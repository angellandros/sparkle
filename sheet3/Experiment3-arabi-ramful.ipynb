{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as Func\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, IDF\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector, VectorUDT, Vectors\n",
    "from pyspark.ml.clustering import KMeans, LDA\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "#import string\n",
    "\n",
    "#Create a spark session.\n",
    "sparkSession = SparkSession.builder.appName(\"Experiment3\").getOrCreate()\n",
    "#Get default configurations\n",
    "sparkSession.sparkContext._conf.getAll()\n",
    "#Update default configurations\n",
    "conf = sparkSession.sparkContext._conf.setAll([('spark.executor.memory', '16g')\\\n",
    "                                        , ('spark.app.name', 'Spark Updated Conf')\\\n",
    "                                        , ('spark.executor.cores', '8')\\\n",
    "                                        , ('spark.cores.max', '8')\\\n",
    "                                        , ('spark.driver.memory','16g')\\\n",
    "                                        ,('spark.driver.maxResultSize','16g')])\n",
    "#Stop the current Spark Session\n",
    "sparkSession.sparkContext.stop()\n",
    "#Create a Spark Session\n",
    "sparkSession = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|stop_word|\n",
      "+---------+\n",
      "|        a|\n",
      "|     able|\n",
      "|    about|\n",
      "|    above|\n",
      "|according|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading stopwords_en.txt data into a dataframe.\n",
    "stopWordsDF = sparkSession.read\\\n",
    "                .load(\"/home/jovyan/work/stopwords_en.txt\", format=\"text\", sep=\" \", inferSchema=\"true\", header=\"false\")\\\n",
    "                .toDF('stop_word')\n",
    "stopWordsDF.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        user_hash_id|        user_library|\n",
      "+--------------------+--------------------+\n",
      "|28d3f81251d94b097...|3929762,503574,58...|\n",
      "|d0c9aaa788153daea...|2080631,6343346,5...|\n",
      "|f05bcffe7951de9e5...|1158654,478707,12...|\n",
      "|ca4f1ba4094011d9a...|              278019|\n",
      "|d1d41a15201915503...|6610569,6493797,6...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading users_libraries.txt data into a dataframe.\n",
    "#Defining the column names.\n",
    "user_columns = ['raw_data']\n",
    "rawUsersDF = sparkSession.read\\\n",
    "            .load(\"/home/jovyan/work/mod_users_libraries2.txt\", format=\"text\", sep=\";\",\\\n",
    "                  inferSchema=\"true\", quote='\"', header=\"false\")\\\n",
    "            .toDF(*user_columns)\n",
    "\n",
    "usersDF = rawUsersDF.select(Func.split(rawUsersDF.raw_data, \";\").getItem(0).alias(\"user_hash_id\"),\\\n",
    "                           Func.split(rawUsersDF.raw_data, \";\").getItem(1).alias(\"user_library\"))\n",
    "usersDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------------------+----------+------+---------+-----+------+------+----+-----+-------------------+-------+--------------------+--------------------+\n",
      "|paper_id|   type|             journal|book_title|series|publisher|pages|volume|number|year|month|          postedate|address|               title|            abstract|\n",
      "+--------+-------+--------------------+----------+------+---------+-----+------+------+----+-----+-------------------+-------+--------------------+--------------------+\n",
      "|   80546|article|biology and philo...|      null|  null|     null|   17|    19|     2|2004|  mar|2005-01-26 21:35:21|   null|the arbitrariness...|the genetic code ...|\n",
      "| 5842862|article|      molecular cell|      null|  null| elsevier|    2|    35|     6|2009|  sep|2009-09-30 17:11:23|   null|how to choose a g...|choosing good pro...|\n",
      "+--------+-------+--------------------+----------+------+---------+-----+------+------+----+-----+-------------------+-------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading papers.csv data into a dataframe.\n",
    "#Defining the column names.\n",
    "paper_columns = ['paper_id', 'type', 'journal', 'book_title', \\\n",
    "           'series', 'publisher', 'pages', 'volume', \\\n",
    "           'number', 'year', 'month', 'postedate',\\\n",
    "           'address', 'title', 'abstract']\n",
    "\n",
    "papersDF = sparkSession.read\\\n",
    "            .load(\"/home/jovyan/work/mod_papers.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", quote='\"', header=\"false\")\\\n",
    "            .toDF(*paper_columns)\n",
    "papersDF.show(2, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1 (Vector representation for the papers)\n",
    "To generate the bag-of-words representation for each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|paper_id|                text|\n",
      "+--------+--------------------+\n",
      "|   80546|the arbitrariness...|\n",
      "| 5842862|how to choose a g...|\n",
      "| 1242600|how to write cons...|\n",
      "| 3467077|defrosting the di...|\n",
      "|  309395|why most publishe...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Concatenate the title and abstract fields of papers together.\n",
    "textDF = papersDF.select(papersDF.paper_id, Func.concat_ws(\" \", papersDF.title, papersDF.abstract).alias(\"text\"))\n",
    "textDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=80546, text=\"the arbitrariness of the genetic code the genetic code has been regarded as arbitrary in the sense that the codon-amino acid assignments could be different than they actually are. this general idea has been spelled out differently by previous, often rather implicit accounts of arbitrariness. they have drawn on the frozen accident theory, on evolutionary contingency, on alternative causal pathways, and on the absence of direct stereochemical interactions between codons and amino acids. it has also been suggested that the arbitrariness of the genetic code justifies attributing semantic information to macromolecules, notably to {dna}. i argue that these accounts of arbitrariness are unsatisfactory. i propose that the code is arbitrary in the sense of jacques monod's concept of chemical arbitrariness: the genetic code is arbitrary in that any codon requires certain chemical and structural properties to specify a particular amino acid, but these properties are not required in virtue of a principle of chemistry. this notion of arbitrariness is compatible with several recent hypotheses about code evolution. i maintain that the code's chemical arbitrariness is neither sufficient nor necessary for attributing semantic information to nucleic acids.\", words=['arbitrariness', 'genetic', 'code', 'genetic', 'code', 'been', 'regarded', 'arbitrary', 'sense', 'that', 'codon-amino', 'acid', 'assignments', 'could', 'different', 'than', 'they', 'actually', 'this', 'general', 'idea', 'been', 'spelled', 'differently', 'previous', 'often', 'rather', 'implicit', 'accounts', 'arbitrariness', 'they', 'have', 'drawn', 'frozen', 'accident', 'theory', 'evolutionary', 'contingency', 'alternative', 'causal', 'pathways', 'absence', 'direct', 'stereochemical', 'interactions', 'between', 'codons', 'amino', 'acids', 'also', 'been', 'suggested', 'that', 'arbitrariness', 'genetic', 'code', 'justifies', 'attributing', 'semantic', 'information', 'macromolecules', 'notably', 'argue', 'that', 'these', 'accounts', 'arbitrariness', 'unsatisfactory', 'propose', 'that', 'code', 'arbitrary', 'sense', 'jacques', 'monod', 'concept', 'chemical', 'arbitrariness', 'genetic', 'code', 'arbitrary', 'that', 'codon', 'requires', 'certain', 'chemical', 'structural', 'properties', 'specify', 'particular', 'amino', 'acid', 'these', 'properties', 'required', 'virtue', 'principle', 'chemistry', 'this', 'notion', 'arbitrariness', 'compatible', 'with', 'several', 'recent', 'hypotheses', 'about', 'code', 'evolution', 'maintain', 'that', 'code', 'chemical', 'arbitrariness', 'neither', 'sufficient', 'necessary', 'attributing', 'semantic', 'information', 'nucleic', 'acids'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Perform tokenization and remove words less than 3 characters.\n",
    "#Keep the words containing \"-\" and \"_\" characters.\n",
    "reTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", minTokenLength=4, pattern=\"[^-_\\\\w]\")\n",
    "wordsDF = reTokenizer.transform(textDF)\n",
    "wordsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.1\n",
    "#Function to remove the characters \"-\" and \"_\" from words.\n",
    "def concatConnectedWords(wordList):\n",
    "    wordSet = set(wordList)\n",
    "    identity = str.maketrans(\"\", \"\", \"-_\")\n",
    "    wordSet = [word.translate(identity) for word in wordSet]\n",
    "    return wordSet\n",
    "\n",
    "udf_concatConnectedWords = Func.udf(concatConnectedWords, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(paper_id=80546, processed_words=['these', 'actually', 'certain', 'stereochemical', 'notably', 'that', 'concept', 'particular', 'code', 'chemical', 'could', 'they', 'direct', 'propose', 'with', 'about', 'chemistry', 'general', 'regarded', 'notion', 'this', 'nucleic', 'information', 'drawn', 'specify', 'evolution', 'neither', 'absence', 'genetic', 'implicit', 'unsatisfactory', 'previous', 'frozen', 'between', 'alternative', 'amino', 'attributing', 'required', 'codonamino', 'been', 'have', 'structural', 'several', 'properties', 'maintain', 'necessary', 'requires', 'sense', 'hypotheses', 'pathways', 'acids', 'causal', 'codons', 'arbitrary', 'often', 'theory', 'also', 'macromolecules', 'assignments', 'rather', 'accounts', 'contingency', 'acid', 'monod', 'virtue', 'sufficient', 'jacques', 'principle', 'argue', 'idea', 'arbitrariness', 'suggested', 'compatible', 'than', 'recent', 'different', 'semantic', 'codon', 'interactions', 'justifies', 'accident', 'spelled', 'evolutionary', 'differently']),\n",
       " Row(paper_id=5842862, processed_words=['profession', 'that', 'lack', 'what', 'explicit', 'results', 'merit', 'discussion', 'within', 'tenure', 'observation', 'this', 'choose', 'problems', 'smart', 'teachers', 'scientific', 'usually', 'explicitly', 'choosing', 'scientist', 'being', 'figure', 'give', 'good', 'resulting', 'subject', 'vacuum', 'publication', 'approaches', 'through', 'scientists', 'problem', 'expected', 'their', 'such', 'journals', 'leaves', 'discussed', 'lead', 'essential', 'valued', 'enough'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#removing the characters \"-\" and \"_\" from words.\n",
    "removedConnectorsDF = wordsDF.select(wordsDF.paper_id,\\\n",
    "                                     Func.lit(udf_concatConnectedWords(wordsDF.words)).alias(\"processed_words\"))\n",
    "removedConnectorsDF.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Creating a list of stop words.\n",
    "stopWordsList = stopWordsDF.agg(Func.collect_list(stopWordsDF.stop_word)).rdd.flatMap(lambda row: row[0])\n",
    "stopWordsList.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(words=['stereochemical', 'notably', 'concept', 'code', 'chemical', 'direct', 'propose', 'chemistry', 'general', 'regarded', 'notion', 'nucleic', 'information', 'drawn', 'evolution', 'absence', 'genetic', 'implicit', 'unsatisfactory', 'previous', 'frozen', 'alternative', 'amino', 'attributing', 'required', 'codonamino', 'structural', 'properties', 'maintain', 'requires', 'sense', 'hypotheses', 'pathways', 'acids', 'causal', 'codons', 'arbitrary', 'theory', 'macromolecules', 'assignments', 'accounts', 'contingency', 'acid', 'monod', 'virtue', 'sufficient', 'jacques', 'principle', 'argue', 'idea', 'arbitrariness', 'suggested', 'compatible', 'recent', 'semantic', 'codon', 'interactions', 'justifies', 'accident', 'spelled', 'evolutionary', 'differently'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Removing the stop words.\n",
    "remover = StopWordsRemover(inputCol=\"processed_words\", outputCol=\"words\", stopWords=stopWordsList.collect())\n",
    "withoutStopWordsDF = remover.transform(removedConnectorsDF)\n",
    "withoutStopWordsDF.select(\"words\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.1\n",
    "#Function to perform stemming.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemming(wordList):\n",
    "    wordSet = set(wordList)\n",
    "    wordSet = [stemmer.stem(word) for word in wordList]\n",
    "    return sorted(wordSet)\n",
    "\n",
    "#User defined function to perform stemming.\n",
    "udf_stemming = Func.udf(stemming, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.1\n",
    "#Performing stemming.\n",
    "stemmedWordsDF = withoutStopWordsDF.withColumn(\"stemmed_words\", udf_stemming(withoutStopWordsDF.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|single_word| df|\n",
      "+-----------+---+\n",
      "|   everyday|  2|\n",
      "|      input|  2|\n",
      "|    persist|  1|\n",
      "| likelihood|  3|\n",
      "|  geneannot|  1|\n",
      "|     import|  4|\n",
      "| photograph|  1|\n",
      "|       oper|  1|\n",
      "|    highest|  1|\n",
      "|      equal|  1|\n",
      "+-----------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Counting the number of papers (frequency) in which a particular word appears. (Document Frequency)\n",
    "explodedDF = stemmedWordsDF.select(stemmedWordsDF.paper_id,\\\n",
    "                                   Func.explode(stemmedWordsDF.stemmed_words).alias(\"single_word\"))\\\n",
    "                            .distinct()\\\n",
    "                            .groupBy(\"single_word\")\\\n",
    "                            .agg(Func.count(\"single_word\")\\\n",
    "                                 .alias(\"df\"))\n",
    "explodedDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|single_word| df|\n",
      "+-----------+---+\n",
      "|   everyday|  2|\n",
      "|      input|  2|\n",
      "| likelihood|  3|\n",
      "|     import|  4|\n",
      "|    explain|  5|\n",
      "|    classif|  3|\n",
      "|     execut|  2|\n",
      "|        map|  4|\n",
      "|  character|  6|\n",
      "|      uncov|  4|\n",
      "+-----------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Counting the number of unique papers.\n",
    "#Setting the upper and lower bounds.\n",
    "uniqPaperCount = stemmedWordsDF.count()\n",
    "upperBoundary = 0.1*uniqPaperCount\n",
    "lowerBoundary = 20\n",
    "#filter the dataframe to select only words that appear in more than 20 papers and\n",
    "#less than 10 percent of the total number of papers.\n",
    "filteredDF = explodedDF.filter((explodedDF.df>=lowerBoundary) & (explodedDF.df<=upperBoundary))\n",
    "filteredDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|single_word|\n",
      "+-----------+\n",
      "|  scientist|\n",
      "|    suggest|\n",
      "|     articl|\n",
      "|      major|\n",
      "|       read|\n",
      "|     access|\n",
      "|    process|\n",
      "|       wide|\n",
      "|      basic|\n",
      "|     exampl|\n",
      "+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Limiting the number of important terms to 1000.\n",
    "termsDF = filteredDF.sort(\"df\", ascending=False).limit(1000).select(\"single_word\")\n",
    "termsDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|single_word|word_id|\n",
      "+-----------+-------+\n",
      "|    process|      0|\n",
      "|     access|      1|\n",
      "|    suggest|      2|\n",
      "|       wide|      3|\n",
      "|      major|      4|\n",
      "|       read|      5|\n",
      "|     articl|      6|\n",
      "|  scientist|      7|\n",
      "|      basic|      8|\n",
      "|     theori|      9|\n",
      "+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Generating an index to the top 1000 important terms starting with the index 0.\n",
    "termsDF = termsDF.withColumn(\"word_id\", Func.monotonically_increasing_id())\n",
    "\n",
    "wordWinSpec = Window.orderBy(\"word_id\")\n",
    "\n",
    "termsDF = termsDF.withColumn(\"word_id\",Func.row_number().over(wordWinSpec)-1)\n",
    "termsDF.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---+\n",
      "|paper_id|single_word| tf|\n",
      "+--------+-----------+---+\n",
      "| 1242600|     public|  1|\n",
      "| 1242600|       talk|  1|\n",
      "|      99|       call|  1|\n",
      "|  740681|       imit|  1|\n",
      "|  740681|      model|  1|\n",
      "|   99857|      degre|  1|\n",
      "| 3614773|      studi|  1|\n",
      "|  117535|   reweight|  1|\n",
      "| 4131662|   techniqu|  1|\n",
      "|  115158|     common|  1|\n",
      "+--------+-----------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Counting the number of times a word appeared in a particular paper (word count per paper) (term frequency).\n",
    "tempDF = stemmedWordsDF.select(stemmedWordsDF.paper_id,\\\n",
    "                               Func.explode(stemmedWordsDF.stemmed_words).alias(\"single_word\"))\\\n",
    "                        .groupBy(\"paper_id\", \"single_word\")\\\n",
    "                        .agg(Func.count(\"single_word\").alias(\"tf\"))\n",
    "tempDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|paper_id|            map_list|\n",
      "+--------+--------------------+\n",
      "|  115158|[[0 -> 1], [1 -> ...|\n",
      "|     101|[[0 -> 1], [3 -> ...|\n",
      "|  740681|[[0 -> 1], [14 ->...|\n",
      "| 4778506|[[0 -> 1], [1 -> ...|\n",
      "|  105906|[[0 -> 1], [69 ->...|\n",
      "|  212874|[[0 -> 1], [1 -> ...|\n",
      "|  430834|[[0 -> 2], [13 ->...|\n",
      "| 5394760|[[0 -> 1], [7 -> ...|\n",
      "| 4200367|[[0 -> 1], [5 -> ...|\n",
      "| 3721754|[[0 -> 1], [4 -> ...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Creating a dataframe with term index and count of the word per paper.\n",
    "#Creating a column containing a mapping of term index -> count per paper.\n",
    "joinedResult = tempDF.join(termsDF, \"single_word\").withColumn(\"map\", Func.create_map(\"word_id\", \"tf\"))\\\n",
    "                    .groupBy(\"paper_id\").agg(Func.collect_list(\"map\").alias(\"map_list\"))\n",
    "joinedResult.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.1\n",
    "#Function to create a sparse vector for each paper.\n",
    "def toSparse(mapList):\n",
    "    index = []\n",
    "    value = []\n",
    "    for map_val in mapList:\n",
    "        for idx, val in map_val.items():\n",
    "            index.append(idx)\n",
    "            value.append(val)\n",
    "    return SparseVector(1000, index, value)\n",
    "\n",
    "#Creating a user defined function.\n",
    "udf_toSparse = Func.udf(toSparse, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=115158, tf_vector=SparseVector(1000, {0: 1.0, 1: 1.0, 12: 1.0, 13: 1.0, 21: 1.0, 22: 2.0, 24: 1.0, 27: 1.0, 31: 1.0, 37: 1.0, 43: 1.0, 51: 1.0, 55: 1.0, 62: 1.0, 69: 1.0, 70: 1.0, 76: 1.0, 79: 1.0, 80: 1.0, 84: 1.0, 90: 1.0, 92: 1.0, 95: 1.0, 105: 2.0, 109: 1.0, 110: 1.0, 111: 2.0, 113: 1.0, 117: 1.0, 120: 1.0, 127: 1.0, 130: 1.0, 145: 1.0, 162: 1.0, 164: 1.0, 169: 1.0, 174: 1.0, 180: 1.0, 202: 1.0, 209: 1.0, 219: 1.0, 222: 1.0, 224: 1.0, 239: 1.0, 245: 1.0, 246: 1.0, 258: 1.0, 263: 2.0, 270: 1.0, 275: 1.0, 317: 2.0, 329: 1.0, 336: 1.0, 339: 1.0, 355: 1.0, 364: 1.0, 369: 1.0, 377: 1.0, 384: 1.0, 387: 1.0, 405: 1.0, 411: 1.0, 418: 2.0, 423: 1.0, 436: 1.0, 440: 1.0, 450: 1.0, 454: 1.0, 459: 1.0, 471: 1.0, 500: 1.0, 534: 1.0, 570: 1.0, 583: 1.0, 584: 1.0, 596: 1.0, 614: 1.0}))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Creating the bag of words dataframe.\n",
    "featurizedDataDF = joinedResult.select(\"paper_id\",\\\n",
    "                                   udf_toSparse(joinedResult.map_list)\\\n",
    "                                   .alias(\"tf_vector\"))\n",
    "featurizedDataDF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2 (TF-IDF representation for the papers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating TF and IDF using functionality provided by pyspark.ml.feature.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=115158, tf_vector=SparseVector(1000, {0: 1.0, 1: 1.0, 12: 1.0, 13: 1.0, 21: 1.0, 22: 2.0, 24: 1.0, 27: 1.0, 31: 1.0, 37: 1.0, 43: 1.0, 51: 1.0, 55: 1.0, 62: 1.0, 69: 1.0, 70: 1.0, 76: 1.0, 79: 1.0, 80: 1.0, 84: 1.0, 90: 1.0, 92: 1.0, 95: 1.0, 105: 2.0, 109: 1.0, 110: 1.0, 111: 2.0, 113: 1.0, 117: 1.0, 120: 1.0, 127: 1.0, 130: 1.0, 145: 1.0, 162: 1.0, 164: 1.0, 169: 1.0, 174: 1.0, 180: 1.0, 202: 1.0, 209: 1.0, 219: 1.0, 222: 1.0, 224: 1.0, 239: 1.0, 245: 1.0, 246: 1.0, 258: 1.0, 263: 2.0, 270: 1.0, 275: 1.0, 317: 2.0, 329: 1.0, 336: 1.0, 339: 1.0, 355: 1.0, 364: 1.0, 369: 1.0, 377: 1.0, 384: 1.0, 387: 1.0, 405: 1.0, 411: 1.0, 418: 2.0, 423: 1.0, 436: 1.0, 440: 1.0, 450: 1.0, 454: 1.0, 459: 1.0, 471: 1.0, 500: 1.0, 534: 1.0, 570: 1.0, 583: 1.0, 584: 1.0, 596: 1.0, 614: 1.0}), idf_vector=SparseVector(1000, {0: 2.1972, 1: 2.1972, 12: 2.2925, 13: 2.2925, 21: 2.2925, 22: 4.7958, 24: 2.3979, 27: 2.3979, 31: 2.3979, 37: 2.3979, 43: 2.5157, 51: 2.5157, 55: 2.5157, 62: 2.5157, 69: 2.6492, 70: 2.6492, 76: 2.6492, 79: 2.6492, 80: 2.6492, 84: 2.6492, 90: 2.6492, 92: 2.8034, 95: 2.8034, 105: 5.6067, 109: 2.8034, 110: 2.8034, 111: 5.6067, 113: 2.8034, 117: 2.8034, 120: 2.8034, 127: 2.8034, 130: 2.8034, 145: 2.8034, 162: 2.9857, 164: 2.9857, 169: 2.9857, 174: 2.9857, 180: 2.9857, 202: 2.9857, 209: 2.9857, 219: 2.9857, 222: 2.9857, 224: 2.9857, 239: 3.2088, 245: 3.2088, 246: 3.2088, 258: 3.2088, 263: 6.4177, 270: 3.2088, 275: 3.2088, 317: 6.4177, 329: 3.2088, 336: 3.2088, 339: 3.2088, 355: 3.2088, 364: 3.4965, 369: 3.4965, 377: 3.4965, 384: 3.4965, 387: 3.4965, 405: 3.4965, 411: 3.4965, 418: 6.993, 423: 3.4965, 436: 3.4965, 440: 3.4965, 450: 3.4965, 454: 3.4965, 459: 3.4965, 471: 3.4965, 500: 3.4965, 534: 3.4965, 570: 3.4965, 583: 3.4965, 584: 3.4965, 596: 3.4965, 614: 3.4965}))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Initializing the IDF object.\n",
    "sparkMlIdf = IDF(inputCol=\"tf_vector\", outputCol=\"idf_vector\")\n",
    "#Training the data and creating a model.\n",
    "sparkMlIdfModel = sparkMlIdf.fit(featurizedDataDF)\n",
    "#Adding the output produced from IDF to the dataset as a separate column.\n",
    "IdfData = sparkMlIdfModel.transform(featurizedDataDF)\n",
    "IdfData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.2\n",
    "#Function to multiply 2 sparse vectors.\n",
    "def multiplySparseVec(x_vec, y_vec):\n",
    "    result = np.multiply(x_vec, y_vec).tolist()\n",
    "    vector_args = len(result), [i for i, x in enumerate(result) if x != 0], [x for x in result if x != 0] \n",
    "    return Vectors.sparse(*vector_args)\n",
    "\n",
    "#Creating a user defined function.\n",
    "udf_multiplySparseVec = Func.udf(multiplySparseVec, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=115158, tf_vector=SparseVector(1000, {0: 1.0, 1: 1.0, 12: 1.0, 13: 1.0, 21: 1.0, 22: 2.0, 24: 1.0, 27: 1.0, 31: 1.0, 37: 1.0, 43: 1.0, 51: 1.0, 55: 1.0, 62: 1.0, 69: 1.0, 70: 1.0, 76: 1.0, 79: 1.0, 80: 1.0, 84: 1.0, 90: 1.0, 92: 1.0, 95: 1.0, 105: 2.0, 109: 1.0, 110: 1.0, 111: 2.0, 113: 1.0, 117: 1.0, 120: 1.0, 127: 1.0, 130: 1.0, 145: 1.0, 162: 1.0, 164: 1.0, 169: 1.0, 174: 1.0, 180: 1.0, 202: 1.0, 209: 1.0, 219: 1.0, 222: 1.0, 224: 1.0, 239: 1.0, 245: 1.0, 246: 1.0, 258: 1.0, 263: 2.0, 270: 1.0, 275: 1.0, 317: 2.0, 329: 1.0, 336: 1.0, 339: 1.0, 355: 1.0, 364: 1.0, 369: 1.0, 377: 1.0, 384: 1.0, 387: 1.0, 405: 1.0, 411: 1.0, 418: 2.0, 423: 1.0, 436: 1.0, 440: 1.0, 450: 1.0, 454: 1.0, 459: 1.0, 471: 1.0, 500: 1.0, 534: 1.0, 570: 1.0, 583: 1.0, 584: 1.0, 596: 1.0, 614: 1.0}), idf_vector=SparseVector(1000, {0: 2.1972, 1: 2.1972, 12: 2.2925, 13: 2.2925, 21: 2.2925, 22: 4.7958, 24: 2.3979, 27: 2.3979, 31: 2.3979, 37: 2.3979, 43: 2.5157, 51: 2.5157, 55: 2.5157, 62: 2.5157, 69: 2.6492, 70: 2.6492, 76: 2.6492, 79: 2.6492, 80: 2.6492, 84: 2.6492, 90: 2.6492, 92: 2.8034, 95: 2.8034, 105: 5.6067, 109: 2.8034, 110: 2.8034, 111: 5.6067, 113: 2.8034, 117: 2.8034, 120: 2.8034, 127: 2.8034, 130: 2.8034, 145: 2.8034, 162: 2.9857, 164: 2.9857, 169: 2.9857, 174: 2.9857, 180: 2.9857, 202: 2.9857, 209: 2.9857, 219: 2.9857, 222: 2.9857, 224: 2.9857, 239: 3.2088, 245: 3.2088, 246: 3.2088, 258: 3.2088, 263: 6.4177, 270: 3.2088, 275: 3.2088, 317: 6.4177, 329: 3.2088, 336: 3.2088, 339: 3.2088, 355: 3.2088, 364: 3.4965, 369: 3.4965, 377: 3.4965, 384: 3.4965, 387: 3.4965, 405: 3.4965, 411: 3.4965, 418: 6.993, 423: 3.4965, 436: 3.4965, 440: 3.4965, 450: 3.4965, 454: 3.4965, 459: 3.4965, 471: 3.4965, 500: 3.4965, 534: 3.4965, 570: 3.4965, 583: 3.4965, 584: 3.4965, 596: 3.4965, 614: 3.4965}), tf_idf=SparseVector(1000, {0: 2.1972, 1: 2.1972, 12: 2.2925, 13: 2.2925, 21: 2.2925, 22: 9.5916, 24: 2.3979, 27: 2.3979, 31: 2.3979, 37: 2.3979, 43: 2.5157, 51: 2.5157, 55: 2.5157, 62: 2.5157, 69: 2.6492, 70: 2.6492, 76: 2.6492, 79: 2.6492, 80: 2.6492, 84: 2.6492, 90: 2.6492, 92: 2.8034, 95: 2.8034, 105: 11.2134, 109: 2.8034, 110: 2.8034, 111: 11.2134, 113: 2.8034, 117: 2.8034, 120: 2.8034, 127: 2.8034, 130: 2.8034, 145: 2.8034, 162: 2.9857, 164: 2.9857, 169: 2.9857, 174: 2.9857, 180: 2.9857, 202: 2.9857, 209: 2.9857, 219: 2.9857, 222: 2.9857, 224: 2.9857, 239: 3.2088, 245: 3.2088, 246: 3.2088, 258: 3.2088, 263: 12.8353, 270: 3.2088, 275: 3.2088, 317: 12.8353, 329: 3.2088, 336: 3.2088, 339: 3.2088, 355: 3.2088, 364: 3.4965, 369: 3.4965, 377: 3.4965, 384: 3.4965, 387: 3.4965, 405: 3.4965, 411: 3.4965, 418: 13.986, 423: 3.4965, 436: 3.4965, 440: 3.4965, 450: 3.4965, 454: 3.4965, 459: 3.4965, 471: 3.4965, 500: 3.4965, 534: 3.4965, 570: 3.4965, 583: 3.4965, 584: 3.4965, 596: 3.4965, 614: 3.4965}))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Calculating the value of TF-IDF.\n",
    "paperTfIdf = IdfData.withColumn(\"tf_idf\", udf_multiplySparseVec(IdfData.tf_vector, IdfData.idf_vector))\n",
    "paperTfIdf.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating TF and IDF WITHOUT using functionality provided by pyspark.ml.feature.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---+\n",
      "|single_word|word_id| df|\n",
      "+-----------+-------+---+\n",
      "|    process|      0| 10|\n",
      "|     access|      1| 10|\n",
      "|    suggest|      2| 10|\n",
      "|       wide|      3| 10|\n",
      "|      major|      4| 10|\n",
      "|       read|      5| 10|\n",
      "|     articl|      6| 10|\n",
      "|  scientist|      7| 10|\n",
      "|      basic|      8| 10|\n",
      "|     theori|      9|  9|\n",
      "+-----------+-------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Creating a dataframe with the terms, ID and document-frequency.\n",
    "termsWithDF = termsDF.join(filteredDF, \"single_word\")\n",
    "termsWithDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 10, 10, 10, 10]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#creating a document frequency array where index of array element is equal to word_id\n",
    "docFreqList = termsWithDF.select(\"word_id\", \"df\").orderBy(\"word_id\", ascending=True).rdd.map(lambda x: x[1]).collect()\n",
    "docFreqList[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.2\n",
    "#Function to calculate the IDF.\n",
    "def calcIdf(m, docFreq):\n",
    "    Idf = math.log((m+1)/(docFreq+1))\n",
    "    return Idf\n",
    "\n",
    "#Function to create the IDF vector.\n",
    "def getIdfSparseVec(m, tf_vector, doc_freq_ls):\n",
    "    index = []\n",
    "    value = []\n",
    "    #retrieving active indices\n",
    "    active_idx = tf_vector.indices\n",
    "\n",
    "    for idx in active_idx:\n",
    "        doc_freq = doc_freq_ls[idx]\n",
    "        index.append(idx)\n",
    "        value.append(calcIdf(m, doc_freq))\n",
    "    return SparseVector(1000, index, value)\n",
    "\n",
    "udf_getIdfSparseVec = Func.udf(lambda x, y: getIdfSparseVec(x, y, docFreqList), VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=115158, tf_vector=SparseVector(1000, {0: 1.0, 1: 1.0, 12: 1.0, 13: 1.0, 21: 1.0, 22: 2.0, 24: 1.0, 27: 1.0, 31: 1.0, 37: 1.0, 43: 1.0, 51: 1.0, 55: 1.0, 62: 1.0, 69: 1.0, 70: 1.0, 76: 1.0, 79: 1.0, 80: 1.0, 84: 1.0, 90: 1.0, 92: 1.0, 95: 1.0, 105: 2.0, 109: 1.0, 110: 1.0, 111: 2.0, 113: 1.0, 117: 1.0, 120: 1.0, 127: 1.0, 130: 1.0, 145: 1.0, 162: 1.0, 164: 1.0, 169: 1.0, 174: 1.0, 180: 1.0, 202: 1.0, 209: 1.0, 219: 1.0, 222: 1.0, 224: 1.0, 239: 1.0, 245: 1.0, 246: 1.0, 258: 1.0, 263: 2.0, 270: 1.0, 275: 1.0, 317: 2.0, 329: 1.0, 336: 1.0, 339: 1.0, 355: 1.0, 364: 1.0, 369: 1.0, 377: 1.0, 384: 1.0, 387: 1.0, 405: 1.0, 411: 1.0, 418: 2.0, 423: 1.0, 436: 1.0, 440: 1.0, 450: 1.0, 454: 1.0, 459: 1.0, 471: 1.0, 500: 1.0, 534: 1.0, 570: 1.0, 583: 1.0, 584: 1.0, 596: 1.0, 614: 1.0}), idf_vector=SparseVector(1000, {0: 2.2172, 1: 2.2172, 12: 2.3125, 13: 2.3125, 21: 2.3125, 22: 2.4179, 24: 2.4179, 27: 2.4179, 31: 2.4179, 37: 2.4179, 43: 2.5357, 51: 2.5357, 55: 2.5357, 62: 2.5357, 69: 2.6692, 70: 2.6692, 76: 2.6692, 79: 2.6692, 80: 2.6692, 84: 2.6692, 90: 2.6692, 92: 2.8234, 95: 2.8234, 105: 2.8234, 109: 2.8234, 110: 2.8234, 111: 2.8234, 113: 2.8234, 117: 2.8234, 120: 2.8234, 127: 2.8234, 130: 2.8234, 145: 2.8234, 162: 3.0057, 164: 3.0057, 169: 3.0057, 174: 3.0057, 180: 3.0057, 202: 3.0057, 209: 3.0057, 219: 3.0057, 222: 3.0057, 224: 3.0057, 239: 3.2288, 245: 3.2288, 246: 3.2288, 258: 3.2288, 263: 3.2288, 270: 3.2288, 275: 3.2288, 317: 3.2288, 329: 3.2288, 336: 3.2288, 339: 3.2288, 355: 3.2288, 364: 3.5165, 369: 3.5165, 377: 3.5165, 384: 3.5165, 387: 3.5165, 405: 3.5165, 411: 3.5165, 418: 3.5165, 423: 3.5165, 436: 3.5165, 440: 3.5165, 450: 3.5165, 454: 3.5165, 459: 3.5165, 471: 3.5165, 500: 3.5165, 534: 3.5165, 570: 3.5165, 583: 3.5165, 584: 3.5165, 596: 3.5165, 614: 3.5165}))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Creating a new dataframe with IDF values.\n",
    "manualIdf = featurizedDataDF.withColumn(\"idf_vector\", udf_getIdfSparseVec(Func.lit(uniqPaperCount),\\\n",
    "                                                                           featurizedDataDF.tf_vector))\n",
    "manualIdf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=115158, tf_vector=SparseVector(1000, {0: 1.0, 1: 1.0, 12: 1.0, 13: 1.0, 21: 1.0, 22: 2.0, 24: 1.0, 27: 1.0, 31: 1.0, 37: 1.0, 43: 1.0, 51: 1.0, 55: 1.0, 62: 1.0, 69: 1.0, 70: 1.0, 76: 1.0, 79: 1.0, 80: 1.0, 84: 1.0, 90: 1.0, 92: 1.0, 95: 1.0, 105: 2.0, 109: 1.0, 110: 1.0, 111: 2.0, 113: 1.0, 117: 1.0, 120: 1.0, 127: 1.0, 130: 1.0, 145: 1.0, 162: 1.0, 164: 1.0, 169: 1.0, 174: 1.0, 180: 1.0, 202: 1.0, 209: 1.0, 219: 1.0, 222: 1.0, 224: 1.0, 239: 1.0, 245: 1.0, 246: 1.0, 258: 1.0, 263: 2.0, 270: 1.0, 275: 1.0, 317: 2.0, 329: 1.0, 336: 1.0, 339: 1.0, 355: 1.0, 364: 1.0, 369: 1.0, 377: 1.0, 384: 1.0, 387: 1.0, 405: 1.0, 411: 1.0, 418: 2.0, 423: 1.0, 436: 1.0, 440: 1.0, 450: 1.0, 454: 1.0, 459: 1.0, 471: 1.0, 500: 1.0, 534: 1.0, 570: 1.0, 583: 1.0, 584: 1.0, 596: 1.0, 614: 1.0}), idf_vector=SparseVector(1000, {0: 2.2172, 1: 2.2172, 12: 2.3125, 13: 2.3125, 21: 2.3125, 22: 2.4179, 24: 2.4179, 27: 2.4179, 31: 2.4179, 37: 2.4179, 43: 2.5357, 51: 2.5357, 55: 2.5357, 62: 2.5357, 69: 2.6692, 70: 2.6692, 76: 2.6692, 79: 2.6692, 80: 2.6692, 84: 2.6692, 90: 2.6692, 92: 2.8234, 95: 2.8234, 105: 2.8234, 109: 2.8234, 110: 2.8234, 111: 2.8234, 113: 2.8234, 117: 2.8234, 120: 2.8234, 127: 2.8234, 130: 2.8234, 145: 2.8234, 162: 3.0057, 164: 3.0057, 169: 3.0057, 174: 3.0057, 180: 3.0057, 202: 3.0057, 209: 3.0057, 219: 3.0057, 222: 3.0057, 224: 3.0057, 239: 3.2288, 245: 3.2288, 246: 3.2288, 258: 3.2288, 263: 3.2288, 270: 3.2288, 275: 3.2288, 317: 3.2288, 329: 3.2288, 336: 3.2288, 339: 3.2288, 355: 3.2288, 364: 3.5165, 369: 3.5165, 377: 3.5165, 384: 3.5165, 387: 3.5165, 405: 3.5165, 411: 3.5165, 418: 3.5165, 423: 3.5165, 436: 3.5165, 440: 3.5165, 450: 3.5165, 454: 3.5165, 459: 3.5165, 471: 3.5165, 500: 3.5165, 534: 3.5165, 570: 3.5165, 583: 3.5165, 584: 3.5165, 596: 3.5165, 614: 3.5165}), tf_idf=SparseVector(1000, {0: 2.2172, 1: 2.2172, 12: 2.3125, 13: 2.3125, 21: 2.3125, 22: 4.8358, 24: 2.4179, 27: 2.4179, 31: 2.4179, 37: 2.4179, 43: 2.5357, 51: 2.5357, 55: 2.5357, 62: 2.5357, 69: 2.6692, 70: 2.6692, 76: 2.6692, 79: 2.6692, 80: 2.6692, 84: 2.6692, 90: 2.6692, 92: 2.8234, 95: 2.8234, 105: 5.6467, 109: 2.8234, 110: 2.8234, 111: 5.6467, 113: 2.8234, 117: 2.8234, 120: 2.8234, 127: 2.8234, 130: 2.8234, 145: 2.8234, 162: 3.0057, 164: 3.0057, 169: 3.0057, 174: 3.0057, 180: 3.0057, 202: 3.0057, 209: 3.0057, 219: 3.0057, 222: 3.0057, 224: 3.0057, 239: 3.2288, 245: 3.2288, 246: 3.2288, 258: 3.2288, 263: 6.4577, 270: 3.2288, 275: 3.2288, 317: 6.4577, 329: 3.2288, 336: 3.2288, 339: 3.2288, 355: 3.2288, 364: 3.5165, 369: 3.5165, 377: 3.5165, 384: 3.5165, 387: 3.5165, 405: 3.5165, 411: 3.5165, 418: 7.033, 423: 3.5165, 436: 3.5165, 440: 3.5165, 450: 3.5165, 454: 3.5165, 459: 3.5165, 471: 3.5165, 500: 3.5165, 534: 3.5165, 570: 3.5165, 583: 3.5165, 584: 3.5165, 596: 3.5165, 614: 3.5165}))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Calculating the value of TF-IDF.\n",
    "manualTfIdf = manualIdf.withColumn(\"tf_idf\", udf_multiplySparseVec(manualIdf.tf_vector, manualIdf.idf_vector))\n",
    "manualTfIdf.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3 (Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Calculating user profile for each user as the summation of the TF-IDF vectors of the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|        user_hash_id|paper_id|\n",
      "+--------------------+--------+\n",
      "|28d3f81251d94b097...| 3929762|\n",
      "|28d3f81251d94b097...|  503574|\n",
      "|28d3f81251d94b097...| 5819422|\n",
      "|28d3f81251d94b097...| 4238883|\n",
      "|28d3f81251d94b097...| 5788061|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Splitting the list of paper_ids into individual rows.\n",
    "explodUsersDF = usersDF.select(usersDF.user_hash_id,\\\n",
    "                               Func.explode(Func.split(usersDF.user_library, \",\"))\\\n",
    "                               .alias(\"paper_id\"))\n",
    "explodUsersDF.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|paper_id|        user_hash_id|           tf_vector|          idf_vector|              tf_idf|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  740681|b656009a6efdc8b1a...|(1000,[0,14,22,37...|(1000,[0,14,22,37...|(1000,[0,14,22,37...|\n",
      "| 4778506|a06e5e3519a8d90a8...|(1000,[0,1,5,33,3...|(1000,[0,1,5,33,3...|(1000,[0,1,5,33,3...|\n",
      "|  105906|d2e2a082f925839b3...|(1000,[0,69,292,5...|(1000,[0,69,292,5...|(1000,[0,69,292,5...|\n",
      "|  105906|f1e1cd4ff25018273...|(1000,[0,69,292,5...|(1000,[0,69,292,5...|(1000,[0,69,292,5...|\n",
      "|  212874|a06e5e3519a8d90a8...|(1000,[0,1,14,37,...|(1000,[0,1,14,37,...|(1000,[0,1,14,37,...|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Creating a dataframe with user_hash_id as well as paper_id, tf, idf, tf-idf information.\n",
    "joinUserPaperTfIdf = explodUsersDF.join(paperTfIdf, \"paper_id\")\n",
    "joinUserPaperTfIdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        user_hash_id|         tf_idf_list|\n",
      "+--------------------+--------------------+\n",
      "|f1e1cd4ff25018273...|[(1000,[0,69,292,...|\n",
      "|cf9c7f356092c34be...|[(1000,[3,19,28,3...|\n",
      "|f3da73f972caebbe0...|[(1000,[7,70,77,2...|\n",
      "|d46f0fdd50ec8aa79...|[(1000,[2,9,21,32...|\n",
      "|d503571e44a0373eb...|[(1000,[2,9,21,32...|\n",
      "|ee1dfee93ebeadade...|[(1000,[35,318,37...|\n",
      "|f05bcffe7951de9e5...|[(1000,[1,30,47,7...|\n",
      "|dad19fd5a1bead8a2...|[(1000,[1,7,15,67...|\n",
      "|d2e2a082f925839b3...|[(1000,[0,69,292,...|\n",
      "|cbd4a69e4b3ed3472...|[(1000,[0,1,14,37...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Grouping the tf-idf vectors with respect to the user.\n",
    "userTfIdfList = joinUserPaperTfIdf.groupBy(\"user_hash_id\").agg(Func.collect_list(\"tf_idf\").alias(\"tf_idf_list\"))\n",
    "userTfIdfList.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Function to add elements of a list of sparse vectors.\n",
    "def addSparseVec(vector_list):\n",
    "    result = SparseVector(1000, list(range(0, 1000)), np.zeros(1000))\n",
    "    for vector in vector_list:\n",
    "        result = np.add(result, vector).tolist()\n",
    "\n",
    "    vector_args = len(result), [i for i, x in enumerate(result) if x != 0], [x for x in result if x != 0] \n",
    "    return Vectors.sparse(*vector_args)\n",
    "    \n",
    "udf_addSparseVec = Func.udf(addSparseVec, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(user_hash_id='f1e1cd4ff25018273aafc0c68fbb5a2f', features=SparseVector(1000, {0: 2.1972, 7: 2.1972, 21: 2.2925, 25: 2.3979, 38: 2.3979, 41: 2.5157, 43: 2.5157, 59: 2.5157, 69: 2.6492, 84: 2.6492, 96: 2.8034, 102: 2.8034, 109: 2.8034, 110: 2.8034, 139: 2.8034, 143: 2.8034, 157: 2.9857, 163: 2.9857, 173: 2.9857, 177: 2.9857, 179: 2.9857, 199: 2.9857, 247: 3.2088, 274: 3.2088, 287: 3.2088, 292: 3.2088, 297: 3.2088, 323: 3.2088, 450: 3.4965, 504: 3.4965, 516: 13.986, 548: 3.4965, 570: 3.4965, 597: 3.4965, 601: 3.4965, 603: 3.4965, 609: 3.4965}))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Creating the user profile.\n",
    "userProfileDF = userTfIdfList.withColumn(\"features\", udf_addSparseVec(userTfIdfList.tf_idf_list))\n",
    "userFeaturesDF = userProfileDF.select(\"user_hash_id\", \"features\")\n",
    "userFeaturesDF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Applying the K-Means algorithm to cluster the users in 50 clusters given their profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 b)\n",
    "#Initializing a Kmeans object.\n",
    "kmeans50 = KMeans(featuresCol=\"features\", k=50, maxIter=10)\n",
    "#Creating the model.\n",
    "userClusterModel = kmeans50.fit(userFeaturesDF.select(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Calculating the Davies-Bouldin index for the 50 Generated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  2.29253476,\n",
       "         0.        ,  0.        ,  2.29253476,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         2.39789527,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        , 10.06271323,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  2.6492097 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , 11.21344152,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  2.80336038,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  2.98568194,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  3.20882549,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  3.20882549,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         3.49650756,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  3.49650756,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         3.49650756,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Getting the list of centroids.\n",
    "centroids50_list = userClusterModel.clusterCenters()\n",
    "centroids50_list[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|        user_hash_id|            features|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|f1e1cd4ff25018273...|(1000,[0,7,21,25,...|         1|\n",
      "|cf9c7f356092c34be...|(1000,[3,19,28,31...|         2|\n",
      "|f3da73f972caebbe0...|(1000,[7,70,77,25...|         3|\n",
      "|d46f0fdd50ec8aa79...|(1000,[2,9,21,32,...|         4|\n",
      "|d503571e44a0373eb...|(1000,[2,9,21,32,...|         4|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Creating a dataframe with the predictions of the KMeans algo.\n",
    "userTransformed = userClusterModel.transform(userFeaturesDF)\n",
    "userTransformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Function to calculate the euclidean distance between 2 points.\n",
    "def calcDistance(point_a, point_b):\n",
    "    return(np.sqrt(np.sum(np.square(np.subtract(point_a, point_b)))))\n",
    "\n",
    "#Function to calculate the inter-cluster distances between a set of points and its centroid.\n",
    "def interClusterDist(prediction, user_points, centroids_list):\n",
    "    distances_list = []\n",
    "    index = int(prediction)\n",
    "    centroid = centroids_list[index]\n",
    "    for point in user_points:\n",
    "        distances_list.append(calcDistance(point, centroid))\n",
    "    average_distance = np.average(distances_list)\n",
    "    return(float(average_distance))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a user-defined function\n",
    "udf_calcInterClustDist50 = Func.udf(lambda x, y: interClusterDist(x, y, centroids50_list), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|prediction|         user_points|\n",
      "+----------+--------------------+\n",
      "|        26|[(1000,[0,35,69,2...|\n",
      "|        12|[(1000,[0,4,10,11...|\n",
      "|        22|[(1000,[0,4,6,7,1...|\n",
      "|         1|[(1000,[0,7,21,25...|\n",
      "|        13|[(1000,[7,39,67,7...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Grouping the user features with respect to the prediction.\n",
    "groupedUserProfDF = userTransformed.groupBy(\"prediction\").agg(Func.collect_list(\"features\").alias(\"user_points\"))\n",
    "groupedUserProfDF.show(5)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|prediction|         user_points|inter_cluster_dist|\n",
      "+----------+--------------------+------------------+\n",
      "|        26|[(1000,[0,35,69,2...|               0.0|\n",
      "|        12|[(1000,[0,4,10,11...|               0.0|\n",
      "|        22|[(1000,[0,4,6,7,1...|               0.0|\n",
      "|         1|[(1000,[0,7,21,25...|               0.0|\n",
      "|        13|[(1000,[7,39,67,7...|               0.0|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Creating a dataframe containing the inter-cluster distance values.\n",
    "interClustDist50DF = groupedUserProfDF.withColumn(\"inter_cluster_dist\",\\\n",
    "                                                udf_calcInterClustDist50(groupedUserProfDF.prediction,\\\n",
    "                                                                         groupedUserProfDF.user_points))\n",
    "interClustDist50DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Creating a list of inter-cluster distances.\n",
    "interClust50List = interClustDist50DF.select(\"prediction\", \"inter_cluster_dist\")\\\n",
    "                                    .orderBy(\"prediction\", ascending=True)\\\n",
    "                                    .rdd.map(lambda x: x[1]).collect()\n",
    "#interClust50List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Function to calculate the Davies-Boulding index.\n",
    "def calcDBIndex(centroids_list, inter_clust_list):\n",
    "    distance = []\n",
    "    counter = 0\n",
    "    #calculate Davies-Bouldin value for pairs of clusters.\n",
    "    while counter < len(centroids_list):\n",
    "        other_counter = counter + 1\n",
    "        other_centroids = list(itertools.islice(centroids_list, counter+1, None))\n",
    "        for other_centroid in other_centroids:\n",
    "            inter_clust_pair = inter_clust_list[counter]+inter_clust_list[other_counter]\n",
    "            #Calculate intra cluster distance (distance between 2 clusters)\n",
    "            intra_clust_pair = calcDistance(centroids_list[counter], other_centroid)\n",
    "            distance.append(inter_clust_pair/intra_clust_pair)\n",
    "            other_counter += 1    \n",
    "        counter += 1\n",
    "        \n",
    "    #calculate the Davies-Bouldin index.\n",
    "    DB_index = sum(distance)/len(centroids_list)\n",
    "    \n",
    "    return(DB_index)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2818357500840474e-16"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Davies-Bouldin Index for a cluster of size 50\n",
    "calcDBIndex(centroids50_list, interClust50List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Calculating the Davies-Bouldin for the 10 generated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Initializing a Kmeans object.\n",
    "kmeans10 = KMeans(featuresCol=\"features\", k=10, maxIter=10)\n",
    "#Creating a model.\n",
    "userCluster10Model = kmeans10.fit(userFeaturesDF.select(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Getting the list of centroids.\n",
    "centroids10_list = userCluster10Model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|        user_hash_id|            features|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|f1e1cd4ff25018273...|(1000,[0,7,21,25,...|         7|\n",
      "|cf9c7f356092c34be...|(1000,[3,19,28,31...|         7|\n",
      "|f3da73f972caebbe0...|(1000,[7,70,77,25...|         7|\n",
      "|d46f0fdd50ec8aa79...|(1000,[2,9,21,32,...|         7|\n",
      "|d503571e44a0373eb...|(1000,[2,9,21,32,...|         7|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Creating a dataframe with the predictions of the KMeans algo.\n",
    "user10Transformed = userCluster10Model.transform(userFeaturesDF)\n",
    "user10Transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|prediction|         user_points|\n",
      "+----------+--------------------+\n",
      "|         1|[(1000,[1,3,6,8,1...|\n",
      "|         6|[(1000,[1,3,7,11,...|\n",
      "|         3|[(1000,[0,1,2,3,4...|\n",
      "|         5|[(1000,[0,13,21,2...|\n",
      "|         9|[(1000,[5,6,46,47...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Grouping the user features with respect to the prediction.\n",
    "groupedUserProf10DF = user10Transformed.groupBy(\"prediction\").agg(Func.collect_list(\"features\").alias(\"user_points\"))\n",
    "groupedUserProf10DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Creating a user defined function\n",
    "udf_calcInterClustDist10 = Func.udf(lambda x, y: interClusterDist(x, y, centroids10_list), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|prediction|         user_points|inter_cluster_dist|\n",
      "+----------+--------------------+------------------+\n",
      "|         1|[(1000,[1,3,6,8,1...|               0.0|\n",
      "|         6|[(1000,[1,3,7,11,...|               0.0|\n",
      "|         3|[(1000,[0,1,2,3,4...|               0.0|\n",
      "|         5|[(1000,[0,13,21,2...|               0.0|\n",
      "|         9|[(1000,[5,6,46,47...|               0.0|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Creating a dataframe containing the inter-cluster distance values.\n",
    "interClustDist10DF = groupedUserProf10DF.withColumn(\"inter_cluster_dist\",\\\n",
    "                                                udf_calcInterClustDist10(groupedUserProf10DF.prediction,\\\n",
    "                                                                        groupedUserProf10DF.user_points))\n",
    "interClustDist10DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 17.07427978515625, 26.09011459350586, 0.0]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Creating a list of inter-cluster distances.\n",
    "interClust10List = interClustDist10DF.select(\"prediction\", \"inter_cluster_dist\")\\\n",
    "                                    .orderBy(\"prediction\", ascending=True)\\\n",
    "                                    .rdd.map(lambda x: x[1]).collect()\n",
    "#interClust10List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9387042199224245"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Davies-Bouldin Index for a cluster of size 10\n",
    "calcDBIndex(centroids10_list, interClust10List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4 (Latent Direchlet Allocation (LDA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Running LDA Algorithm with k=40 and showing the top 5 terms for each extracted topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Initializing an LDA object.\n",
    "lda = LDA(featuresCol=\"tf_vector\", k=40, maxIter=10)\n",
    "#Creating the model.\n",
    "ldaModel = lda.fit(featurizedDataDF.select(\"tf_vector\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------+------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices              |termWeights                                                                                                       |\n",
      "+-----+-------------------------+------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[726, 779, 743, 175, 588]|[0.0013339958833439576, 0.001320473180729142, 0.0012953853029319666, 0.001294426871207656, 0.001291593299519583]  |\n",
      "|1    |[851, 204, 320, 838, 927]|[0.001317823910699884, 0.001312613168814432, 0.0013094725739818392, 0.0012925885391767084, 0.0012878698150576423] |\n",
      "|2    |[388, 901, 596, 668, 845]|[0.001314594528894793, 0.0013080723360205605, 0.0013052458040161991, 0.0012926310130867316, 0.0012798505196279092]|\n",
      "|3    |[38, 738, 220, 420, 830] |[0.0013183847054659037, 0.001302549516896273, 0.0012938533467724634, 0.0012854476346241682, 0.0012742175780598984]|\n",
      "|4    |[475, 198, 11, 211, 416] |[0.0013590445426179183, 0.0012920545911756776, 0.001267700621288466, 0.0012634399005790361, 0.0012602968195140538]|\n",
      "+-----+-------------------------+------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Showing the top 5 terms for each extracted latent topic.\n",
    "topTopics_5 = ldaModel.describeTopics(5)\n",
    "topTopics_5.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------+\n",
      "|word_id|topic|single_word|\n",
      "+-------+-----+-----------+\n",
      "|      0|   12|    process|\n",
      "|      5|   37|       read|\n",
      "|      5|   29|       read|\n",
      "|      5|   11|       read|\n",
      "|      8|   29|      basic|\n",
      "|     11|    4|   interest|\n",
      "|     14|   22|   knowledg|\n",
      "|     18|   29|       call|\n",
      "|     23|   20|      power|\n",
      "|     24|   37|    program|\n",
      "|     26|   20|       link|\n",
      "|     29|   28|     explor|\n",
      "|     30|   19|      singl|\n",
      "|     34|   18|     scienc|\n",
      "|     35|   33|      learn|\n",
      "|     36|   37|      align|\n",
      "|     38|    3|     number|\n",
      "|     40|   29|    collect|\n",
      "|     45|   10|     featur|\n",
      "|     46|   30|     provid|\n",
      "+-------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Showing the top 5 terms for each extracted latent topic.\n",
    "topTopics_5.select(\"topic\", Func.explode(\"termIndices\").alias(\"word_id\"))\\\n",
    ".join(termsDF, \"word_id\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=115158, tf_vector=SparseVector(1000, {0: 1.0, 1: 1.0, 12: 1.0, 13: 1.0, 21: 1.0, 22: 2.0, 24: 1.0, 27: 1.0, 31: 1.0, 37: 1.0, 43: 1.0, 51: 1.0, 55: 1.0, 62: 1.0, 69: 1.0, 70: 1.0, 76: 1.0, 79: 1.0, 80: 1.0, 84: 1.0, 90: 1.0, 92: 1.0, 95: 1.0, 105: 2.0, 109: 1.0, 110: 1.0, 111: 2.0, 113: 1.0, 117: 1.0, 120: 1.0, 127: 1.0, 130: 1.0, 145: 1.0, 162: 1.0, 164: 1.0, 169: 1.0, 174: 1.0, 180: 1.0, 202: 1.0, 209: 1.0, 219: 1.0, 222: 1.0, 224: 1.0, 239: 1.0, 245: 1.0, 246: 1.0, 258: 1.0, 263: 2.0, 270: 1.0, 275: 1.0, 317: 2.0, 329: 1.0, 336: 1.0, 339: 1.0, 355: 1.0, 364: 1.0, 369: 1.0, 377: 1.0, 384: 1.0, 387: 1.0, 405: 1.0, 411: 1.0, 418: 2.0, 423: 1.0, 436: 1.0, 440: 1.0, 450: 1.0, 454: 1.0, 459: 1.0, 471: 1.0, 500: 1.0, 534: 1.0, 570: 1.0, 583: 1.0, 584: 1.0, 596: 1.0, 614: 1.0}), topicDistribution=DenseVector([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.9886, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003]))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Creating a dataframe with the topic distibution generated from LDA.\n",
    "ldaTransformed = ldaModel.transform(featurizedDataDF)\n",
    "ldaTransformed.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+\n",
      "|paper_id|        user_hash_id|           tf_vector|   topicDistribution|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "|  740681|b656009a6efdc8b1a...|(1000,[0,14,22,37...|[8.70247031601365...|\n",
      "| 4778506|a06e5e3519a8d90a8...|(1000,[0,1,5,33,3...|[8.70247031601365...|\n",
      "|  105906|d2e2a082f925839b3...|(1000,[0,69,292,5...|[0.00488615736766...|\n",
      "|  105906|f1e1cd4ff25018273...|(1000,[0,69,292,5...|[0.00488615736766...|\n",
      "|  212874|a06e5e3519a8d90a8...|(1000,[0,1,14,37,...|[8.70247031601365...|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Joining the topic distribution to the users.\n",
    "joinUserLDA = explodUsersDF.join(ldaTransformed, \"paper_id\")\n",
    "joinUserLDA.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(user_hash_id='f1e1cd4ff25018273aafc0c68fbb5a2f', topic_distribution_ls=[DenseVector([0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0053, 0.0049, 0.8077, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.006, 0.0049, 0.0049, 0.0049, 0.005, 0.0049, 0.0049, 0.0049, 0.005, 0.0049, 0.0049]), DenseVector([0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0066, 0.0061, 0.0064, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.0061, 0.7605, 0.0061, 0.0061, 0.0061, 0.0062, 0.0061, 0.0061, 0.0061, 0.0063, 0.0061, 0.0061]), DenseVector([0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.9659, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009]), DenseVector([0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0053, 0.0049, 0.0051, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.0049, 0.8085, 0.0049, 0.0049, 0.0049, 0.005, 0.0049, 0.0049, 0.0049, 0.005, 0.0049, 0.0049])])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Grouping the topic distribution with respect to the users.\n",
    "grpUserTopicDistDF = joinUserLDA.groupBy(\"user_hash_id\")\\\n",
    "                                .agg(Func.collect_list(\"topicDistribution\").alias(\"topic_distribution_ls\"))\n",
    "grpUserTopicDistDF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Calculating the LDA based user profiles as the summation of the paper topics vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 b)\n",
    "#Function to sum a list of dense vectors.\n",
    "def addDenseVec(dense_vec_list):\n",
    "    result = DenseVector(np.zeros(40))\n",
    "    for dense_vector in dense_vec_list:\n",
    "        result = np.add(result, dense_vector).tolist() \n",
    "    return DenseVector(result)\n",
    "\n",
    "#Creating a user defined function.\n",
    "udf_addDenseVec = Func.udf(addDenseVec, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(user_hash_id='f1e1cd4ff25018273aafc0c68fbb5a2f', features=DenseVector([0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0181, 0.0168, 0.8201, 0.0168, 0.0168, 0.0168, 0.0168, 0.0168, 0.0169, 2.5408, 0.0168, 0.0168, 0.0168, 0.017, 0.0168, 0.0168, 0.0168, 0.0173, 0.0168, 0.0168]))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.4 b)\n",
    "#Summation of the paper topics vectors.\n",
    "userLDAProfile = grpUserTopicDistDF.withColumn(\"features\",\\\n",
    "                                               Func.lit(udf_addDenseVec(grpUserTopicDistDF.topic_distribution_ls)))\n",
    "\n",
    "userLDAProfile.select(\"user_hash_id\", \"features\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Applying the K-means algorithm to cluster the users using their LDA profiles in 50 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 c)\n",
    "#Initializing the K-means object.\n",
    "UsrLDAKmeans = KMeans(featuresCol=\"features\", k=50, maxIter=10)\n",
    "#Creating the model.\n",
    "userLDAClustModel = UsrLDAKmeans.fit(userLDAProfile.select(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00243919, 0.00243919, 0.00243919, 0.00243919, 0.00243919,\n",
       "        0.00243919, 0.00243919, 0.00243919, 0.00243919, 0.00245016,\n",
       "        0.00243919, 0.00243919, 0.00243919, 0.00243919, 0.00243919,\n",
       "        0.00243919, 0.00243919, 0.00243919, 0.00243919, 0.00245023,\n",
       "        0.00263719, 0.00243919, 0.0025533 , 0.00243919, 0.00243919,\n",
       "        0.00243919, 0.00243919, 0.00243919, 0.00245382, 0.90441067,\n",
       "        0.00243919, 0.00243919, 0.00243919, 0.00247763, 0.00243919,\n",
       "        0.00243919, 0.00243919, 0.00251302, 0.00243919, 0.00243919])]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Getting a list of centroids.\n",
    "topicCentroids_list = userLDAClustModel.clusterCenters()\n",
    "topicCentroids_list[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+--------------------+----------+\n",
      "|        user_hash_id|topic_distribution_ls|            features|prediction|\n",
      "+--------------------+---------------------+--------------------+----------+\n",
      "|f1e1cd4ff25018273...| [[0.0048861573676...|[0.01675513477498...|         1|\n",
      "|cf9c7f356092c34be...| [[7.6141206089542...|[7.61412060895422...|         2|\n",
      "|f3da73f972caebbe0...| [[0.0024391869663...|[0.00243918696632...|         0|\n",
      "|d46f0fdd50ec8aa79...| [[7.8598634786179...|[7.85986347861791...|         3|\n",
      "|d503571e44a0373eb...| [[7.8598634786179...|[7.85986347861791...|         3|\n",
      "+--------------------+---------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Creating a dataframe with the predictions of the KMeans algo.\n",
    "userTopicTransformed = userLDAClustModel.transform(userLDAProfile)\n",
    "userTopicTransformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|prediction|     paper_topic_pts|\n",
      "+----------+--------------------+\n",
      "|        12|[[0.0011627482152...|\n",
      "|        22|[[8.7024703160136...|\n",
      "|         1|[[0.0167551347749...|\n",
      "|        13|[[0.0013224931929...|\n",
      "|         6|[[0.0130471140319...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Grouping the user features with respect to the prediction.\n",
    "grpUserTopicProfDF = userTopicTransformed.groupBy(\"prediction\")\\\n",
    "                                            .agg(Func.collect_list(\"features\").alias(\"paper_topic_pts\"))\n",
    "grpUserTopicProfDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Creating a user defined function.\n",
    "udf_calcTopicInterClust = Func.udf(lambda x, y: interClusterDist(x, y, topicCentroids_list), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|prediction|     paper_topic_pts|inter_cluster_dist|\n",
      "+----------+--------------------+------------------+\n",
      "|        12|[[0.0011627482152...|               0.0|\n",
      "|        22|[[8.7024703160136...|               0.0|\n",
      "|         1|[[0.0167551347749...|               0.0|\n",
      "|        13|[[0.0013224931929...|               0.0|\n",
      "|         6|[[0.0130471140319...|               0.0|\n",
      "|        16|[[0.0109467675524...|               0.0|\n",
      "|         3|[[7.8598634786179...|     2.4243498E-19|\n",
      "|        20|[[0.0363485545860...|               0.0|\n",
      "|         5|[[0.0022598097566...|               0.0|\n",
      "|        19|[[0.0048861573676...|               0.0|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Creating a dataframe containing the inter-cluster distance values.\n",
    "topicInterClustDist = grpUserTopicProfDF.withColumn(\"inter_cluster_dist\",\\\n",
    "                                                udf_calcTopicInterClust(grpUserTopicProfDF.prediction,\\\n",
    "                                                                        grpUserTopicProfDF.paper_topic_pts))\n",
    "topicInterClustDist.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Creating a list of inter-cluster distances.\n",
    "topicInterClustList = topicInterClustDist.select(\"prediction\", \"inter_cluster_dist\")\\\n",
    "                                    .orderBy(\"prediction\", ascending=True)\\\n",
    "                                    .rdd.map(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005571902894568843"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Davies-Bouldin Index for a cluster of size 50\n",
    "calcDBIndex(topicCentroids_list, topicInterClustList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
