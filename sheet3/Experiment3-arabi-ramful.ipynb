{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as Func\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, IDF\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector, VectorUDT, Vectors\n",
    "from pyspark.ml.clustering import KMeans, LDA\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "#import string\n",
    "\n",
    "#Create a spark session.\n",
    "sparkSession = SparkSession.builder.appName(\"Experiment3\").getOrCreate()\n",
    "#Get default configurations\n",
    "sparkSession.sparkContext._conf.getAll()\n",
    "#Update default configurations\n",
    "conf = sparkSession.sparkContext._conf.setAll([('spark.executor.memory', '16g')\\\n",
    "                                        , ('spark.app.name', 'Spark Updated Conf')\\\n",
    "                                        , ('spark.executor.cores', '8')\\\n",
    "                                        , ('spark.cores.max', '8')\\\n",
    "                                        , ('spark.driver.memory','16g')\\\n",
    "                                        ,('spark.driver.maxResultSize','16g')])\n",
    "#Stop the current Spark Session\n",
    "sparkSession.sparkContext.stop()\n",
    "#Create a Spark Session\n",
    "sparkSession = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|stop_word|\n",
      "+---------+\n",
      "|        a|\n",
      "|     able|\n",
      "|    about|\n",
      "|    above|\n",
      "|according|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading stopwords_en.txt data into a dataframe.\n",
    "stopWordsDF = sparkSession.read\\\n",
    "                .load(\"/home/jovyan/work/stopwords_en.txt\", format=\"text\", sep=\" \", inferSchema=\"true\", header=\"false\")\\\n",
    "                .toDF('stop_word')\n",
    "stopWordsDF.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        user_hash_id|        user_library|\n",
      "+--------------------+--------------------+\n",
      "|28d3f81251d94b097...|3929762,503574,58...|\n",
      "|d0c9aaa788153daea...|2080631,6343346,5...|\n",
      "|f05bcffe7951de9e5...|1158654,478707,12...|\n",
      "|ca4f1ba4094011d9a...|              278019|\n",
      "|d1d41a15201915503...|6610569,6493797,6...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading users_libraries.txt data into a dataframe.\n",
    "#Defining the column names.\n",
    "user_columns = ['raw_data']\n",
    "rawUsersDF = sparkSession.read\\\n",
    "            .load(\"/home/jovyan/work/users_libraries.txt\", format=\"text\", sep=\";\",\\\n",
    "                  inferSchema=\"true\", quote='\"', header=\"false\")\\\n",
    "            .toDF(*user_columns)\n",
    "\n",
    "usersDF = rawUsersDF.select(Func.split(rawUsersDF.raw_data, \";\").getItem(0).alias(\"user_hash_id\"),\\\n",
    "                           Func.split(rawUsersDF.raw_data, \";\").getItem(1).alias(\"user_library\"))\n",
    "usersDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------------------+----------+------+---------+-----+------+------+----+-----+-------------------+-------+--------------------+--------------------+\n",
      "|paper_id|   type|             journal|book_title|series|publisher|pages|volume|number|year|month|          postedate|address|               title|            abstract|\n",
      "+--------+-------+--------------------+----------+------+---------+-----+------+------+----+-----+-------------------+-------+--------------------+--------------------+\n",
      "|   80546|article|biology and philo...|      null|  null|     null|   17|    19|     2|2004|  mar|2005-01-26 21:35:21|   null|the arbitrariness...|the genetic code ...|\n",
      "| 5842862|article|      molecular cell|      null|  null| elsevier|    2|    35|     6|2009|  sep|2009-09-30 17:11:23|   null|how to choose a g...|choosing good pro...|\n",
      "+--------+-------+--------------------+----------+------+---------+-----+------+------+----+-----+-------------------+-------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading papers.csv data into a dataframe.\n",
    "#Defining the column names.\n",
    "paper_columns = ['paper_id', 'type', 'journal', 'book_title', \\\n",
    "           'series', 'publisher', 'pages', 'volume', \\\n",
    "           'number', 'year', 'month', 'postedate',\\\n",
    "           'address', 'title', 'abstract']\n",
    "\n",
    "papersDF = sparkSession.read\\\n",
    "            .load(\"/home/jovyan/work/papers.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", quote='\"', header=\"false\")\\\n",
    "            .toDF(*paper_columns)\n",
    "papersDF.show(2, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1 (Vector representation for the papers)\n",
    "To generate the bag-of-words representation for each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|paper_id|                text|\n",
      "+--------+--------------------+\n",
      "|   80546|the arbitrariness...|\n",
      "| 5842862|how to choose a g...|\n",
      "| 1242600|how to write cons...|\n",
      "| 3467077|defrosting the di...|\n",
      "|  309395|why most publishe...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Concatenate the title and abstract fields of papers together.\n",
    "textDF = papersDF.select(papersDF.paper_id, Func.concat_ws(\" \", papersDF.title, papersDF.abstract).alias(\"text\"))\n",
    "textDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=80546, text=\"the arbitrariness of the genetic code the genetic code has been regarded as arbitrary in the sense that the codon-amino acid assignments could be different than they actually are. this general idea has been spelled out differently by previous, often rather implicit accounts of arbitrariness. they have drawn on the frozen accident theory, on evolutionary contingency, on alternative causal pathways, and on the absence of direct stereochemical interactions between codons and amino acids. it has also been suggested that the arbitrariness of the genetic code justifies attributing semantic information to macromolecules, notably to {dna}. i argue that these accounts of arbitrariness are unsatisfactory. i propose that the code is arbitrary in the sense of jacques monod's concept of chemical arbitrariness: the genetic code is arbitrary in that any codon requires certain chemical and structural properties to specify a particular amino acid, but these properties are not required in virtue of a principle of chemistry. this notion of arbitrariness is compatible with several recent hypotheses about code evolution. i maintain that the code's chemical arbitrariness is neither sufficient nor necessary for attributing semantic information to nucleic acids.\", words=['arbitrariness', 'genetic', 'code', 'genetic', 'code', 'been', 'regarded', 'arbitrary', 'sense', 'that', 'codon-amino', 'acid', 'assignments', 'could', 'different', 'than', 'they', 'actually', 'this', 'general', 'idea', 'been', 'spelled', 'differently', 'previous', 'often', 'rather', 'implicit', 'accounts', 'arbitrariness', 'they', 'have', 'drawn', 'frozen', 'accident', 'theory', 'evolutionary', 'contingency', 'alternative', 'causal', 'pathways', 'absence', 'direct', 'stereochemical', 'interactions', 'between', 'codons', 'amino', 'acids', 'also', 'been', 'suggested', 'that', 'arbitrariness', 'genetic', 'code', 'justifies', 'attributing', 'semantic', 'information', 'macromolecules', 'notably', 'argue', 'that', 'these', 'accounts', 'arbitrariness', 'unsatisfactory', 'propose', 'that', 'code', 'arbitrary', 'sense', 'jacques', 'monod', 'concept', 'chemical', 'arbitrariness', 'genetic', 'code', 'arbitrary', 'that', 'codon', 'requires', 'certain', 'chemical', 'structural', 'properties', 'specify', 'particular', 'amino', 'acid', 'these', 'properties', 'required', 'virtue', 'principle', 'chemistry', 'this', 'notion', 'arbitrariness', 'compatible', 'with', 'several', 'recent', 'hypotheses', 'about', 'code', 'evolution', 'maintain', 'that', 'code', 'chemical', 'arbitrariness', 'neither', 'sufficient', 'necessary', 'attributing', 'semantic', 'information', 'nucleic', 'acids'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Perform tokenization and remove words less than 3 characters.\n",
    "#Keep the words containing \"-\" and \"_\" characters.\n",
    "reTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", minTokenLength=4, pattern=\"[^-_\\\\w]\")\n",
    "wordsDF = reTokenizer.transform(textDF)\n",
    "wordsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.1\n",
    "#Function to remove the characters \"-\" and \"_\" from words.\n",
    "def concatConnectedWords(wordList):\n",
    "    wordSet = set(wordList)\n",
    "    identity = str.maketrans(\"\", \"\", \"-_\")\n",
    "    wordSet = [word.translate(identity) for word in wordSet]\n",
    "    return wordSet\n",
    "\n",
    "udf_concatConnectedWords = Func.udf(concatConnectedWords, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(paper_id=80546, processed_words=['these', 'actually', 'certain', 'stereochemical', 'notably', 'that', 'concept', 'particular', 'code', 'chemical', 'could', 'they', 'direct', 'propose', 'with', 'about', 'chemistry', 'general', 'regarded', 'notion', 'this', 'nucleic', 'information', 'drawn', 'specify', 'evolution', 'neither', 'absence', 'genetic', 'implicit', 'unsatisfactory', 'previous', 'frozen', 'between', 'alternative', 'amino', 'attributing', 'required', 'codonamino', 'been', 'have', 'structural', 'several', 'properties', 'maintain', 'necessary', 'requires', 'sense', 'hypotheses', 'pathways', 'acids', 'causal', 'codons', 'arbitrary', 'often', 'theory', 'also', 'macromolecules', 'assignments', 'rather', 'accounts', 'contingency', 'acid', 'monod', 'virtue', 'sufficient', 'jacques', 'principle', 'argue', 'idea', 'arbitrariness', 'suggested', 'compatible', 'than', 'recent', 'different', 'semantic', 'codon', 'interactions', 'justifies', 'accident', 'spelled', 'evolutionary', 'differently']),\n",
       " Row(paper_id=5842862, processed_words=['profession', 'that', 'lack', 'what', 'explicit', 'results', 'merit', 'discussion', 'within', 'tenure', 'observation', 'this', 'choose', 'problems', 'smart', 'teachers', 'scientific', 'usually', 'explicitly', 'choosing', 'scientist', 'being', 'figure', 'give', 'good', 'resulting', 'subject', 'vacuum', 'publication', 'approaches', 'through', 'scientists', 'problem', 'expected', 'their', 'such', 'journals', 'leaves', 'discussed', 'lead', 'essential', 'valued', 'enough'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#removing the characters \"-\" and \"_\" from words.\n",
    "removedConnectorsDF = wordsDF.select(wordsDF.paper_id,\\\n",
    "                                     Func.lit(udf_concatConnectedWords(wordsDF.words)).alias(\"processed_words\"))\n",
    "removedConnectorsDF.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Creating a list of stop words.\n",
    "stopWordsList = stopWordsDF.agg(Func.collect_list(stopWordsDF.stop_word)).rdd.flatMap(lambda row: row[0])\n",
    "stopWordsList.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(words=['stereochemical', 'notably', 'concept', 'code', 'chemical', 'direct', 'propose', 'chemistry', 'general', 'regarded', 'notion', 'nucleic', 'information', 'drawn', 'evolution', 'absence', 'genetic', 'implicit', 'unsatisfactory', 'previous', 'frozen', 'alternative', 'amino', 'attributing', 'required', 'codonamino', 'structural', 'properties', 'maintain', 'requires', 'sense', 'hypotheses', 'pathways', 'acids', 'causal', 'codons', 'arbitrary', 'theory', 'macromolecules', 'assignments', 'accounts', 'contingency', 'acid', 'monod', 'virtue', 'sufficient', 'jacques', 'principle', 'argue', 'idea', 'arbitrariness', 'suggested', 'compatible', 'recent', 'semantic', 'codon', 'interactions', 'justifies', 'accident', 'spelled', 'evolutionary', 'differently'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Removing the stop words.\n",
    "remover = StopWordsRemover(inputCol=\"processed_words\", outputCol=\"words\", stopWords=stopWordsList.collect())\n",
    "withoutStopWordsDF = remover.transform(removedConnectorsDF)\n",
    "withoutStopWordsDF.select(\"words\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.1\n",
    "#Function to perform stemming.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemming(wordList):\n",
    "    wordSet = [stemmer.stem(word) for word in wordList]\n",
    "    return sorted(wordSet)\n",
    "\n",
    "#User defined function to perform stemming.\n",
    "udf_stemming = Func.udf(stemming, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.1\n",
    "#Performing stemming.\n",
    "stemmedWordsDF = withoutStopWordsDF.withColumn(\"stemmed_words\", udf_stemming(withoutStopWordsDF.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+\n",
      "|single_word|  df|\n",
      "+-----------+----+\n",
      "| likelihood|1388|\n",
      "|      input|3770|\n",
      "|  viewpoint| 444|\n",
      "|    persist|1629|\n",
      "|    nucleas| 117|\n",
      "|     synopt|  35|\n",
      "|     harder| 135|\n",
      "|   ineffici| 321|\n",
      "|  hematolog|  38|\n",
      "|      still|  11|\n",
      "+-----------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Counting the number of papers (frequency) in which a particular word appears. (Document Frequency)\n",
    "explodedDF = stemmedWordsDF.select(stemmedWordsDF.paper_id,\n",
    "                                   Func.explode(stemmedWordsDF.stemmed_words).alias(\"single_word\"))\\\n",
    "                            .distinct()\\\n",
    "                            .groupBy(\"single_word\")\\\n",
    "                            .agg(Func.count(\"single_word\")\n",
    "                                 .alias(\"df\"))\n",
    "explodedDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+\n",
      "|single_word|  df|\n",
      "+-----------+----+\n",
      "| likelihood|1388|\n",
      "|      input|3770|\n",
      "|  viewpoint| 444|\n",
      "|    persist|1629|\n",
      "|    nucleas| 117|\n",
      "|     synopt|  35|\n",
      "|     harder| 135|\n",
      "|   ineffici| 321|\n",
      "|  hematolog|  38|\n",
      "|     imagin| 397|\n",
      "+-----------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Counting the number of unique papers.\n",
    "#Setting the upper and lower bounds.\n",
    "uniqPaperCount = stemmedWordsDF.count()\n",
    "upperBoundary = 0.1*uniqPaperCount\n",
    "lowerBoundary = 20\n",
    "#filter the dataframe to select only words that appear in more than 20 papers and\n",
    "#less than 10 percent of the total number of papers.\n",
    "filteredDF = explodedDF.filter((explodedDF.df>=lowerBoundary) & (explodedDF.df<=upperBoundary))\n",
    "filteredDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|single_word|\n",
      "+-----------+\n",
      "|     applic|\n",
      "|       time|\n",
      "|   interact|\n",
      "|      activ|\n",
      "|     compar|\n",
      "|    network|\n",
      "|   identifi|\n",
      "|     design|\n",
      "|       find|\n",
      "|       gene|\n",
      "+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Limiting the number of important terms to 1000.\n",
    "termsDF = filteredDF.sort(\"df\", ascending=False).limit(1000).select(\"single_word\")\n",
    "termsDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|single_word|word_id|\n",
      "+-----------+-------+\n",
      "|     applic|      0|\n",
      "|       time|      1|\n",
      "|   interact|      2|\n",
      "|      activ|      3|\n",
      "|     compar|      4|\n",
      "|    network|      5|\n",
      "|   identifi|      6|\n",
      "|     design|      7|\n",
      "|       find|      8|\n",
      "|       gene|      9|\n",
      "+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Generating an index to the top 1000 important terms starting with the index 0.\n",
    "termsDF = termsDF.withColumn(\"word_id\", Func.monotonically_increasing_id())\n",
    "\n",
    "wordWinSpec = Window.orderBy(\"word_id\")\n",
    "\n",
    "termsDF = termsDF.withColumn(\"word_id\",Func.row_number().over(wordWinSpec)-1)\n",
    "termsDF.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---+\n",
      "|paper_id|single_word| tf|\n",
      "+--------+-----------+---+\n",
      "| 1242600|     public|  1|\n",
      "| 1242600|       talk|  1|\n",
      "|      99|       call|  1|\n",
      "|  740681|       imit|  1|\n",
      "|  740681|      model|  1|\n",
      "|   99857|      degre|  1|\n",
      "| 3614773|      studi|  1|\n",
      "|  117535|   reweight|  1|\n",
      "| 4131662|   techniqu|  1|\n",
      "|  115158|     common|  1|\n",
      "+--------+-----------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Counting the number of times a word appeared in a particular paper (word count per paper) (term frequency).\n",
    "tempDF = stemmedWordsDF.select(stemmedWordsDF.paper_id,\n",
    "                               Func.explode(stemmedWordsDF.stemmed_words).alias(\"single_word\"))\\\n",
    "                        .groupBy(\"paper_id\", \"single_word\")\\\n",
    "                        .agg(Func.count(\"single_word\").alias(\"tf\"))\n",
    "tempDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|paper_id|            map_list|\n",
      "+--------+--------------------+\n",
      "|     148|[[55 -> 1], [386 ...|\n",
      "|     496|[[75 -> 1], [2 ->...|\n",
      "|    1238|[[557 -> 1], [87 ...|\n",
      "|    1959|[[743 -> 1], [619...|\n",
      "|    4101|[[534 -> 1], [135...|\n",
      "|    4935|[[761 -> 2], [161...|\n",
      "|   29719|[[652 -> 1], [761...|\n",
      "|   78113|         [[11 -> 1]]|\n",
      "|   81501|[[45 -> 1], [82 -...|\n",
      "|   89863|[[392 -> 1], [580...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Creating a dataframe with term index and count of the word per paper.\n",
    "#Creating a column containing a mapping of term index -> count per paper.\n",
    "joinedResult = tempDF.join(termsDF, \"single_word\").withColumn(\"map\", Func.create_map(\"word_id\", \"tf\"))\\\n",
    "                    .groupBy(\"paper_id\").agg(Func.collect_list(\"map\").alias(\"map_list\"))\n",
    "joinedResult.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.1\n",
    "#Function to create a sparse vector for each paper.\n",
    "def toSparse(mapList):\n",
    "    pairs = []\n",
    "    for map_val in mapList:\n",
    "        pairs += map_val.items()\n",
    "    pairs = sorted(pairs, key=lambda pair: pair[0])\n",
    "    return SparseVector(1000, [x[0] for x in pairs], [x[1] for x in pairs])\n",
    "\n",
    "#Creating a user defined function.\n",
    "udf_toSparse = Func.udf(toSparse, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=148, tf_vector=SparseVector(1000, {2: 1.0, 5: 2.0, 7: 1.0, 25: 1.0, 41: 1.0, 47: 1.0, 49: 1.0, 52: 1.0, 55: 1.0, 61: 1.0, 67: 1.0, 71: 1.0, 75: 1.0, 86: 1.0, 91: 1.0, 101: 1.0, 106: 1.0, 116: 1.0, 118: 1.0, 145: 1.0, 149: 1.0, 158: 1.0, 160: 1.0, 163: 1.0, 196: 1.0, 222: 1.0, 230: 1.0, 285: 1.0, 311: 1.0, 313: 1.0, 317: 1.0, 344: 1.0, 348: 1.0, 354: 1.0, 371: 1.0, 372: 1.0, 386: 1.0, 416: 1.0, 490: 1.0, 526: 1.0, 607: 1.0, 642: 1.0, 685: 1.0, 700: 1.0, 727: 1.0, 733: 1.0, 862: 1.0, 880: 1.0, 915: 1.0, 937: 2.0}))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Creating the bag of words dataframe.\n",
    "featurizedDataDF = joinedResult.select(\"paper_id\",\n",
    "                                   udf_toSparse(joinedResult.map_list)\n",
    "                                   .alias(\"tf_vector\"))\n",
    "featurizedDataDF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2 (TF-IDF representation for the papers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating TF and IDF using functionality provided by pyspark.ml.feature.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=148, tf_vector=SparseVector(1000, {2: 1.0, 5: 2.0, 7: 1.0, 25: 1.0, 41: 1.0, 47: 1.0, 49: 1.0, 52: 1.0, 55: 1.0, 61: 1.0, 67: 1.0, 71: 1.0, 75: 1.0, 86: 1.0, 91: 1.0, 101: 1.0, 106: 1.0, 116: 1.0, 118: 1.0, 145: 1.0, 149: 1.0, 158: 1.0, 160: 1.0, 163: 1.0, 196: 1.0, 222: 1.0, 230: 1.0, 285: 1.0, 311: 1.0, 313: 1.0, 317: 1.0, 344: 1.0, 348: 1.0, 354: 1.0, 371: 1.0, 372: 1.0, 386: 1.0, 416: 1.0, 490: 1.0, 526: 1.0, 607: 1.0, 642: 1.0, 685: 1.0, 700: 1.0, 727: 1.0, 733: 1.0, 862: 1.0, 880: 1.0, 915: 1.0, 937: 2.0}), idf_vector=SparseVector(1000, {2: 2.2937, 5: 4.6545, 7: 2.3549, 25: 2.5051, 41: 2.6222, 47: 2.691, 49: 2.6938, 52: 2.7091, 55: 2.7245, 61: 2.7501, 67: 2.767, 71: 2.7875, 75: 2.7938, 86: 2.8662, 91: 2.9035, 101: 2.9508, 106: 2.9795, 116: 3.014, 118: 3.0221, 145: 3.1298, 149: 3.1438, 158: 3.1718, 160: 3.173, 163: 3.2011, 196: 3.291, 222: 3.3608, 230: 3.3891, 285: 3.565, 311: 3.6248, 313: 3.6291, 317: 3.6403, 344: 3.7059, 348: 3.7088, 354: 3.7118, 371: 3.7563, 372: 3.7576, 386: 3.7861, 416: 3.8516, 490: 3.9993, 526: 4.0806, 607: 4.2174, 642: 4.2784, 685: 4.3578, 700: 4.3966, 727: 4.4319, 733: 4.4405, 862: 4.623, 880: 4.6478, 915: 4.6954, 937: 9.4347}))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Initializing the IDF object.\n",
    "sparkMlIdf = IDF(inputCol=\"tf_vector\", outputCol=\"idf_vector\")\n",
    "#Training the data and creating a model.\n",
    "sparkMlIdfModel = sparkMlIdf.fit(featurizedDataDF)\n",
    "#Adding the output produced from IDF to the dataset as a separate column.\n",
    "IdfData = sparkMlIdfModel.transform(featurizedDataDF)\n",
    "IdfData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.2\n",
    "#Function to multiply 2 sparse vectors.\n",
    "def multiplySparseVec(x_vec, y_vec):\n",
    "    result = np.multiply(x_vec, y_vec).tolist()\n",
    "    vector_args = len(result), [i for i, x in enumerate(result) if x != 0], [x for x in result if x != 0] \n",
    "    return Vectors.sparse(*vector_args)\n",
    "\n",
    "#Creating a user defined function.\n",
    "udf_multiplySparseVec = Func.udf(multiplySparseVec, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=148, tf_vector=SparseVector(1000, {2: 1.0, 5: 2.0, 7: 1.0, 25: 1.0, 41: 1.0, 47: 1.0, 49: 1.0, 52: 1.0, 55: 1.0, 61: 1.0, 67: 1.0, 71: 1.0, 75: 1.0, 86: 1.0, 91: 1.0, 101: 1.0, 106: 1.0, 116: 1.0, 118: 1.0, 145: 1.0, 149: 1.0, 158: 1.0, 160: 1.0, 163: 1.0, 196: 1.0, 222: 1.0, 230: 1.0, 285: 1.0, 311: 1.0, 313: 1.0, 317: 1.0, 344: 1.0, 348: 1.0, 354: 1.0, 371: 1.0, 372: 1.0, 386: 1.0, 416: 1.0, 490: 1.0, 526: 1.0, 607: 1.0, 642: 1.0, 685: 1.0, 700: 1.0, 727: 1.0, 733: 1.0, 862: 1.0, 880: 1.0, 915: 1.0, 937: 2.0}), idf_vector=SparseVector(1000, {2: 2.2937, 5: 4.6545, 7: 2.3549, 25: 2.5051, 41: 2.6222, 47: 2.691, 49: 2.6938, 52: 2.7091, 55: 2.7245, 61: 2.7501, 67: 2.767, 71: 2.7875, 75: 2.7938, 86: 2.8662, 91: 2.9035, 101: 2.9508, 106: 2.9795, 116: 3.014, 118: 3.0221, 145: 3.1298, 149: 3.1438, 158: 3.1718, 160: 3.173, 163: 3.2011, 196: 3.291, 222: 3.3608, 230: 3.3891, 285: 3.565, 311: 3.6248, 313: 3.6291, 317: 3.6403, 344: 3.7059, 348: 3.7088, 354: 3.7118, 371: 3.7563, 372: 3.7576, 386: 3.7861, 416: 3.8516, 490: 3.9993, 526: 4.0806, 607: 4.2174, 642: 4.2784, 685: 4.3578, 700: 4.3966, 727: 4.4319, 733: 4.4405, 862: 4.623, 880: 4.6478, 915: 4.6954, 937: 9.4347}), tf_idf=SparseVector(1000, {2: 2.2937, 5: 9.3091, 7: 2.3549, 25: 2.5051, 41: 2.6222, 47: 2.691, 49: 2.6938, 52: 2.7091, 55: 2.7245, 61: 2.7501, 67: 2.767, 71: 2.7875, 75: 2.7938, 86: 2.8662, 91: 2.9035, 101: 2.9508, 106: 2.9795, 116: 3.014, 118: 3.0221, 145: 3.1298, 149: 3.1438, 158: 3.1718, 160: 3.173, 163: 3.2011, 196: 3.291, 222: 3.3608, 230: 3.3891, 285: 3.565, 311: 3.6248, 313: 3.6291, 317: 3.6403, 344: 3.7059, 348: 3.7088, 354: 3.7118, 371: 3.7563, 372: 3.7576, 386: 3.7861, 416: 3.8516, 490: 3.9993, 526: 4.0806, 607: 4.2174, 642: 4.2784, 685: 4.3578, 700: 4.3966, 727: 4.4319, 733: 4.4405, 862: 4.623, 880: 4.6478, 915: 4.6954, 937: 18.8695}))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Calculating the value of TF-IDF.\n",
    "paperTfIdf = IdfData.withColumn(\"tf_idf\", udf_multiplySparseVec(IdfData.tf_vector, IdfData.idf_vector))\n",
    "paperTfIdf.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating TF and IDF WITHOUT using functionality provided by pyspark.ml.feature.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----+\n",
      "|single_word|word_id|  df|\n",
      "+-----------+-------+----+\n",
      "| likelihood|    993|1388|\n",
      "|      input|    386|3770|\n",
      "|    persist|    864|1629|\n",
      "|       oper|    221|5775|\n",
      "|    classif|    403|3602|\n",
      "|     execut|    674|2182|\n",
      "|        map|    273|4874|\n",
      "|      equal|    839|1690|\n",
      "|    explain|    250|5240|\n",
      "|     growth|    327|4223|\n",
      "+-----------+-------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Creating a dataframe with the terms, ID and document-frequency.\n",
    "termsWithDF = termsDF.join(filteredDF, \"single_word\")\n",
    "termsWithDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17098, 17014, 16771, 16598, 16395]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#creating a document frequency array where index of array element is equal to word_id\n",
    "docFreqList = termsWithDF.select(\"word_id\", \"df\").orderBy(\"word_id\", ascending=True).rdd.map(lambda x: x[1]).collect()\n",
    "docFreqList[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.2\n",
    "#Function to calculate the IDF.\n",
    "def calcIdf(m, docFreq):\n",
    "    Idf = math.log((m+1)/(docFreq+1))\n",
    "    return Idf\n",
    "\n",
    "#Function to create the IDF vector.\n",
    "def getIdfSparseVec(m, tf_vector, doc_freq_ls):\n",
    "    index = []\n",
    "    value = []\n",
    "    #retrieving active indices\n",
    "    active_idx = tf_vector.indices\n",
    "\n",
    "    for idx in active_idx:\n",
    "        doc_freq = doc_freq_ls[idx]\n",
    "        index.append(idx)\n",
    "        value.append(calcIdf(m, doc_freq))\n",
    "    return SparseVector(1000, index, value)\n",
    "\n",
    "udf_getIdfSparseVec = Func.udf(lambda x, y: getIdfSparseVec(x, y, docFreqList), VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=148, tf_vector=SparseVector(1000, {2: 1.0, 5: 2.0, 7: 1.0, 25: 1.0, 41: 1.0, 47: 1.0, 49: 1.0, 52: 1.0, 55: 1.0, 61: 1.0, 67: 1.0, 71: 1.0, 75: 1.0, 86: 1.0, 91: 1.0, 101: 1.0, 106: 1.0, 116: 1.0, 118: 1.0, 145: 1.0, 149: 1.0, 158: 1.0, 160: 1.0, 163: 1.0, 196: 1.0, 222: 1.0, 230: 1.0, 285: 1.0, 311: 1.0, 313: 1.0, 317: 1.0, 344: 1.0, 348: 1.0, 354: 1.0, 371: 1.0, 372: 1.0, 386: 1.0, 416: 1.0, 490: 1.0, 526: 1.0, 607: 1.0, 642: 1.0, 685: 1.0, 700: 1.0, 727: 1.0, 733: 1.0, 862: 1.0, 880: 1.0, 915: 1.0, 937: 2.0}), idf_vector=SparseVector(1000, {2: 2.3282, 5: 2.3618, 7: 2.3894, 25: 2.5396, 41: 2.6567, 47: 2.7255, 49: 2.7283, 52: 2.7436, 55: 2.759, 61: 2.7846, 67: 2.8015, 71: 2.822, 75: 2.8283, 86: 2.9007, 91: 2.938, 101: 2.9853, 106: 3.014, 116: 3.0485, 118: 3.0566, 145: 3.1643, 149: 3.1783, 158: 3.2063, 160: 3.2075, 163: 3.2356, 196: 3.3255, 222: 3.3953, 230: 3.4236, 285: 3.5995, 311: 3.6593, 313: 3.6636, 317: 3.6748, 344: 3.7404, 348: 3.7433, 354: 3.7463, 371: 3.7908, 372: 3.7921, 386: 3.8206, 416: 3.8861, 490: 4.0338, 526: 4.1151, 607: 4.2519, 642: 4.3129, 685: 4.3923, 700: 4.4311, 727: 4.4664, 733: 4.475, 862: 4.6575, 880: 4.6823, 915: 4.7299, 937: 4.7519}))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Creating a new dataframe with IDF values.\n",
    "manualIdf = featurizedDataDF.withColumn(\"idf_vector\", udf_getIdfSparseVec(Func.lit(uniqPaperCount),\\\n",
    "                                                                           featurizedDataDF.tf_vector))\n",
    "manualIdf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=148, tf_vector=SparseVector(1000, {2: 1.0, 5: 2.0, 7: 1.0, 25: 1.0, 41: 1.0, 47: 1.0, 49: 1.0, 52: 1.0, 55: 1.0, 61: 1.0, 67: 1.0, 71: 1.0, 75: 1.0, 86: 1.0, 91: 1.0, 101: 1.0, 106: 1.0, 116: 1.0, 118: 1.0, 145: 1.0, 149: 1.0, 158: 1.0, 160: 1.0, 163: 1.0, 196: 1.0, 222: 1.0, 230: 1.0, 285: 1.0, 311: 1.0, 313: 1.0, 317: 1.0, 344: 1.0, 348: 1.0, 354: 1.0, 371: 1.0, 372: 1.0, 386: 1.0, 416: 1.0, 490: 1.0, 526: 1.0, 607: 1.0, 642: 1.0, 685: 1.0, 700: 1.0, 727: 1.0, 733: 1.0, 862: 1.0, 880: 1.0, 915: 1.0, 937: 2.0}), idf_vector=SparseVector(1000, {2: 2.3282, 5: 2.3618, 7: 2.3894, 25: 2.5396, 41: 2.6567, 47: 2.7255, 49: 2.7283, 52: 2.7436, 55: 2.759, 61: 2.7846, 67: 2.8015, 71: 2.822, 75: 2.8283, 86: 2.9007, 91: 2.938, 101: 2.9853, 106: 3.014, 116: 3.0485, 118: 3.0566, 145: 3.1643, 149: 3.1783, 158: 3.2063, 160: 3.2075, 163: 3.2356, 196: 3.3255, 222: 3.3953, 230: 3.4236, 285: 3.5995, 311: 3.6593, 313: 3.6636, 317: 3.6748, 344: 3.7404, 348: 3.7433, 354: 3.7463, 371: 3.7908, 372: 3.7921, 386: 3.8206, 416: 3.8861, 490: 4.0338, 526: 4.1151, 607: 4.2519, 642: 4.3129, 685: 4.3923, 700: 4.4311, 727: 4.4664, 733: 4.475, 862: 4.6575, 880: 4.6823, 915: 4.7299, 937: 4.7519}), tf_idf=SparseVector(1000, {2: 2.3282, 5: 4.7236, 7: 2.3894, 25: 2.5396, 41: 2.6567, 47: 2.7255, 49: 2.7283, 52: 2.7436, 55: 2.759, 61: 2.7846, 67: 2.8015, 71: 2.822, 75: 2.8283, 86: 2.9007, 91: 2.938, 101: 2.9853, 106: 3.014, 116: 3.0485, 118: 3.0566, 145: 3.1643, 149: 3.1783, 158: 3.2063, 160: 3.2075, 163: 3.2356, 196: 3.3255, 222: 3.3953, 230: 3.4236, 285: 3.5995, 311: 3.6593, 313: 3.6636, 317: 3.6748, 344: 3.7404, 348: 3.7433, 354: 3.7463, 371: 3.7908, 372: 3.7921, 386: 3.8206, 416: 3.8861, 490: 4.0338, 526: 4.1151, 607: 4.2519, 642: 4.3129, 685: 4.3923, 700: 4.4311, 727: 4.4664, 733: 4.475, 862: 4.6575, 880: 4.6823, 915: 4.7299, 937: 9.5037}))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Calculating the value of TF-IDF.\n",
    "manualTfIdf = manualIdf.withColumn(\"tf_idf\", udf_multiplySparseVec(manualIdf.tf_vector, manualIdf.idf_vector))\n",
    "manualTfIdf.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3 (Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Calculating user profile for each user as the summation of the TF-IDF vectors of the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|        user_hash_id|paper_id|\n",
      "+--------------------+--------+\n",
      "|28d3f81251d94b097...| 3929762|\n",
      "|28d3f81251d94b097...|  503574|\n",
      "|28d3f81251d94b097...| 5819422|\n",
      "|28d3f81251d94b097...| 4238883|\n",
      "|28d3f81251d94b097...| 5788061|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Splitting the list of paper_ids into individual rows.\n",
    "explodUsersDF = usersDF.select(usersDF.user_hash_id,\\\n",
    "                               Func.explode(Func.split(usersDF.user_library, \",\"))\\\n",
    "                               .alias(\"paper_id\"))\n",
    "explodUsersDF.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|paper_id|        user_hash_id|           tf_vector|          idf_vector|              tf_idf|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     148|8ac80c1b48f33b5c2...|(1000,[2,5,7,25,4...|(1000,[2,5,7,25,4...|(1000,[2,5,7,25,4...|\n",
      "|     148|add58a98787fee1a1...|(1000,[2,5,7,25,4...|(1000,[2,5,7,25,4...|(1000,[2,5,7,25,4...|\n",
      "|     148|20e72d3b3cbe48c98...|(1000,[2,5,7,25,4...|(1000,[2,5,7,25,4...|(1000,[2,5,7,25,4...|\n",
      "|     148|e571f7858c3c6d226...|(1000,[2,5,7,25,4...|(1000,[2,5,7,25,4...|(1000,[2,5,7,25,4...|\n",
      "|     148|e6dc6cf6460b94a70...|(1000,[2,5,7,25,4...|(1000,[2,5,7,25,4...|(1000,[2,5,7,25,4...|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Creating a dataframe with user_hash_id as well as paper_id, tf, idf, tf-idf information.\n",
    "joinUserPaperTfIdf = explodUsersDF.join(paperTfIdf, \"paper_id\")\n",
    "joinUserPaperTfIdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        user_hash_id|         tf_idf_list|\n",
      "+--------------------+--------------------+\n",
      "|03237605301d9dd8e...|[(1000,[9,10,12,1...|\n",
      "|2086aff81a8d6f9d9...|[(1000,[614],[4.2...|\n",
      "|42407318182edf74b...|[(1000,[27,62,474...|\n",
      "|4dccc6267cfee33b5...|[(1000,[87,557],[...|\n",
      "|dcb0db5e7f1e041d8...|[(1000,[7,11,12,1...|\n",
      "|e6db138f507c636de...|[(1000,[684,933],...|\n",
      "|e8e6b6347145fe653...|[(1000,[0,19,29,3...|\n",
      "|f0baca0d108f14901...|[(1000,[17,21,30,...|\n",
      "|f1109606cf8f36542...|[(1000,[2,9,10,12...|\n",
      "|f1e1cd4ff25018273...|[(1000,[1,3,22,61...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Grouping the tf-idf vectors with respect to the user.\n",
    "userTfIdfList = joinUserPaperTfIdf.groupBy(\"user_hash_id\").agg(Func.collect_list(\"tf_idf\").alias(\"tf_idf_list\"))\n",
    "userTfIdfList.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Function to add elements of a list of sparse vectors.\n",
    "def addSparseVec(vector_list):\n",
    "    result = SparseVector(1000, list(range(0, 1000)), np.zeros(1000))\n",
    "    for vector in vector_list:\n",
    "        result = np.add(result, vector).tolist()\n",
    "\n",
    "    vector_args = len(result), [i for i, x in enumerate(result) if x != 0], [x for x in result if x != 0] \n",
    "    return Vectors.sparse(*vector_args)\n",
    "    \n",
    "udf_addSparseVec = Func.udf(addSparseVec, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(user_hash_id='03237605301d9dd8e883b91a7f0423de', features=SparseVector(1000, {0: 11.3722, 1: 4.5587, 2: 13.7624, 3: 2.3041, 4: 25.4805, 5: 2.3273, 6: 30.3518, 7: 4.7098, 8: 9.4399, 9: 94.555, 10: 21.339, 11: 11.8563, 12: 7.1328, 13: 19.1858, 15: 12.084, 16: 14.5671, 17: 4.8583, 18: 4.898, 19: 22.0523, 20: 29.4257, 21: 26.9874, 22: 7.4243, 23: 2.4942, 24: 7.5135, 25: 20.0406, 27: 115.6691, 28: 7.5669, 29: 5.0448, 30: 134.1892, 31: 2.5366, 32: 5.0786, 33: 12.8502, 34: 10.3152, 35: 2.5822, 36: 5.1812, 37: 12.9671, 38: 15.5881, 39: 54.7407, 40: 15.6431, 41: 2.6222, 42: 15.8054, 43: 5.2718, 44: 2.6569, 45: 5.3194, 46: 2.664, 47: 2.691, 48: 10.7745, 49: 13.469, 50: 8.0972, 51: 5.4049, 53: 10.8706, 54: 2.7224, 56: 10.9312, 57: 2.7341, 58: 13.6927, 59: 8.2209, 61: 22.0004, 62: 110.1265, 63: 5.5084, 65: 8.2777, 66: 16.5765, 67: 2.767, 68: 11.0705, 69: 8.3054, 70: 27.7937, 71: 33.4505, 72: 8.3682, 74: 8.3814, 76: 8.3826, 77: 11.2076, 78: 11.2679, 79: 2.8177, 80: 14.1855, 81: 8.5172, 82: 5.6789, 84: 25.7197, 85: 17.1528, 86: 11.4647, 87: 17.204, 88: 14.4642, 89: 2.8948, 90: 5.7902, 91: 2.9035, 92: 8.7116, 94: 5.8163, 95: 11.6397, 96: 11.7078, 97: 2.9276, 98: 11.7249, 99: 5.8629, 100: 2.938, 101: 5.9016, 102: 5.906, 103: 32.539, 104: 5.9346, 105: 26.7499, 106: 2.9795, 107: 5.9631, 108: 14.9178, 109: 5.9829, 111: 20.961, 112: 5.9929, 113: 18.0508, 115: 3.0138, 116: 9.0419, 118: 9.0662, 119: 12.0952, 120: 24.2163, 122: 3.0272, 123: 3.029, 124: 9.0904, 125: 3.0323, 126: 12.1405, 127: 9.1103, 129: 9.1428, 130: 18.3268, 131: 3.055, 132: 24.4715, 133: 3.0643, 135: 9.2272, 136: 9.2304, 137: 3.0814, 138: 15.4285, 139: 18.5913, 140: 6.215, 141: 18.6475, 142: 3.1185, 143: 3.1194, 144: 3.1241, 145: 84.5055, 146: 3.1366, 147: 25.1094, 148: 15.7003, 149: 3.1438, 151: 9.4441, 152: 3.1482, 154: 3.1524, 157: 9.5138, 158: 12.6873, 159: 19.037, 160: 3.173, 161: 6.3653, 162: 19.109, 163: 9.6033, 165: 25.6277, 166: 6.4105, 167: 6.4129, 168: 28.9088, 169: 16.0619, 170: 16.1085, 171: 3.2276, 172: 3.2288, 173: 3.2296, 174: 3.232, 175: 3.2323, 176: 19.4048, 177: 3.2437, 178: 3.2467, 182: 16.2936, 184: 3.2641, 186: 6.5382, 187: 22.8848, 189: 13.0846, 190: 6.5471, 191: 9.844, 192: 9.8513, 194: 6.5749, 195: 3.2886, 196: 75.693, 198: 6.5843, 199: 3.2949, 200: 3.2973, 201: 6.5973, 202: 6.6002, 203: 6.6248, 204: 3.3127, 205: 3.3132, 206: 3.3239, 207: 3.325, 210: 3.3301, 211: 3.3321, 212: 3.3328, 213: 3.3346, 214: 13.3581, 215: 10.041, 216: 10.0431, 217: 3.3513, 218: 3.3537, 219: 16.7711, 220: 13.4189, 222: 3.3608, 223: 20.2001, 224: 13.4668, 225: 10.1053, 226: 6.7474, 227: 13.5102, 228: 13.5442, 229: 3.3875, 230: 3.3891, 232: 13.5964, 233: 13.6073, 237: 17.0672, 238: 6.8283, 241: 27.3957, 243: 24.0427, 245: 3.4386, 246: 3.4388, 248: 3.4427, 249: 6.8983, 250: 3.4569, 252: 6.9277, 254: 20.8304, 258: 3.4848, 260: 3.4917, 262: 10.4828, 263: 3.4986, 264: 3.4988, 265: 3.5016, 267: 14.0265, 269: 10.5398, 270: 3.5203, 271: 7.0448, 273: 14.1173, 274: 14.1288, 275: 3.533, 276: 7.0665, 277: 3.5388, 278: 14.1802, 279: 3.5528, 280: 3.5534, 282: 10.6716, 283: 7.1203, 284: 3.5627, 286: 3.565, 288: 3.5697, 294: 3.5807, 295: 10.7427, 296: 21.5126, 298: 3.5937, 299: 10.7877, 300: 3.5968, 302: 3.6027, 303: 18.0502, 305: 25.2796, 307: 7.2294, 308: 32.5486, 309: 3.6174, 312: 3.628, 315: 7.2664, 316: 3.6366, 318: 3.6449, 319: 18.229, 320: 3.6477, 321: 7.2999, 323: 3.6544, 324: 10.9694, 325: 7.3392, 326: 3.672, 332: 3.6817, 333: 3.687, 334: 3.6882, 335: 36.8866, 336: 11.0732, 338: 3.6964, 339: 7.3937, 340: 3.6976, 341: 7.3957, 343: 7.4025, 344: 3.7059, 346: 3.7074, 347: 3.7078, 348: 3.7088, 349: 33.3883, 352: 3.7105, 353: 3.711, 354: 3.7118, 356: 3.7145, 358: 11.1516, 359: 3.7172, 360: 3.7252, 364: 11.2033, 366: 26.2208, 367: 26.2297, 370: 15.0233, 371: 30.0506, 372: 3.7576, 374: 3.761, 378: 3.7716, 379: 3.7758, 382: 3.7808, 383: 3.7819, 386: 3.7861, 387: 11.3751, 391: 3.7965, 392: 3.7987, 398: 3.8201, 399: 15.2859, 400: 7.6451, 401: 3.8289, 402: 3.8314, 403: 7.6634, 404: 3.8342, 407: 3.835, 409: 7.6712, 410: 3.8364, 411: 3.84, 413: 30.7563, 416: 3.8516, 417: 3.8516, 421: 3.8593, 422: 3.861, 423: 3.8633, 427: 7.7513, 429: 3.8789, 430: 3.8791, 431: 3.8806, 434: 3.8882, 436: 23.3363, 439: 3.8944, 440: 3.8956, 441: 3.8962, 442: 7.7953, 444: 3.9033, 447: 7.8228, 448: 58.7566, 450: 3.9201, 452: 3.9229, 453: 3.9244, 457: 3.928, 460: 3.9363, 461: 39.3664, 463: 3.941, 465: 7.9006, 467: 3.9537, 468: 7.9138, 469: 11.8716, 470: 19.8049, 474: 3.9667, 475: 15.882, 476: 7.9436, 477: 3.9731, 479: 11.9249, 482: 7.9538, 483: 3.9804, 484: 3.9811, 485: 3.9875, 486: 7.9835, 492: 4.0095, 493: 4.0099, 498: 4.0249, 499: 8.0613, 508: 8.0954, 509: 4.0532, 510: 4.0539, 511: 4.0543, 512: 8.1218, 515: 8.1281, 519: 8.1323, 520: 12.2174, 522: 16.3026, 528: 4.0838, 531: 12.273, 534: 4.0957, 536: 40.9858, 538: 16.3987, 539: 16.4001, 540: 49.2309, 542: 4.1073, 543: 4.1081, 544: 8.2198, 549: 4.1198, 553: 4.1314, 554: 16.539, 556: 4.1385, 557: 4.1404, 559: 4.1446, 560: 33.1596, 561: 4.1449, 565: 4.1499, 570: 8.309, 574: 4.1587, 576: 8.3228, 579: 4.163, 582: 4.1777, 584: 4.1797, 587: 4.1829, 590: 8.3697, 591: 4.1848, 593: 50.2465, 596: 16.7663, 597: 4.1932, 599: 8.3943, 604: 4.2104, 605: 4.2161, 611: 8.4495, 613: 8.4552, 620: 4.2392, 624: 4.243, 626: 4.2569, 627: 17.0293, 633: 4.2684, 636: 4.2719, 637: 4.2753, 638: 4.2771, 645: 8.5628, 649: 8.5829, 651: 4.295, 654: 8.6131, 655: 4.3074, 657: 4.3132, 664: 4.3241, 666: 8.65, 667: 4.3254, 671: 4.3282, 672: 4.33, 673: 4.3318, 674: 4.3328, 676: 4.336, 677: 4.3364, 681: 4.3536, 682: 4.355, 684: 4.3569, 687: 4.3606, 691: 4.3744, 692: 4.3806, 694: 4.382, 696: 21.9271, 697: 4.3878, 699: 4.3956, 707: 4.4064, 712: 8.8257, 714: 8.8366, 716: 4.4238, 719: 4.4258, 723: 4.4288, 728: 8.8678, 729: 4.4359, 734: 4.4415, 735: 4.4436, 736: 4.4436, 738: 4.4451, 739: 13.3384, 741: 4.4492, 746: 4.4643, 750: 4.4748, 755: 22.4082, 756: 4.4843, 760: 4.4886, 762: 17.9693, 766: 35.9732, 767: 4.5004, 770: 4.5026, 771: 4.5059, 772: 18.0256, 781: 9.0281, 782: 4.5141, 786: 45.1737, 792: 4.5234, 794: 4.5307, 795: 9.0625, 800: 9.0703, 801: 9.0726, 802: 4.5391, 803: 13.6173, 805: 4.5402, 807: 4.5408, 813: 4.5476, 814: 4.5481, 816: 4.5493, 819: 36.4124, 824: 9.1191, 828: 4.5682, 829: 4.57, 831: 9.1481, 832: 13.7221, 835: 9.1551, 851: 4.6036, 853: 4.6066, 856: 9.2181, 869: 4.6292, 870: 4.6298, 874: 4.636, 877: 4.6453, 889: 93.1079, 896: 4.6752, 897: 4.6752, 905: 4.6869, 906: 9.3777, 908: 9.379, 909: 4.6901, 911: 4.6901, 912: 9.3855, 916: 9.3921, 917: 4.7013, 926: 9.4186, 927: 4.71, 928: 4.7107, 931: 4.7127, 936: 4.7174, 938: 4.7174, 940: 4.7207, 948: 4.7282, 950: 4.7343, 952: 4.7357, 961: 37.9683, 963: 4.7467, 970: 4.7558, 978: 4.767, 979: 14.3032, 980: 4.7713, 981: 4.7727, 985: 4.7763, 988: 19.1136, 998: 4.7863}))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Creating the user profile.\n",
    "userProfileDF = userTfIdfList.withColumn(\"features\", udf_addSparseVec(userTfIdfList.tf_idf_list))\n",
    "userFeaturesDF = userProfileDF.select(\"user_hash_id\", \"features\")\n",
    "userFeaturesDF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Applying the K-Means algorithm to cluster the users in 50 clusters given their profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 b)\n",
    "#Initializing a Kmeans object.\n",
    "kmeans50 = KMeans(featuresCol=\"features\", k=50, maxIter=10)\n",
    "#Creating the model.\n",
    "userClusterModel = kmeans50.fit(userFeaturesDF.select(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Calculating the Davies-Bouldin index for the 50 Generated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2.80984482, 2.17845141, 2.73838195, 2.76097062, 2.18150292,\n",
       "        3.16367322, 2.31596347, 3.18632578, 2.02938087, 2.25164002,\n",
       "        1.78562005, 1.71686914, 2.70850743, 1.91131523, 3.12124388,\n",
       "        1.86098396, 1.88025302, 2.22703824, 1.70804603, 2.4263263 ,\n",
       "        2.03899846, 1.78754908, 2.07838753, 2.64912025, 2.04582905,\n",
       "        2.3203221 , 2.16036808, 2.38924492, 1.83421976, 1.76515535,\n",
       "        2.95020505, 2.12709653, 1.99264003, 1.66137374, 1.89430865,\n",
       "        2.54181706, 1.8599884 , 1.53951585, 2.16569755, 1.71878419,\n",
       "        1.9876657 , 2.17888803, 1.27929436, 1.86330959, 2.50580459,\n",
       "        2.04128101, 1.91569372, 1.6112169 , 1.450521  , 1.50372528,\n",
       "        1.81006994, 1.3514131 , 1.69836131, 1.71648713, 2.76856002,\n",
       "        1.72894839, 1.31591783, 1.50705642, 1.5986712 , 1.69053138,\n",
       "        2.37190699, 1.96538919, 2.21784286, 1.66622038, 1.89837264,\n",
       "        1.80774909, 1.67063764, 1.54398705, 1.33556325, 1.72555655,\n",
       "        1.98596221, 1.72369906, 1.62369048, 2.0653144 , 1.71898694,\n",
       "        1.82788433, 1.47057438, 1.5360753 , 1.62586416, 2.13145441,\n",
       "        1.48738034, 1.81217092, 1.95777752, 1.81221599, 2.06639113,\n",
       "        1.403738  , 1.0771292 , 1.31418752, 2.33114236, 1.58372475,\n",
       "        1.71978985, 1.19279053, 1.17191661, 1.66168178, 1.36550779,\n",
       "        1.39002112, 2.37067726, 1.37121093, 2.12734301, 1.3478186 ,\n",
       "        2.04419044, 1.04409435, 1.35479047, 1.90473454, 1.24984235,\n",
       "        0.99320017, 0.86899885, 1.3394326 , 1.38917709, 1.46142412,\n",
       "        1.8491799 , 1.24976571, 1.51158247, 1.29997168, 1.52656054,\n",
       "        1.55809266, 1.24238171, 1.30937312, 1.32813023, 1.24794596,\n",
       "        2.09490149, 1.47490946, 1.72923723, 1.53758966, 1.38332846,\n",
       "        1.82017902, 1.3242602 , 2.24828929, 1.6279647 , 1.250736  ,\n",
       "        1.34360632, 1.32048512, 1.31970918, 1.44383524, 1.5242043 ,\n",
       "        1.21277314, 1.10313528, 1.2039817 , 1.27013796, 1.25244931,\n",
       "        1.13143836, 1.29446505, 1.33989508, 1.08471275, 1.44499739,\n",
       "        1.54220111, 1.71729142, 0.95842074, 1.21470719, 1.40224639,\n",
       "        1.25637064, 1.17522207, 2.12719538, 1.60084198, 1.4442603 ,\n",
       "        1.60862636, 1.23904779, 1.32041497, 1.27920414, 1.13786097,\n",
       "        1.31496253, 1.37725523, 1.32246491, 1.34427266, 1.23451401,\n",
       "        1.07221517, 1.32717177, 1.50875461, 1.20814656, 1.25619419,\n",
       "        1.17431983, 1.1361889 , 1.24173696, 1.36674317, 0.82177488,\n",
       "        1.37696447, 0.84937258, 1.15372164, 1.30295284, 0.98818319,\n",
       "        1.54365998, 1.14593259, 1.02307336, 0.95848664, 1.27920112,\n",
       "        1.19078476, 1.39943735, 1.1212893 , 1.15797618, 1.89406096,\n",
       "        1.26249103, 0.86622278, 1.02350944, 1.22949037, 0.91837534,\n",
       "        1.17773202, 1.09987714, 1.04710543, 1.35913036, 1.0146136 ,\n",
       "        1.42866634, 1.35181759, 1.12485186, 0.98945157, 1.36748472,\n",
       "        1.41338385, 1.07884549, 0.83580564, 1.22050709, 1.26497506,\n",
       "        1.16337502, 0.99969879, 1.07208673, 1.10238822, 0.82676231,\n",
       "        1.20274383, 1.12654445, 1.0782021 , 1.07412355, 0.67190456,\n",
       "        0.91805705, 1.38901629, 1.00770166, 1.01936903, 1.04266106,\n",
       "        1.46739711, 1.3757135 , 1.03258683, 0.95097301, 1.23643215,\n",
       "        0.95182294, 0.83229021, 0.82030577, 0.85956836, 1.17399266,\n",
       "        0.83099218, 1.05391089, 1.35148492, 1.2623868 , 1.23378111,\n",
       "        1.1038163 , 1.3018067 , 0.95887667, 1.08685702, 0.95111637,\n",
       "        1.02388324, 1.11941337, 0.81168496, 0.8719298 , 1.03606942,\n",
       "        1.16018068, 0.70162431, 0.75071858, 0.84674805, 0.89638844,\n",
       "        0.91721288, 1.0195035 , 0.80282467, 0.8063176 , 0.89349433,\n",
       "        0.99485109, 1.02705156, 0.63707248, 1.04128258, 1.11775748,\n",
       "        0.78424154, 1.00006707, 1.0409647 , 1.13186267, 1.20577017,\n",
       "        0.75819149, 1.03322094, 0.77292359, 0.99711765, 0.95640298,\n",
       "        0.86460575, 1.23850969, 0.95483094, 0.88950684, 0.87474124,\n",
       "        0.71502093, 1.15651571, 0.87968982, 0.9993883 , 0.9458115 ,\n",
       "        0.82633423, 1.25675023, 1.20477502, 1.02757724, 0.96416396,\n",
       "        1.11529863, 0.92361565, 1.13367796, 1.14178109, 0.78689097,\n",
       "        0.68589942, 0.91131614, 1.06529741, 0.93600809, 0.99202527,\n",
       "        0.81564016, 1.28370646, 0.97061077, 0.57264393, 0.77522407,\n",
       "        0.59000219, 0.82542799, 1.02026168, 1.12395528, 0.81884382,\n",
       "        1.47131046, 0.7136554 , 0.73708154, 0.8190346 , 0.83613871,\n",
       "        1.16627457, 0.92120291, 0.83110231, 0.86115873, 0.85329845,\n",
       "        0.84119493, 0.81746644, 0.98939449, 0.87628881, 0.7985858 ,\n",
       "        0.69757982, 0.985609  , 0.72573182, 1.07147783, 0.87747552,\n",
       "        0.95398258, 0.85992319, 0.82956978, 0.81191795, 0.81034803,\n",
       "        0.66117278, 0.78110721, 0.65740391, 0.80312583, 0.78467581,\n",
       "        0.80573013, 0.71615869, 0.8907255 , 0.70962144, 0.7381605 ,\n",
       "        0.83099321, 0.71900699, 0.94896661, 0.89035285, 0.77433741,\n",
       "        0.87750411, 0.92128705, 0.94898676, 0.94944817, 0.80546275,\n",
       "        0.85201625, 0.53831925, 1.3194392 , 0.89539027, 0.7397433 ,\n",
       "        0.78983798, 0.85211699, 0.86077179, 0.64027193, 0.57244977,\n",
       "        0.94627396, 0.87924942, 0.88378448, 1.11468036, 0.51434769,\n",
       "        0.65045279, 0.41427461, 0.8019871 , 0.7177647 , 0.603218  ,\n",
       "        0.82844008, 0.70785621, 0.74032341, 0.73113185, 1.00938983,\n",
       "        0.77720077, 0.89223401, 0.82420695, 0.67001546, 0.78772504,\n",
       "        0.61456611, 0.68600245, 0.74273668, 0.53925793, 0.69492476,\n",
       "        0.71949292, 0.77045988, 0.83582372, 0.84574176, 0.69517751,\n",
       "        0.91949613, 0.98489531, 0.76438055, 0.56191573, 0.63780395,\n",
       "        0.68119983, 0.74137019, 1.02528881, 0.66844386, 1.05221196,\n",
       "        0.91011063, 0.77596509, 0.7288532 , 0.6340886 , 0.67120429,\n",
       "        0.70951526, 0.94685803, 0.63934188, 0.38002032, 0.6784091 ,\n",
       "        0.88850775, 0.80218885, 0.81403185, 0.621152  , 0.77320368,\n",
       "        1.0997658 , 0.84148098, 1.03093139, 0.72269122, 1.61134676,\n",
       "        0.75700642, 1.60532678, 0.70695895, 0.68084668, 0.71999483,\n",
       "        0.74092237, 0.6775036 , 0.79326323, 0.72972969, 0.69061143,\n",
       "        0.70982916, 0.7785797 , 0.78561687, 0.92588625, 0.64028984,\n",
       "        0.75709612, 0.58719981, 0.61703214, 0.66091657, 0.6745829 ,\n",
       "        0.66031297, 0.63010647, 0.99398387, 1.01846548, 1.40981443,\n",
       "        0.87620276, 0.75353104, 0.73029362, 1.29984829, 0.90664455,\n",
       "        0.54476629, 0.70811276, 0.58738678, 0.79345609, 0.56584444,\n",
       "        0.64273599, 0.76828185, 0.65496938, 0.84807003, 0.74250009,\n",
       "        0.5501661 , 0.32318441, 0.76428904, 0.73823662, 0.57474662,\n",
       "        0.70568698, 0.68519881, 0.79700368, 0.76176108, 0.88014988,\n",
       "        0.82534026, 0.5598026 , 0.67011086, 0.80491642, 0.53167448,\n",
       "        0.91819207, 0.72195705, 0.55998499, 0.73957111, 0.71700865,\n",
       "        0.55120653, 0.68626367, 0.53413313, 0.66084689, 0.62010859,\n",
       "        0.82282539, 0.61911378, 0.61570549, 1.1872712 , 0.64938764,\n",
       "        0.57755593, 0.64252577, 0.62243333, 0.60405179, 0.57101336,\n",
       "        0.68571107, 0.59320084, 0.83710169, 0.60035572, 1.19807086,\n",
       "        0.84509493, 0.61398714, 0.69698293, 0.66220212, 0.52399693,\n",
       "        0.59062847, 0.62807764, 0.6803332 , 0.678363  , 0.79305309,\n",
       "        0.51491784, 0.59690036, 0.79863222, 0.58794334, 0.49164982,\n",
       "        0.62316209, 0.81228368, 0.53850556, 0.89726845, 0.77899772,\n",
       "        0.49382626, 0.61149362, 0.77649061, 0.64170533, 0.72209706,\n",
       "        0.41818769, 0.57586624, 0.68198406, 0.60845819, 0.64653144,\n",
       "        0.60407036, 0.58768277, 0.67583427, 0.59191712, 0.82668088,\n",
       "        0.93404821, 0.74796912, 0.6492963 , 0.61263175, 0.6069579 ,\n",
       "        0.66039427, 0.43536552, 0.61884671, 0.77971998, 0.62202611,\n",
       "        0.61826126, 0.54562073, 1.5757579 , 0.48963718, 0.53696577,\n",
       "        0.5416108 , 0.49984038, 0.7563745 , 0.67486264, 0.46458886,\n",
       "        0.38422524, 0.78850517, 0.66904937, 0.70311475, 0.88107133,\n",
       "        0.55563364, 0.73359394, 0.59214678, 0.50215029, 0.80850574,\n",
       "        0.74354266, 0.63349146, 0.63636999, 0.49414764, 0.47952553,\n",
       "        0.59515306, 0.65050793, 0.52453052, 0.48938139, 0.60331269,\n",
       "        0.64286738, 0.46682456, 0.63813871, 0.49324101, 0.57720295,\n",
       "        0.67353015, 0.72161285, 0.57234194, 0.55026427, 0.62200119,\n",
       "        0.78833494, 0.52189817, 0.63580593, 0.80392908, 0.67764365,\n",
       "        0.55737989, 0.63304825, 0.56332361, 0.59011433, 0.55702754,\n",
       "        0.69508783, 0.60754827, 0.67123958, 0.44077074, 0.44817517,\n",
       "        0.38129223, 0.64889424, 0.65658509, 0.67203639, 0.72040524,\n",
       "        0.68498446, 0.42753609, 0.66040947, 0.54654139, 0.66870526,\n",
       "        0.69810716, 0.66463867, 0.74410426, 0.60463152, 0.50667227,\n",
       "        0.51909753, 0.53856429, 0.61218788, 0.7000145 , 0.46391863,\n",
       "        0.51958505, 0.55206048, 0.53921746, 0.85683569, 0.44244244,\n",
       "        0.56471635, 0.65275963, 0.40142275, 0.30032531, 0.53648592,\n",
       "        0.67149549, 0.50088147, 0.43961645, 0.49472046, 0.68038569,\n",
       "        0.55995343, 0.5458337 , 0.4639173 , 0.48171765, 0.51400168,\n",
       "        0.35118608, 0.73013085, 0.46066241, 0.88923377, 0.52413574,\n",
       "        0.30106794, 0.5562225 , 0.38972151, 0.63702665, 0.43542381,\n",
       "        0.69300496, 0.54473174, 0.36042847, 0.76474027, 0.71820122,\n",
       "        0.54226518, 0.7864602 , 0.39048128, 0.45852089, 0.45186341,\n",
       "        0.66630124, 0.27007984, 0.46413239, 0.70282471, 0.55167664,\n",
       "        0.63853637, 0.64506813, 0.50358466, 0.49674578, 0.72862307,\n",
       "        0.45202851, 0.59780009, 0.45158398, 0.53072404, 0.65908517,\n",
       "        0.45947295, 0.48526223, 0.48522181, 0.46888198, 0.45528834,\n",
       "        0.50366413, 0.36978859, 0.66094504, 0.67010779, 0.73737502,\n",
       "        0.48933676, 0.67410663, 0.56572459, 0.619541  , 0.41611096,\n",
       "        0.37791944, 0.38053896, 0.39384376, 0.50616365, 0.60980031,\n",
       "        0.3097392 , 0.48618782, 0.49036367, 0.54045525, 0.57312649,\n",
       "        0.33132872, 0.55876954, 0.3988973 , 0.60313125, 0.51386861,\n",
       "        0.6696045 , 0.53869795, 0.28374616, 0.45874985, 0.3758007 ,\n",
       "        0.48863401, 0.56090264, 0.52775643, 0.38295696, 0.46569658,\n",
       "        0.4253675 , 0.55047559, 0.54907438, 0.4948525 , 0.62028844,\n",
       "        0.26203494, 0.47003369, 0.42725512, 0.36449567, 0.25875392,\n",
       "        0.48122198, 0.27937357, 0.3321093 , 0.45278504, 0.34142419,\n",
       "        0.46635906, 0.41733209, 0.45596321, 0.44742535, 0.42018632,\n",
       "        0.51149643, 0.52267994, 0.71618494, 0.38958927, 0.24262104,\n",
       "        0.40144504, 0.3603299 , 0.69579138, 0.64801397, 0.47755689,\n",
       "        0.36299881, 0.5389599 , 0.44050683, 0.54730592, 0.43007702,\n",
       "        0.39821101, 0.37614256, 0.64692045, 0.39919825, 0.45261309,\n",
       "        0.77634307, 0.65414228, 0.43978787, 0.23868395, 0.51872849,\n",
       "        0.52427722, 0.37738062, 0.41554949, 0.90752119, 0.47656852,\n",
       "        0.41228859, 0.52098066, 0.40183734, 0.51517986, 0.39555601,\n",
       "        0.45943078, 0.40156632, 0.4960527 , 0.41405715, 1.0619917 ,\n",
       "        0.36761613, 0.30046138, 0.37088201, 0.58078576, 0.44974113,\n",
       "        0.27327595, 0.32437602, 0.6787793 , 0.7653363 , 0.60644843,\n",
       "        0.35033735, 0.38183295, 0.59828506, 0.4306345 , 0.40871352,\n",
       "        0.40712475, 0.35778937, 0.28155933, 0.37463898, 0.36313649,\n",
       "        0.28463905, 0.40552642, 0.31793107, 0.32285705, 0.55239424,\n",
       "        0.33854001, 0.77870362, 0.3381714 , 0.54719295, 0.45875457,\n",
       "        0.54568262, 0.49888394, 0.48173744, 0.49413555, 0.4499807 ,\n",
       "        0.34452096, 0.48156681, 0.39218112, 0.43997282, 0.31509951,\n",
       "        0.41275825, 0.46875338, 0.40410886, 0.39560734, 0.26411048,\n",
       "        0.78046238, 0.50186149, 0.43650136, 0.42945226, 0.39489946,\n",
       "        0.3720564 , 0.33196157, 0.28894101, 0.33928714, 0.45986882,\n",
       "        0.45785771, 0.44423741, 0.55869167, 0.58995984, 0.39066017,\n",
       "        0.40725753, 0.56480882, 0.39127697, 0.35056609, 0.48477157,\n",
       "        0.49807007, 0.69404961, 0.51710018, 0.50121426, 0.43968604,\n",
       "        0.35555018, 0.36949741, 0.74085353, 0.37307166, 0.45213699,\n",
       "        0.36170959, 0.48330891, 0.37107176, 0.40807754, 0.41897569,\n",
       "        0.32942932, 0.57927803, 0.31607615, 0.85698929, 0.43624146,\n",
       "        0.38866338, 0.40571504, 0.56020813, 0.28028326, 0.39478285,\n",
       "        0.38855489, 0.43927626, 0.38582964, 0.52861699, 0.46894307,\n",
       "        0.48123046, 0.44876248, 0.3348246 , 0.44789354, 0.38528928,\n",
       "        0.39574347, 0.66799929, 0.35170355, 0.47558937, 0.54884387,\n",
       "        0.33663061, 0.36560341, 0.38944052, 0.46732862, 0.30986835,\n",
       "        0.38638793, 0.38865504, 0.36397024, 0.34512243, 0.35707551,\n",
       "        0.66945033, 0.3369777 , 0.28877763, 0.36413793, 0.48050853,\n",
       "        0.52584713, 0.40460815, 0.3379614 , 0.29530991, 0.33061394,\n",
       "        0.31154006, 0.3131545 , 0.34432153, 0.32825376, 0.33296534,\n",
       "        0.47993667, 0.36371692, 0.24063303, 0.46393071, 0.43364281,\n",
       "        0.50000738, 0.34379001, 0.40263802, 0.31764834, 0.49496689,\n",
       "        0.31042574, 0.45076605, 0.42118319, 0.38952719, 0.54804753,\n",
       "        0.36685544, 0.38649854, 0.34268007, 0.5834269 , 0.52499896,\n",
       "        0.3958799 , 0.34138305, 0.39205455, 0.33219574, 0.36609762,\n",
       "        0.626617  , 0.33767059, 0.3843856 , 0.40806307, 0.57895942,\n",
       "        0.40002956, 0.43842767, 0.43031259, 0.37788646, 0.30577204,\n",
       "        0.32334   , 0.32834965, 0.45175951, 0.30509268, 0.32050327,\n",
       "        0.36157732, 0.34492913, 0.42249761, 0.4871678 , 0.29486981,\n",
       "        0.34023426, 0.70972851, 0.44254061, 0.58321671, 0.35726851,\n",
       "        0.24332883, 0.17404627, 0.27838589, 0.42562777, 0.34535305,\n",
       "        0.22568377, 0.4340377 , 0.2524214 , 0.28120818, 0.32765876,\n",
       "        0.33224843, 0.34154571, 0.52660887, 0.23015699, 0.29634204,\n",
       "        0.23370543, 0.24063373, 0.28962179, 0.39964142, 0.23369773,\n",
       "        0.38360101, 0.2278984 , 0.32134537, 0.4106598 , 0.33557341,\n",
       "        0.31229836, 0.29933992, 0.41609028, 0.31675994, 0.43088752,\n",
       "        0.48191083, 0.35954137, 0.35018627, 0.26547733, 0.2633137 ,\n",
       "        0.31134628, 0.36505923, 0.37674458, 0.28828223, 0.41078053])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Getting the list of centroids.\n",
    "centroids50_list = userClusterModel.clusterCenters()\n",
    "centroids50_list[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|        user_hash_id|            features|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|03237605301d9dd8e...|(1000,[0,1,2,3,4,...|        20|\n",
      "|2086aff81a8d6f9d9...|(1000,[1,2,3,4,6,...|         1|\n",
      "|42407318182edf74b...|(1000,[0,1,2,3,4,...|        20|\n",
      "|4dccc6267cfee33b5...|(1000,[0,1,2,5,6,...|         0|\n",
      "|dcb0db5e7f1e041d8...|(1000,[0,1,2,3,4,...|         1|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Creating a dataframe with the predictions of the KMeans algo.\n",
    "userTransformed = userClusterModel.transform(userFeaturesDF)\n",
    "userTransformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Function to calculate the euclidean distance between 2 points.\n",
    "def calcDistance(point_a, point_b):\n",
    "    return(np.sqrt(np.sum(np.square(np.subtract(point_a, point_b)))))\n",
    "\n",
    "#Function to calculate the inter-cluster distances between a set of points and its centroid.\n",
    "def interClusterDist(prediction, user_points, centroids_list):\n",
    "    distances_list = []\n",
    "    index = int(prediction)\n",
    "    centroid = centroids_list[index]\n",
    "    for point in user_points:\n",
    "        distances_list.append(calcDistance(point, centroid))\n",
    "    average_distance = np.average(distances_list)\n",
    "    return(float(average_distance))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a user-defined function\n",
    "udf_calcInterClustDist50 = Func.udf(lambda x, y: interClusterDist(x, y, centroids50_list), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|prediction|         user_points|\n",
      "+----------+--------------------+\n",
      "|        31|[(1000,[0,1,2,3,4...|\n",
      "|        34|[(1000,[0,1,2,3,4...|\n",
      "|        28|[(1000,[0,1,2,3,4...|\n",
      "|        27|[(1000,[0,1,2,3,4...|\n",
      "|        26|[(1000,[0,1,2,3,4...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Grouping the user features with respect to the prediction.\n",
    "groupedUserProfDF = userTransformed.groupBy(\"prediction\").agg(Func.collect_list(\"features\").alias(\"user_points\"))\n",
    "groupedUserProfDF.show(5)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|prediction|         user_points|inter_cluster_dist|\n",
      "+----------+--------------------+------------------+\n",
      "|        31|[(1000,[0,1,2,3,4...|         1633.9684|\n",
      "|        34|[(1000,[0,1,2,3,4...|          669.6046|\n",
      "|        28|[(1000,[0,1,2,3,4...|         1924.6855|\n",
      "|        27|[(1000,[0,1,2,3,4...|         1550.7633|\n",
      "|        26|[(1000,[0,1,2,3,4...|         928.39703|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Creating a dataframe containing the inter-cluster distance values.\n",
    "interClustDist50DF = groupedUserProfDF.withColumn(\"inter_cluster_dist\",\\\n",
    "                                                udf_calcInterClustDist50(groupedUserProfDF.prediction,\\\n",
    "                                                                         groupedUserProfDF.user_points))\n",
    "interClustDist50DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Creating a list of inter-cluster distances.\n",
    "interClust50List = interClustDist50DF.select(\"prediction\", \"inter_cluster_dist\")\\\n",
    "                                    .orderBy(\"prediction\", ascending=True)\\\n",
    "                                    .rdd.map(lambda x: x[1]).collect()\n",
    "#interClust50List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Function to calculate the Davies-Boulding index.\n",
    "def calcDBIndex(centroids_list, inter_clust_list):\n",
    "    distance = []\n",
    "    counter = 0\n",
    "    #calculate Davies-Bouldin value for pairs of clusters.\n",
    "    while counter < len(centroids_list):\n",
    "        other_counter = counter + 1\n",
    "        other_centroids = list(itertools.islice(centroids_list, counter+1, None))\n",
    "        for other_centroid in other_centroids:\n",
    "            inter_clust_pair = inter_clust_list[counter]+inter_clust_list[other_counter]\n",
    "            #Calculate intra cluster distance (distance between 2 clusters)\n",
    "            intra_clust_pair = calcDistance(centroids_list[counter], other_centroid)\n",
    "            distance.append(inter_clust_pair/intra_clust_pair)\n",
    "            other_counter += 1    \n",
    "        counter += 1\n",
    "        \n",
    "    #calculate the Davies-Bouldin index.\n",
    "    DB_index = sum(distance)/len(centroids_list)\n",
    "    \n",
    "    return(DB_index)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.787159323220504"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.3 c)\n",
    "#Davies-Bouldin Index for a cluster of size 50\n",
    "calcDBIndex(centroids50_list, interClust50List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Calculating the Davies-Bouldin for the 10 generated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Initializing a Kmeans object.\n",
    "kmeans10 = KMeans(featuresCol=\"features\", k=10, maxIter=10)\n",
    "#Creating a model.\n",
    "userCluster10Model = kmeans10.fit(userFeaturesDF.select(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Getting the list of centroids.\n",
    "centroids10_list = userCluster10Model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|        user_hash_id|            features|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|03237605301d9dd8e...|(1000,[0,1,2,3,4,...|         0|\n",
      "|2086aff81a8d6f9d9...|(1000,[1,2,3,4,6,...|         0|\n",
      "|42407318182edf74b...|(1000,[0,1,2,3,4,...|         4|\n",
      "|4dccc6267cfee33b5...|(1000,[0,1,2,5,6,...|         0|\n",
      "|dcb0db5e7f1e041d8...|(1000,[0,1,2,3,4,...|         0|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Creating a dataframe with the predictions of the KMeans algo.\n",
    "user10Transformed = userCluster10Model.transform(userFeaturesDF)\n",
    "user10Transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|prediction|         user_points|\n",
      "+----------+--------------------+\n",
      "|         1|[(1000,[0,1,2,3,4...|\n",
      "|         6|[(1000,[0,1,2,3,4...|\n",
      "|         3|[(1000,[0,1,2,3,4...|\n",
      "|         5|[(1000,[0,1,2,3,4...|\n",
      "|         9|[(1000,[0,1,2,3,4...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Grouping the user features with respect to the prediction.\n",
    "groupedUserProf10DF = user10Transformed.groupBy(\"prediction\").agg(Func.collect_list(\"features\").alias(\"user_points\"))\n",
    "groupedUserProf10DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Creating a user defined function\n",
    "udf_calcInterClustDist10 = Func.udf(lambda x, y: interClusterDist(x, y, centroids10_list), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|prediction|         user_points|inter_cluster_dist|\n",
      "+----------+--------------------+------------------+\n",
      "|         1|[(1000,[0,1,2,3,4...|         2373.3965|\n",
      "|         6|[(1000,[0,1,2,3,4...|          3202.719|\n",
      "|         3|[(1000,[0,1,2,3,4...|         3173.4805|\n",
      "|         5|[(1000,[0,1,2,3,4...|         2194.3276|\n",
      "|         9|[(1000,[0,1,2,3,4...|         1834.0939|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Creating a dataframe containing the inter-cluster distance values.\n",
    "interClustDist10DF = groupedUserProf10DF.withColumn(\"inter_cluster_dist\",\\\n",
    "                                                udf_calcInterClustDist10(groupedUserProf10DF.prediction,\\\n",
    "                                                                        groupedUserProf10DF.user_points))\n",
    "interClustDist10DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-7f6b329140b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Creating a list of inter-cluster distances.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minterClust10List\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterClustDist10DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inter_cluster_dist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                     \u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                                     \u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#interClust10List\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \"\"\"\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_rdd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjavaToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_rdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Creating a list of inter-cluster distances.\n",
    "interClust10List = interClustDist10DF.select(\"prediction\", \"inter_cluster_dist\")\\\n",
    "                                    .orderBy(\"prediction\", ascending=True)\\\n",
    "                                    .rdd.map(lambda x: x[1]).collect()\n",
    "#interClust10List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 d)\n",
    "#Davies-Bouldin Index for a cluster of size 10\n",
    "calcDBIndex(centroids10_list, interClust10List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4 (Latent Direchlet Allocation (LDA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Running LDA Algorithm with k=40 and showing the top 5 terms for each extracted topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Initializing an LDA object.\n",
    "lda = LDA(featuresCol=\"tf_vector\", k=40, maxIter=10)\n",
    "#Creating the model.\n",
    "ldaModel = lda.fit(featurizedDataDF.select(\"tf_vector\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Showing the top 5 terms for each extracted latent topic.\n",
    "topTopics_5 = ldaModel.describeTopics(5)\n",
    "topTopics_5.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Showing the top 5 terms for each extracted latent topic.\n",
    "topTopics_5.select(\"topic\", Func.explode(\"termIndices\").alias(\"word_id\"))\\\n",
    ".join(termsDF, \"word_id\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Creating a dataframe with the topic distibution generated from LDA.\n",
    "ldaTransformed = ldaModel.transform(featurizedDataDF)\n",
    "ldaTransformed.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Joining the topic distribution to the users.\n",
    "joinUserLDA = explodUsersDF.join(ldaTransformed, \"paper_id\")\n",
    "joinUserLDA.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 a)\n",
    "#Grouping the topic distribution with respect to the users.\n",
    "grpUserTopicDistDF = joinUserLDA.groupBy(\"user_hash_id\")\\\n",
    "                                .agg(Func.collect_list(\"topicDistribution\").alias(\"topic_distribution_ls\"))\n",
    "grpUserTopicDistDF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Calculating the LDA based user profiles as the summation of the paper topics vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 b)\n",
    "#Function to sum a list of dense vectors.\n",
    "def addDenseVec(dense_vec_list):\n",
    "    result = DenseVector(np.zeros(40))\n",
    "    for dense_vector in dense_vec_list:\n",
    "        result = np.add(result, dense_vector).tolist() \n",
    "    return DenseVector(result)\n",
    "\n",
    "#Creating a user defined function.\n",
    "udf_addDenseVec = Func.udf(addDenseVec, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 b)\n",
    "#Summation of the paper topics vectors.\n",
    "userLDAProfile = grpUserTopicDistDF.withColumn(\"features\",\\\n",
    "                                               Func.lit(udf_addDenseVec(grpUserTopicDistDF.topic_distribution_ls)))\n",
    "\n",
    "userLDAProfile.select(\"user_hash_id\", \"features\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Applying the K-means algorithm to cluster the users using their LDA profiles in 50 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 c)\n",
    "#Initializing the K-means object.\n",
    "UsrLDAKmeans = KMeans(featuresCol=\"features\", k=50, maxIter=10)\n",
    "#Creating the model.\n",
    "userLDAClustModel = UsrLDAKmeans.fit(userLDAProfile.select(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Getting a list of centroids.\n",
    "topicCentroids_list = userLDAClustModel.clusterCenters()\n",
    "topicCentroids_list[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Creating a dataframe with the predictions of the KMeans algo.\n",
    "userTopicTransformed = userLDAClustModel.transform(userLDAProfile)\n",
    "userTopicTransformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Grouping the user features with respect to the prediction.\n",
    "grpUserTopicProfDF = userTopicTransformed.groupBy(\"prediction\")\\\n",
    "                                            .agg(Func.collect_list(\"features\").alias(\"paper_topic_pts\"))\n",
    "grpUserTopicProfDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Creating a user defined function.\n",
    "udf_calcTopicInterClust = Func.udf(lambda x, y: interClusterDist(x, y, topicCentroids_list), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Creating a dataframe containing the inter-cluster distance values.\n",
    "topicInterClustDist = grpUserTopicProfDF.withColumn(\"inter_cluster_dist\",\\\n",
    "                                                udf_calcTopicInterClust(grpUserTopicProfDF.prediction,\\\n",
    "                                                                        grpUserTopicProfDF.paper_topic_pts))\n",
    "topicInterClustDist.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Creating a list of inter-cluster distances.\n",
    "topicInterClustList = topicInterClustDist.select(\"prediction\", \"inter_cluster_dist\")\\\n",
    "                                    .orderBy(\"prediction\", ascending=True)\\\n",
    "                                    .rdd.map(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.4 d)\n",
    "#Davies-Bouldin Index for a cluster of size 50\n",
    "calcDBIndex(topicCentroids_list, topicInterClustList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
