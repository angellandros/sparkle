{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as Func\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, IDF\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT, Vectors\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import numpy as np\n",
    "#import string\n",
    "\n",
    "#Create a spark session.\n",
    "sparkSession = SparkSession.builder.appName(\"Experiment3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|stop_word|\n",
      "+---------+\n",
      "|        a|\n",
      "|     able|\n",
      "|    about|\n",
      "|    above|\n",
      "|according|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading stopwords_en.txt data into a dataframe.\n",
    "stopWordsDF = sparkSession.read\\\n",
    "                .load(\"/home/jovyan/work/stopwords_en.txt\", format=\"text\", sep=\" \", inferSchema=\"true\", header=\"false\")\\\n",
    "                .toDF('stop_word')\n",
    "stopWordsDF.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        user_hash_id|        user_library|\n",
      "+--------------------+--------------------+\n",
      "|d0c9aaa788153daea...|2080631,6343346,5...|\n",
      "|ca4f1ba4094011d9a...|              278019|\n",
      "|d1d41a15201915503...|6610569,6493797,6...|\n",
      "|f2f77383828ea6d39...|943458,238121,763429|\n",
      "|9c883d02115400f7b...|3509971,3509965,2...|\n",
      "|1eac022a97d683eac...|3973229,322433,57...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading users_libraries.txt data into a dataframe.\n",
    "#Defining the column names.\n",
    "user_columns = ['raw_data']\n",
    "rawUsersDF = sparkSession.read\\\n",
    "            .load(\"/home/jovyan/work/mod_users_libraries2.txt\", format=\"text\", sep=\";\", inferSchema=\"true\", quote='\"', header=\"false\")\\\n",
    "            .toDF(*user_columns)\n",
    "\n",
    "usersDF = rawUsersDF.select(Func.split(rawUsersDF.raw_data, \";\").getItem(0).alias(\"user_hash_id\"),\\\n",
    "                           Func.split(rawUsersDF.raw_data, \";\").getItem(1).alias(\"user_library\"))\n",
    "usersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------------------+----------+------+---------+-----+------+------+----+-----+-------------------+-------+--------------------+--------------------+\n",
      "|paper_id|   type|             journal|book_title|series|publisher|pages|volume|number|year|month|          postedate|address|               title|            abstract|\n",
      "+--------+-------+--------------------+----------+------+---------+-----+------+------+----+-----+-------------------+-------+--------------------+--------------------+\n",
      "|   80546|article|biology and philo...|      null|  null|     null|   17|    19|     2|2004|  mar|2005-01-26 21:35:21|   null|the arbitrariness...|the genetic code ...|\n",
      "| 5842862|article|      molecular cell|      null|  null| elsevier|    2|    35|     6|2009|  sep|2009-09-30 17:11:23|   null|how to choose a g...|choosing good pro...|\n",
      "+--------+-------+--------------------+----------+------+---------+-----+------+------+----+-----+-------------------+-------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading papers.csv data into a dataframe.\n",
    "#Defining the column names.\n",
    "paper_columns = ['paper_id', 'type', 'journal', 'book_title', \\\n",
    "           'series', 'publisher', 'pages', 'volume', \\\n",
    "           'number', 'year', 'month', 'postedate',\\\n",
    "           'address', 'title', 'abstract']\n",
    "papersDF = sparkSession.read\\\n",
    "            .load(\"/home/jovyan/work/mod_papers.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", quote='\"', header=\"false\")\\\n",
    "            .toDF(*paper_columns)\n",
    "papersDF.show(2, truncate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1 (Vector representation for the papers)\n",
    "To generate the bag-of-words representation for each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|paper_id|                text|\n",
      "+--------+--------------------+\n",
      "|   80546|the arbitrariness...|\n",
      "| 5842862|how to choose a g...|\n",
      "| 1242600|how to write cons...|\n",
      "| 3467077|defrosting the di...|\n",
      "|  309395|why most publishe...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Concatenate the title and abstract fields together.\n",
    "textDF = papersDF.select(papersDF.paper_id, Func.concat_ws(\" \", papersDF.title, papersDF.abstract).alias(\"text\"))\n",
    "textDF.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=80546, text=\"the arbitrariness of the genetic code the genetic code has been regarded as arbitrary in the sense that the codon-amino acid assignments could be different than they actually are. this general idea has been spelled out differently by previous, often rather implicit accounts of arbitrariness. they have drawn on the frozen accident theory, on evolutionary contingency, on alternative causal pathways, and on the absence of direct stereochemical interactions between codons and amino acids. it has also been suggested that the arbitrariness of the genetic code justifies attributing semantic information to macromolecules, notably to {dna}. i argue that these accounts of arbitrariness are unsatisfactory. i propose that the code is arbitrary in the sense of jacques monod's concept of chemical arbitrariness: the genetic code is arbitrary in that any codon requires certain chemical and structural properties to specify a particular amino acid, but these properties are not required in virtue of a principle of chemistry. this notion of arbitrariness is compatible with several recent hypotheses about code evolution. i maintain that the code's chemical arbitrariness is neither sufficient nor necessary for attributing semantic information to nucleic acids.\", words=['arbitrariness', 'genetic', 'code', 'genetic', 'code', 'been', 'regarded', 'arbitrary', 'sense', 'that', 'codon-amino', 'acid', 'assignments', 'could', 'different', 'than', 'they', 'actually', 'this', 'general', 'idea', 'been', 'spelled', 'differently', 'previous', 'often', 'rather', 'implicit', 'accounts', 'arbitrariness', 'they', 'have', 'drawn', 'frozen', 'accident', 'theory', 'evolutionary', 'contingency', 'alternative', 'causal', 'pathways', 'absence', 'direct', 'stereochemical', 'interactions', 'between', 'codons', 'amino', 'acids', 'also', 'been', 'suggested', 'that', 'arbitrariness', 'genetic', 'code', 'justifies', 'attributing', 'semantic', 'information', 'macromolecules', 'notably', 'argue', 'that', 'these', 'accounts', 'arbitrariness', 'unsatisfactory', 'propose', 'that', 'code', 'arbitrary', 'sense', 'jacques', 'monod', 'concept', 'chemical', 'arbitrariness', 'genetic', 'code', 'arbitrary', 'that', 'codon', 'requires', 'certain', 'chemical', 'structural', 'properties', 'specify', 'particular', 'amino', 'acid', 'these', 'properties', 'required', 'virtue', 'principle', 'chemistry', 'this', 'notion', 'arbitrariness', 'compatible', 'with', 'several', 'recent', 'hypotheses', 'about', 'code', 'evolution', 'maintain', 'that', 'code', 'chemical', 'arbitrariness', 'neither', 'sufficient', 'necessary', 'attributing', 'semantic', 'information', 'nucleic', 'acids'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Perform tokenization and remove words less than 3 characters.\n",
    "#Keep the words containing \"-\" and \"_\" characters.\n",
    "reTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", minTokenLength=4, pattern=\"[^-_\\\\w]\")\n",
    "wordsDF = reTokenizer.transform(textDF)\n",
    "wordsDF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.1\n",
    "#Function to remove the characters \"-\" and \"_\" from words.\n",
    "def concatConnectedWords(wordList):\n",
    "    wordSet = set(wordList)\n",
    "    identity = str.maketrans(\"\", \"\", \"-_\")\n",
    "    wordSet = [word.translate(identity) for word in wordSet]\n",
    "    return wordSet\n",
    "\n",
    "udf_concatConnectedWords = Func.udf(concatConnectedWords, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(paper_id=80546, processed_words=['these', 'actually', 'certain', 'stereochemical', 'notably', 'that', 'concept', 'particular', 'code', 'chemical', 'could', 'they', 'direct', 'propose', 'with', 'about', 'chemistry', 'general', 'regarded', 'notion', 'this', 'nucleic', 'information', 'drawn', 'specify', 'evolution', 'neither', 'absence', 'genetic', 'implicit', 'unsatisfactory', 'previous', 'frozen', 'between', 'alternative', 'amino', 'attributing', 'required', 'codonamino', 'been', 'have', 'structural', 'several', 'properties', 'maintain', 'necessary', 'requires', 'sense', 'hypotheses', 'pathways', 'acids', 'causal', 'codons', 'arbitrary', 'often', 'theory', 'also', 'macromolecules', 'assignments', 'rather', 'accounts', 'contingency', 'acid', 'monod', 'virtue', 'sufficient', 'jacques', 'principle', 'argue', 'idea', 'arbitrariness', 'suggested', 'compatible', 'than', 'recent', 'different', 'semantic', 'codon', 'interactions', 'justifies', 'accident', 'spelled', 'evolutionary', 'differently']),\n",
       " Row(paper_id=5842862, processed_words=['profession', 'that', 'lack', 'what', 'explicit', 'results', 'merit', 'discussion', 'within', 'tenure', 'observation', 'this', 'choose', 'problems', 'smart', 'teachers', 'scientific', 'usually', 'explicitly', 'choosing', 'scientist', 'being', 'figure', 'give', 'good', 'resulting', 'subject', 'vacuum', 'publication', 'approaches', 'through', 'scientists', 'problem', 'expected', 'their', 'such', 'journals', 'leaves', 'discussed', 'lead', 'essential', 'valued', 'enough'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#removing the characters \"-\" and \"_\" from words.\n",
    "removedConnectorsDF = wordsDF.select(wordsDF.paper_id,\\\n",
    "                                     Func.lit(udf_concatConnectedWords(wordsDF.words)).alias(\"processed_words\"))\n",
    "removedConnectorsDF.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'b',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'c',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " 'course',\n",
       " 'currently',\n",
       " 'd',\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'f',\n",
       " 'far',\n",
       " 'few',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'g',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'h',\n",
       " 'had',\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " 'have',\n",
       " 'having',\n",
       " 'he',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'i',\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'j',\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'known',\n",
       " 'l',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'm',\n",
       " 'mainly',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'o',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'p',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provides',\n",
       " 'q',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'r',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 't',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " 'thats',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'theres',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'think',\n",
       " 'third',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'u',\n",
       " 'un',\n",
       " 'under',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlikely',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'uucp',\n",
       " 'v',\n",
       " 'value',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vs',\n",
       " 'w',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " 'way',\n",
       " 'we',\n",
       " 'welcome',\n",
       " 'well',\n",
       " 'went',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wonder',\n",
       " 'would',\n",
       " 'would',\n",
       " 'x',\n",
       " 'y',\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'z',\n",
       " 'zero']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Creating a list of stop words.\n",
    "stopWordsList = stopWordsDF.agg(Func.collect_list(stopWordsDF.stop_word)).rdd.flatMap(lambda row: row[0])\n",
    "stopWordsList.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.1\n",
    "#Removing the stop words.\n",
    "remover = StopWordsRemover(inputCol=\"processed_words\", outputCol=\"words\", stopWords=stopWordsList.collect())\n",
    "withoutStopWordsDF = remover.transform(removedConnectorsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(words=['stereochemical', 'notably', 'concept', 'code', 'chemical', 'direct', 'propose', 'chemistry', 'general', 'regarded', 'notion', 'nucleic', 'information', 'drawn', 'evolution', 'absence', 'genetic', 'implicit', 'unsatisfactory', 'previous', 'frozen', 'alternative', 'amino', 'attributing', 'required', 'codonamino', 'structural', 'properties', 'maintain', 'requires', 'sense', 'hypotheses', 'pathways', 'acids', 'causal', 'codons', 'arbitrary', 'theory', 'macromolecules', 'assignments', 'accounts', 'contingency', 'acid', 'monod', 'virtue', 'sufficient', 'jacques', 'principle', 'argue', 'idea', 'arbitrariness', 'suggested', 'compatible', 'recent', 'semantic', 'codon', 'interactions', 'justifies', 'accident', 'spelled', 'evolutionary', 'differently']),\n",
       " Row(words=['profession', 'lack', 'explicit', 'results', 'merit', 'discussion', 'tenure', 'observation', 'choose', 'problems', 'smart', 'teachers', 'scientific', 'explicitly', 'choosing', 'scientist', 'figure', 'give', 'good', 'resulting', 'subject', 'vacuum', 'publication', 'approaches', 'scientists', 'problem', 'expected', 'journals', 'leaves', 'discussed', 'lead', 'essential', 'valued']),\n",
       " Row(words=['typically', 'publications', 'discuss', 'lost', 'students', 'write', 'adventurous', 'top10', 'make', 'structure', 'scientific', 'contributions', 'stilted', 'accessible', 'allure', 'exciting', 'language', 'literature', 'enthusiasm', 'list', 'laymen', 'present', 'predictable', 'scientists', 'recommendations', 'research', 'talk', 'insist', 'boring', 'consistently', 'prospective']),\n",
       " Row(words=['cold', 'publications', 'tools', 'library', 'thought', 'citation', 'electronically', 'information', 'defrosting', 'bulk', 'organizing', 'material', 'libraries', 'storage', 'bibliographic', 'generation', 'scientists', 'manage', 'digital']),\n",
       " Row(words=['concern', 'importantly', 'greater', 'power', 'study', 'discuss', 'false', 'effect', 'sizes', 'financial', 'implications', 'flexibility', 'framework', 'field', 'involved', 'problems', 'depend', 'current', 'bias', 'scientific', 'fields', 'claim', 'outcomes', 'interpretation', 'ratio', 'conduct', 'significance', 'studies', 'prejudice', 'published', 'prevailing', 'settings', 'definitions', 'relationships', 'smaller', 'tested', 'probability', 'teams', 'number', 'analytical', 'probed', 'essay', 'interest', 'claimed', 'findings', 'simply', 'simulations', 'accurate', 'increasing', 'research', 'question', 'finding', 'statistical', 'show', 'preselection', 'chase', 'lesser', 'designs', 'true', 'modes', 'measures', 'conducted']),\n",
       " Row(words=['systems', 'collaborative', 'structure', 'tagging']),\n",
       " Row(words=['build', 'enhances', 'enjoyable', 'group', 'motivated', 'performance', 'choice', 'behavior', 'building', 'innovation', 'full', 'sense', 'members', 'describes', 'principles', 'essay', 'research', 'shows', 'experience', 'high']),\n",
       " Row(words=['find', 'topology', 'graph', 'coupling', 'network', 'path', 'caenorhabditis', 'infectious', 'oscillators1', 'easily', 'networks', 'collaboration', 'dynamics', 'call', 'film', 'elegans', 'computational', 'media7', 'technological', 'display', 'random', 'networks12', 'rewired', 'extremes', 'analogy', 'collective', 'dynamical', 'highly', 'graphs', 'tuned', 'model', 'networks8', 'separation15', 'regular', 'enhanced', 'signalpropagation', 'completely', 'popularly', 'spread', 'western', 'small', 'characteristic', 'coupled', 'worm', 'speed', 'models', 'clustered', 'synchronizability', 'united', 'social', 'control', 'power', 'smallworld', 'selforganizing', 'connection', 'biological', 'games11', 'spatial', 'degrees', 'neural', 'ground', 'excitable', 'increasing', 'disorder', 'ordinarily', 'simple', 'middle', 'assumed', 'actors', 'states', 'lattices', 'grid', 'systems', 'explore', 'josephson', 'genetic', 'junction', 'introduce', 'lengths', 'phenomenon13', 'shown', 'arrays5', 'amounts', 'diseases']),\n",
       " Row(words=['means', 'aids', 'human', 'albertl', 'connected', 'brain', 'physicist', 'linked', 'epidemic']),\n",
       " Row(words=['organism', 'gene', 'clear', 'genes', 'functions', 'protein', 'worldwide', 'constructed', 'dynamic', 'transferred', 'biology', 'cells', 'genomic', 'function', 'unification', 'large', 'biological', 'eukaryotes', 'core', 'consortium', 'independent', 'fraction', 'changing', 'tool', 'applied', 'accumulating', 'cellular', 'produce', 'roles', 'accessible', 'goal', 'shared', 'organisms', 'sequencing', 'http', 'made', 'component', 'controlled', 'knowledge', 'process', 'proteins', 'geneontology', 'vocabulary', 'ontologies', 'role', 'molecular', 'ontology']),\n",
       " Row(words=['popularity', 'regularities', 'tags', 'discovered', 'relates', 'stable', 'dynamic', 'collaborative', 'photographs', 'form', 'usage', 'predicts', 'specifically', 'systems', 'remarkable', 'tagging', 'relative', 'content', 'activity', 'model', 'bookmarking', 'imitation', 'structure', 'analyze', 'sites', 'shared', 'frequencies', 'bursts', 'describes', 'proportions', 'keywords', 'recently', 'user', 'bookmarks', 'metadata', 'stability', 'present', 'patterns', 'kinds', 'aspects', 'knowledge', 'process', 'grown', 'users', 'paper']),\n",
       " Row(words=['universal', 'interconnections', 'describe', 'found', 'blocks', 'elements', 'significantly', 'motifs', 'food', 'defined', 'occurring', 'perform', 'webs', 'neurons', 'distinct', 'information', 'studied', 'cell', 'define', 'higher', 'escherichia', 'genetic', 'basic', 'network', 'fields', 'randomized', 'engineering', 'structural', 'caenorhabditis', 'saccharomyces', 'approach', 'biomolecules', 'complex', 'building', 'coli', 'uncover', 'similar', 'shared', 'neurobiology', 'networks', 'synaptic', 'numbers', 'principles', 'processing', 'classes', 'patterns', 'elegans', 'design', 'ecology', 'science', 'biochemistry', 'ecological', 'world', 'wide', 'cerevisiae', 'connections', 'simple']),\n",
       " Row(words=['confining', 'aspect', 'micro', 'dyadic', 'social', 'mobility', 'cohesive', 'power', 'diffusion', 'opportunity', 'lends', 'segments', 'levels', 'smallscale', 'interaction', 'discussion', 'community', 'defined', 'implications', 'welldefined', 'elaboration', 'information', 'deal', 'primary', 'degree', 'models', 'overlap', 'network', 'illustrated', 'friendship', 'structure', 'tool', 'linking', 'weak', 'macro', 'emphasis', 'groups', 'stress', 'varies', 'easily', 'strength', 'networks', 'theory', 'procedure', 'explored', 'laid', 'sociological', 'implicitly', 'argued', 'strong', 'individuals', 'relations', 'influence', 'small', 'terms', 'applicability', 'ties', 'directly', 'suggested', 'organization', 'analysis', 'principle', 'impact']),\n",
       " Row(words=['technologies', 'characterizing', 'precise', 'methods', 'levels', 'altered', 'seq', 'transcriptomics', 'view', 'isoforms', 'eukaryote', 'approach', 'measurement', 'extent', 'tool', 'method', 'complexity', 'application', 'studies', 'transcripts', 'deepsequencing', 'describes', 'advances', 'recently', 'revolutionary', 'challenges', 'made', 'developed', 'article', 'eukaryotic', 'transcriptomes', 'transcriptome', 'profiling']),\n",
       " Row(words=['development', 'concepts', 'undergone', 'learning', 'recognition', 'developments', 'students', 'field', 'aimed', 'book', 'reflects', 'basic', 'machine', 'researchers', 'year', 'grounding', 'providing', 'pattern', 'practitioners', 'years', 'substantial', 'advanced', 'undergraduates']),\n",
       " Row(words=['guide', 'programming', 'developing', 'bioinformatics', 'quick', 'skills', 'effective']),\n",
       " Row(words=['implemented', 'rapid', 'protein', 'score', 'gene', 'faster', 'comparison', 'maximal', 'results', 'performance', 'tools', 'generates', 'magnitude', 'flexibility', 'searches', 'segment', 'algorithm', 'alignments', 'mathematical', 'pair', 'variety', 'basic', 'approach', 'similarity', 'optimize', 'identification', 'multiple', 'tool', 'applied', 'sequence', 'method', 'significance', 'properties', 'scores', 'blast', 'addition', 'database', 'straightforward', 'number', 'order', 'search', 'robust', 'tractability', 'ways', 'directly', 'alignment', 'including', 'local', 'approximates', 'comparable', 'statistical', 'recent', 'stochastic', 'contexts', 'measure', 'motif', 'regions', 'analysis', 'sequences', 'existing', 'sensitivity', 'long', 'simple']),\n",
       " Row(words=['empirical', 'data', 'distributions', 'powerlaw']),\n",
       " Row(words=['statistical', 'learning', 'elements']),\n",
       " Row(words=['behaviour', 'applicable', 'iteratively', 'situations', 'monotone', 'levels', 'incomplete', 'factor', 'derived', 'mixture', 'estimation', 'generality', 'computing', 'showing', 'applications', 'examples', 'likelihood', 'algorithm', 'truncated', 'data', 'finite', 'missing', 'variance', 'broadly', 'reweighted', 'convergence', 'estimates', 'sketched', 'theory', 'grouped', 'component', 'analysis', 'maximum', 'presented', 'including', 'hyperparameter', 'censored', 'squares', 'models']),\n",
       " Row(words=['guide', 'computational', 'projects', 'quick', 'biology', 'organizing']),\n",
       " Row(words=['network', 'folksonomy', 'complex']),\n",
       " Row(words=['rating', 'compare', 'human', 'surfer', 'interests', 'readers', 'depends', 'objectively', 'large', 'efficiently', 'citation', 'idealized', 'relative', 'compute', 'matter', 'attention', 'bringing', 'method', 'inherently', 'devoted', 'attitudes', 'ranking', 'describes', 'order', 'pages', 'measuring', 'pagerank', 'page', 'knowledge', 'importance', 'paper', 'random', 'subjective', 'mechanically', 'show', 'interest', 'effectively']),\n",
       " Row(words=['cbcb', 'human', 'memory', 'extends', 'simultaneously', 'greater', 'source', 'techniques', 'gigabytes', 'large', 'processor', 'genome', 'qualityaware', 'hour', 'footprint', 'memoryefficient', 'algorithm', 'approximately', 'backtracking', 'previous', 'multiple', 'sequence', 'mismatches', 'million', 'reads', 'aligning', 'achieve', 'indexing', 'http', 'genomes', 'burrowswheeler', 'bowtie', 'program', 'ultrafast', 'cores', 'alignment', 'short', 'permits', 'sequences', 'align', 'speeds', 'open']),\n",
       " Row(words=['development', 'improve', 'elements', 'response', 'developing', 'browser', 'readers', 'ease', 'creation', 'require', 'find', 'advantage', 'selection', 'capabilities', 'experienced', 'recurring', 'content', 'network', 'text', 'reference', 'tutorial', 'programming', 'graphical', 'literature', 'include', 'online', 'compiled', 'search', 'work', 'design', 'contents', 'demand', 'process', 'directly', 'version', 'creating', 'programmers', 'smalltalk', 'paste', 'contained', 'bestseller', 'consistent', 'valuable', 'structures', 'classic', 'reusability', 'examples', 'problems', 'languages', 'offering', 'note', 'resource', 'navigation', 'timeless', 'crossreferences', 'demonstrating', 'internationally', 'user', 'succinct', 'acclaimed', 'numerous', 'implementation', 'standard', 'presents', 'realworld', 'enables', 'wellengineered', 'elegant', 'flow', 'control', 'modern', 'javabased', 'dynamic', 'designers', 'inheritance', 'composition', 'complete', 'object', 'objectoriented', 'flexibility', 'install', 'readytoinstall', 'evaluate', 'book', 'solutions', 'enhancing', 'html', 'common', 'larger', 'published', 'explain', 'composing', 'authors', 'mechanism', 'sample', 'designs', 'electronic', 'simple', 'hyperlinked', 'implemented', 'describe', 'code', 'documents', 'format', 'applicationa', 'systems', 'objects', 'demonstrates', 'environment', 'seach', 'software', 'components', 'create', 'managing', 'coordinating', '1995', 'pattern', 'accessed', 'describes', 'computer', 'created', 'reusable', 'reader', 'catalog', 'patterns', 'landmark', 'allowing', 'systematically', 'virtuoso', 'essential']),\n",
       " Row(words=['guide', 'teaching', 'programming', 'students', 'computational', 'quick', 'biology']),\n",
       " Row(words=['tags', 'ht06', 'resources', 'systems', 'tagging', 'read', 'academic', 'popular', 'flickr', 'taxonomy', 'increasingly', 'years', 'keywords', 'article', 'internet', 'users', 'paper', 'recent', 'enable']),\n",
       " Row(words=['coefficient', 'detecting', 'important', 'score', 'informationbased', 'gene', 'human', 'determination', 'maximal', 'microbiota', 'functional', 'class', 'function', 'large', 'health', 'range', 'captures', 'expression', 'information', 'relative', 'belongs', 'mine', 'identifying', 'baseball', 'twovariable', 'data', 'global', 'majorleague', 'equals', 'exploration', 'identify', 'roughly', 'larger', 'variables', 'relationships', 'sets', 'increasingly', 'regression', 'nonparametric', 'present', 'dependence', 'interesting', 'associations', 'wide', 'pairs', 'apply', 'statistics', 'measure', 'classifying']),\n",
       " Row(words=['development', 'retrieval', 'free', 'content', 'evolution', 'strengths', 'inadequate', 'restrictions', 'tool', 'usefulness', 'online', 'biomedical', 'journal', 'updated', 'search', 'article', 'science', 'includes', 'obscure', 'world', 'journals', 'recent', 'index', 'limited', 'keyword', 'offered', 'optimal', 'general', 'range', 'information', 'pubmed', 'offering', 'google', 'extract', 'coverage', 'numerous', 'research', 'articles', 'wide', 'medical', 'analysis', 'offers', 'frequency', 'ability', 'citation', 'evaluate', 'searching', 'accuracy', 'published', 'facilities', 'subject', 'pages', 'performing', 'importance', 'scholar', 'compared', 'electronic', 'utility', 'covers', 'update', 'official', 'comparison', 'results', 'wider', 'perform', 'practical', 'specific', 'scopus', 'citations', 'covered', 'marred', 'rate', '1995', 'accessed', 'remains', 'weaknesses', 'number', 'inconsistent', 'early', 'databases']),\n",
       " Row(words=['behaviour', 'instance', 'solar', 'physics', 'earthquakes', 'laws', 'distribution', 'pareto', 'zipf', 'power', 'social', 'planetary', 'economics', 'biology', 'fortunes', 'existence', 'review', 'community', 'sizes', 'theories', 'demography', 'cities', 'forms', 'follow', 'debate', 'finance', 'moon', 'origin', 'widely', 'evidence', 'fires', 'craters', 'scientific', 'quantity', 'inversely', 'explain', 'variously', 'varies', 'topic', 'probability', 'computer', 'sciences', 'forest', 'measuring', 'century', 'proposed', 'flares', 'science', 'distributions', 'personal', 'empirical', 'earth', 'people', 'powerlaw']),\n",
       " Row(words=['interpreted', 'commonly', 'clear', 'communicate', 'publications', 'types', 'biology', 'correct', 'rules', 'illustrate', 'information', 'figures', 'make', 'basic', 'data', 'interpretation', 'quantities', 'explain', 'figure', 'confidence', 'deviations', 'give', 'legends', 'features', 'experimental', 'intervals', 'represent', 'bars', 'article', 'standard', 'assist', 'errors', 'error', 'show', 'unsure', 'biologists', 'suggest', 'effective', 'simple']),\n",
       " Row(words=['information', 'learning', 'algorithms', 'theory', 'inference']),\n",
       " Row(words=['lung', 'biologically', 'gene', 'genes', 'chromosomal', 'describe', 'share', 'insight', 'power', 'cancerrelated', 'notably', 'embodied', 'regulation', 'derives', 'function', 'freely', 'genomewide', 'location', 'focusing', 'defined', 'biological', 'expression', 'powerful', 'information', 'singlegene', 'initial', 'independent', 'insights', 'extracting', 'data', 'approach', 'similarity', 'software', 'finds', 'tool', 'package', 'method', 'common', 'studies', 'profiles', 'yields', 'reveals', 'challenge', 'groups', 'pathways', 'gsea', 'enrichment', 'remains', 'sets', 'database', 'patient', 'major', 'biomedical', 'analytical', 'genomewide', 'cancer', 'leukemia', 'knowledgebased', 'research', 'interpreting', 'including', 'routine', 'called', 'analysis', 'survival', 'demonstrate']),\n",
       " Row(words=['protein', 'comparisons', 'gapped', 'score', 'significant', 'biologically', 'superfamily', 'tools', 'execution', 'extension', 'positionspecific', 'programs', 'similarities', 'introduced', 'cases', 'statistically', 'combining', 'alignments', 'combined', 'approximately', 'relevant', 'enhancing', 'searching', 'widely', 'variety', 'substantially', 'sequence', 'weak', 'method', 'matrix', 'uncover', 'yields', 'iteration', 'blast', 'times', 'definitional', 'members', 'criterion', 'resulting', 'addition', 'database', 'runs', 'refinements', 'search', 'time', 'algorithmic', 'interesting', 'program', 'generation', 'original', 'automatically', 'generating', 'word', 'hits', 'statistical', 'triggering', 'heuristic', 'permits', 'decreased', 'sensitivity', 'speed', 'iterated', 'produced', 'sensitive', 'brct', 'databases']),\n",
       " Row(words=['flexible', 'universal', 'sourceforge', '1000', 'storing', 'format', 'efficient', 'tools', 'released', 'variant', 'style', 'read', 'alignments', 'compact', 'implements', 'samtools', 'reference', 'sequence', 'utilities', 'postprocessing', 'reads', 'sequencing', 'caller', 'indexing', 'http', 'platforms', 'genomes', 'processing', 'project', 'generic', 'size', 'alignment', 'short', 'viewer', 'random', 'sequences', 'produced', 'long', 'access', 'supporting']),\n",
       " Row(words=['production', 'technologies', 'development', 'greater', 'catalysed', 'broad', 'methods', 'outline', 'large', 'fast', 'advantage', 'conventional', 'review', 'genome', 'range', 'applications', 'platform', 'information', 'selection', 'biological', 'primary', 'nextgeneration', 'guidelines', 'current', 'data', 'preparation', 'questions', 'volumes', 'template', 'sequence', 'instruments', 'providing', 'challenge', 'nearterm', 'sequencing', 'deliver', 'advances', 'addition', 'commercially', 'revolutionary', 'present', 'interest', 'approaches', 'inexpensive', 'demand', 'accurate', 'generation', 'assembly', 'alignment', 'address', 'recent', 'imaging', 'technical']),\n",
       " Row(words=['networks', 'structure', 'complex', 'function']),\n",
       " Row(words=['rules', 'published', 'simple']),\n",
       " Row(words=['033340', 'research', 'importance', 'stupidity', 'scientific', '1242']),\n",
       " Row(words=['remaining', 'precursors', 'untranscribed', 'events', 'represented', 'spanned', 'reference', 'mapping', 'revised', 'mammalian', 'sequencing', 'skeletal', 'detection', 'transcript', 'unknown', 'directly', 'uniquely', 'mouse', 'composed', 'observed', 'promoters', 'measured', 'readily', 'splices', 'methods', 'mapped', 'linear', 'seq', 'report', 'prevalence', 'range', 'deeply', 'muscle', 'microrna', 'million', 'serial', 'poly', 'quantify', 'standard', 'frequently', 'including', 'tissues', 'orders', 'analysis', 'magnitude', 'models', 'expressing', 'standards', 'genes', 'additional', '25basepair', 'adult', 'fell', 'changed', 'candidate', 'brain', 'alternative', 'selected', 'microarray', 'reads', 'test', 'splicecrossing', 'previously', 'measurements', '4152', 'sample', 'transcriptomes', 'liver', 'regions', 'suggest', 'splice', 'gene', 'presence', 'detected', 'expression', 'quantifying', 'distinct', 'alternate', 'internal', 'quantified', 'data', 'sequence', 'transcripts', 'prominent', 'exons', 'measure', 'recording', 'digital']),\n",
       " Row(words=['development', 'emergence', 'found', 'distribution', 'connected', 'continuously', 'phenomena', 'large', 'systems', 'connectivities', 'selforganizing', 'follow', 'based', 'scaling', 'stationary', 'expand', 'topology', 'model', 'diverse', 'genetic', 'governed', 'attach', 'individual', 'sites', 'complex', 'common', 'particulars', 'ingredients', 'property', 'addition', 'networks', 'vertices', 'feature', 'robust', 'preferentially', 'generic', 'consequence', 'reproduces', 'wide', 'world', 'observed', 'distributions', 'random', 'scalefree', 'powerlaw', 'mechanisms', 'vertex']),\n",
       " Row(words=['development', 'mobile', 'insight', 'governing', 'evolution7', 'find', 'community', 'algorithm', 'suggesting', 'rich', 'evolution', 'tendency', 'whole17', 'structure', 'network', 'clique', 'capable', 'friends', 'uncover', 'families', 'relationships', 'change', 'society1', 'networks', 'collaboration', 'dynamics', 'dependence', 'investigate', 'overlapping', 'calls', 'displays', 'show', 'commitment', 'interactions', 'limited', 'condition', 'highly', 'based', 'membership', 'society', 'communities', 'time', 'persist', 'small', 'altering', 'mechanisms', 'percolation23', 'behaviour', 'social', 'connected', 'characterizing', 'composition', 'large', 'ability', 'communication', 'fundamental', 'institutions', 'complex', 'network3', 'cliques', 'groups', 'subject', 'lifetime', 'individuals', 'knowledge', 'activity', 'phone', 'longer', 'opposite', 'group', 'results', 'selfoptimization', 'estimating', 'professional', 'constant', 'quantifying', 'circles', 'scale', 'basic', 'dynamically', 'differences', 'deeper', 'frequent', 'members', 'remains', 'stability', 'unchanged', 'developed', 'findings', 'patterns', 'scientists', 'offer', 'capturing', 'understanding', 'underlying', 'users', 'focus', 'essential', 'adaptability']),\n",
       " Row(words=['topics', 'account', 'recognition', 'classification', 'examples', 'based', 'edition', 'solutions', 'fundamental', 'enlarged', 'manual', 'revised', 'completely', 'pattern', 'formatted', 'major', 'extensive', 'exercises', 'principles', 'systematic', 'includes', 'colour']),\n",
       " Row(words=['indels', 'technologies', 'development', 'longer', 'gapped', 'concern', 'methods', 'scaled', 'hundreds', 'programs', 'fast', 'read', 'amount', 'rich', 'hash', 'support', 'singleend', 'wheeler', 'individual', 'generated', 'makes', 'reads', 'sequencing', 'single', 'resequencing', 'feature', 'enormous', 'tablebased', 'developed', 'call', 'individuals', 'unsuitable', 'accurate', 'frequently', 'generation', 'alignment', 'short', 'including', 'burrows', 'motivation', 'transform', 'align', 'occur', 'speed']),\n",
       " Row(words=['collaborative', 'tripartite', 'tagging', 'network']),\n",
       " Row(words=['typically', 'publications', 'discuss', 'lost', 'students', 'write', 'adventurous', 'top10', 'make', 'structure', 'scientific', 'contributions', 'stilted', 'accessible', 'allure', 'exciting', 'language', 'literature', 'enthusiasm', 'list', 'laymen', 'present', 'predictable', 'scientists', 'recommendations', 'research', 'talk', 'insist', 'boring', 'consistently', 'prospective']),\n",
       " Row(words=['molecules', 'clear', 'rapid', 'universal', 'laws', 'catalogue', 'biology', 'surrounded', 'functional', 'cells', 'function', 'isolation', 'framework', 'potentially', 'view', 'cell', 'revolutionize', 'determine', 'network', 'governed', 'pathologies', 'conceptual', 'cellular', 'complex', 'machinery', 'postgenomic', 'advances', 'networks', 'biomedical', 'disease', 'living', 'enormously', 'century', 'offer', 'research', 'understanding', 'understand', 'organization', 'systematically', 'interactions', 'twentyfirst']),\n",
       " Row(words=['detecting', 'significant', 'found', 'social', 'centrality', 'propose', 'concentrated', 'smallworld', 'detects', 'find', 'food', 'tightly', 'community', 'graphs', 'reliability', 'systems', 'biological', 'built', 'cases', 'worldwide', 'networked', 'degree', 'communities', 'structure', 'network', 'divisions', 'researchers', 'knit', 'nodes', 'properties', 'common', 'studies', 'highlight', 'method', 'joined', 'property', 'groups', 'test', 'informative', 'networks', 'focused', 'number', 'collaboration', 'boundaries', 'looser', 'transitivity', 'article', 'idea', 'indices', 'distributions', 'statistical', 'recent', 'computergenerated', 'apply', 'sensitivity', 'connections', 'powerlaw', 'high', 'realworld']),\n",
       " Row(words=['wikis', 'research', 'communication', 'groups', 'blogs', 'valuable', 'tools', 'software']),\n",
       " Row(words=['social', 'results', 'collaborative', 'methodology', 'document', 'infrastructure', 'community', 'selected', 'systems', 'prototypes', 'preliminary', 'tagging', 'potential', 'society', 'identification', 'support', 'recommendation', 'folksonomies', 'user', 'challenges', 'harvesting', 'management', 'meet', 'design', 'knowledge', 'generation', 'presented', 'technological', 'paper', 'organization', 'designs', 'evaluation', 'presents', 'ontology', 'activities', 'enhance']),\n",
       " Row(words=['physics', 'collectively', 'power', 'metabolic', 'diagram', 'interacting', 'study', 'dynamical', 'perspective', 'food', 'beginning', 'wiring', 'neurons', 'lasers', 'systems', 'issues', 'topology', 'escherichia', 'behave', 'basic', 'pervades', 'network', 'coupling', 'structure', 'structural', 'individual', 'researchers', 'complex', 'coli', 'nonlinear', 'neurobiology', 'networks', 'dynamics', 'unifying', 'enormous', 'bacterium', 'principles', 'stations', 'characterize', 'exploring', 'science', 'internet', 'underlying', 'statistical', 'understand', 'architecture', 'unravel']),\n",
       " Row(words=['projected', 'technologies', 'development', 'productionscale', 'significant', 'highthroughput', 'past', 'format', 'broad', 'phenomena', 'parallel', 'comprehensive', 'dramatically', 'interactomes', 'genome', 'range', 'biological', 'cost', 'democratizing', 'field', 'widespread', 'potential', 'protocols', 'nextgeneration', 'widely', 'data', 'sequence', 'individual', 'building', 'investigators', 'efforts', 'center', 'requiring', 'nearterm', 'dataanalysis', 'sequencing', 'include', 'single', 'massively', 'libraries', 'years', 'major', 'platforms', 'challenges', 'hands', 'evolving', 'experimental', 'biomedical', 'genomes', 'collection', 'reducing', 'robust', 'approaches', 'rapidly', 'rethinking', 'design', 'accelerate', 'inexpensive', 'research', 'putting', 'represents', 'generating', 'orders', 'transcriptomes', 'routine', 'capacity', 'analysis', 'magnitude', 'effective', 'enabling']),\n",
       " Row(words=['compare', 'provide', 'highthroughput', '1000', 'undertook', 'functional', 'haplotype', 'mutations', 'location', 'inform', 'selection', 'deep', 'approximately', 'structure', 'identify', 'pilot', 'substitution', 'sequencing', 'exontargeted', 'single', 'implicated', 'linked', 'majority', 'polymorphisms', 'insertions', 'novo', 'directly', 'short', 'show', 'highcoverage', 'methods', 'disorders', 'relationship', 'genomewide', 'trios', 'foundation', 'variation', 'deletions', 'germline', 'support', 'structural', 'sites', 'strategies', 'million', 'catalogued', 'variants', 'lowcoverage', 'marked', 'research', 'reduction', 'populationscale', 'demonstrate', 'person', 'human', 'genes', 'frequency', 'allele', 'natural', 'carry', 'inherited', 'signatures', 'average', 'vast', 'designed', 'characterization', 'common', 'studies', 'neighbourhood', 'present', 'genomes', 'previously', 'individuals', 'project', 'local', 'investigating', 'phenotype', 'describe', 'found', 'undescribed', 'results', 'estimate', 'genome', 'explore', 'genotype', 'pair', 'genetic', 'data', 'sequence', 'individual', 'accessible', 'phase', 'regard', 'projects', 'develop', 'annotated', 'rate', 'association', 'platforms', 'populations', 'generation', 'public', 'nucleotide', 'lossoffunction', 'base', 'motherfatherchild', 'wholegenome', 'aims']),\n",
       " Row(words=['development', 'pipeline', 'relying', 'require', 'levels', 'generates', 'fragments', 'algorithm', 'free', 'current', 'tophat', 'reference', 'identify', 'mapping', 'protocol', 'millions', 'mammalian', 'sequencing', 'single', 'online', 'junctions', 'initio', 'process', 'short', 'recent', 'cbcb', 'mapped', 'seq', 'hour', 'cell', 'sites', 'recovered', 'million', 'variants', 'challenges', 'messenger', 'standard', 'annotationbased', 'unique', 'genes', 'faster', 'desktop', 'relies', 'designed', 'experiment', 'site', 'reads', 'aligning', 'supplementary', 'previously', 'readmapping', 'reported', 'align', 'splice', 'gene', 'describe', 'study', 'efficient', 'genome', 'systems', 'expression', 'discovering', 'data', 'previous', 'software', 'opensource', 'sequence', 'unreported', 'bioinformatics', 'computer', 'http', 'sufficient', 'entire', 'measure', 'discovery']),\n",
       " Row(words=['cognitive', 'computational', 'situated', 'learning', 'social', 'legitimate', 'participation', 'peripheral', 'perspectives']),\n",
       " Row(words=['managing', 'nextgeneration', 'data', 'analyzing', 'sequence']),\n",
       " Row(words=['control', 'power', 'faults', 'study', 'false', 'falsely', 'gain', 'point', 'sequential', 'practical', 'proportion', 'powerful', 'problems', 'examples', 'potential', 'independent', 'approach', 'multiple', 'illustrated', 'significance', 'desired', 'common', 'appropriateness', 'testing', 'rate', 'rejected', 'hypotheses', 'smaller', 'bonferronitype', 'proved', 'test', 'criterion', 'multiplicity', 'procedure', 'simulation', 'substantial', 'fwer', 'controlling', 'problem', 'expected', 'familywise', 'presented', 'calls', 'error', 'hypothesesthe', 'shows', 'equivalent', 'statistics', 'true', 'discovery', 'simple']),\n",
       " Row(words=['means', 'retrieval', 'highthroughput', 'close', 'improved', 'methods', 'tools', 'require', 'owing', 'facts', 'laboratories', 'growing', 'biological', 'information', 'considerably', 'average', 'linguists', 'extracting', 'pubmed', 'data', 'scientific', 'encourage', 'text', 'handson', 'literature', 'mining', 'annotate', 'number', 'experimental', 'sets', 'biomedical', 'body', 'search', 'computational', 'generation', 'automatically', 'increasing', 'biologist', 'systemwide', 'hypothesis', 'collaborations', 'openaccess', 'journals', 'policies', 'integration', 'discovery', 'biologists', 'keyword', 'analyse']),\n",
       " Row(words=['incorporated', 'expression', 'studies', 'highdimensional', 'principal', 'explore', 'data', 'analysis', 'genomewide', 'component']),\n",
       " Row(words=['review', 'data', 'clustering']),\n",
       " Row(words=['level', 'increases', 'provide', 'highthroughput', 'tools', 'functional', 'genomic', 'community', 'task', 'experienced', 'approximately', 'tool', 'identify', 'investigators', 'advantages', 'collected', 'recent', 'uniquely', 'details', 'enabling', 'toolbytool', 'interests', 'categorized', 'trends', 'algorithms', 'pitfalls', 'make', 'scanning', 'approach', 'challenging', 'strategy', 'questions', 'geneannotation', 'developers', 'choices', 'promising', 'classifications', 'enrichment', 'survey', 'research', 'unique', 'analysis', 'uptodate', 'categories', 'toolclass', 'designers', 'pertinent', 'derived', 'comprehensive', 'large', 'emerging', 'biological', 'likelihood', 'issues', 'view', 'daunting', 'classes', 'paths', 'approaches', 'proteomic', 'understand', 'gene', 'study', 'simpler', 'cases', 'collections', 'bioinformatics', 'processes', 'major', 'lists', 'underlying', 'users']),\n",
       " Row(words=['interest', 'management', 'decentralized', 'blogging', 'information', 'knowledge', 'effective', 'collective', 'structured', 'communities', 'semantic', 'metadata', 'snippets', 'tapping', 'access']),\n",
       " Row(words=['provide', 'types', 'state', 'tutorial', 'description', 'leftright', 'single', 'cointossing', 'hidden', 'acquiring', 'details', 'source', 'recognition', 'background', 'methods', 'classic', 'discrete', 'selected', 'observation', 'problems', 'examples', 'system', 'studied', 'ergodic', 'ballsinurns', 'implementation', 'research', 'including', 'pursue', 'models', 'effectively', 'baum', '1966', 'techniques', 'applications', 'fundamental', 'speech', 'required', 'author', 'noted', 'chains', 'petrie', 'original', 'shows', 'area', 'simple', 'concept', 'results', 'function', 'states', 'solving', 'practical', 'originated', 'markov', 'distinct', 'reviews', 'combined', 'basic', 'hmms', 'illustrated', 'probabilistic', 'theory', 'number', 'sources', 'overview']),\n",
       " Row(words=['assembly', 'novo', 'bruijn', 'read', 'collectively', 'short', 'velvet', 'algorithms', 'called', 'graphs', 'developed']),\n",
       " Row(words=['basis', 'development', 'laboratory', 'adam', 'describe', 'relates', 'experimentally', 'units', 'functional', 'levels', 'hypotheticodeductive', 'report', 'genomics', 'robot', 'deep', 'detail', 'confirmed', 'structure', 'scientific', 'logical', 'biomass', 'machine', 'contributed', 'manual', 'saccharomyces', 'method', 'autonomously', 'scientist', 'description', 'generated', 'hypotheses', 'language', 'formalization', 'tested', 'involves', 'nested', 'million', 'advances', 'yeast', 'resulting', 'describes', 'conclusions', 'treelike', 'developed', 'sufficient', 'measurements', 'knowledge', 'research', 'science', 'experiments', 'reproducibility', 'cerevisiae', 'recording', 'automation', 'ontology', 'enable']),\n",
       " Row(words=['development', 'gleaned', 'human', 'extraordinary', 'international', 'holds', 'results', 'trove', 'describing', 'draft', 'freely', 'report', 'genome', 'information', 'initial', 'evolution', 'make', 'insights', 'data', 'sequence', 'produce', 'medicine', 'sequencing', 'collaboration', 'present', 'physiology', 'analysis']),\n",
       " Row(words=['protein', 'development', 'place', 'obtain', 'future', 'systems', 'biological', 'information', 'rcsb', 'worldwide', 'resource', 'data', 'structural', 'archive', 'goals', 'nearterm', 'single', 'describes', 'http', 'macromolecules', 'bank', 'deposition', 'paper', 'access', 'plans']),\n",
       " Row(words=['selection', 'strategies', 'variable', 'spike', 'frequentist', 'slab', 'bayesian']),\n",
       " Row(words=['diagram', 'regulation', 'community', 'unified', 'entity', 'tool', 'makes', 'graphical', 'networks', 'work', 'accurate', 'unambiguous', 'process', 'recent', 'interactions', 'reuse', 'enabling', 'enable', 'visual', 'pressing', 'relationship', 'examples', 'languages', 'information', 'visualization', 'support', 'notation', 'removing', 'circuit', 'storage', 'represent', 'standard', 'kinds', 'ratios', 'highest', 'foster', 'exchange', 'promoting', 'deficit', 'flow', 'metabolism', 'deluge', 'biological', 'communication', 'modeling', 'signaling', 'complementary', 'cellular', 'complex', 'sbgn', 'language', 'biochemical', 'consists', 'present', 'accelerate', 'knowledge', 'representation', 'modelers', 'activity', 'notations', 'concern', 'gene', 'ironically', 'biology', 'efficient', 'systems', 'biochemists', 'software', 'addressing', 'lacks', 'goal', 'computer', 'regularity', 'developed', 'scientists', 'ambiguity', 'diagrams', 'textual']),\n",
       " Row(words=['research', 'funding', 'white', 'lies', 'scientific', 'lives', 'real']),\n",
       " Row(words=['simple', 'academia', 'industry', 'choosing', 'rules']),\n",
       " Row(words=['gene', 'international', 'comparison', 'biology', 'seq', 'genome', 'expression', 'arrays', 'insights', 'technical', 'organisms', 'outstanding', 'sciences', 'assessment', 'journal', 'peerreviewed', 'original', 'research', 'reproducibility', 'featuring', 'offers']),\n",
       " Row(words=['interconnected', 'developing', 'chemical', 'units', 'review', 'topology', 'structure', 'network', 'achieved', 'mimic', 'networks', 'dynamics', 'recently', 'ideas', 'science', 'world', 'cope', 'interactions', 'real', 'composed', 'links', 'basis', 'dynamical', 'highly', 'hand', 'graphs', 'wiring', 'examples', 'model', 'approach', 'global', 'reproduce', 'questions', 'mechanics', 'engineering', 'structural', 'medicine', 'nonlinear', 'capture', 'represent', 'internet', 'wide', 'coupled', 'stand', 'models', 'ensemble', 'concepts', 'collectively', 'social', 'learning', 'characterizing', 'large', 'biological', 'applications', 'issues', 'arise', 'nodes', 'complex', 'neural', 'disciplines', 'species', 'interacting', 'study', 'results', 'biology', 'growth', 'systems', 'relevant', 'behave', 'revealing', 'properties', 'ranging', 'interact', 'number', 'major', 'unifying', 'principles', 'scientists', 'summarize', 'studying', 'statistical', 'architecture']),\n",
       " Row(words=['human', 'provide', 'connected', 'modularityboth', 'animals', 'diffusion', 'smallworld', 'functional', 'developments', 'highly', 'review', 'imaging', 'nonhuman', 'moving', 'future', 'systems', 'field', 'tensor', 'based', 'hubs', 'topology', 'brain', 'scale', 'diverse', 'graph', 'modalities', 'basic', 'network', 'questions', 'structural', 'cellular', 'complex', 'studies', 'accessible', 'highlight', 'theoretical', 'technical', 'electroencephalography', 'features', 'networks', 'theory', 'experimental', 'neuroimaging', 'networkssuch', 'introduction', 'principles', 'challenges', 'wholebrain', 'quantitative', 'article', 'rapidly', 'addressed', 'largely', 'including', 'translated', 'recent', 'organization', 'humans', 'analysis', 'investigating', 'magnetoencephalography']),\n",
       " Row(words=['displaying', 'basis', 'tags', 'social', 'algorithms', 'movielens', 'emergent', 'effect', 'community', 'selection', 'expression', 'forms', 'tagging', 'based', 'evaluate', 'system', 'model', 'recommender', 'explore', 'evolution', 'tendency', 'communities', 'navigation', 'analyze', 'applied', 'introducing', 'adoption', 'shared', 'members', 'features', 'user', 'present', 'influence', 'satisfaction', 'usercentric', 'vocabulary', 'personal', 'utility']),\n",
       " Row(words=['guide', 'commonly', 'learning', 'purpose', 'performance', 'serve', 'graphs', 'pitfalls', 'apparently', 'data', 'decision', 'machine', 'organizing', 'practice', 'common', 'misconceptions', 'characteristics', 'visualizing', 'years', 'increasingly', 'mining', 'introduction', 'classifiers', 'article', 'research', 'medical', 'recent', 'making', 'analysis', 'operating', 'receiver', 'simple']),\n",
       " Row(words=['protein', 'technique', 'insight', 'units', 'exhibit', 'functional', 'clustering', 'degree', 'science1', 'structure', 'network', 'path', 'capable', 'tool', 'organizing', 'invaluable', 'networks', 'prediction', 'inferring', 'short', 'distributions', 'recent', 'show', 'organization', 'techniques8', 'links', 'partly', 'observed', 'commonly', 'simultaneously', 'phenomena', 'describing', 'structures', 'general', 'correspond', 'divide', 'subdivide', 'communities', 'offering', 'reproduce', 'ecological', 'hierarchy', 'rightskewed', 'social', 'existence', 'interaction', 'central', 'accuracy', 'complex', 'studies', 'explain', 'groups', 'biochemical', 'years', 'present', 'knowledge', 'modules', 'suggest', 'topological', 'connections', 'competing', 'principle', 'coefficients', 'found', 'hierarchical', 'scales', 'metabolic', 'quantitatively', 'results', 'food', 'branches', 'webs', 'systems', 'cases', 'quantifying', 'regulatory', 'genetic', 'data', 'multiple', 'missing', 'properties', 'lengths', 'niches', 'vertices', 'networks4', 'high', 'predict', 'emerged']),\n",
       " Row(words=['development', 'improve', 'highthroughput', 'require', 'mrna', 'estimation', 'start', 'annotation', 'quantification', 'restricted', 'splicing', 'sequencing', 'transcript', 'series', 'called', 'mouse', 'observed', 'supported', 'account', 'algorithms', 'simultaneous', 'seq', 'cell', 'independent', 'model', 'switches', 'differentiation', 'showed', 'muscle', 'analyzed', 'complexity', 'isoform', 'reveals', 'sequenced', 'million', 'transcription', 'abundance', 'time', 'assembly', 'program', 'prior', 'promises', 'paired', 'shifts', 'unannotated', 'genes', 'subtle', 'complete', 'flexibility', 'dominant', 'alternative', 'myoblast', 'site', 'homologous', 'reads', 'test', 'line', 'previously', 'substantial', 'wellstudied', '75bp', 'suggest', 'transcriptomebased', 'splice', 'gene', 'detected', 'species', 'results', 'genome', 'expression', 'regulatory', 'data', 'software', 'opensource', 'introduce', 'cufflinks', 'transcripts', 'illuminate', 'switching', 'annotations', 'discovery']),\n",
       " Row(words=['measured', 'strongly', 'types', 'results', 'collaborative', 'uncorrelated', 'evaluated', 'class', 'domain', 'review', 'systems', 'reviewing', 'userbased', 'evaluating', 'system', 'content', 'recommender', 'equivalence', 'correlated', 'accuracy', 'researchers', 'strategies', 'roughly', 'attributes', 'collapsed', 'tested', 'addition', 'user', 'prediction', 'present', 'classes', 'decisions', 'article', 'ways', 'datasets', 'incomparable', 'filtering', 'prior', 'tasks', 'metrics', 'empirical', 'equivalency', 'evaluation', 'analysis', 'quality']),\n",
       " Row(words=['respond', 'means', 'computers', 'human', 'emergence', 'significant', 'place', 'implied', 'misinterprets', 'interacting', 'keyboards', 'sitting', 'interaction', 'poses', 'domain', 'computing', 'conventional', 'conventionallydesigned', 'systems', 'notion', 'opportunities', 'representational', 'interpretations', 'everyday', 'wellunderstood', 'model', 'engendered', 'constrained', 'alternative', 'paradigm', 'ubiquitous', 'interactive', 'settings', 'contextaware', 'sense', 'desks', 'single', 'proposes', 'mice', 'challenges', 'stance', 'surrounds', 'considerable', 'suggests', 'aspects', 'plays', 'design', 'computational', 'confusion', 'traditionally', 'directions', 'includes', 'talk', 'users', 'employing', 'context', 'experience', 'role', 'paper', 'interest', 'activity', 'humancomputer', 'screens']),\n",
       " Row(words=['main', 'computers', 'interplay', 'describe', 'connected', 'chemical', 'discuss', 'motivated', 'tools', 'modeled', 'smallworld', 'graphs', 'focusing', 'systems', 'range', 'examples', 'reactions', 'recognized', 'real', 'reviews', 'field', 'cited', 'cell', 'topology', 'evolution', 'society', 'emerging', 'data', 'mechanics', 'network', 'governed', 'robustness', 'organizing', 'complex', 'physical', 'include', 'advances', 'networks', 'linked', 'increasingly', 'dynamics', 'authors', 'theory', 'analytical', 'principles', 'evolving', 'failures', 'interest', 'robust', 'article', 'frequently', 'traditionally', 'internet', 'wide', 'attacks', 'empirical', 'statistical', 'random', 'recent', 'scalefree', 'covering', 'routers', 'reviewing', 'nature', 'links', 'models', 'chemicals']),\n",
       " Row(words=['expressible', 'functional', 'find', 'utilize', 'programming', 'easily', 'sets', 'failures', 'clusters', 'processing', 'commodity', 'jobs', 'partitioning', 'terabytes', 'world', 'details', 'real', 'mapreduce', 'programmers', 'generate', 'input', 'execution', 'highly', 'style', 'cluster', 'system', 'model', 'scalable', 'machine', 'google', 'upwards', 'map', 'thousand', 'runtime', 'machines', 'implementation', 'scheduling', 'program', 'automatically', 'distributed', 'generating', 'computation', 'experience', 'written', 'parallelized', 'hundreds', 'large', 'programs', 'intermediate', 'communication', 'required', 'values', 'easy', 'takes', 'executed', 'runs', 'pairs', 'paper', 'implemented', 'merges', 'parallel', 'function', 'resources', 'systems', 'inter', 'pair', 'data', 'managing', 'care', 'shown', 'processes', 'typical', 'handling', 'tasks', 'users', 'reduce', 'simplified', 'thousands']),\n",
       " Row(words=['evaluating', 'finding', 'networks', 'structure', 'community']),\n",
       " Row(words=['rapid', 'working', 'place', 'simultaneously', 'retrieval', 'emergence', 'interoperable', 'accelerated', 'reading', 'trends', 'tools', 'fragments', '1980s', 'resources', 'future', 'advantage', 'read', 'content', 'environment', 'filter', 'publishing', 'current', 'engage', 'navigation', 'scientific', 'strategic', 'analyze', 'shaping', 'enhanced', 'link', 'indirect', 'intensified', 'literature', 'strategically', 'annotate', 'online', 'indexing', 'practices', 'search', 'transforming', 'ways', 'promised', 'scientists', 'scan', 'revolution', 'articles', 'evolution', 'digital', 'ontologies', 'disciplines', 'recent', 'increase', 'widespread', 'observed']),\n",
       " Row(words=['constraints', 'impose', 'fmri', 'resonance', 'mere', 'technology', 'study', 'methods', 'limitations', 'methodology', 'functional', 'physiological', 'magnetic', 'promise', 'neuroscience', 'cartography', 'forward', 'draw', 'acquisition', 'mainstay', 'drawn', 'protocols', 'brain', 'current', 'fundamental', 'data', 'state', 'questions', 'interpretation', 'abound', 'push', 'cognitive', 'give', 'scanner', 'advances', 'experimental', 'neuroimaging', 'conclusions', 'ignore', 'present', 'image', 'haemodynamic', 'analysis', 'design', 'understanding', 'actual', 'organization', 'true', 'imaging', 'signals', 'overview']),\n",
       " Row(words=['improve', 'computers', 'united', 'smartest', 'things', 'cars', 'bestseller', 'stove', 'fail', 'designers', 'classic', 'switch', 'rules', 'states', 'burner', 'doors', 'everyday', 'fault', 'examples', 'objects', 'feel', 'pull', 'inept', 'diverse', 'lies', 'slide', 'turns', 'usability', 'push', 'product', 'cognitive', 'psychology', 'figure', 'publisher', 'description', 'good', 'ignore', 'principles', 'light', 'work', 'aspects', 'design', 'users', 'designs', 'door', 'telephones', 'simple']),\n",
       " Row(words=['functions', 'genes', 'found', 'blocks', 'control', 'motifs', 'regulation', 'serve', 'review', 'recurring', 'expression', 'biological', 'suggesting', 'diverse', 'basic', 'network', 'building', 'emphasis', 'studies', 'organisms', 'neuronal', 'signalling', 'networks', 'experimental', 'theory', 'transcription', 'recently', 'made', 'patterns', 'approaches', 'small', 'wellstudied', 'including', 'microorganisms', 'called', 'bacteria', 'humans', 'mentioned']),\n",
       " Row(words=['gene', 'human', 'crucial', 'describe', 'genes', 'functions', 'catalogue', 'species', 'nonredundant', 'derived', 'intestinal', 'large', 'bacterial', 'prevalent', 'health', 'proportion', 'genome', 'overwhelming', 'complement', 'potential', 'define', 'genetic', 'characterization', 'sequence', 'individual', 'established', 'metagenome', 'harbours', 'larger', 'shared', 'indicating', 'frequent', 'million', 'times', 'sequencing', 'minimal', 'microbes', 'gigabases', 'majority', 'present', 'terms', 'wellbeing', 'individuals', 'metagenomic', 'assembly', 'entire', 'european', 'includes', 'assess', 'largely', 'faecal', 'samples', 'understand', 'cohort', 'bacteria', 'illuminabased', 'microbial', 'impact']),\n",
       " Row(words=['discipline', 'scientist', 'stages', 'types', 'scientific']),\n",
       " Row(words=['sequence', 'human', 'genome']),\n",
       " Row(words=['felt', 'writing', 'bloggers', 'provide', 'document', 'commentary', 'form', 'community', 'articulate', 'deeply', 'driven', 'maintain', 'express', 'ideas', 'lives', 'forums', 'opinions', 'blog', 'emotions']),\n",
       " Row(words=['guide', 'artificial', 'robotics', 'topics', 'learning', 'edition', 'agents', 'modern', 'intelligence', 'planning', 'approach', 'covering', 'problemsolving', 'logical', 'presents', 'intelligent', 'uncertainty']),\n",
       " Row(words=['annotations', 'gene', 'misuse', 'ontology']),\n",
       " Row(words=['interactome', 'functional', 'find', 'mutations', 'peripheral', 'products', 'tendency', 'network', 'single', 'linked', 'prediction', 'majority', 'disease', 'cancer', 'proteins', 'explains', 'show', 'role', 'interactions', 'observed', 'disorders', 'somatic', 'model', 'confirm', 'indicating', 'contrast', 'suggests', 'tissues', 'play', 'offers', 'higher', 'supporting', 'human', 'genes', 'existence', 'likelihood', 'central', 'origin', 'vast', 'widely', 'periphery', 'expressed', 'difference', 'common', 'disordergene', 'diseasespecific', 'graphtheoretic', 'modules', 'profiling', 'phenotype', 'gene', 'localized', 'platform', 'framework', 'expression', 'distinct', 'explore', 'genetic', 'similarity', 'encode', 'caused', 'physical', 'similar', 'transcripts', 'pattern', 'selectionbased', 'associations', 'nonessential', 'essential', 'diseases']),\n",
       " Row(words=['technique', 'double', 'tools', '1999', 'successful', 'deal', 'clustering', 'identifiable', 'edge', 'attention', 'include', 'features', 'networks', 'regression', 'great', 'updated', 'work', 'hidden', 'alchemy', 'heart', 'rules', 'technology', 'algorithms', 'references', 'methods', 'input', 'visualizationin', '2005', 'uncovered', 'information', 'witten', 'reports', 'miningincluding', 'machine', 'enhanced', 'revision', 'interactive', 'frank', 'weka', 'sigkdd', 'extract', 'body', 'section', 'algorithmic', 'output', 'reflect', 'including', 'recipients', 'burgeoning', 'thirty', 'workbench', 'secrets', 'learning', 'power', 'techniques', 'performance', 'comprehensive', 'preprocessing', 'exaggerated', 'core', 'enjoys', 'leading', 'edition', 'book', 'highlights', 'hype', 'neural', 'mining', 'years', 'authors', 'service', 'collection', 'setting', 'oceans', 'award', 'shows', 'interface', 'downloadable', 'true', 'place', 'classification', 'surrounded', 'bayesian', 'sections', 'eibe', 'commercial', 'practical', 'magic', 'basic', 'data', 'appeared', 'describes', 'association', 'remains', 'major', 'loose', 'transforming', 'tasks', 'improvement']),\n",
       " Row(words=['time', 'visualizing', 'tags']),\n",
       " Row(words=['tens', 'probabilities', 'evaluated', 'efficiently', 'algorithm', 'assemblies', 'userfriendly', 'correlated', 'reference', 'mapping', 'consensus', 'confidence', 'makes', 'sequencing', 'accurate', 'errors', 'short', 'calls', 'aligned', 'final', 'real', 'typically', 'build', 'algorithms', 'derive', 'information', 'model', 'issue', 'full', 'scores', 'position', 'probability', 'variants', 'simulated', 'shotgun', 'alignment', 'diploid', 'matepair', 'effectively', 'human', 'calling', 'lack', 'sourceforge', 'derived', 'read', 'accuracy', 'site', 'reads', 'estimates', 'aligning', 'pairs', 'sample', 'empirical', 'technologies', 'describe', 'concept', 'haplotypes', 'efficient', 'bayesian', 'freely', 'promise', 'incorporates', 'genome', 'qualities', 'genotype', 'data', 'software', 'sequence', 'produce', 'introduce', 'requires', 'http', 'major', 'versatile', 'sampling', 'handling', 'ambiguity', 'error', 'statistical', 'measure', 'quality', 'base']),\n",
       " Row(words=['applications', 'social', 'methods', 'network', 'sciences', 'analysis', 'structural']),\n",
       " Row(words=['foundations', 'language', 'statistical', 'natural', 'processing']),\n",
       " Row(words=['combining', 'social', 'filtering', 'collaborative', 'networks', 'referral', 'abstract'])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withoutStopWordsDF.select(\"words\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.1\n",
    "#Function to perform stemming.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemming(wordList):\n",
    "    wordSet = set(wordList)\n",
    "    wordSet = [stemmer.stem(word) for word in wordList]\n",
    "    return sorted(wordSet)\n",
    "\n",
    "#User defined function to perform stemming.\n",
    "udf_stemming = Func.udf(stemming, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.1\n",
    "#Performing stemming.\n",
    "stemmedWordsDF = withoutStopWordsDF.withColumn(\"stemmed_words\", udf_stemming(withoutStopWordsDF.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|single_word| df|\n",
      "+-----------+---+\n",
      "|   everyday|  2|\n",
      "|      input|  2|\n",
      "|    persist|  1|\n",
      "| likelihood|  3|\n",
      "|  geneannot|  1|\n",
      "|     import|  4|\n",
      "| photograph|  1|\n",
      "|       oper|  1|\n",
      "|    highest|  1|\n",
      "|      equal|  1|\n",
      "+-----------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Counting the number of papers (frequency) in which a particular word appears. (Document Frequency)\n",
    "explodedDF = stemmedWordsDF.select(stemmedWordsDF.paper_id,\\\n",
    "                                   Func.explode(stemmedWordsDF.stemmed_words).alias(\"single_word\"))\\\n",
    "                            .distinct()\\\n",
    "                            .groupBy(\"single_word\")\\\n",
    "                            .agg(Func.count(\"single_word\")\\\n",
    "                                 .alias(\"df\"))\n",
    "explodedDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1449"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explodedDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|single_word| df|\n",
      "+-----------+---+\n",
      "|   everyday|  2|\n",
      "|      input|  2|\n",
      "| likelihood|  3|\n",
      "|     import|  4|\n",
      "|    explain|  5|\n",
      "|    classif|  3|\n",
      "|     execut|  2|\n",
      "|        map|  4|\n",
      "|  character|  6|\n",
      "|      uncov|  4|\n",
      "+-----------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Counting the number of unique papers.\n",
    "#Setting the upper and lower bounds.\n",
    "uniqPaperCount = stemmedWordsDF.count()\n",
    "upperBoundary = 0.1*uniqPaperCount\n",
    "lowerBoundary = 2\n",
    "#filter the dataframe to select only words that appear in more than 20 papers and\n",
    "#less than 10 percent of the total number of papers.\n",
    "filteredDF = explodedDF.filter((explodedDF.df>=lowerBoundary) & (explodedDF.df<=upperBoundary))\n",
    "filteredDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|single_word|\n",
      "+-----------+\n",
      "|  scientist|\n",
      "|    suggest|\n",
      "|     articl|\n",
      "|      major|\n",
      "|       read|\n",
      "|     access|\n",
      "|    process|\n",
      "|       wide|\n",
      "|      basic|\n",
      "|     exampl|\n",
      "+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Limiting the number of important terms to 1000.\n",
    "termsDF = filteredDF.sort(\"df\", ascending=False).limit(1000).select(\"single_word\")\n",
    "termsDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|single_word|word_id|\n",
      "+-----------+-------+\n",
      "|    process|      0|\n",
      "|     access|      1|\n",
      "|    suggest|      2|\n",
      "|       wide|      3|\n",
      "|      major|      4|\n",
      "|       read|      5|\n",
      "|     articl|      6|\n",
      "|  scientist|      7|\n",
      "|      basic|      8|\n",
      "|     theori|      9|\n",
      "+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Generating an index to the top 1000 important terms starting with the index 0.\n",
    "termsDF = termsDF.withColumn(\"word_id\", Func.monotonically_increasing_id())\n",
    "\n",
    "wordWinSpec = Window.orderBy(\"word_id\")\n",
    "\n",
    "termsDF = termsDF.withColumn(\"word_id\",Func.row_number().over(wordWinSpec)-1)\n",
    "termsDF.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---+\n",
      "|paper_id|single_word| tf|\n",
      "+--------+-----------+---+\n",
      "| 1242600|     public|  1|\n",
      "| 1242600|       talk|  1|\n",
      "|      99|       call|  1|\n",
      "|  740681|       imit|  1|\n",
      "|  740681|      model|  1|\n",
      "|   99857|      degre|  1|\n",
      "| 3614773|      studi|  1|\n",
      "|  117535|   reweight|  1|\n",
      "| 4131662|   techniqu|  1|\n",
      "|  115158|     common|  1|\n",
      "+--------+-----------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Counting the number of times a word appeared in a particular paper (word count per paper) (term frequency).\n",
    "tempDF = stemmedWordsDF.select(stemmedWordsDF.paper_id,\\\n",
    "                               Func.explode(stemmedWordsDF.stemmed_words).alias(\"single_word\"))\\\n",
    "                        .groupBy(\"paper_id\", \"single_word\")\\\n",
    "                        .agg(Func.count(\"single_word\").alias(\"tf\"))\n",
    "tempDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|paper_id|            map_list|\n",
      "+--------+--------------------+\n",
      "|  115158|[[0 -> 1], [1 -> ...|\n",
      "|     101|[[0 -> 1], [3 -> ...|\n",
      "|  740681|[[0 -> 1], [14 ->...|\n",
      "| 4778506|[[0 -> 1], [1 -> ...|\n",
      "|  105906|[[0 -> 1], [69 ->...|\n",
      "|  212874|[[0 -> 1], [1 -> ...|\n",
      "|  430834|[[0 -> 2], [13 ->...|\n",
      "| 5394760|[[0 -> 1], [7 -> ...|\n",
      "| 4200367|[[0 -> 1], [5 -> ...|\n",
      "| 3721754|[[0 -> 1], [4 -> ...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Creating a dataframe with term index and count of the word per paper.\n",
    "#Creating a column containing a mapping of term index -> count per paper.\n",
    "joinedResult = tempDF.join(termsDF, \"single_word\").withColumn(\"map\", Func.create_map(\"word_id\", \"tf\"))\\\n",
    "                    .groupBy(\"paper_id\").agg(Func.collect_list(\"map\").alias(\"map_list\"))\n",
    "joinedResult.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.1\n",
    "#Function to create a sparse vector for each paper.\n",
    "def to_sparse(mapList):\n",
    "    index = []\n",
    "    value = []\n",
    "    for map_val in mapList:\n",
    "        for idx, val in map_val.items():\n",
    "            index.append(idx)\n",
    "            value.append(val)\n",
    "    return SparseVector(1000, index, value)\n",
    "\n",
    "#Creating a user defined function.\n",
    "udf_to_sparse = Func.udf(to_sparse, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=115158, tf_vector=SparseVector(1000, {0: 1.0, 1: 1.0, 12: 1.0, 13: 1.0, 21: 1.0, 22: 2.0, 24: 1.0, 27: 1.0, 31: 1.0, 37: 1.0, 43: 1.0, 51: 1.0, 55: 1.0, 62: 1.0, 69: 1.0, 70: 1.0, 76: 1.0, 79: 1.0, 80: 1.0, 84: 1.0, 90: 1.0, 92: 1.0, 95: 1.0, 105: 2.0, 109: 1.0, 110: 1.0, 111: 2.0, 113: 1.0, 117: 1.0, 120: 1.0, 127: 1.0, 130: 1.0, 145: 1.0, 162: 1.0, 164: 1.0, 169: 1.0, 174: 1.0, 180: 1.0, 202: 1.0, 209: 1.0, 219: 1.0, 222: 1.0, 224: 1.0, 239: 1.0, 245: 1.0, 246: 1.0, 258: 1.0, 263: 2.0, 270: 1.0, 275: 1.0, 317: 2.0, 329: 1.0, 336: 1.0, 339: 1.0, 355: 1.0, 364: 1.0, 369: 1.0, 377: 1.0, 384: 1.0, 387: 1.0, 405: 1.0, 411: 1.0, 418: 2.0, 423: 1.0, 436: 1.0, 440: 1.0, 450: 1.0, 454: 1.0, 459: 1.0, 471: 1.0, 500: 1.0, 534: 1.0, 570: 1.0, 583: 1.0, 584: 1.0, 596: 1.0, 614: 1.0}))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.1\n",
    "#Creating the bag of words dataframe.\n",
    "featurizedDataDF = joinedResult.select(\"paper_id\",\\\n",
    "                                   udf_to_sparse(joinedResult.map_list)\\\n",
    "                                   .alias(\"tf_vector\"))\n",
    "featurizedDataDF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2 (TF-IDF representation for the papers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.2\n",
    "#Calculate TF and IDF using spark functionalities.\n",
    "sparkMlIdf = IDF(inputCol=\"tf_vector\", outputCol=\"idf_vector\")\n",
    "sparkMlIdfModel = sparkMlIdf.fit(featurizedDataDF)\n",
    "IdfData = sparkMlIdfModel.transform(featurizedDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=115158, tf_vector=SparseVector(1000, {0: 1.0, 1: 1.0, 12: 1.0, 13: 1.0, 21: 1.0, 22: 2.0, 24: 1.0, 27: 1.0, 31: 1.0, 37: 1.0, 43: 1.0, 51: 1.0, 55: 1.0, 62: 1.0, 69: 1.0, 70: 1.0, 76: 1.0, 79: 1.0, 80: 1.0, 84: 1.0, 90: 1.0, 92: 1.0, 95: 1.0, 105: 2.0, 109: 1.0, 110: 1.0, 111: 2.0, 113: 1.0, 117: 1.0, 120: 1.0, 127: 1.0, 130: 1.0, 145: 1.0, 162: 1.0, 164: 1.0, 169: 1.0, 174: 1.0, 180: 1.0, 202: 1.0, 209: 1.0, 219: 1.0, 222: 1.0, 224: 1.0, 239: 1.0, 245: 1.0, 246: 1.0, 258: 1.0, 263: 2.0, 270: 1.0, 275: 1.0, 317: 2.0, 329: 1.0, 336: 1.0, 339: 1.0, 355: 1.0, 364: 1.0, 369: 1.0, 377: 1.0, 384: 1.0, 387: 1.0, 405: 1.0, 411: 1.0, 418: 2.0, 423: 1.0, 436: 1.0, 440: 1.0, 450: 1.0, 454: 1.0, 459: 1.0, 471: 1.0, 500: 1.0, 534: 1.0, 570: 1.0, 583: 1.0, 584: 1.0, 596: 1.0, 614: 1.0}), idf_vector=SparseVector(1000, {0: 2.1972, 1: 2.1972, 12: 2.2925, 13: 2.2925, 21: 2.2925, 22: 4.7958, 24: 2.3979, 27: 2.3979, 31: 2.3979, 37: 2.3979, 43: 2.5157, 51: 2.5157, 55: 2.5157, 62: 2.5157, 69: 2.6492, 70: 2.6492, 76: 2.6492, 79: 2.6492, 80: 2.6492, 84: 2.6492, 90: 2.6492, 92: 2.8034, 95: 2.8034, 105: 5.6067, 109: 2.8034, 110: 2.8034, 111: 5.6067, 113: 2.8034, 117: 2.8034, 120: 2.8034, 127: 2.8034, 130: 2.8034, 145: 2.8034, 162: 2.9857, 164: 2.9857, 169: 2.9857, 174: 2.9857, 180: 2.9857, 202: 2.9857, 209: 2.9857, 219: 2.9857, 222: 2.9857, 224: 2.9857, 239: 3.2088, 245: 3.2088, 246: 3.2088, 258: 3.2088, 263: 6.4177, 270: 3.2088, 275: 3.2088, 317: 6.4177, 329: 3.2088, 336: 3.2088, 339: 3.2088, 355: 3.2088, 364: 3.4965, 369: 3.4965, 377: 3.4965, 384: 3.4965, 387: 3.4965, 405: 3.4965, 411: 3.4965, 418: 6.993, 423: 3.4965, 436: 3.4965, 440: 3.4965, 450: 3.4965, 454: 3.4965, 459: 3.4965, 471: 3.4965, 500: 3.4965, 534: 3.4965, 570: 3.4965, 583: 3.4965, 584: 3.4965, 596: 3.4965, 614: 3.4965}))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Calculate TF and IDF using spark functionalities.\n",
    "IdfData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.2\n",
    "#Calculate TF and IDF using spark functionalities.\n",
    "def multiply_sparse_vec(x_vec, y_vec):\n",
    "    result = np.multiply(x_vec, y_vec).tolist()\n",
    "    vector_args = len(result), [i for i, x in enumerate(result) if x != 0], [x for x in result if x != 0] \n",
    "    return Vectors.sparse(*vector_args)\n",
    "\n",
    "udf_multi_sparse_vec = Func.udf(multiply_sparse_vec, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(paper_id=115158, tf_vector=SparseVector(1000, {0: 1.0, 1: 1.0, 12: 1.0, 13: 1.0, 21: 1.0, 22: 2.0, 24: 1.0, 27: 1.0, 31: 1.0, 37: 1.0, 43: 1.0, 51: 1.0, 55: 1.0, 62: 1.0, 69: 1.0, 70: 1.0, 76: 1.0, 79: 1.0, 80: 1.0, 84: 1.0, 90: 1.0, 92: 1.0, 95: 1.0, 105: 2.0, 109: 1.0, 110: 1.0, 111: 2.0, 113: 1.0, 117: 1.0, 120: 1.0, 127: 1.0, 130: 1.0, 145: 1.0, 162: 1.0, 164: 1.0, 169: 1.0, 174: 1.0, 180: 1.0, 202: 1.0, 209: 1.0, 219: 1.0, 222: 1.0, 224: 1.0, 239: 1.0, 245: 1.0, 246: 1.0, 258: 1.0, 263: 2.0, 270: 1.0, 275: 1.0, 317: 2.0, 329: 1.0, 336: 1.0, 339: 1.0, 355: 1.0, 364: 1.0, 369: 1.0, 377: 1.0, 384: 1.0, 387: 1.0, 405: 1.0, 411: 1.0, 418: 2.0, 423: 1.0, 436: 1.0, 440: 1.0, 450: 1.0, 454: 1.0, 459: 1.0, 471: 1.0, 500: 1.0, 534: 1.0, 570: 1.0, 583: 1.0, 584: 1.0, 596: 1.0, 614: 1.0}), idf_vector=SparseVector(1000, {0: 2.1972, 1: 2.1972, 12: 2.2925, 13: 2.2925, 21: 2.2925, 22: 4.7958, 24: 2.3979, 27: 2.3979, 31: 2.3979, 37: 2.3979, 43: 2.5157, 51: 2.5157, 55: 2.5157, 62: 2.5157, 69: 2.6492, 70: 2.6492, 76: 2.6492, 79: 2.6492, 80: 2.6492, 84: 2.6492, 90: 2.6492, 92: 2.8034, 95: 2.8034, 105: 5.6067, 109: 2.8034, 110: 2.8034, 111: 5.6067, 113: 2.8034, 117: 2.8034, 120: 2.8034, 127: 2.8034, 130: 2.8034, 145: 2.8034, 162: 2.9857, 164: 2.9857, 169: 2.9857, 174: 2.9857, 180: 2.9857, 202: 2.9857, 209: 2.9857, 219: 2.9857, 222: 2.9857, 224: 2.9857, 239: 3.2088, 245: 3.2088, 246: 3.2088, 258: 3.2088, 263: 6.4177, 270: 3.2088, 275: 3.2088, 317: 6.4177, 329: 3.2088, 336: 3.2088, 339: 3.2088, 355: 3.2088, 364: 3.4965, 369: 3.4965, 377: 3.4965, 384: 3.4965, 387: 3.4965, 405: 3.4965, 411: 3.4965, 418: 6.993, 423: 3.4965, 436: 3.4965, 440: 3.4965, 450: 3.4965, 454: 3.4965, 459: 3.4965, 471: 3.4965, 500: 3.4965, 534: 3.4965, 570: 3.4965, 583: 3.4965, 584: 3.4965, 596: 3.4965, 614: 3.4965}), tf_idf=SparseVector(1000, {0: 2.1972, 1: 2.1972, 12: 2.2925, 13: 2.2925, 21: 2.2925, 22: 9.5916, 24: 2.3979, 27: 2.3979, 31: 2.3979, 37: 2.3979, 43: 2.5157, 51: 2.5157, 55: 2.5157, 62: 2.5157, 69: 2.6492, 70: 2.6492, 76: 2.6492, 79: 2.6492, 80: 2.6492, 84: 2.6492, 90: 2.6492, 92: 2.8034, 95: 2.8034, 105: 11.2134, 109: 2.8034, 110: 2.8034, 111: 11.2134, 113: 2.8034, 117: 2.8034, 120: 2.8034, 127: 2.8034, 130: 2.8034, 145: 2.8034, 162: 2.9857, 164: 2.9857, 169: 2.9857, 174: 2.9857, 180: 2.9857, 202: 2.9857, 209: 2.9857, 219: 2.9857, 222: 2.9857, 224: 2.9857, 239: 3.2088, 245: 3.2088, 246: 3.2088, 258: 3.2088, 263: 12.8353, 270: 3.2088, 275: 3.2088, 317: 12.8353, 329: 3.2088, 336: 3.2088, 339: 3.2088, 355: 3.2088, 364: 3.4965, 369: 3.4965, 377: 3.4965, 384: 3.4965, 387: 3.4965, 405: 3.4965, 411: 3.4965, 418: 13.986, 423: 3.4965, 436: 3.4965, 440: 3.4965, 450: 3.4965, 454: 3.4965, 459: 3.4965, 471: 3.4965, 500: 3.4965, 534: 3.4965, 570: 3.4965, 583: 3.4965, 584: 3.4965, 596: 3.4965, 614: 3.4965}))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Calculate TF and IDF using spark functionalities.\n",
    "paperTfIdf = IdfData.withColumn(\"tf_idf\", udf_multi_sparse_vec(IdfData.tf_vector, IdfData.idf_vector))\n",
    "paperTfIdf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.2\n",
    "#Calculate TF and IDF without using spark functionalities.\n",
    "def calcIdf(m, docFreq):\n",
    "    Idf = math.log((m+1)/(docFreq+1))\n",
    "    return Idf\n",
    "\n",
    "udf_calcIdf = Func.udf(calcIdf, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---+\n",
      "|single_word|word_id| df|\n",
      "+-----------+-------+---+\n",
      "|    process|      0| 10|\n",
      "|     access|      1| 10|\n",
      "|    suggest|      2| 10|\n",
      "|       wide|      3| 10|\n",
      "|      major|      4| 10|\n",
      "|       read|      5| 10|\n",
      "|     articl|      6| 10|\n",
      "|  scientist|      7| 10|\n",
      "|      basic|      8| 10|\n",
      "|     theori|      9|  9|\n",
      "+-----------+-------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Calculate TF and IDF without using spark functionalities.\n",
    "termsWithDF = termsDF.join(filteredDF, \"single_word\")\n",
    "termsWithDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---+---------+\n",
      "|single_word|word_id| df|      idf|\n",
      "+-----------+-------+---+---------+\n",
      "|    process|      0| 10|2.2172253|\n",
      "|     access|      1| 10|2.2172253|\n",
      "|    suggest|      2| 10|2.2172253|\n",
      "|       wide|      3| 10|2.2172253|\n",
      "|      major|      4| 10|2.2172253|\n",
      "|       read|      5| 10|2.2172253|\n",
      "|     articl|      6| 10|2.2172253|\n",
      "|  scientist|      7| 10|2.2172253|\n",
      "|      basic|      8| 10|2.2172253|\n",
      "|     theori|      9|  9|2.3125355|\n",
      "+-----------+-------+---+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Calculate TF and IDF without using spark functionalities.\n",
    "termsWithIDF = termsWithDF.withColumn(\"idf\",\\\n",
    "                                            udf_calcIdf(Func.lit(uniqPaperCount), termsWithDF.df))\n",
    "\n",
    "termsWithIDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---+---------+---------+\n",
      "|single_word|word_id| df|      idf|   tf_idf|\n",
      "+-----------+-------+---+---------+---------+\n",
      "|    process|      0| 10|2.2172253|22.172253|\n",
      "|     access|      1| 10|2.2172253|22.172253|\n",
      "|    suggest|      2| 10|2.2172253|22.172253|\n",
      "|       wide|      3| 10|2.2172253|22.172253|\n",
      "|      major|      4| 10|2.2172253|22.172253|\n",
      "|       read|      5| 10|2.2172253|22.172253|\n",
      "|     articl|      6| 10|2.2172253|22.172253|\n",
      "|  scientist|      7| 10|2.2172253|22.172253|\n",
      "|      basic|      8| 10|2.2172253|22.172253|\n",
      "|     theori|      9|  9|2.3125355| 20.81282|\n",
      "+-----------+-------+---+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.2\n",
    "#Calculate TF and IDF without using spark functionalities.\n",
    "termsWithTfIdf = termsWithIDF.withColumn(\"tf_idf\", termsWithIDF.df*termsWithIDF.idf)\n",
    "termsWithTfIdf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3 (Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_hash_id='d0c9aaa788153daeaf1f1538b3d46bbb', user_library='2080631,6343346,5184704,7756088,2653863,6607628,4236212,1277953,226864,3140015,8806369,311570,5687747,767516,4781370,2841637,2445106,1959511,2688186,2363430,6614346,853030,5336762,4226226,239571,4089758,4140337,913868,7562861,3190274,2782576,12571584,2049617,5761055,5441098,3466838,2080691,1805577,7570111,5760287,2855355,3281547,1012525,3512183,678653'),\n",
       " Row(user_hash_id='ca4f1ba4094011d9a8757b1bfcadae5b', user_library='278019'),\n",
       " Row(user_hash_id='d1d41a152019155035ceb2db7d331c44', user_library='6610569,6493797,6609079,7469737,7469738,6609102,6610585,6609240,7499869,7364976,7427963,7364991,7499833,7363519,7364990,6609245,7499801,7364971,7363487,7363498,7364986,7363513,7363526,6609222,7241881,6609169,7364933,6609099,1153734,7499794,3105791,6743243,6609238,7469728,7465494,7329626,1391614,1397150,7364987,7339380,7364973,7364823'),\n",
       " Row(user_hash_id='f2f77383828ea6d39438e525e40d54ba', user_library='943458,238121,763429'),\n",
       " Row(user_hash_id='9c883d02115400f7bab2c3f27d121d7c', user_library='3509971,3509965,2336685,2081051,3365220,1595391,4174633,1734381,3878624'),\n",
       " Row(user_hash_id='1eac022a97d683eace8815545ce3153f', user_library='3973229,322433,5732042,8004203,421656,3106933,1121661,368203,6439894,12786786,1421977,9801263,13116791,2649917,921555,5317666,3919640,6915167,431014,6874387,1283783,12804731,563003,5777260,7152677,9730873,3169,484976,927157,12306465,3088458,3501508,7812276,3469193,1375462,4566381,238188,349730,210441,12942060,877556,9676804,387967,1042553,854207,8759544,7459440,7083832,1044819,2886520,388250,14068158,13458037,3469180,9559119,8380800,7680635,604166,3104003,8310640,707256,7888067,440894,4466244,3108251,3285218,2193077,3442598,691774,6614713,10611025,8465912,1151050,3225867,1031887,6967343,7072014,13553940,276166,423124,1457751,989242,833661,3945377,707273,12827868,10049547,3272953,7273281,9853751,8299207,6392873,10408179,615665,13126861,388199,9125124,12335203,750835,507246,7278363,6868738,12552322,2971783,9782549,1311268,7708598,2551277,878839,486957,2822072,4146263,4191871,2348956,878840,12798026,3859628,10610321,8020025,2036373,5692630,1427756,259037,388242,10007880,1370957,10613615,8240584,7878119,363455,116929,216611,11784956,1610761,10751725,13644093,10259199,10783113,1256312,280456,1030272,4520149,472232,1362884,9336752,177002,2500019,12737160,423382,5431922,163457,1128173,10714063,417678,12688649,82240,6604119,613232,970776,2514768,854459,226925,2231283,3332739,339200,118812,898520,8667279,1121,3090210,1242600,2894296,10010024,6703484,1928278,5230237,5234060,2187038,6334982,7562461,4868207,878528,766338,3884819,9599338,2214041,7179733,7572779,1509584,5190745,1860938,1065281,4222657,600359,6130269,3245965,5455140,77208,2801583,1699344,1938374,9394991,4787859,658723,12904793,6729137,591475,5632727,480374,11840356,494240,523772,877557,4133700,1666572,5183399,531300,11044613,9428511,315242,332173,2214031,2766991,6076091,677408,9491406,1986332,220803,1065319,1475365,6478477,1290688,1095192,6627520,1693394,1544695,10615526,10430893,12729502,7802517,1036197,13833129,543467,6600388,4860650,2105931,2794764,4186128,8322198,12850545,1016800,5403808,6452999,423237,2283010,241163,938091,993264,3428435,14120961,1091573,310636,9182771,854229,4067576,2492402,1069581,9485312,7736741,1248423,10686202,10282613,329170,3910339,1572527,3095772,441788,3035664,3358506,2999253,749845,10806038,423869,255030,6819940,6331715,8277825,1046016,436811,9183673,2481463,13223978,4544623,6804489,2297448,976006,3010240,2214204,9153283,9778927,5489039,9919599,447594,7179444,13242469,2734016,5127341,3072883,3206364,4163167,423130,10690995,225127,308879,2058201,2970470,322839,2180593,3558378,4099915,1786202,2525144,6351182,6056800,8222585,3123957,527579,4191928,7094068,9888308,1786213,8240785,1536548,6564060')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usersDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|        user_hash_id|paper_id|\n",
      "+--------------------+--------+\n",
      "|d0c9aaa788153daea...| 2080631|\n",
      "|d0c9aaa788153daea...| 6343346|\n",
      "|d0c9aaa788153daea...| 5184704|\n",
      "|d0c9aaa788153daea...| 7756088|\n",
      "|d0c9aaa788153daea...| 2653863|\n",
      "|d0c9aaa788153daea...| 6607628|\n",
      "|d0c9aaa788153daea...| 4236212|\n",
      "|d0c9aaa788153daea...| 1277953|\n",
      "|d0c9aaa788153daea...|  226864|\n",
      "|d0c9aaa788153daea...| 3140015|\n",
      "|d0c9aaa788153daea...| 8806369|\n",
      "|d0c9aaa788153daea...|  311570|\n",
      "|d0c9aaa788153daea...| 5687747|\n",
      "|d0c9aaa788153daea...|  767516|\n",
      "|d0c9aaa788153daea...| 4781370|\n",
      "|d0c9aaa788153daea...| 2841637|\n",
      "|d0c9aaa788153daea...| 2445106|\n",
      "|d0c9aaa788153daea...| 1959511|\n",
      "|d0c9aaa788153daea...| 2688186|\n",
      "|d0c9aaa788153daea...| 2363430|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3\n",
    "explodUsersDF = usersDF.select(usersDF.user_hash_id,\\\n",
    "                               Func.explode(Func.split(usersDF.user_library, \",\"))\\\n",
    "                               .alias(\"paper_id\"))\n",
    "explodUsersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explodUsersDF.select(\"user_hash_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|paper_id|        user_hash_id|           tf_vector|          idf_vector|              tf_idf|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "| 1242600|1eac022a97d683eac...|(1000,[1,7,15,67,...|(1000,[1,7,15,67,...|(1000,[1,7,15,67,...|\n",
      "|  255030|1eac022a97d683eac...|(1000,[1,30,47,72...|(1000,[1,30,47,72...|(1000,[1,30,47,72...|\n",
      "|  238188|1eac022a97d683eac...|(1000,[3,11,24,36...|(1000,[3,11,24,36...|(1000,[3,11,24,36...|\n",
      "| 1042553|1eac022a97d683eac...|(1000,[12,18,23,3...|(1000,[12,18,23,3...|(1000,[12,18,23,3...|\n",
      "| 2058201|1eac022a97d683eac...|(1000,[27,159,337...|(1000,[27,159,337...|(1000,[27,159,337...|\n",
      "| 2492402|1eac022a97d683eac...|(1000,[29,219,359...|(1000,[29,219,359...|(1000,[29,219,359...|\n",
      "|  523772|1eac022a97d683eac...|(1000,[79,135],[1...|(1000,[79,135],[2...|(1000,[79,135],[2...|\n",
      "| 3010240|1eac022a97d683eac...|  (1000,[153],[1.0])|(1000,[153],[2.98...|(1000,[153],[2.98...|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3\n",
    "joinUserPaperTfIdf = explodUsersDF.join(paperTfIdf, \"paper_id\")\n",
    "joinUserPaperTfIdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 3.3\n",
    "joinUserPaperTfIdf.select(\"user_hash_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        user_hash_id|         tf_idf_list|\n",
      "+--------------------+--------------------+\n",
      "|1eac022a97d683eac...|[(1000,[1,7,15,67...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3.3\n",
    "userTfIdfList = joinUserPaperTfIdf.groupBy(\"user_hash_id\").agg(Func.collect_list(\"tf_idf\").alias(\"tf_idf_list\"))\n",
    "userTfIdfList.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_hash_id='1eac022a97d683eace8815545ce3153f', tf_idf_list=[SparseVector(1000, {1: 2.1972, 7: 2.1972, 15: 2.2925, 67: 2.6492, 69: 2.6492, 77: 2.6492, 81: 2.6492, 110: 2.8034, 148: 2.8034, 192: 2.9857, 220: 2.9857, 224: 2.9857, 301: 3.2088, 304: 3.2088, 309: 3.2088, 310: 3.2088, 314: 3.2088, 379: 3.4965, 386: 3.4965, 398: 3.4965, 407: 3.4965, 519: 3.4965, 578: 3.4965, 580: 3.4965, 590: 3.4965, 610: 3.4965, 616: 3.4965}), SparseVector(1000, {1: 2.1972, 30: 2.3979, 47: 2.5157, 72: 2.6492, 75: 2.6492, 120: 2.8034, 223: 2.9857, 280: 3.2088, 291: 3.2088, 332: 3.2088, 335: 3.2088, 518: 3.4965, 575: 3.4965}), SparseVector(1000, {3: 2.1972, 11: 2.2925, 24: 9.5916, 36: 2.3979, 58: 2.5157, 62: 2.5157, 66: 2.6492, 75: 2.6492, 84: 10.5968, 104: 2.8034, 122: 2.8034, 129: 2.8034, 141: 11.2134, 147: 2.8034, 149: 2.8034, 155: 2.9857, 166: 2.9857, 175: 2.9857, 187: 11.9427, 205: 2.9857, 212: 2.9857, 218: 2.9857, 228: 2.9857, 235: 12.8353, 259: 12.8353, 274: 3.2088, 290: 3.2088, 363: 3.4965, 410: 13.986, 424: 3.4965, 480: 3.4965, 513: 3.4965, 540: 3.4965, 551: 3.4965, 562: 3.4965, 563: 3.4965, 566: 3.4965, 571: 3.4965, 576: 3.4965}), SparseVector(1000, {12: 2.2925, 18: 2.2925, 23: 9.5916, 31: 2.3979, 58: 2.5157, 61: 10.0627, 71: 2.6492, 107: 2.8034, 113: 11.2134, 117: 11.2134, 157: 2.9857, 166: 2.9857, 171: 2.9857, 184: 11.9427, 194: 2.9857, 217: 2.9857, 221: 2.9857, 276: 3.2088, 296: 3.2088, 313: 3.2088, 341: 3.2088, 376: 13.986, 389: 3.4965, 426: 3.4965, 476: 3.4965, 535: 3.4965, 566: 3.4965, 582: 3.4965}), SparseVector(1000, {27: 2.3979, 159: 2.9857, 337: 3.2088, 448: 3.4965}), SparseVector(1000, {29: 2.3979, 219: 2.9857, 359: 3.2088, 383: 3.4965}), SparseVector(1000, {79: 2.6492, 135: 2.8034}), SparseVector(1000, {153: 2.9857})])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userTfIdfList.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3\n",
    "#\n",
    "def add_sparse_vec(vector_list):\n",
    "    result = SparseVector(1000, list(range(0, 1000)), np.zeros(1000))\n",
    "    for vector in vector_list:\n",
    "        result = np.add(result, vector).tolist()\n",
    "\n",
    "    vector_args = len(result), [i for i, x in enumerate(result) if x != 0], [x for x in result if x != 0] \n",
    "    return Vectors.sparse(*vector_args)\n",
    "    \n",
    "udf_add_sparse_vec = Func.udf(add_sparse_vec, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(user_hash_id='1eac022a97d683eace8815545ce3153f', features=SparseVector(1000, {1: 4.3944, 3: 2.1972, 7: 2.1972, 11: 2.2925, 12: 2.2925, 15: 2.2925, 18: 2.2925, 23: 9.5916, 24: 9.5916, 27: 2.3979, 29: 2.3979, 30: 2.3979, 31: 2.3979, 36: 2.3979, 47: 2.5157, 58: 5.0314, 61: 10.0627, 62: 2.5157, 66: 2.6492, 67: 2.6492, 69: 2.6492, 71: 2.6492, 72: 2.6492, 75: 5.2984, 77: 2.6492, 79: 2.6492, 81: 2.6492, 84: 10.5968, 104: 2.8034, 107: 2.8034, 110: 2.8034, 113: 11.2134, 117: 11.2134, 120: 2.8034, 122: 2.8034, 129: 2.8034, 135: 2.8034, 141: 11.2134, 147: 2.8034, 148: 2.8034, 149: 2.8034, 153: 2.9857, 155: 2.9857, 157: 2.9857, 159: 2.9857, 166: 5.9714, 171: 2.9857, 175: 2.9857, 184: 11.9427, 187: 11.9427, 192: 2.9857, 194: 2.9857, 205: 2.9857, 212: 2.9857, 217: 2.9857, 218: 2.9857, 219: 2.9857, 220: 2.9857, 221: 2.9857, 223: 2.9857, 224: 2.9857, 228: 2.9857, 235: 12.8353, 259: 12.8353, 274: 3.2088, 276: 3.2088, 280: 3.2088, 290: 3.2088, 291: 3.2088, 296: 3.2088, 301: 3.2088, 304: 3.2088, 309: 3.2088, 310: 3.2088, 313: 3.2088, 314: 3.2088, 332: 3.2088, 335: 3.2088, 337: 3.2088, 341: 3.2088, 359: 3.2088, 363: 3.4965, 376: 13.986, 379: 3.4965, 383: 3.4965, 386: 3.4965, 389: 3.4965, 398: 3.4965, 407: 3.4965, 410: 13.986, 424: 3.4965, 426: 3.4965, 448: 3.4965, 476: 3.4965, 480: 3.4965, 513: 3.4965, 518: 3.4965, 519: 3.4965, 535: 3.4965, 540: 3.4965, 551: 3.4965, 562: 3.4965, 563: 3.4965, 566: 6.993, 571: 3.4965, 575: 3.4965, 576: 3.4965, 578: 3.4965, 580: 3.4965, 582: 3.4965, 590: 3.4965, 610: 3.4965, 616: 3.4965}))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userProfileDF = userTfIdfList.withColumn(\"features\", udf_add_sparse_vec(userTfIdfList.tf_idf_list))\n",
    "userFeaturesDF = userProfileDF.select(\"user_hash_id\", \"features\")\n",
    "userFeaturesDF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.3 b)\n",
    "kmeans = KMeans(featuresCol=\"features\", k=50, maxIter=10)\n",
    "userClusterModel = kmeans.fit(userFeaturesDF.select(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          4.39444915  0.          2.19722458  0.          0.\n",
      "  0.          2.19722458  0.          0.          0.          2.29253476\n",
      "  2.29253476  0.          0.          2.29253476  0.          0.\n",
      "  2.29253476  0.          0.          0.          0.          9.59158109\n",
      "  9.59158109  0.          0.          2.39789527  0.          2.39789527\n",
      "  2.39789527  2.39789527  0.          0.          0.          0.\n",
      "  2.39789527  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          2.51567831\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          5.03135662  0.\n",
      "  0.         10.06271323  2.51567831  0.          0.          0.\n",
      "  2.6492097   2.6492097   0.          2.6492097   0.          2.6492097\n",
      "  2.6492097   0.          0.          5.2984194   0.          2.6492097\n",
      "  0.          2.6492097   0.          2.6492097   0.          0.\n",
      " 10.5968388   0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          2.80336038  0.          0.          2.80336038\n",
      "  0.          0.          2.80336038  0.          0.         11.21344152\n",
      "  0.          0.          0.         11.21344152  0.          0.\n",
      "  2.80336038  0.          2.80336038  0.          0.          0.\n",
      "  0.          0.          0.          2.80336038  0.          0.\n",
      "  0.          0.          0.          2.80336038  0.          0.\n",
      "  0.          0.          0.         11.21344152  0.          0.\n",
      "  0.          0.          0.          2.80336038  2.80336038  2.80336038\n",
      "  0.          0.          0.          2.98568194  0.          2.98568194\n",
      "  0.          2.98568194  0.          2.98568194  0.          0.\n",
      "  0.          0.          0.          0.          5.97136388  0.\n",
      "  0.          0.          0.          2.98568194  0.          0.\n",
      "  0.          2.98568194  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         11.94272775  0.\n",
      "  0.         11.94272775  0.          0.          0.          0.\n",
      "  2.98568194  0.          2.98568194  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          2.98568194  0.          0.          0.          0.\n",
      "  0.          0.          2.98568194  0.          0.          0.\n",
      "  0.          2.98568194  2.98568194  2.98568194  2.98568194  2.98568194\n",
      "  0.          2.98568194  2.98568194  0.          0.          0.\n",
      "  2.98568194  0.          0.          0.          0.          0.\n",
      "  0.         12.83530196  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         12.83530196  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          3.20882549  0.\n",
      "  3.20882549  0.          0.          0.          3.20882549  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          3.20882549  3.20882549  0.          0.\n",
      "  0.          0.          3.20882549  0.          0.          0.\n",
      "  0.          3.20882549  0.          0.          3.20882549  0.\n",
      "  0.          0.          0.          3.20882549  3.20882549  0.\n",
      "  0.          3.20882549  3.20882549  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          3.20882549  0.          0.          3.20882549\n",
      "  0.          3.20882549  0.          0.          0.          3.20882549\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          3.20882549\n",
      "  0.          0.          0.          3.49650756  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         13.98603025  0.\n",
      "  0.          3.49650756  0.          0.          0.          3.49650756\n",
      "  0.          0.          3.49650756  0.          0.          3.49650756\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          3.49650756  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          3.49650756\n",
      "  0.          0.         13.98603025  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          3.49650756  0.\n",
      "  3.49650756  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          3.49650756  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          3.49650756  0.          0.          0.\n",
      "  3.49650756  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          3.49650756  0.          0.\n",
      "  0.          0.          3.49650756  3.49650756  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          3.49650756  0.          0.          0.          0.\n",
      "  3.49650756  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          3.49650756\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          3.49650756  3.49650756\n",
      "  0.          0.          6.99301512  0.          0.          0.\n",
      "  0.          3.49650756  0.          0.          0.          3.49650756\n",
      "  3.49650756  0.          3.49650756  0.          3.49650756  0.\n",
      "  3.49650756  0.          0.          0.          0.          0.\n",
      "  0.          0.          3.49650756  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          3.49650756  0.\n",
      "  0.          0.          0.          0.          3.49650756  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "userCenters = userClusterModel.clusterCenters()\n",
    "for center in userCenters:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.        ,  4.39444915,  0.        ,  2.19722458,  0.        ,\n",
       "         0.        ,  0.        ,  2.19722458,  0.        ,  0.        ,\n",
       "         0.        ,  2.29253476,  2.29253476,  0.        ,  0.        ,\n",
       "         2.29253476,  0.        ,  0.        ,  2.29253476,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  9.59158109,  9.59158109,\n",
       "         0.        ,  0.        ,  2.39789527,  0.        ,  2.39789527,\n",
       "         2.39789527,  2.39789527,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  2.39789527,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  2.51567831,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  5.03135662,  0.        ,\n",
       "         0.        , 10.06271323,  2.51567831,  0.        ,  0.        ,\n",
       "         0.        ,  2.6492097 ,  2.6492097 ,  0.        ,  2.6492097 ,\n",
       "         0.        ,  2.6492097 ,  2.6492097 ,  0.        ,  0.        ,\n",
       "         5.2984194 ,  0.        ,  2.6492097 ,  0.        ,  2.6492097 ,\n",
       "         0.        ,  2.6492097 ,  0.        ,  0.        , 10.5968388 ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  2.80336038,\n",
       "         0.        ,  0.        ,  2.80336038,  0.        ,  0.        ,\n",
       "         2.80336038,  0.        ,  0.        , 11.21344152,  0.        ,\n",
       "         0.        ,  0.        , 11.21344152,  0.        ,  0.        ,\n",
       "         2.80336038,  0.        ,  2.80336038,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  2.80336038,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         2.80336038,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , 11.21344152,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  2.80336038,  2.80336038,  2.80336038,\n",
       "         0.        ,  0.        ,  0.        ,  2.98568194,  0.        ,\n",
       "         2.98568194,  0.        ,  2.98568194,  0.        ,  2.98568194,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  5.97136388,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  2.98568194,  0.        ,  0.        ,  0.        ,\n",
       "         2.98568194,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        , 11.94272775,\n",
       "         0.        ,  0.        , 11.94272775,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  2.98568194,  0.        ,  2.98568194,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         2.98568194,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  2.98568194,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  2.98568194,  2.98568194,  2.98568194,\n",
       "         2.98568194,  2.98568194,  0.        ,  2.98568194,  2.98568194,\n",
       "         0.        ,  0.        ,  0.        ,  2.98568194,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        12.83530196,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        , 12.83530196,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  3.20882549,\n",
       "         0.        ,  3.20882549,  0.        ,  0.        ,  0.        ,\n",
       "         3.20882549,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         3.20882549,  3.20882549,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  3.20882549,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  3.20882549,  0.        ,  0.        ,  3.20882549,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  3.20882549,\n",
       "         3.20882549,  0.        ,  0.        ,  3.20882549,  3.20882549,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  3.20882549,  0.        ,  0.        ,\n",
       "         3.20882549,  0.        ,  3.20882549,  0.        ,  0.        ,\n",
       "         0.        ,  3.20882549,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  3.20882549,\n",
       "         0.        ,  0.        ,  0.        ,  3.49650756,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , 13.98603025,  0.        ,  0.        ,  3.49650756,\n",
       "         0.        ,  0.        ,  0.        ,  3.49650756,  0.        ,\n",
       "         0.        ,  3.49650756,  0.        ,  0.        ,  3.49650756,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  3.49650756,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  3.49650756,  0.        ,  0.        ,\n",
       "        13.98603025,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  3.49650756,\n",
       "         0.        ,  3.49650756,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  3.49650756,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  3.49650756,  0.        ,  0.        ,  0.        ,\n",
       "         3.49650756,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  3.49650756,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  3.49650756,  3.49650756,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         3.49650756,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         3.49650756,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  3.49650756,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  3.49650756,  3.49650756,  0.        ,\n",
       "         0.        ,  6.99301512,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  3.49650756,  0.        ,  0.        ,  0.        ,\n",
       "         3.49650756,  3.49650756,  0.        ,  3.49650756,  0.        ,\n",
       "         3.49650756,  0.        ,  3.49650756,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         3.49650756,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         3.49650756,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  3.49650756,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userCenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(user_hash_id='1eac022a97d683eace8815545ce3153f', prediction=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userTransformed = userClusterModel.transform(userFeaturesDF).select('user_hash_id', 'prediction')\n",
    "userTransformed.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
