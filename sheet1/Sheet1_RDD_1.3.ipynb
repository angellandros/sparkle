{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.appName(\"myApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading txt and csv files\n",
    "userLibRDD = sparkSession.sparkContext.textFile(\"./mod_users_libraries.txt\")\n",
    "stopWordRDD = sparkSession.sparkContext.textFile(\"./stopwords_en.txt\")\n",
    "papersRDD = sparkSession.sparkContext.textFile(\"./papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('28d3f81251d94b09735497477a5e4e02',\n",
       "  '3929762,503574,5819422,4238883,5788061,462949,635215,635216,4810441,3481823,4165233,3366480,5984302,4238942,5490453,4636156,5996865,4194836,5828780,4450195'),\n",
       " ('d0c9aaa788153daeaf1f1538b3d46bbb',\n",
       "  '2080631,6343346,5184704,7756088,2653863,6607628,4236212,1277953,226864,3140015,8806369,311570,5687747,767516,4781370,2841637,2445106,1959511,2688186,2363430,6614346,853030,5336762,4226226,239571,4089758,4140337,913868,7562861,3190274,2782576,12571584,2049617,5761055,5441098,3466838,2080691,1805577,7570111,5760287,2855355,3281547,1012525,3512183,678653'),\n",
       " ('f05bcffe7951de9e5a32fff4a42eb088',\n",
       "  '1158654,478707,12054725,6670515,781057,13329754,781055,2825,553840,988447,12938139,12037219,12788583,6595566,4027225,920055,3129258,2242776,3112352,144287,706033,525396,622633,9172127,7357993,230211,12790816,503161,12937120,942241,166220,8493138,2945819,227173,11191048,949352,227096,11597926,921623,833638,882809,12738996,99,7154210,11852474,72879,10723139,11923443,2862386,802634,1959297,4488840,12735311,5102465,5177766,129,1467354,9396734,494336,1218954,2783692,3733678,8423325,1160828,507529,7804574,5201306,407124,504896,3281478,13687046,6486855,13207613,11847579,556147,10362464,6531882,714977,9946377,742892,3044720,1119267,7538620,1426283,2705980,9001253,2121165,12901647,10462916,261290,921399,6196237,5664256,213068,1769635,1055600,876703,7623038,1405472,1320137,4492925,1825619,13408130,1320157,682116,405672,1814546,12732269,12394780,1835552,9445526,80543,5413529,1153421,12925513,1784935,10802130,6305290,3856997,2097471,3748163,12796779,878326,10098153,3459771,827938,4448813,239,9478785,579614,7355647,423698,556149,13329593,9081047,151946,1453872,671699,3578102,137486,703464,9069826,622635,6792520,247,12716731,10045309,255030,968854,3726797,8217163,2945839,11839662,10685085,2308256,9983047,10138849,11536389,945310,1036492,6982661,1649749,1325105,6475030,488726,100186,3317637,336911,1321106,3956160')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 1.2\n",
    "#Creating a pair RDD from user_libraries.txt using the user hash as key and liked paper(s) as value(s)\n",
    "userRatingsRDD = userLibRDD.map(lambda line: (line.split(';')[0], line.split(';')[1]))\n",
    "#Displaying first 'n' elements of dataset\n",
    "userRatingsRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('28d3f81251d94b09735497477a5e4e02', '3929762'),\n",
       " ('28d3f81251d94b09735497477a5e4e02', '503574'),\n",
       " ('28d3f81251d94b09735497477a5e4e02', '5819422'),\n",
       " ('28d3f81251d94b09735497477a5e4e02', '4238883'),\n",
       " ('28d3f81251d94b09735497477a5e4e02', '5788061')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 1.2\n",
    "#converting the user ratings into key value pairs rdd\n",
    "userRatingsList = []\n",
    "for keyListVal in userRatingsRDD.collect():\n",
    "    for val in keyListVal[1].split(','):\n",
    "        userRatingsList.append((keyListVal[0],val))\n",
    "\n",
    "#print(userRatingsList[0:30])  \n",
    "userRatingsKVPairRDD = sparkSession.sparkContext.parallelize(userRatingsList)\n",
    "userRatingsKVPairRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('80546',\n",
       "  '\"the genetic code has been regarded as arbitrary in the sense that the codon-amino acid assignments could be different than they actually are. this general idea has been spelled out differently by previous, often rather implicit accounts of arbitrariness. they have drawn on the frozen accident theory, on evolutionary contingency, on alternative causal pathways, and on the absence of direct stereochemical interactions between codons and amino acids. it has also been suggested that the arbitrariness of the genetic code justifies attributing semantic information to macromolecules, notably to {dna}. i argue that these accounts of arbitrariness are unsatisfactory. i propose that the code is arbitrary in the sense of jacques monod\\'s concept of chemical arbitrariness: the genetic code is arbitrary in that any codon requires certain chemical and structural properties to specify a particular amino acid, but these properties are not required in virtue of a principle of chemistry. this notion of arbitrariness is compatible with several recent hypotheses about code evolution. i maintain that the code\\'s chemical arbitrariness is neither sufficient nor necessary for attributing semantic information to nucleic acids.\"'),\n",
       " ('5842862',\n",
       "  '\"choosing good problems is essential for being a good scientist. but what is a good problem, and how do you choose one? the subject is not usually discussed explicitly within our profession. scientists are expected to be smart enough to figure it out on their own and through the observation of their teachers. this lack of explicit discussion leaves a vacuum that can lead to approaches such as choosing problems that can give results that merit publication in valued journals, resulting in a job and tenure.\"'),\n",
       " ('1242600',\n",
       "  '\"although scientists typically insist that their research is very exciting and adventurous when they talk to laymen and prospective students, the allure of this enthusiasm is too often lost in the predictable, stilted structure and language of their scientific publications. i present here, a top-10 list of recommendations for how to write consistently boring scientific publications. i then discuss why we should and how we could make these contributions more accessible and exciting.\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 1.2\n",
    "#Creating a pair RDD from papers.csv using paper_id as key and abstract as value(s)\n",
    "paperTermsRDD = papersRDD.map(lambda line: (line.split(',')[0], \",\".join(line.split(',')[14:])))\n",
    "#Displaying first 'n' elements of dataset\n",
    "#saveRDDAsFile(paperTermsRDD)\n",
    "paperTermsRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 1.3\n",
    "#Creating a broadcast variable holding stop words\n",
    "stopWordBrdcast = sparkSession.sparkContext.broadcast(stopWordRDD.collect())\n",
    "#stopWordBrdcast.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3929762', '28d3f81251d94b09735497477a5e4e02'),\n",
       " ('503574', '28d3f81251d94b09735497477a5e4e02'),\n",
       " ('5819422', '28d3f81251d94b09735497477a5e4e02'),\n",
       " ('4238883', '28d3f81251d94b09735497477a5e4e02'),\n",
       " ('5788061', '28d3f81251d94b09735497477a5e4e02')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exercise 1.3\n",
    "#Create RDD containg the paperID as key and UserHash as value\n",
    "paperIDUserHashRDD = userRatingsKVPairRDD.map(lambda x: (x[1], x[0]))\n",
    "paperIDUserHashRDD.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('503574', ('28d3f81251d94b09735497477a5e4e02', '')),\n",
       " ('5788061',\n",
       "  ('28d3f81251d94b09735497477a5e4e02',\n",
       "   ' and its impact on {cmb} anomalies\",')),\n",
       " ('4238942', ('28d3f81251d94b09735497477a5e4e02', '')),\n",
       " ('5490453', ('28d3f81251d94b09735497477a5e4e02', '')),\n",
       " ('5828780', ('28d3f81251d94b09735497477a5e4e02', ''))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Join to match paperIDUserHashRDD with paperTermsRDD\n",
    "joinResultRDD = paperIDUserHashRDD.join(paperTermsRDD)\n",
    "joinResultRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('f05bcffe7951de9e5a32fff4a42eb088',\n",
       "  'a protein domain database for functional characterization and annotation.\",\"{prosite} consists of documentation entries describing protein domains, families and functional sites, as well as associated patterns and profiles to identify them. it is complemented by {prorule}, a collection of rules based on profiles and patterns, which increases the discriminatory power of these profiles and patterns by providing additional information about functionally and/or structurally critical amino acids. {prosite} is largely used for the annotation of domain features of {uniprotkb}/{swiss-prot} entries. among the 983 ({dna}-binding) domains, repeats and zinc fingers present in {swiss-prot} (release 57.8 of 22 september 2009), 696 ( approximately 70\\\\%) are annotated with {prosite} descriptors using information from {prorule}. in order to allow better functional characterization of domains, {prosite} developments focus on subfamily specific profiles and a new profile building method giving more weight to functionally important residues. here, we describe {amsa}, an annotated multiple sequence alignment format used to build a new generation of generalized profiles, the migration of {scanprosite} to {vital-it}, a cluster of 633 {cpus}, and the adoption of the distributed annotation system ({das}) to facilitate {prosite} data integration and interchange with other sources. the latest version of {prosite} (release 20.54, of 22 september 2009) contains 1308 patterns, 863 profiles and 869 {prorules}. {prosite} is accessible at: http://www.expasy.org/prosite/.\"\"impressive advances in {ngs} have enabled an immense diversity of novel applications. the barrier of the \\\\$1000 genome has recently been broken. important novel tools for clinical diagnostics based on {ngs} are appearing. third-generation technologies may further revolutionize genomics research. significant challenges for {ngs} remain, in particular data storage and processing. ten years ago next-generation sequencing ({ngs}) technologies appeared on the market. during the past decade, tremendous progress has been made in terms of speed, read length, and throughput, along with a sharp reduction in per-base cost. together, these advances democratized {ngs} and paved the way for the development of a large number of novel {ngs} applications in basic science as well as in translational research areas such as clinical diagnostics, agrigenomics, and forensic science. here we provide an overview of the evolution of {ngs} and discuss the most significant improvements in sequencing technologies and library preparation protocols. we also explore the current landscape of {ngs} applications and provide a perspective for future developments.\"{dna} methylation: roles in mammalian development,\"{dna} methylation is among the best studied epigenetic modifications and is essential to mammalian development. although the methylation status of most {cpg} dinucleotides in the genome is stably propagated through mitosis, improvements to methods for measuring methylation have identified numerous regions in which it is dynamically regulated. in this review, we discuss key concepts in the function of {dna} methylation in mammals, stemming from more than two decades of research, including many recent studies that have elucidated when and where {dna} methylation has a regulatory role in the genome. we include insights from early development, embryonic stem cells and adult lineages, particularly haematopoiesis, to highlight the general features of this modification as it participates in both global and localized epigenetic regulation.\"storing, and querying biological pathways.\",\"biological pathways, including metabolic pathways, protein interaction networks, signal transduction pathways, and gene regulatory networks, are currently represented in over 220 diverse databases. these data are crucial for the study of specific biological processes, including human diseases. standard exchange formats for pathway information, such as {biopax}, {cellml}, {sbml} and {psi}-{mi}, enable convenient collection of this data for biological research, but mechanisms for common storage and communication are required. we have developed {cpath}, an open source database and web application for collecting, storing, and querying biological pathway data. {cpath} makes it easy to aggregate custom pathway data sets available in standard exchange formats from multiple databases, present pathway data to biologists via a customizable web interface, and export pathway data via a web service to third-party software, such as cytoscape, for visualization and analysis. {cpath} is software only, and does not include new pathway information. key features include: a built-in identifier mapping service for linking identical interactors and linking to external resources; built-in support for {psi}-{mi} and {biopax} standard pathway exchange formats; a web service interface for searching and retrieving pathway data sets; and thorough documentation. the {cpath} software is freely available under the {lgpl} open source license for academic and commercial use. {cpath} is a robust, scalable, modular, professional-grade software platform for collecting, storing, and querying biological pathways. it can serve as the core data handling component in information systems for pathway visualization, analysis and modeling.\"new haven, connecticut 06520, usa.\",annotation transfer between genomes: protein-protein interologs and {protein-dna} regulogs.,texts, and trees.\",\"{bionames} is a web database of taxonomic names for animals, linked to the primary literature and, wherever possible, to phylogenetic trees. it aims to provide a taxonomic \"\"dashboard\"\" where at a glance we can see a summary of the taxonomic and phylogenetic information we have for a given taxon and hence provide a quick answer to the basic question \"\"what is this taxon?\"\" {bionames} combines classifications from the global biodiversity information facility ({gbif}) and {genbank}, images from the encyclopedia of life ({eol}), animal names from the index of organism names ({ion}), and bibliographic data from multiple sources including the biodiversity heritage library ({bhl}) and {crossref}. the user interface includes display of full text articles, interactive timelines of taxonomic publications, and zoomable phylogenies. it is available at http://bionames.org.\"\"the prediction of regulatory elements is a problem where computational methods offer great hope. over the past few years, numerous tools have become available for this task. the purpose of the current assessment is twofold: to provide some guidance to users regarding the accuracy of currently available tools in various settings, and to provide a benchmark of data sets for assessing future tools.\"\"unraveling the mechanisms that regulate gene expression is a major challenge in biology. an important task in this challenge is to identify regulatory elements, especially the binding sites in deoxyribonucleic acid ({dna}) for transcription factors. these binding sites are short {dna} segments that are called motifs. recent advances in genome sequence availability and in high-throughput gene expression analysis technologies have allowed for the development of computational methods for motif finding. as a result, a large number of motif finding algorithms have been implemented and applied to various motif models over the past decade. this survey reviews the latest developments in {dna} motif finding algorithms. earlier algorithms use promoter sequences of coregulated genes from single genome and search for statistically overrepresented motifs. recent algorithms are designed to use phylogenetic footprinting or orthologous sequences and also an integrated approach where promoter sequences of coregulated genes and phylogenetic footprinting are used. all the algorithms studied have been reported to correctly detect the motifs that have been previously detected by laboratory experimental approaches, and some algorithms were able to find novel motifs. however, most of these motif finding algorithms have been shown to work successfully in yeast and other lower organisms, but perform significantly worse in higher organisms. despite considerable efforts to date, {dna} motif finding remains a complex challenge for biologists and computer scientists. researchers have taken many different approaches in developing motif discovery tools and the progress made in this area of research is very encouraging. performance comparison of different motif finding tools and identification of the best tools have proven to be a difficult task because tools are designed based on algorithms and motif models that are diverse and complex and our incomplete understanding of the biology of regulatory mechanism does not always provide adequate evaluation of underlying algorithms over motif models.\"\"the gibbs motif sampler is a software package for locating common elements in collections of biopolymer sequences. in this paper we describe a new variation of the gibbs motif sampler, the gibbs recursive sampler, which has been developed specifically for locating multiple transcription factor binding sites for multiple transcription factors simultaneously in unaligned {dna} sequences that may be heterogeneous in {dna} composition. here we describe the basic operation of the web-based version of this sampler. the sampler may be acces-sed at http://bayesweb.wadsworth.org/gibbs/gibbs.html and at http://www.bioinfo.rpi.edu/applications/bayesian/gibbs/gibbs.html. an online user guide is available at http://bayesweb.wadsworth.org/gibbs/bernoulli.html and at http://www.bioinfo.rpi.edu/applications/bayesian/gibbs/manual/bernoulli.html. solaris, solaris.x86 and linux versions of the sampler are available as stand-alone programs for academic and not-for-profit users. commercial licenses are also available. the gibbs recursive sampler is distributed in accordance with the {iscb} level 0 guidelines and a requirement for citation of use in scientific publications.\"wellcome trust genome campus, hinxton, cambridge, cb10 1sd, uk.\",the ontology lookup service: more data and better tools for controlled vocabulary queries,\"the ontology lookup service ({ols}) (http://www.ebi.ac.uk/ols) provides interactive and programmatic interfaces to query, browse and navigate an ever increasing number of biomedical ontologies and controlled vocabularies. the volume of data available for querying has more than quadrupled since it went into production and {ols} functionality has been integrated into several high-usage databases and data entry tools. improvements have been made to both {ols} query interfaces, based on user feedback and requirements, to improve usability and service interoperability and provide novel ways to perform queries.\"\"in a chromatin immunoprecipitation followed by high-throughput sequencing ({chip}-seq) experiment, an important consideration in experimental design is the minimum number of sequenced reads required to obtain statistically significant results. we present an extensive evaluation of the impact of sequencing depth on identification of enriched regions for key histone modifications ({h3k4me3}, {h3k36me3}, {h3k27me3} and {h3k9me2}/me3) using deep-sequenced datasets in human and fly. we propose to define sufficient sequencing depth as the number of reads at which detected enrichment regions increase <1\\\\% for an additional million reads. although the required depth depends on the nature of the mark and the state of the cell in each experiment, we observe that sufficient depth is often reached at <20 million reads for fly. for human, there are no clear saturation points for the examined datasets, but our analysis suggests 40–50 million reads as a practical minimum for most marks. we also devise a mathematical model to estimate the sufficient depth and total genomic coverage of a mark. lastly, we find that the five algorithms tested do not agree well for broad enrichment profiles, especially at lower depths. our findings suggest that sufficient sequencing depth and an appropriate peak-calling algorithm are essential for ensuring robustness of conclusions derived from {chip}-seq data.\"\"all inferences in comparative biology depend on accurate estimates of evolutionary relationships. recent phylogenetic analyses have turned away from maximum parsimony towards the probabilistic techniques of maximum likelihood and bayesian markov chain monte carlo ({bmcmc}). these probabilistic techniques represent a parametric approach to statistical phylogenetics, because their criterion for evaluating a topology—the probability of the data, given the tree—is calculated with reference to an explicit evolutionary model from which the data are assumed to be identically distributed. maximum parsimony can be considered nonparametric, because trees are evaluated on the basis of a general metric—the minimum number of character state changes required to generate the data on a given tree—without assuming a specific distribution1. the shift to parametric methods was spurred, in large part, by studies showing that although both approaches perform well most of the time2, maximum parsimony is strongly biased towards recovering an incorrect tree under certain combinations of branch lengths, whereas maximum likelihood is not3, 4, 5, 6. all these evaluations simulated sequences by a largely homogeneous evolutionary process in which data are identically distributed. there is ample evidence, however, that real-world gene sequences evolve heterogeneously and are not identically distributed7, 8, 9, 10, 11, 12, 13, 14, 15, 16. here we show that maximum likelihood and {bmcmc} can become strongly biased and statistically inconsistent when the rates at which sequence sites evolve change non-identically over time. maximum parsimony performs substantially better than current parametric methods over a wide range of conditions tested, including moderate heterogeneity and phylogenetic problems not normally considered difficult.\"java library, r library, and {tool-chain} for mass spectrometry data analysis\",\"the recent proliferation of high-resolution mass spectrometers has generated a wealth of new data analysis methods. however, flexible integration of these methods into configurations best suited to the research question is hampered by heterogeneous file formats and monolithic software development. the {mzxml}, {mzdata}, and {mzml} file formats have enabled uniform access to unprocessed raw data. in this paper we present our efforts to produce an equally simple and powerful format, {peakml}, to uniformly exchange processed intermediary and result data. to demonstrate the versatility of {peakml}, we have developed an open source java toolkit for processing, filtering, and annotating mass spectra in a customizable pipeline ({mzmatch}), as well as a user-friendly data visualization environment ({peakml} viewer). the {peakml} format in particular enables the flexible exchange of processed data between software created by different groups or companies, as we illustrate by providing a {peakml}-based integration of the widely used {xcms} package with {mzmatch} data processing tools. as an added advantage, downstream analysis can benefit from direct access to the full mass trace information underlying summarized mass spectrometry results, providing the user with the means to rapidly verify results. the {peakml}/{mzmatch} software is freely available at http://mzmatch.sourceforge.net, with documentation, tutorials, and a community forum.\"braunschweig, germany.\",{transfac}: a database on transcription factors and their {dna} binding sites,{transfac} is a database about eukaryotic transcription regulating {dna} sequence elements and the transcription factors binding to and acting through them. this report summarizes the present status of this database and accompanying retrieval tools.hinxton, cambridge cb10 1sd and department of genetics, university of cambridge, cambridge cb2 3eh, uk.\",{chebi}: a database and ontology for chemical entities of biological interest,\"chemical entities of biological interest ({chebi}) is a freely available dictionary of molecular entities focused on \\'small\\' chemical compounds. the molecular entities in question are either natural products or synthetic products used to intervene in the processes of living organisms. genome-encoded macromolecules (nucleic acids, proteins and peptides derived from proteins by cleavage) are not as a rule included in {chebi}. in addition to molecular entities, {chebi} contains groups (parts of molecular entities) and classes of entities. {chebi} includes an ontological classification, whereby the relationships between molecular entities or classes of entities and their parents and/or children are specified. {chebi} is available online at http://www.ebi.ac.uk/chebi/\"\"we offer a publicly available service that can interactively map protein identifiers and protein sequences to the majority of commonly used protein databases. programmatic access is available through a standards-compliant {soap} interface or a lightweight {rest} interface. the {picr} interface, documentation and code examples are available at {http://www.ebi.ac.uk/tools}/picr.\"\"differential gene expression is the fundamental mechanism underlying animal development and cell differentiation. however, it is a challenge to identify comprehensively and accurately the {dna} sequences that are required to regulate gene expression: namely, cis-regulatory modules ({crms}). three major features, either singly or in combination, are used to predict {crms}: clusters of transcription factor binding site motifs, non-coding {dna} that is under evolutionary constraint and biochemical marks associated with {crms}, such as histone modifications and protein occupancy. the validation rates for predictions indicate that identifying diagnostic biochemical marks is the most reliable method, and understanding is enhanced by the analysis of motifs and conservation patterns within those predicted {crms}.\"\"reactome (http://www.reactome.org) is a manually curated open-source open-data resource of human pathways and reactions. the current version 46 describes 7088 human proteins (34\\\\% of the predicted human proteome), participating in 6744 reactions based on data extracted from 15 107 research publications with {pubmed} links. the reactome web site and analysis tool set have been completely redesigned to increase speed, flexibility and user friendliness. the data model has been extended to support annotation of disease processes due to infectious agents and to mutation.\"{piana}: protein interactions and network analysis.,\"we present a software framework and tool called protein interactions and network analysis ({piana}) that facilitates working with protein interaction networks by (1) integrating data from multiple sources, (2) providing a library that handles graph-related tasks and (3) automating the analysis of protein-protein interaction networks. {piana} can also be used as a stand-alone application to create protein interaction networks and perform tasks such as predicting protein interactions and helping to identify spots in a {2d} electrophoresis gel. availability: {piana} is under the {gnu} {gpl}. source code, database and detailed documentation may be freely downloaded from http://sbi.imim.es/piana.\"university of california, berkeley, usa. amoses@ocf.berkeley.edu\",phylogenetic motif detection by expectation-maximization on evolutionary mixtures.,\"the preferential conservation of transcription factor binding sites implies that non-coding sequence data from related species will prove a powerful asset to motif discovery. we present a unified probabilistic framework for motif discovery that incorporates evolutionary information. we treat aligned {dna} sequence as a mixture of evolutionary models, for motif and background, and, following the example of the {meme} program, provide an algorithm to estimate the parameters by {expectation-maximization}. we examine a variety of evolutionary models and show that our approach can take advantage of phylogenic information to avoid false positives and discover motifs upstream of groups of characterized target genes. we compare our method to traditional motif finding on only conserved regions. an implementation will be made available at http://rana.lbl.gov.\"harpenden, uk. jacob.koehler@bbsrc.ac.uk\",\"linking experimental results, biological networks and sequence analysis methods using ontologies and generalised data structures.\",\"the structure of a closely integrated data warehouse is described that is designed to link different types and varying numbers of biological networks, sequence analysis methods and experimental results such as those coming from microarrays. the data schema is inspired by a combination of graph based methods and generalised data structures and makes use of ontologies and meta-data. the core idea is to consider and store biological networks as graphs, and to use generalised data structures ({gds}) for the storage of further relevant information. this is possible because many biological networks can be stored as graphs: protein interactions, signal transduction networks, metabolic pathways, gene regulatory networks etc. nodes in biological graphs represent entities such as promoters, proteins, genes and transcripts whereas the edges of such graphs specify how the nodes are related. the semantics of the nodes and edges are defined using ontologies of node and relation types. besides generic attributes that most biological entities possess (name, attribute description), further information is stored using generalised data structures. by directly linking to underlying sequences (exons, introns, promoters, amino acid sequences) in a systematic way, close interoperability to sequence analysis methods can be achieved. this approach allows us to store, query and update a wide variety of biological information in a way that is semantically compact without requiring changes at the database schema level when new kinds of biological information is added. we describe how this datawarehouse is being implemented by extending the text-mining framework {ondex} to link, support and complement different bioinformatics applications and research activities such as microarray analysis, sequence analysis and modelling/simulation of biological systems. the system is developed under the {gpl} license and can be downloaded from http://sourceforge.net/projects/ondex/\"national library of medicine, national institutes of health, bethesda, md 20894.\",detecting subtle sequence signals: a gibbs sampling strategy for multiple alignment.,\"a wealth of protein and {dna} sequence data is being generated by genome projects and other sequencing efforts. a crucial barrier to deciphering these sequences and understanding the relations among them is the difficulty of detecting subtle local residue patterns common to multiple sequences. such patterns frequently reflect similar molecular structures and biological properties. a mathematical definition of this \"\"local multiple alignment\"\" problem suitable for full computer automation has been used to develop a new and sensitive algorithm, based on the statistical method of iterative sampling. this algorithm finds an optimized local alignment model for n sequences in n-linear time, requiring only seconds on current workstations, and allows the simultaneous detection and optimization of multiple patterns and pattern repeats. the method is illustrated as applied to helix-turn-helix proteins, lipocalins, and prenyltransferases.\"usa\",readings in speech recognition,an abstract is not available.\"{background}:the {kegg} {pathway} database provides a plethora of pathways for a diversity of organisms. all pathway components are directly linked to other {kegg} databases, such as {kegg} {compound} or {kegg} {reaction}. therefore, the pathways can be extended with an enormous amount of information and provide a foundation for initial structural modeling approaches. as a drawback, {kgml}-formatted {kegg} pathways are primarily designed for visualization purposes and often omit important details for the sake of a clear arrangement of its entries. thus, a direct conversion into systems biology models would produce incomplete and erroneous {models.results}:here, we present a precise method for processing and converting {kegg} pathways into initial metabolic and signaling models encoded in the standardized community pathway formats {sbml} (levels 2 and 3) and {biopax} (levels 2 and 3). this method involves correcting invalid or incomplete {kgml} content, creating complete and valid stoichiometric reactions, translating relations to signaling models and augmenting the pathway content with various information, such as cross-references to entrez gene, {omim}, {uniprot} {chebi}, and many {more.finally}, we compare several existing conversion tools for {kegg} pathways and show that the conversion from {kegg} to {biopax} does not involve a loss of information, whilst lossless translations to {sbml} can only be performed using {sbml} level 3, including its recently proposed qualitative models and groups extension {packages.conclusions}:building correct {biopax} and {sbml} signaling models from the {kegg} database is a unique characteristic of the proposed method. further, there is no other approach that is able to appropriately construct metabolic models from {kegg} pathways, including correct reactions with stoichiometry. the resulting initial models, which contain valid and comprehensive {sbml} or {biopax} code and a multitude of cross-references, lay the foundation to facilitate further modeling steps.\"ondex web: web-based visualization and exploration of heterogeneous biological networks.,\"ondex web is a new web-based implementation of the network visualization and exploration tools from the ondex data integration platform. new features such as context-sensitive menus and annotation tools provide users with intuitive ways to explore and manipulate the appearance of heterogeneous biological networks. ondex web is open source, written in java and can be easily embedded into web sites as an applet. ondex web supports loading data from a variety of network formats, such as {xgmml}, {nwb}, pajek and {oxl}. {http://ondex.rothamsted.ac.uk/ondexweb}.\"\"{kegg} (kyoto encyclopedia of genes and genomes) is a knowledge base for systematic analysis of gene functions, linking genomic information with higher order functional information. the genomic information is stored in the {genes} database, which is a collection of gene catalogs for all the completely sequenced genomes and some partial genomes with up-to-date annotation of gene functions. the higher order functional information is stored in the {pathway} database, which contains graphical representations of cellular processes, such as metabolism, membrane transport, signal transduction and cell cycle. the {pathway} database is supplemented by a set of ortholog group tables for the information about conserved subpathways (pathway motifs), which are often encoded by positionally coupled genes on the chromosome and which are especially useful in predicting gene functions. a third database in {kegg} is {ligand} for the information about chemical compounds, enzyme molecules and enzymatic reactions. {kegg} provides java graphics tools for browsing genome maps, comparing two genome maps and manipulating expression maps, as well as computational tools for sequence comparison, graph comparison and path computation. the {kegg} databases are daily updated and made freely available (http://www. genome.ad.jp/kegg/).\"44 binney st, boston, ma 02115, usa. rgentlem@jimmy.harvard.edu\",bioconductor: open software development for computational biology and bioinformatics.,\"the bioconductor project is an initiative for the collaborative creation of extensible software for computational biology and bioinformatics. the goals of the project include: fostering collaborative development and widespread use of innovative software, reducing barriers to entry into interdisciplinary scientific research, and promoting the achievement of remote reproducibility of research results. we describe details of our aims and methods, identify current challenges, compare bioconductor to other open bioinformatics projects, and provide working examples.\"\"{metabolights} (http://www.ebi.ac.uk/metabolights) is the first general-purpose, open-access repository for metabolomics studies, their raw experimental data and associated metadata, maintained by one of the major open-access data providers in molecular biology. metabolomic profiling is an important tool for research into biological functioning and into the systemic perturbations caused by diseases, diet and the environment. the effectiveness of such methods depends on the availability of public open data across a broad range of experimental methods and conditions. the {metabolights} repository, powered by the open source {isa} framework, is cross-species and cross-technique. it will cover metabolite structures and their reference spectra as well as their biological roles, locations, concentrations and raw data from metabolic experiments. studies automatically receive a stable unique accession number that can be used as a publication reference (e.g. {mtbls1}). at present, the repository includes 15 submitted studies, encompassing 93 protocols for 714 assays, and span over 8 different species including human, caenorhabditis elegans, mus musculus and arabidopsis thaliana. eight hundred twenty-seven of the metabolites identified in these studies have been mapped to {chebi}. these studies cover a variety of techniques, including {nmr} spectroscopy and mass spectrometry.\"with increased coverage and integration.\",\"complete knowledge of all direct and indirect interactions between proteins in a given cell would represent an important milestone towards a comprehensive description of cellular mechanisms and functions. although this goal is still elusive, considerable progress has been made-particularly for certain model organisms and functional systems. currently, protein interactions and associations are annotated at various levels of detail in online resources, ranging from raw data repositories to highly formalized pathway databases. for many applications, a global view of all the available interaction data is desirable, including lower-quality data and/or computational predictions. the {string} database (http://string-db.org/) aims to provide such a global perspective for as many organisms as feasible. known and predicted associations are scored and integrated, resulting in comprehensive protein networks covering >1100 organisms. here, we describe the update to version 9.1 of {string}, introducing several improvements: (i) we extend the automated mining of scientific texts for interaction information, to now also include full-text articles; (ii) we entirely re-designed the algorithm for transferring interactions from one model organism to the other; and (iii) we provide users with statistical information on any functional enrichment observed in their networks.\"\"{background}:biologically active sequence motifs often have positional preferences with respect to a genomic landmark. for example, many known transcription factor binding sites ({tfbss}) occur within an interval [-300, 0] bases upstream of a transcription start site ({tss}). although some programs for identifying sequence motifs exploit positional information, most of them model it only implicitly and with ad hoc methods, making them unsuitable for general motif {searches.results}:{a-glam}, a user-friendly computer program for identifying sequence motifs, now incorporates a bayesian model systematically combining sequence and positional information. {a-glam}\\'s predictions with and without positional information were compared on two human {tfbs} datasets, each containing sequences corresponding to the interval [-2000, 0] bases upstream of a known {tss}. a rigorous statistical analysis showed that positional information significantly improved the prediction of sequence motifs, and an extensive cross-validation study showed that {a-glam}\\'s model was robust against mild misspecification of its parameters. as expected, when sequences in the datasets were successively truncated to the intervals [-1000, 0], [-500, 0] and [-250, 0], positional information aided motif prediction less and less, but never hurt it {significantly.conclusion}:although sequence truncation is a viable strategy when searching for biologically active motifs with a positional preference, a probabilistic model (used reasonably) generally provides a superior and more robust strategy, particularly when the sequence motifs\\' positional preferences are not well characterized.\"\"sumary: two sample logo is a web-based tool that detects and displays statistically significant differences in position-specific symbol compositions between two sets of multiple sequence alignments. in a typical scenario, two groups of aligned sequences will share a common motif but will differ in their functional annotation. the inclusion of the background alignment provides an appropriate underlying amino acid or nucleotide distribution and addresses intersite symbol correlations. in addition, the difference detection process is sensitive to the sizes of the aligned groups. two sample logo extends {weblogo}, a widely-used sequence logo generator. the source code is distributed under the {mit} open source license agreement and is available for download free of charge.\"new york, ny 10021, usa. saurabh@lonnrot.rockefeller.edu\",{phyme}: a probabilistic algorithm for finding motifs in sets of orthologous sequences.,\"this paper addresses the problem of discovering transcription factor binding sites in heterogeneous sequence data, which includes regulatory sequences of one or more genes, as well as their orthologs in other species. we propose an algorithm that integrates two important aspects of a motif\\'s significance - overrepresentation and cross-species conservation - into one probabilistic score. the algorithm allows the input orthologous sequences to be related by any user-specified phylogenetic tree. it is based on the {expectation-maximization} technique, and scales well with the number of species and the length of input sequences. we evaluate the algorithm on synthetic data, and also present results for data sets from yeast, fly, and human. the results demonstrate that the new approach improves motif discovery by exploiting multiple species information.\"\"despite its increasing role in communication, the {world-wide} web remains uncontrolled: any individual or institution can create a website with any number of documents and links. this unregulated growth leads to a huge and complex web, which becomes a large directed graph whose vertices are documents and whose edges are links ({urls}) that point from one document to another. the topology of this graph determines the web\\'s connectivity and consequently how effectively we can locate information on it. but its enormous size (estimated to be at least 8108 documents1) and the continual changing of documents and links make it impossible to catalogue all the vertices and edges.\"pittsburgh usa.\",penalized and weighted k-means for clustering with scattered objects and prior information in high-throughput biological data,\"motivation: cluster analysis is one of the most important data mining tools for investigating high-throughput biological data. the existence of many scattered objects that should not be clustered has been found to hinder performance of most traditional clustering algorithms in such a high-dimensional complex situation. very often, additional prior knowledge from databases or previous experiments is also available in the analysis. excluding scattered objects and incorporating existing prior information are desirable to enhance the clustering performance.  results: in this article, a class of loss functions is proposed for cluster analysis and applied in high-throughput genomic and proteomic data. two major extensions from k-means are involved: penalization and weighting. the additive penalty term is used to allow a set of scattered objects without being clustered. weights are introduced to account for prior information of preferred or prohibited cluster patterns to be identified. their relationship with the classification likelihood of gaussian mixture models is explored. incorporation of good prior information is also shown to improve the global optimization issue in clustering. applications of the proposed method on simulated data as well as high-throughput data sets from tandem mass spectrometry ({ms}/{ms}) and microarray experiments are presented. our results demonstrate its superior performance over most existing methods and its computational simplicity and extensibility in the application of large complex biological data sets.  availability: http://www.pitt.edu/\\\\~{}ctseng/research/software.html  contact: ctseng@pitt.edu  supplementary information: supplementary data are available at bioinformatics online. 10.1093/bioinformatics/btm320\"\"a method for studying the sequence-specific binding of proteins to {dba} is described. the technique is a simple conjoining of the {maxam-gilbert} {dna}-sequencing method and the technique of {dnaase}-protected fragment isolation. fragments of a 5\\' end-labelled, double-stranded {dna} segment, partially degraded by {dnaase} in the presence and absence of the binding protein, are visualized by electrophoresis and autoradiography alongside the base-specific reaction products of the {maxam-gilbert} sequencing method. it is then possible to see the protective \"\"footprint\"\" of the binding protein on the {dna} sequence. the binding of lac repressor to lac operator is visualized by \"\"footprinting\"\" as an example. equillibrium estimates indicate that 10-fold sequence-specificity (differential binding constant) could be studied easily using this technique.\"regulation of cytoplasmic {mrna} decay.,\"discoveries made over the past 20 years highlight the importance of {mrna} decay as a means of modulating gene expression and thereby protein production. up until recently, studies largely focused on identifying cis-acting sequences that serve as {mrna} stability or instability elements, the proteins that bind these elements, how the process of translation influences {mrna} decay and the ribonucleases that catalyse decay. now, current studies have begun to elucidate how the decay process is regulated. this review examines our current understanding of how mammalian cell {mrna} decay is controlled by different signalling pathways and lays out a framework for future research.\"431 state hall, detroit, mi-48202, us.\",babel\\'s tower revisited: a universal resource for cross-referencing across annotation databases,\"motivation: annotation databases are widely used as public repositories of biological knowledge. however, most of these resources have been developed by independent groups which used different designs and different identifiers for the same biological entities. as we show in this article, incoherent name spaces between various databases represent a serious impediment to using the existing annotations at their full potential. navigating between various such name spaces by mapping {ids} from one database to another is a very important issue which is not properly addressed at the {moment.results}: we have developed a web-based resource, {onto-translate} ({ot}), which effectively addresses this problem. {ot} is able to map onto each other different types of biological entities from the following annotation databases: {swiss-prot}, {trembl}, {nref}, {pir}, gene ontology, {kegg}, entrez gene, {genbank}, {genpept}, {image}, {refseq}, {unigene}, {omim}, {pdb}, eukaryotic promoter database, {hugo} gene nomenclature committee and {netaffx}. currently, {ot} is able to perform 462 types of mappings between 29 different types of {ids} from 17 databases concerning 53 organisms. among these, over 300 types of translations and 15 types of {ids} are not currently supported by any other tool or resource. on average, {ot} is able to correctly map between 96 and 99\\\\% of the biological entities provided as input. in terms of speed, sets of ∼20 000 {ids} can be translated in <30 s, in most {cases.availability}: {ot} is a part of {onto-tools}, which is freely available at {http://vortex.cs.wayne.edu/projects}.{htmlcontact}:sorin@wayne.edu\"\"motivation: a significant and stubbornly intractable problem in genome sequence analysis has been the de novo identification of transcription factor binding sites in promoter regions. although theoretically pleasing, probabilistic methods have faced difficulties due to model mismatch and the nature of the biological sequence. these problems result in inference in a high dimensional, highly multimodal space, and consequently often display only local convergence and hence unsatisfactory performance.  algorithm: in this article, we derive and demonstrate a novel method utilizing a sequential monte carlo-based expectation-maximization ({em}) optimization to improve performance in this scenario. the monte carlo element should increase the robustness of the algorithm compared to classical {em}. furthermore, the parallel nature of the sequential monte carlo algorithm should be more robust than gibbs sampling approaches to multimodality problems.  results: we demonstrate the superior performance of this algorithm on both semi-synthetic and real data from escherichia coli.  availability: http://sigproc-eng.cam.ac.uk/[\\\\~{}]ej230/smc\\\\_em\\\\_tfbsid.tar.gz  contact: ej230@cam.ac.uk  supplementary information: supplementary data are available at bioinformatics online. 10.1093/bioinformatics/btm054\"ny 11274, usa.\",identifying tissue-selective transcription factor binding sites in vertebrate promoters,\"we present a computational method aimed at systematically identifying tissue-selective transcription factor binding sites. our method focuses on the differences between sets of promoters that are associated with differentially expressed genes, and it is effective at identifying the highly degenerate motifs that characterize vertebrate transcription factor binding sites. results on simulated data indicate that our method detects motifs with greater accuracy than the leading methods, and its detection of strongly overrepresented motifs is nearly perfect. we present motifs identified by our method as the most overrepresented in promoters of liver- and muscle-selective genes, demonstrating that our method accurately identifies known transcription factor binding sites and previously uncharacterized motifs.\"berzelius v\\\\\"\"{a}g 35, s-17177 stockholm, sweden.\",{jaspar}: an open‐access database for eukaryotic transcription factor binding profiles,\"the analysis of regulatory regions in genome sequences is strongly based on the detection of potential transcription factor binding sites. the preferred models for representation of transcription factor binding specificity have been termed position‐specific scoring matrices. {jaspar} is an open‐access database of annotated, high‐quality, matrix‐based transcription factor binding site profiles for multicellular eukaryotes. the profiles were derived exclusively from sets of nucleotide sequences experimentally demonstrated to bind transcription factors. the database is complemented by a web interface for browsing, searching and subset selection, an online sequence analysis utility and a suite of programming tools for genome‐wide and comparative genomic analysis of regulatory regions. {jaspar} is available at http://jaspar. cgb.ki.se.\"notre dame, in 46556, usa.\",functional and topological characterization of protein interaction networks,\"the elucidation of the cell\\'s large-scale organization is a primary challenge for post-genomic biology, and understanding the structure of protein interaction networks offers an important starting point for such studies. we compare four available databases that approximate the protein interaction network of the yeast, saccharomyces cerevisiae, aiming to uncover the network\\'s generic large-scale properties and the impact of the proteins\\' function and cellular localization on the network topology. we show how each database supports a scale-free, topology with hierarchical modularity, indicating that these features represent a robust and generic property of the protein interactions network. we also find strong correlations between the network\\'s structure and the functional role and subcellular localization of its protein constituents, concluding that most functional and/or localization classes appear as relatively segregated subnetworks of the full protein interaction network. the uncovered systematic differences between the four protein interaction databases reflect their relative coverage for different functional and localization classes and provide a guide for their utility in various bioinformatics studies.\"\"{enologos} is a web-based tool that generates sequence logos from various input sources. sequence logos have become a popular way to graphically represent {dna} and amino acid sequence patterns from a set of aligned sequences. each position of the alignment is represented by a column of stacked symbols with its total height reflecting the information content in this position. currently, the available web servers are able to create logo images from a set of aligned sequences, but none of them generates weighted sequence logos directly from energy measurements or other sources. with the advent of high-throughput technologies for estimating the contact energy of different {dna} sequences, tools that can create logos directly from binding affinity data are useful to researchers. {enologos} generates sequence logos from a variety of input data, including energy measurements, probability matrices, alignment matrices, count matrices and aligned sequences. furthermore, {enologos} can represent the mutual information of different positions of the consensus sequence, a unique feature of this tool. another web interface for our software, {c2h2}-{enologos}, generates logos for the {dna}-binding preferences of the {c2h2} zinc-finger transcription factor family members. {enologos} and {c2h2}-{enologos} are accessible over the web at http://biodev.hgen.pitt.edu/enologos/.\"where to publish and find ontologies? a survey of ontology libraries,\"one of the key promises of the semantic web is its potential to enable and facilitate data interoperability. the ability of data providers and application developers to share and reuse ontologies is a critical component of this data interoperability: if different applications and data sources use the same set of well defined terms for describing their domain and data, it will be much easier for them to  ” talk” to one another. ontology libraries are the systems that collect ontologies from different sources and facilitate the tasks of finding, exploring, and using these ontologies. thus ontology libraries can serve as a link in enabling diverse users and applications to discover, evaluate, use, and publish ontologies. in this paper, we provide a survey of the growing—and surprisingly diverse—landscape of ontology libraries. we highlight how the varying scope and intended use of the libraries affects their features, content, and potential exploitation in applications. from reviewing 11 ontology libraries, we identify a core set of questions that ontology practitioners and users should consider in choosing an ontology library for finding ontologies or publishing their own. we also discuss the research challenges that emerge from this survey, for the developers of ontology libraries to address.\"\"the {pazar} database unites independently created and maintained data collections of transcription factor and regulatory sequence annotation. the flexible {pazar} schema permits the representation of diverse information derived from experiments ranging from biochemical {protein–dna} binding to cellular reporter gene assays. data collections can be made available to the public, or restricted to specific system users. the data \\'boutiques\\' within the shopping-mall-inspired system facilitate the analysis of genomics data and the creation of predictive models of gene regulation. since its initial release, {pazar} has grown in terms of data, features and through the addition of an associated package of software tools called the {orca} toolkit ({orcatk}). {orcatk} allows users to rapidly develop analyses based on the information stored in the {pazar} system. {pazar} is available at http://www.pazar.info. {orcatk} can be accessed through convenient buttons located in the {pazar} pages or via our website at {http://www.cisreg.ca/orcatk}.\"\"proteins are traditionally identified on the basis of their individual actions as catalysts, signalling molecules, or building blocks in cells and microorganisms. but our post-genomic view is expanding the protein\\'s role into an element in a network of protein–protein interactions as well, in which it has a contextual or cellular function within functional modules1, 2. here we provide quantitative support for this idea by demonstrating that the phenotypic consequence of a single gene deletion in the yeast saccharomyces cerevisiae is affected to a large extent by the topological position of its protein product in the complex hierarchical web of molecular interactions.\"250 longwood avenue, boston, ma 02115, usa.\",\"the synergizer service for translating gene, protein and other biological identifiers\",summary: the synergizer is a database and web service that provides translations of biological database identifiers. it is accessible both programmatically and {interactively.availability}: the synergizer is freely available to all users inter-actively via a web application (http://llama.med.harvard.edu/synergizer/translate) and programmatically via a web service. clients implementing the synergizer application programming interface ({api}) are also freely available. please visit http://llama.med.harvard.edu/synergizer/doc for {details.contact}: fritz\\\\_roth@hms.harvard.edula jolla, california 92037, usa.\",\"{xcms}: processing mass spectrometry data for metabolite profiling using nonlinear peak alignment, matching, and identification.\",\"metabolite profiling in biomarker discovery, enzyme substrate assignment, drug activity/specificity determination, and basic metabolic research requires new data preprocessing approaches to correlate specific metabolites to their biological origin. here we introduce an {lc}/{ms}-based data analysis approach, {xcms}, which incorporates novel nonlinear retention time alignment, matched filtration, peak detection, and peak matching. without using internal standards, the method dynamically identifies hundreds of endogenous metabolites for use as standards, calculating a nonlinear retention time correction profile for each sample. following retention time correction, the relative metabolite ion intensities are directly compared to identify changes in specific endogenous metabolites, such as potential biomarkers. the software is demonstrated using data sets from a previously reported enzyme knockout study and a large-scale study of plasma samples. {xcms} is freely available under an open-source license at http://metlin.scripps.edu/download/.\"\"mapping the chromosomal locations of transcription factors, nucleosomes, histone modifications, chromatin remodeling enzymes, chaperones, and polymerases is one of the key tasks of modern biology, as evidenced by the encyclopedia of {dna} elements ({encode}) project. to this end, chromatin immunoprecipitation followed by high-throughput sequencing ({chip}-seq) is the standard methodology. mapping such {protein-dna} interactions in vivo using {chip}-seq presents multiple challenges not only in sample preparation and sequencing but also for computational analysis. here, we present step-by-step guidelines for the computational analysis of {chip}-seq data. we address all the major steps in the analysis of {chip}-seq data: sequencing depth selection, quality checking, mapping, data normalization, assessment of reproducibility, peak calling, differential binding analysis, controlling the false discovery rate, peak annotation, visualization, and motif analysis. at each step in our guidelines we discuss some of the software tools most frequently used. we also highlight the challenges and problems associated with each step in {chip}-seq data analysis. we present a concise workflow for the analysis of {chip}-seq data in figure 1 that complements and expands on the recommendations of the {encode} and {modencode} projects. each step in the workflow is described in detail in the following sections.\"divergence measures based on the shannon entropy,\"a novel class of information-theoretic divergence measures based on the shannon entropy is introduced. unlike the well-known kullback divergences, the new measures do not require the condition of absolute continuity to be satisfied by the probability distributions involved. more importantly, their close relationship with the variational distance and the probability of misclassification error are established in terms of bounds. these bounds are crucial in many applications of divergence measures. the measures are also well characterized by the properties of nonnegativity, finiteness, semiboundedness, and boundedness\"liivi 2, 50409 tartu, estonia.\",{g:profiler}--a web-based toolset for functional profiling of gene lists from large-scale experiments.,\"{g:profiler} (http://biit.cs.ut.ee/gprofiler/) is a public web server for characterising and manipulating gene lists resulting from mining high-throughput genomic data. {g:profiler} has a simple, user-friendly web interface with powerful visualisation for capturing gene ontology ({go}), pathway, or transcription factor binding site enrichments down to individual gene levels. besides standard multiple testing corrections, a new improved method for estimating the true effect of multiple testing over complex structures like {go} has been introduced. interpreting ranked gene lists is supported from the same interface with very efficient algorithms. such ordered lists may arise when studying the most significantly affected genes from high-throughput data or genes co-expressed with the query gene. other important aspects of practical data analysis are supported by modules tightly integrated with {g:profiler}. these are: {g:convert} for converting between different database identifiers; {g:orth} for finding orthologous genes from other species; and {g:sorter} for searching a large body of public gene expression data for co-expression. {g:profiler} supports 31 different species, and underlying data is updated regularly from sources like the ensembl database. bioinformatics communities wishing to integrate with {g:profiler} can use alternative simple textual outputs.\"\"we present model-based analysis of {chip}-seq data, {macs}, which analyzes data generated by short read sequencers such as solexa\\'s genome analyzer. {macs} empirically models the shift size of {chip}-seq tags, and uses it to improve the spatial resolution of predicted binding sites. {macs} also uses a dynamic poisson distribution to effectively capture local biases in the genome, allowing for more robust predictions. {macs} compares favorably to existing {chip}-seq peak-finding algorithms, and is freely available.\"protein and metabolite identifier mapping services.\",\"many complementary solutions are available for the identifier mapping problem. this creates an opportunity for bioinformatics tool developers. tools can be made to flexibly support multiple mapping services or mapping services could be combined to get broader coverage. this approach requires an interface layer between tools and mapping services. here we present {bridgedb}, a software framework for gene, protein and metabolite identifier mapping. this framework provides a standardized interface layer through which bioinformatics tools can be connected to different identifier mapping services. this approach makes it easier for tool developers to support identifier mapping. mapping services can be combined or merged to support multi-omics experiments or to integrate custom microarray annotations. {bridgedb} provides its own ready-to-go mapping services, both in webservice and local database forms. however, the framework is intended for customization and adaptation to any identifier mapping service. {bridgedb} has already been integrated into several bioinformatics applications. by uncoupling bioinformatics tools from mapping services, {bridgedb} improves capability and flexibility of those tools. all described software is open source and available at http://www.bridgedb.org.\"\"motivation: probabilistic approaches for inferring transcription factor binding sites ({tfbss}) and regulatory motifs from {dna} sequences have been developed for over two decades. previous work has shown that prediction accuracy can be significantly improved by incorporating features such as the competition of multiple transcription factors ({tfs}) for binding to nearby sites, the tendency of {tfbss} for co-regulated {tfs} to cluster and form cis-regulatory modules and explicit evolutionary modeling of conservation of {tfbss} across orthologous sequences. however, currently available tools only incorporate some of these features, and significant methodological hurdles hampered their synthesis into a single consistent probabilistic framework.\"\"we have developed {netpath} as a resource of curated human signaling pathways. as an initial step, {netpath} provides detailed maps of a number of immune signaling pathways, which include approximately 1,600 reactions annotated from the literature and more than 2,800 instances of transcriptionally regulated genes - all linked to over 5,500 published articles. we anticipate {netpath} to become a consolidated resource for human signaling pathways that should enable systems biology approaches.\"\"we describe a set of best practices for scientific software development, based on research and experience, that will improve scientists\\' productivity and the reliability of their software.\"\"the {uniprot} consortium was formed in 2002 by groups from the swiss institute of bioinformatics ({sib}), the european bioinformatics institute ({ebi}) and the protein information resource ({pir}) at georgetown university, and soon afterwards the website http://www.uniprot.org was set up as a central entry point to {uniprot} resources. requests to this address were redirected to one of the three organisations\\' websites. while these sites shared a set of static pages with general information about {uniprot}, their pages for searching and viewing data were different. to provide users with a consistent view and to cut the cost of maintaining three separate sites, the consortium decided to develop a common website for {uniprot}. following several years of intense development and a year of public beta testing, the http://www.uniprot.org domain was switched to the newly developed site described in this paper in july 2008.\"\"summary: pathview is a novel tool set for pathway-based data integration and visualization. it maps and renders user data on relevant pathway graphs. users only need to supply their data and specify the target pathway. pathview automatically downloads the pathway graph data, parses the data file, maps and integrates user data onto the pathway and renders pathway graphs with the mapped data. although built as a stand-alone program, pathview may seamlessly integrate with pathway and functional analysis tools for large-scale and fully automated analysis pipelines.\"\"motivation: there is accumulating evidence that the chromatin environment of transcription factor ({tf}) binding sites in promoter regions has a critical influence on their regulatory potential. recent studies have mapped {tf} binding sites and nucleosome positions throughout the yeast genome; however, there is a lack of computation tools to integrate these data types.  results: we have developed the ceres software to facilitate the integrated analysis of {tf} binding sites and nucleosome positions in the model eukaryote s.cerevisiae. ceres enables users to dynamically display the spatial organization of {tf} binding sites and nucleosome positions of individual genes, or the average profiles for large gene sets. ceres provides novel statistical tools to test for the enrichment of {tf} binding sites and chromatin environments for user-selected gene sets. ceres also enables users to search the genome for combinations of {tf} binding sites that are associated with specific chromatin environments. preliminary analysis using the ceres software indicates that functional and conserved {tf} binding sites are often associated with specific chromatin environments.  availability: {http://bioinformatics1.smb.wsu.edu/ceres}  contact: jwyrick@wsu.edu  supplementary information: supplementary data are available at bioinformatics online. 10.1093/bioinformatics/btp657\"\"promiscuous individuals are the vulnerable nodes to target in safe-sex campaigns. unlike clearly defined \\'real-world\\' networks1, social networks tend to be subjective to some extent2, 3 because the perception of what constitutes a social link may differ between individuals. one unambiguous type of connection, however, is sexual contact, and here we analyse the sexual behaviour of a random sample of individuals4 to reveal the mathematical features of a sexual-contact network.\"new york state department of health, albany, new york 12208, usa. thompson@wadsworth.org\",decoding human regulatory circuits.,\"clusters of transcription factor binding sites ({tfbss}) which direct gene expression constitute cis-regulatory modules ({crms}). we present a novel algorithm, based on gibbs sampling, which locates, de novo, the cis features of these {crms}, their component {tfbss}, and the properties of their spatial distribution. the algorithm finds 69\\\\% of experimentally reported {tfbss} and 85\\\\% of the {crms} in a reference data set of regions upstream of genes differentially expressed in skeletal muscle cells. a discriminant procedure based on the output of the model specifically discriminated regulatory sequences in muscle-specific genes in an independent test set. application of the method to the analysis of 2710 10-kb fragments upstream of annotated human genes identified 17 novel candidate modules with a false discovery rate </=0.05, demonstrating the applicability of the method to genome-scale data.\"link communities reveal multiscale complexity in networks.,\"networks have become a key approach to understanding systems of interacting objects, unifying the study of diverse phenomena including biological organisms and human society. one crucial step when studying the structure and dynamics of networks is to identify communities: groups of related nodes that correspond to functional subunits such as protein complexes or social spheres. communities in networks often overlap such that nodes simultaneously belong to several groups. meanwhile, many networks are known to possess hierarchical organization, where communities are recursively grouped into a hierarchical structure. however, the fact that many real networks have communities with pervasive overlap, where each and every node belongs to more than one group, has the consequence that a global hierarchy of nodes cannot capture the relationships between overlapping groups. here we reinvent communities as groups of links rather than nodes and show that this unorthodox approach successfully reconciles the antagonistic organizing principles of overlapping communities and hierarchy. in contrast to the existing literature, which has entirely focused on grouping nodes, link communities naturally incorporate overlap while revealing hierarchical organization. we find relevant link communities in many networks, including major biological networks such as protein-protein interaction and metabolic networks, and show that a large social network contains hierarchically organized community structures spanning inner-city to regional scales while maintaining pervasive overlap. our results imply that link communities are fundamental building blocks that reveal overlap and hierarchical organization in networks to be two aspects of the same phenomenon.\"recent advances in genomic {dna} sequencing of microbial species from single cells,\"the vast majority of microbial species remain uncultivated and, until recently, about half of all known bacterial phyla were identified only from their {16s} ribosomal {rna} gene sequence. with the advent of single-cell sequencing, genomes of uncultivated species are rapidly filling in unsequenced branches of\"wellcome trust genome campus, hinxton, cambridge cb10 1sd, uk. brazma@ebi.ac.uk\",minimum information about a microarray experiment ({miame})—toward standards for microarray data,\"microarray analysis has become a widely used tool for the generation of gene expression data on a genomic scale. although many significant results have been derived from microarray studies, one limitation has been the lack of standards for presenting and exchanging such data. here we present a proposal, the minimum information about a microarray experiment ({miame}), that describes the minimum information required to ensure that microarray data can be easily interpreted and that results derived from its analysis can be independently verified. the ultimate goal of this work is to establish a standard for recording and reporting microarray-based gene expression data, which will in turn facilitate the establishment of databases and public repositories and enable the development of data analysis tools. with respect to {miame}, we concentrate on defining the content and structure of the necessary information rather than the technical format for capturing it.\"uji, kyoto 611-0011, japan.\",{kegg}: kyoto encyclopedia of genes and genomes,\"kyoto encyclopedia of genes and genomes ({kegg}) is a knowledge base for systematic analysis of gene functions in terms of the networks of genes and molecules. the major component of {kegg} is the {pathway} database that consists of graphical diagrams of biochemical pathways including most of the known metabolic pathways and some of the known regulatory pathways. the pathway information is also represented by the ortholog group tables summarizing orthologous and paralogous gene groups among different organisms. {kegg} maintains the {genes} database for the gene catalogs of all organisms with complete genomes and selected organisms with partial genomes, which are continuously re-annotated, as well as the {ligand} database for chemical compounds and enzymes. each gene catalog is associated with the graphical genome map for chromosomal locations that is represented by java applet. in addition to the data collection efforts, {kegg} develops and provides various computational tools, such as for reconstructing biochemical pathways from the complete genome sequence and for predicting gene regulatory networks from the gene expression profiles. the {kegg} databases are daily updated and made freely available (http://www.genome.ad.jp/kegg/).\"harvard school of public health, and channing laboratory, boston, ma, usa. jzhang@jimmy.harvard.edu\",an extensible application for assembling annotation for genomic data,\"summary: {annbuilder} is an r package for assembling genomic annotation data. the system currently provides parsers to process annotation data from {locuslink}, gene ontology consortium, and human gene project and can be extended to new data sources via user defined parsers. {annbuilder} differs from other existing systems in that it provides users with unlimited ability to assemble data from user selected sources. the products of {annbuilder} are files in {xml} format that can be easily used by different {systems.availability}: (http://www.bioconductor.org). open {source.contact}: jzhang@jimmy.harvard.edu\"{plogo}: a probabilistic approach to visualizing sequence motifs,\"here, we describe the development of {wikipathways} (http://www.wikipathways.org), a public wiki for pathway curation, since it was first published in 2008. new features are discussed, as well as developments in the community of contributors. new features include a zoomable pathway viewer, support for pathway ontology annotations, the ability to mark pathways as private for a limited time and the availability of stable hyperlinks to pathways and the elements therein. {wikipathways} content is freely available in a variety of formats such as the {biopax} standard, and the content is increasingly adopted by external databases and tools, including wikipedia. a recent development is the use of {wikipathways} as a staging ground for centrally curated databases such as reactome. {wikipathways} is seeing steady growth in the number of users, page views and edits for each pathway. to assess whether the community curation experiment can be considered successful, here we analyze the relation between use and contribution, which gives results in line with other wiki projects. the novel use of pathway pages as supplementary material to publications, as well as the addition of tailored content for research domains, is expected to stimulate growth further.\"glasgow g12 8qw, uk. raya@stats.gla.ac.uk\",how scale-free are biological networks.,\"the concept of scale-free network has emerged as a powerful unifying paradigm in the study of complex systems in biology and in physical and social studies. metabolic, protein, and gene interaction networks have been reported to exhibit scale-free behavior based on the analysis of the distribution of the number of connections of the network nodes. here we study 10 published datasets of various biological interactions and perform goodness-of-fit tests to determine whether the given data is drawn from the power-law distribution. our analysis did not identify a single interaction network that has a nonzero probability of being drawn from the power-law distribution.\"\"liquid chromatography coupled to mass spectrometry ({lc}/{ms}) is an important analytical technology for e.g. metabolomics experiments. determining the boundaries, centres and intensities of the two-dimensional signals in the {lc}/{ms} raw data is called feature detection. for the subsequent analysis of complex samples such as plant extracts, which may contain hundreds of compounds, corresponding to thousands of features – a reliable feature detection is mandatory.\"ankara 06533, turkey.\",patika: an integrated visual environment for collaborative construction and analysis of cellular pathways,\"motivation: availability of the sequences of entire genomes shifts the scientific curiosity towards the identification of function of the genomes in large scale as in genome studies. in the near future, data produced about cellular processes at molecular level will accumulate with an accelerating rate as a result of proteomics studies. in this regard, it is essential to develop tools for storing, integrating, accessing, and analyzing this data effectively.\"\"biometrics laboratory, wadsworth center for laboratories and research, new york state department of health, albany, new york 12201\",an expectation maximization ({em}) algorithm for the identification and characterization of common sites in unaligned biopolymer sequences,\"statistical methodology for the identification and characterization of protein binding sites in a set of unaligned {dna} fragments is presented. each sequence must contain at least one common site. no alignment of the sites is required. instead, the uncertainty in the location of the sites is handled by employing the missing information principle to develop an  ” expectation maximization” ({em}) algorithm. this approach allows for the simultaneous identification of the sites and characterization of the binding motifs. the reliability of the algorithm increases with the number of fragments, but the computations increase only linearly. the method is illustrated with an example, using known cyclic adenosine monophophate receptor protein ({crp}) binding sites. the final motif is utilized in a search for undiscovered {crp} binding sites.\"\"{background}:protein-protein interaction ({ppi}) databases have become a major resource for investigating biological networks and pathways in cells. a number of publicly available repositories for human {ppis} are currently available. each of these databases has their own unique features with a large variation in the type and depth of their {annotations.results}:we analyzed the major publicly available primary databases that contain literature curated {ppi} information for human proteins. this included {bind}, {dip}, {hprd}, {intact}, {mint}, {mips}, {pdzbase} and reactome databases. the number of binary non-redundant human {ppis} ranged from 101 in {pdzbase} and 346 in {mips} to 11,367 in {mint} and 36,617 in {hprd}. the number of genes annotated with at least one interactor was 9,427 in {hprd}, 4,975 in {mint}, 4,614 in {intact}, 3,887 in {bind} and <1,000 in the remaining databases. the number of literature citations for the {ppis} included in the databases was 43,634 in {hprd}, 11,480 in {mint}, 10,331 in {intact}, 8,020 in {bind} and <2,100 in the remaining {databases.conclusion}:given the importance of {ppis}, we suggest that submission of {ppis} to repositories be made mandatory by scientific journals at the time of manuscript submission as this will minimize annotation errors, promote standardization and help keep the information up to date. we hope that our analysis will help guide biomedical scientists in selecting the most appropriate database for their needs especially in light of the dramatic differences in their content.\"\"motivation: transcriptome analysis allows detection and clustering of genes that are coexpressed under various biological circumstances. under the assumption that coregulated genes share cis-acting regulatory elements, it is important to investigate the upstream sequences controlling the transcription of these genes. to improve the robustness of the gibbs sampling algorithm to noisy data sets we propose an extension of this algorithm for motif finding with a higher-order background model.\"\"an international, peer-reviewed genome sciences journal featuring outstanding original research that offers novel insights into the biology of all organisms\"department of molecular virology, immunology and medical genetics , the ohio state university, columbus, 43210, usa.\",{agris} and {atregnet}. a platform to link {cis-regulatory} elements and transcription factors into regulatory networks,\"gene regulatory pathways converge at the level of transcription, where interactions among regulatory genes and between regulators and target genes result in the establishment of spatiotemporal patterns of gene expression. the growing identification of direct target genes for key transcription factors ({tfs}) through traditional and high-throughput experimental approaches has facilitated the elucidation of regulatory networks at the genome level. to integrate this information into a web-based knowledgebase, we have developed the arabidopsis gene regulatory information server ({agris}). {agris}, which contains all arabidopsis (arabidopsis thaliana) promoter sequences, {tfs}, and their target genes and functions, provides the scientific community with a platform to establish regulatory networks. {agris} currently houses three linked databases: {atcisdb} (arabidopsis thaliana cis-regulatory database), {attfdb} (arabidopsis thaliana transcription factor database), and {atregnet} (arabidopsis thaliana regulatory network). {attfdb} contains 1,690 arabidopsis {tfs} and their sequences (protein and {dna}) grouped into 50 (october 2005) families with information on available mutants in the corresponding genes. {atcisdb} consists of 25,806 (september 2005) promoter sequences of annotated arabidopsis genes with a description of putative cis-regulatory elements. {atregnet} links, in direct interactions, several hundred genes with the {tfs} that control their expression. the current release of {atregnet} contains a total of 187 (september 2005) direct targets for 66 {tfs}. {agris} can be accessed at {http://arabidopsis}.med.ohio-state.edu.\"dept of molecular virology, immunology and medical genetics, the ohio state university, columbus, oh 43210, usa. davuluri-1@medctr.osu.edu\",\"{agris}: arabidopsis gene regulatory information server, an information resource of arabidopsis cis-regulatory elements and transcription factors\",\"{background}:the gene regulatory information is hardwired in the promoter regions formed by cis-regulatory elements that bind specific transcription factors ({tfs}). hence, establishing the architecture of plant promoters is fundamental to understanding gene expression. the determination of the regulatory circuits controlled by each {tf} and the identification of the cis-regulatory sequences for all genes have been identified as two of the goals of the multinational coordinated arabidopsis thaliana functional genomics project by the multinational arabidopsis steering committee (june {2002).results}:{agris} is an information resource of arabidopsis promoter sequences, transcription factors and their target genes. {agris} currently contains two databases, {attfdb} (arabidopsis thaliana transcription factor database) and {atcisdb} (arabidopsis thaliana cis-regulatory database). {attfdb} contains information on approximately 1,400 transcription factors identified through motif searches and grouped into 34 families. {attfdb} links the sequence of the transcription factors with available mutants and, when known, with the possible genes they may regulate. {atcisdb} consists of the 5\\' regulatory sequences of all 29,388 annotated genes with a description of the corresponding cis-regulatory elements. users can search the databases for (i) promoter sequences, (ii) a transcription factor, (iii) a direct target genes for a specific transcription factor, or (vi) a regulatory network that consists of transcription factors and their target {genes.conclusion}:{agris} provides the necessary software tools on arabidopsis transcription factors and their putative binding sites on all genes to initiate the identification of transcriptional regulatory networks in the model dicotyledoneous plant arabidopsis thaliana. {agris} can be accessed from http://arabidopsis.med.ohio-state.edu webcite.\"\"the study of the metabolite complement of biological samples, known as metabolomics, is creating large amounts of data, and support for handling these data sets is required to facilitate meaningful analyses that will answer biological questions. we present a data model for plant metabolomics known as {armet} (architecture for metabolomics). it encompasses the entire experimental time line from experiment definition and description of biological source material, through sample growth and preparation to the results of chemical analysis. such formal data descriptions, which specify the full experimental context, enable principled comparison of data sets, allow proper interpretation of experimental results, permit the repetition of experiments and provide a basis for the design of systems for data storage and transmission. the current design and example implementations are freely available (http://www.armet.org/). we seek to advance discussion and community adoption of a standard for metabolomics, which would promote principled collection, storage and transmission of experiment data.\"washington 98103, usa.\",cytoscape: a software environment for integrated models of biomolecular interaction networks,\"an international, peer-reviewed genome sciences journal featuring outstanding original research that offers novel insights into the biology of all organisms\"md 20850, usa\",{resourcerer}: a database for annotating and linking microarray resources within and across species.,\"microarray expression analysis is providing unprecedented data on gene expression in humans and mammalian model systems. although such studies provide a tremendous resource for understanding human disease states, one of the significant challenges is cross-referencing the data derived from different species, across diverse expression analysis platforms, in order to properly derive inferences regarding gene expression and disease state. to address this problem, we have developed {resourcerer}, a microarray-resource annotation and cross-reference database built using the analysis of expressed sequence tags ({ests}) and gene sequences provided by the {tigr} gene index ({tgi}) and {tigr} orthologous gene alignment ({toga}) databases [now called eukaryotic gene orthologs ({ego})].\"mn, usa\",toward principles for the design of ontologies used for knowledge sharing?,an abstract is not available.london, nw7 1aa, uk. cedric.notredame@europe.com\",t-coffee: a novel method for fast and accurate multiple sequence alignment,\"we describe a new method ({t-coffee}) for multiple sequence alignment that provides a dramatic improvement in accuracy with a modest sacrifice in speed as compared to the most commonly used alternatives. the method is broadly based on the popular progressive approach to multiple alignment but avoids the most serious pitfalls caused by the greedy nature of this algorithm. with {t-coffee} we pre-process a data set of all pair-wise alignments between the sequences. this provides us with a library of alignment information that can be used to guide the progressive alignment. intermediate alignments are then based not only on the sequences to be aligned next but also on how all of the sequences align with each other. this alignment information can be derived from heterogeneous sources such as a mixture of alignment programs and/or structure superposition. here, we illustrate the power of the approach by using a combination of local and global pair-wise alignments to generate the library. the resulting alignments are significantly more reliable, as determined by comparison with a set of 141 test cases, than any of the popular alternatives that we tried. the improvement, especially clear with the more difficult test cases, is always visible, regardless of the phylogenetic spread of the sequences in the tests.\"kanazawa university, kanazawa 920-0934, japan. titolab@kenroku.kanazawa-u.ac.jp\",a comprehensive two-hybrid analysis to explore the yeast protein interactome,\"protein–protein interactions play crucial roles in the execution of various biological functions. accordingly, their comprehensive description would contribute considerably to the functional interpretation of fully sequenced genomes, which are flooded with novel genes of unpredictable functions. we previously developed a system to examine two-hybrid interactions in all possible combinations between the ≈6,000 proteins of the budding yeast saccharomyces cerevisiae. here we have completed the comprehensive analysis using this system to identify 4,549 two-hybrid interactions among 3,278 proteins. unexpectedly, these data do not largely overlap with those obtained by the other project [uetz, p., et al. (2000) nature (london) 403, 623–627] and hence have substantially expanded our knowledge on the protein interaction space or interactome of the yeast. cumulative connection of these binary interactions generates a single huge network linking the vast majority of the proteins. bioinformatics-aided selection of biologically relevant interactions highlights various intriguing subnetworks. they include, for instance, the one that had successfully foreseen the involvement of a novel protein in spindle pole body function as well as the one that may uncover a hitherto unidentified multiprotein complex potentially participating in the process of vesicular transport. our data would thus significantly expand and improve the protein interaction map for the exploration of genome functions that eventually leads to thorough understanding of the cell as a molecular system.\"3900 reservoir road, nw, box 571414, washington, dc 20057-1414, usa. pirmail@georgetown.edu\",the protein information resource.,\"the protein information resource ({pir}) is an integrated public resource of protein informatics that supports genomic and proteomic research and scientific discovery. {pir} maintains the protein sequence database ({psd}), an annotated protein database containing over 283 000 sequences covering the entire taxonomic range. family classification is used for sensitive identification, consistent annotation, and detection of annotation errors. the superfamily curation defines signature domain architecture and categorizes memberships to improve automated classification. to increase the amount of experimental annotation, the {pir} has developed a bibliography system for literature searching, mapping, and user submission, and has conducted retrospective attribution of citations for experimental features. {pir} also maintains {nref}, a non-redundant reference database, and {iproclass}, an integrated database of protein family, function, and structure information. {pir}-{nref} provides a timely and comprehensive collection of protein sequences, currently consisting of more than 1 000 000 entries from {pir}-{psd}, {swiss}-{prot}, {trembl}, {refseq}, {genpept}, and {pdb}. the {pir} web site (http://pir.georgetown.edu) connects data analysis tools to underlying databases for information retrieval and knowledge discovery, with functionalities for interactive queries, combinations of sequence and text searches, and sorting and visual exploration of search results. the {ftp} site provides free download for {psd} and {nref} biweekly releases and auxiliary databases and files.\"ithaca, new york 14853, usa. djw24@columbia.edu\",collective dynamics of \\'small-world\\' networks,\"networks of coupled dynamical systems have been used to model biological oscillators1, 2, 3, 4, josephson junction arrays5,6, excitable media7, neural networks8, 9, 10, spatial games11, genetic control networks12 and many other self-organizing systems. ordinarily, the connection topology is assumed to be either completely regular or completely random. but many biological, technological and social networks lie somewhere between these two extremes. here we explore simple models of networks that can be tuned through this middle ground: regular networks \\'rewired\\' to introduce increasing amounts of disorder. we find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. we call them \\'small-world\\' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). the neural network of the worm caenorhabditis elegans, the power grid of the western united states, and the collaboration graph of film actors are shown to be small-world networks. models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. in particular, infectious diseases spread more easily in small-world networks than in regular lattices.\"\"gene ontology ({go}) has established itself as the undisputed standard for protein function annotation. most annotations are inferred electronically, i.e. without individual curator supervision, but they are widely considered unreliable. at the same time, we crucially depend on those automated annotations, as most newly sequenced genomes are non-model organisms. here, we introduce a methodology to systematically and quantitatively evaluate electronic annotations. by exploiting changes in successive releases of the {uniprot} gene ontology annotation database, we assessed the quality of electronic annotations in terms of specificity, reliability, and coverage. overall, we not only found that electronic annotations have significantly improved in recent years, but also that their reliability now rivals that of annotations inferred by curators when they use evidence other than experiments from primary literature. this work provides the means to identify the subset of electronic annotations that can be relied upon—an important outcome given that >98\\\\% of all annotations are inferred without direct curation. in the {uniprot} gene ontology annotation database, the largest repository of functional annotations, over 98\\\\% of all function annotations are inferred in silico, without curator oversight. yet these  ” electronic {go} annotations” are generally perceived as unreliable; they are disregarded in many studies. in this article, we introduce novel methodology to systematically evaluate the quality of electronic annotations. we then provide the first comprehensive assessment of the reliability of electronic {go} annotations. overall, we found that electronic annotations are more reliable than generally believed, to an extent that they are competitive with annotations inferred by curators when they use evidence other than experiments from primary literature. but we also report significant variations among inference methods, types of annotations, and organisms. this work provides guidance for gene ontology users and lays the foundations for improving computational approaches to {go} function inference.\"\"signaling networks in eukaryotes are made up of upstream and downstream subnetworks. the upstream subnetwork contains the intertwined network of signaling pathways, while the downstream regulatory part contains transcription factors and their binding sites on the {dna} as well as {micrornas} and their {mrna} targets. currently, most signaling and regulatory databases contain only a subsection of this network, making comprehensive analyses highly time-consuming and dependent on specific data handling expertise. the need for detailed mapping of signaling systems is also supported by the fact that several drug development failures were caused by undiscovered cross-talk or regulatory effects of drug targets. we previously created a uniformly curated signaling pathway resource, {signalink}, to facilitate the analysis of pathway cross-talks. here, we present {signalink} 2, which significantly extends the coverage and applications of its predecessor. we developed a novel concept to integrate and utilize different subsections (i.e., layers) of the signaling network. the multi-layered (onion-like) database structure is made up of signaling pathways, their pathway regulators (e.g., scaffold and endocytotic proteins) and modifier enzymes (e.g., phosphatases, ubiquitin ligases), as well as transcriptional and post-transcriptional regulators of all of these components. the user-friendly website allows the interactive exploration of how each signaling protein is regulated. the customizable download page enables the analysis of any user-specified part of the signaling network. compared to other signaling resources, distinctive features of {signalink} 2 are the following: 1) it involves experimental data not only from humans but from two invertebrate model organisms, c. elegans and d. melanogaster; 2) combines manual curation with large-scale datasets; 3) provides confidence scores for each interaction; 4) operates a customizable download page with multiple file formats (e.g., {biopax}, cytoscape, {sbml}). non-profit users can access {signalink} 2 free of charge at {http://signalink}.org. with {signalink} 2 as a single resource, users can effectively analyze signaling pathways, scaffold proteins, modifier enzymes, transcription factors and {mirnas} that are important in the regulation of signaling processes. this integrated resource allows the systems-level examination of how cross-talks and signaling flow are regulated, as well as provide data for cross-species comparisons and drug discovery analyses.\"\"{biomart} central portal (www.biomart.org) offers a one-stop shop solution to access a wide array of biological databases. these include major biomolecular sequence, pathway and annotation databases such as ensembl, uniprot, reactome, {hgnc}, wormbase and {pride}; for a complete list, visit, http://www.biomart.org/biomart/martview. moreover, the web server features seamless data federation making cross querying of these data sources in a user friendly and unified way. the web server not only provides access through a web interface ({martview}), it also supports programmatic access through a perl {api} as well as {restful} and {soap} oriented web services. the website is free and open to all users and there is no login requirement.\"\"{background}:cellular processes and pathways, whose deregulation may contribute to the development of cancers, are often represented as cascades of proteins transmitting a signal from the cell surface to the nucleus. however, recent functional genomic experiments have identified thousands of interactions for the signalling canonical proteins, challenging the traditional view of pathways as independent functional entities. combining information from pathway databases and interaction networks obtained from functional genomic experiments is therefore a promising strategy to obtain more robust pathway and process representations, facilitating the study of cancer-related {pathways.results}:we present a methodology for extending pre-defined protein sets representing cellular pathways and processes by mapping them onto a protein-protein interaction network, and extending them to include densely interconnected interaction partners. the added proteins display distinctive network topological features and molecular function annotations, and can be proposed as putative new components, and/or as regulators of the communication between the different cellular processes. finally, these extended pathways and processes are used to analyse their enrichment in pancreatic mutated genes. significant associations between mutated genes and certain processes are identified, enabling an analysis of the influence of previously non-annotated cancer mutated {genes.conclusions}:the proposed method for extending cellular pathways helps to explain the functions of cancer mutated genes by exploiting the synergies of canonical knowledge and large-scale interaction data.\"\"the ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. it is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. it is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. the ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. the ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.\"new york, new york, united states of america.\",{phylogibbs}: a gibbs sampling motif finder that incorporates phylogeny,\"a central problem in the bioinformatics of gene regulation is to find the binding sites for regulatory proteins. one of the most promising approaches toward identifying these short and fuzzy sequence patterns is the comparative analysis of orthologous intergenic regions of related species. this analysis is complicated by various factors. first, one needs to take the phylogenetic relationship between the species into account in order to distinguish conservation that is due to the occurrence of functional sites from spurious conservation that is due to evolutionary proximity. second, one has to deal with the complexities of multiple alignments of orthologous intergenic regions, and one has to consider the possibility that functional sites may occur outside of conserved segments. here we present a new motif sampling algorithm, {phylogibbs}, that runs on arbitrary collections of multiple local sequence alignments of orthologous sequences. the algorithm searches over all ways in which an arbitrary number of binding sites for an arbitrary number of transcription factors ({tfs}) can be assigned to the multiple sequence alignments. these binding site configurations are scored by a bayesian probabilistic model that treats aligned sequences by a model for the evolution of binding sites and  ” background” intergenic {dna}. this model takes the phylogenetic relationship between the species in the alignment explicitly into account. the algorithm uses simulated annealing and monte carlo markov-chain sampling to rigorously assign posterior probabilities to all the binding sites that it reports. in tests on synthetic data and real data from five saccharomyces species our algorithm performs significantly better than four other motif-finding algorithms, including algorithms that also take phylogeny into account. our results also show that, in contrast to the other algorithms, {phylogibbs} can make realistic estimates of the reliability of its predictions. our tests suggest that, running on the five-species multiple alignment of a single gene\\'s upstream region, {phylogibbs} on average recovers over 50\\\\% of all binding sites in s. cerevisiae at a specificity of about 50\\\\%, and 33\\\\% of all binding sites at a specificity of about 85\\\\%. we also tested {phylogibbs} on collections of multiple alignments of intergenic regions that were recently annotated, based on {chip}-on-chip data, to contain binding sites for the same {tf}. we compared {phylogibbs}\\'s results with the previous analysis of these data using six other motif-finding algorithms. for 16 of 21 {tfs} for which all other motif-finding methods failed to find a significant motif, {phylogibbs} did recover a motif that matches the literature consensus. in 11 cases where there was disagreement in the results we compiled lists of known target genes from the literature, and found that running {phylogibbs} on their regulatory regions yielded a binding motif matching the literature consensus in all but one of the cases. interestingly, these literature gene lists had little overlap with the targets annotated based on the {chip}-on-chip data. the {phylogibbs} code can be downloaded from http://www.biozentrum.unibas.ch/\\\\~{}nimwege\\u200bn/cgi-bin/phylogibbs.cgi or http://www.imsc.res.in/\\\\~{}rsidd/phylogibbs. the full set of predicted sites from our tests on yeast are available at http://www.swissregulon.unibas.ch. computational discovery of regulatory sites in intergenic {dna} is one of the central problems in bioinformatics. up until recently motif finders would typically take one of the following two general approaches. given a known set of co-regulated genes, one searches their promoter regions for significantly overrepresented sequence motifs. alternatively, in a  ” phylogenetic footprinting” approach one searches multiple alignments of orthologous intergenic regions for short segments that are significantly more conserved than expected based on the phylogeny of the species. in this work the authors present an algorithm, {phylogibbs}, that combines these two approaches into one integrated bayesian framework. the algorithm searches over all ways in which an arbitrary number of binding sites for an arbitrary number of transcription factors can be assigned to arbitrary collections of multiple sequence alignments while taking into account the phylogenetic relations between the sequences. the authors perform a number of tests on synthetic data and real data from saccharomyces genomes in which {phylogibbs} significantly outperforms other existing methods. finally, a novel anneal-and-track strategy allows {phylogibbs} to make accurate estimates of the reliability of its predictions.\"ontologies, and gene expression data\",\"the explosion in the number of functional genomic datasets generated with tools such as {dna} microarrays has created a critical need for resources that facilitate the interpretation of large-scale biological data. {source} is a web-based database that brings together information from a broad range of resources, and provides it in manner particularly useful for genome-scale analyses. {source}\\'s {genereports} include aliases, chromosomal location, functional descriptions, {geneontology} annotations, gene expression data, and links to external databases. we curate published microarray gene expression datasets and allow users to rapidly identify sets of co-regulated genes across a variety of tissues and a large number of conditions using a simple and intuitive interface. {source} provides content both in gene and {cdna} clone-centric pages, and thus simplifies analysis of datasets generated using {cdna} microarrays. {source} is continuously updated and contains the most recent and accurate information available for human, mouse, and rat genes. by allowing dynamic linking to individual gene or clone reports, {source} facilitates browsing of large genomic datasets. finally, {sources} batch interface allows rapid extraction of data for thousands of genes or clones at once and thus facilitates statistical analyses such as assessing the enrichment of functional attributes within clusters of genes. {source} is available at http://source.stanford.edu.\"\"the purpose of this article is to provide a brief history of the development and application of computer algorithms for the analysis and prediction of {dna} binding sites. this problem can be conveniently divided into two subproblems. the first is, given a collection of known binding sites, develop a representation of those sites that can be used to search new sequences and reliably predict where additional binding sites occur. the second is, given a set of sequences known to contain binding sites for a common factor, but not knowing where the sites are, discover the location of the sites in each sequence and a representation for the specificity of the protein.\"\"motivation: we propose a new class of variable-order bayesian network ({vobn}) models for the identification of transcription factor binding sites ({tfbss}). the proposed models generalize the widely used position weight matrix ({pwm}) models, markov models and bayesian network models. in contrast to these models, where for each position a fixed subset of the remaining positions is used to model dependencies, in {vobn} models, these subsets may vary based on the specific nucleotides observed, which are called the context. this flexibility turns out to be of advantage for the classification and analysis of {tfbss}, as statistical dependencies between nucleotides in different {tfbs} positions (not necessarily adjacent) may be taken into account efficiently--in a position-specific and context-specific manner.  results: we apply the {vobn} model to a set of 238 experimentally verified sigma-70 binding sites in escherichia coli. we find that the {vobn} model can distinguish these 238 sites from a set of 472 intergenic  non-promoter\\' sequences with a higher accuracy than fixed-order markov models or bayesian trees. we use a replicated stratified-holdout experiment having a fixed true-negative rate of 99.9\\\\%. we find that for a foreground inhomogeneous {vobn} model of order 1 and a background homogeneous variable-order markov ({vom}) model of order 5, the obtained mean true-positive ({tp}) rate is 47.56\\\\%. in comparison, the best {tp} rate for the conventional models is 44.39\\\\%, obtained from a foreground {pwm} model and a background 2nd-order markov model. as the standard deviation of the estimated {tp} rate is [\\\\~{}=]0.01\\\\%, this improvement is highly significant.  availability: all datasets are available upon request from the authors. a web server for utilizing the {vobn} and {vom} models is available at http://www.eng.tau.ac.il/\\\\~{}bengal/  contact: bengal@eng.tau.ac.il 10.1093/bioinformatics/bti410\"beyond regression: new tools for prediction and analysis in the behavioral sciences,\"the use of ontologies has become a mainstream activity within bioinformatics. in a largely descriptive science such as biology, the need to have a common understanding of things described is obvious. the need to be able to apply computational methods to the large quantities of data being produced also suggests a computational requirement to standardise descriptions in biology.   as a mechanism for describing the categories of entities and their characteristics, ontologies offer many of the features that can support a descriptive science. the main use of ontologies in bioinformatics has been the delivery of controlled vocabularies. in this chapter we explore this use of ontology, but also other uses, especially those that have a deeper computational aspect. we take a broad view of ontology to include many ontology-like resources and classify the uses of ontology and ontology-like artifacts. we present a series of case studies and conclude by describing the current state and future directions for bio-ontologies.\"national research council canada, saskatoon, saskatchewan. mwilkinson@gene.pbi.nrc.ca\",{biomoby}: an open source biological web services proposal.,\"{biomoby} is an open source research project which aims to generate an architecture for the discovery and distribution of biological data through web services; data and services are decentralised, but the availability of these resources, and the instructions for interacting with them, are registered in a central location called {moby} central. {biomoby} adds to the web services paradigm, as exemplified by universal data discovery and integration ({uddi}), by having an object-driven registry query system with object and service ontologies. this allows users to traverse expansive and disparate data sets where each possible next step is presented based on the data object currently in-hand. moreover, a path from the current data object to a desired final data object could be automatically discovered using the registry. native {biomoby} objects are lightweight {xml}, and make up both the query and the response of a simple object access protocol ({soap}) transaction.\"\"the {biocyc} database collection is a set of 160 pathway/genome databases ({pgdbs}) for most eukaryotic and prokaryotic species whose genomes have been completely sequenced to date. each {pgdb} in the {biocyc} collection describes the genome and predicted metabolic network of a single organism, inferred from the {metacyc} database, which is a reference source on metabolic pathways from multiple organisms. in addition, each bacterial {pgdb} includes predicted operons for the corresponding species. the {biocyc} collection provides a unique resource for computational systems biology, namely global and comparative analyses of genomes and metabolic networks, and a supplement to the {biocyc} resource of curated {pgdbs}. the omics viewer available through the {biocyc} website allows scientists to visualize combinations of gene expression, proteomics and metabolomics data on the metabolic maps of these organisms. this paper discusses the computational methodology by which the {biocyc} collection has been expanded, and presents an aggregate analysis of the collection that includes the range of number of pathways present in these organisms, and the most frequently observed pathways. we seek scientists to adopt and curate individual {pgdbs} within the {biocyc} collection. only by harnessing the expertise of many scientists we can hope to produce biological databases, which accurately reflect the depth and breadth of knowledge that the biomedical research community is producing.\"are graph databases ready for bioinformatics?,contact: lars.juhl.jensen{at}gmail.comusa\",{wordnet}: a lexical database for english,\"because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. this information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. but dictionary entries evolved for the convenience of human readers, not for machines. {wordnet1} provides a more effective combination of traditional lexicographic information and modern computing. {wordnet} is an online lexical database designed for use under program control. english nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. semantic relations link the synonym sets [4].\"systems and communication, university of milan-bicocca, via bicocca degli arcimboldi 8, milan, i-20126, italy. pavesi@disco.unimib.it\",an algorithm for finding signals of unknown length in {dna} sequences.,\"pattern discovery in unaligned {dna} sequences is a challenging problem in both computer science and molecular biology. several different methods and techniques have been proposed so far, but in most of the cases signals in {dna} sequences are very complicated and avoid detection. exact exhaustive methods can solve the problem only for short signals with a limited number of mutations. in this work, we extend exhaustive enumeration also to longer patterns. more in detail, the basic version of algorithm presented in this paper, given as input a set of sequences and an error ratio epsilon < 1, finds all patterns that occur in at least q sequences of the set with at most epsilonm mutations, where m is the length of the pattern. the only restriction is imposed on the location of mutations along the signal. that is, a valid occurrence of a pattern can present at most [epsiloni] mismatches in the first i nucleotides, and so on. however, we show how the algorithm can be used also when no assumption can be made on the position of mutations. in this case, it is also possible to have an estimate of the probability of finding a signal according to the signal length, the error ratio, and the input parameters. finally, we discuss some significance measures that can be used to sort the patterns output by the algorithm.\"\"valuable binding-site annotation data are stored in databases. however, several types of errors can, and do, occur in the process of manually incorporating annotation data from the scientific literature into these databases. here, we introduce {motifadjuster} {http://dig.ipk-gatersleben.de/motifadjuster}.html webcite, a tool that helps to detect these errors, and we demonstrate its efficacy on public data sets.\"an update\",\"the arabidopsis gene regulatory information server ({agris}; http://arabidopsis.med.ohio-state.edu/) provides a comprehensive resource for gene regulatory studies in the model plant arabidopsis thaliana. three interlinked databases, {attfdb}, {atcisdb} and {atregnet}, furnish comprehensive and updated information on transcription factors ({tfs}), predicted and experimentally verified cis-regulatory elements ({cres}) and their interactions, respectively. in addition to significant contributions in the identification of the entire set of {tf}–{dna} interactions, which are the key to understand the gene regulatory networks that govern arabidopsis gene expression, tools recently incorporated into {agris} include the complete set of words length 5–15 present in the arabidopsis genome and the integration of {atregnet} with visualization tools, such as the recently developed {rein} application. all the information in {agris} is publicly available and downloadable upon registration.\"oxford road, manchester m13 9pl, uk. garwood@cs.man.ac.uk\",\"{pedro}: a database for storing, searching and disseminating experimental proteomics data\",\"{background}:proteomics is rapidly evolving into a high-throughput technology, in which substantial and systematic studies are conducted on samples from a wide range of physiological, developmental, or pathological conditions. reference maps from {2d} gels are widely circulated. however, there is, as yet, no formally accepted standard representation to support the sharing of proteomics data, and little systematic dissemination of comprehensive proteomic data {sets.results}:this paper describes the design, implementation and use of a proteome experimental data repository ({pedro}), which makes comprehensive proteomics data sets available for browsing, searching and downloading. it is also serves to extend the debate on the level of detail at which proteomics data should be captured, the sorts of facilities that should be provided by proteome data management systems, and the techniques by which such facilities can be made {available.conclusions}:the {pedro} database provides access to a collection of comprehensive descriptions of experimental data sets in proteomics. not only are these data sets interesting in and of themselves, they also provide a useful early validation of the {pedro} data model, which has served as a starting point for the ongoing standardisation activity through the proteome standards initiative of the human proteome organisation.\"globally integrated and scored.\",\"an essential prerequisite for any systems-level understanding of cellular functions is to correctly uncover and annotate all functional interactions among proteins in the cell. toward this goal, remarkable progress has been made in recent years, both in terms of experimental measurements and computational prediction techniques. however, public efforts to collect and present protein interaction information have struggled to keep up with the pace of interaction discovery, partly because protein-protein interaction information can be error-prone and require considerable effort to annotate. here, we present an update on the online database resource search tool for the retrieval of interacting genes ({string}); it provides uniquely comprehensive coverage and ease of access to both experimental as well as predicted interaction information. interactions in {string} are provided with a confidence score, and accessory information such as protein domains and {3d} structures is made available, all within a stable and consistent identifier space. new features in {string} include an interactive network viewer that can cluster networks on demand, updated on-screen previews of structural information including homology models, extensive data updates and strongly improved connectivity and integration with third-party resources. version 9.0 of {string} covers more than 1100 completely sequenced organisms; the resource can be reached at http://string-db.org.\"\"we present an online server that generates a {3d} representation of properties of user-submitted {rna} or {dna} alignments. the visualized properties are information of single alignment columns, mutual information of two alignment positions as well as the position-specific fraction of gaps. the nucleotide composition of both single columns and column pairs is visualized with the help of color-coded {3d} bars labeled with letters. the server generates both {vrml} and {jvx} output that can be viewed with a {vrml} viewer or the {javaview} applet, respectively. we show that combining these different features of an alignment into one {3d} representation is helpful in identifying correlations between bases and potential {rna} and {dna} base pairs. significant known correlations between the {trna} 3\\' anticodon cardinal nucleotide and the extended anticodon were observed, as were correlations within the amino acid acceptor stem and between the cardinal nucleotide and the acceptor stem. the online server can be accessed using the {url} http://correlogo.abcc.ncifcrf.gov.\"\"a genetic interaction network containing ∼1000 genes and ∼4000 interactions was mapped by crossing mutations in 132 different query genes into a set of ∼4700 viable gene yeast deletion mutants and scoring the double mutant progeny for fitness defects. network connectivity was predictive of function because interactions often occurred among functionally related genes, and similar patterns of interactions tended to identify components of the same pathway. the genetic network exhibited dense local neighborhoods; therefore, the position of a gene on a partially mapped network is predictive of other genetic interactions. because digenic interactions are common in yeast, similar networks may underlie the complex genetics associated with inherited phenotypes in other organisms.\"friedrich-schiller-university jena, jena, germany.\",towards de novo identification of metabolites by analyzing tandem mass spectra.,\"mass spectrometry is among the most widely used technologies in proteomics and metabolomics. being a high-throughput method, it produces large amounts of data that necessitates an automated analysis of the spectra. clearly, database search methods for protein analysis can easily be adopted to analyze metabolite mass spectra. but for metabolites, de novo interpretation of spectra is even more important than for protein data, because metabolite spectra databases cover only a small fraction of naturally occurring metabolites: even the model plant arabidopsis thaliana has a large number of enzymes whose substrates and products remain unknown. the field of bio-prospection searches biologically diverse areas for metabolites which might serve as pharmaceuticals. de novo identification of metabolite mass spectra requires new concepts and methods since, unlike proteins, metabolites possess a non-linear molecular structure. in this work, we introduce a method for fully automated de novo identification of metabolites from tandem mass spectra. mass spectrometry data is usually assumed to be insufficient for identification of molecular structures, so we want to estimate the molecular formula of the unknown metabolite, a crucial step for its identification. the method first calculates all molecular formulas that explain the parent peak mass. then, a graph is build where vertices correspond to molecular formulas of all peaks in the fragmentation mass spectra, whereas edges correspond to hypothetical fragmentation steps. our algorithm afterwards calculates the maximum scoring subtree of this graph: each peak in the spectra must be scored at most once, so the subtree shall contain only one explanation per peak. unfortunately, finding this subtree is {np}-hard. we suggest three exact algorithms (including one fixed parameter tractable algorithm) as well as two heuristics to solve the problem. tests on real mass spectra show that the {fpt} algorithm and the heuristics solve the problem suitably fast and provide excellent results: for all 32 test compounds the correct solution was among the top five suggestions, for 26 compounds the first suggestion of the exact algorithm was correct. http://www.bio.inf.uni-jena.de/tandemms\"167a castetter hall, albuquerque, nm 87131-1091, usa.\",the small world inside large metabolic networks.,\"the metabolic network of the catabolic, energy and biosynthetic metabolism of escherichia coli is a paradigmatic case for the large genetic and metabolic networks that functional genomics efforts are beginning to elucidate. to analyse the structure of previously unknown networks involving hundreds or thousands of components by simple visual inspection is impossible, and quantitative approaches are needed to analyse them. we have undertaken a graph theoretical analysis of the e. coli metabolic network and find that this network is a small-world graph, a type of graph distinct from both regular and random networks and observed in a variety of seemingly unrelated areas, such as friendship networks in sociology, the structure of electrical power grids, and the nervous system of caenorhabditis elegans. moreover, the connectivity of the metabolites follows a power law, another unusual but by no means rare statistical distribution. this provides an objective criterion for the centrality of the tricarboxylic acid cycle to metabolism. the small-world architecture may serve to minimize transition times between metabolic states, and contains evidence about the evolutionary history of metabolism.\"usa.\",the value of prior knowledge in discovering motifs with {meme}.,\"{meme} is a tool for discovering motifs in sets of protein or {dna} sequences. this paper describes several extensions to {meme} which increase its ability to find motifs in a totally unsupervised fashion, but which also allow it to benefit when prior knowledge is available. when no background knowledge is asserted. {meme} obtains increased robustness from a method for determining motif widths automatically, and from probabilistic models that allow motifs to be absent in some input sequences. on the other hand, {meme} can exploit prior knowledge about a motif being present in all input sequences, about the length of a motif and whether it is a palindrome, and (using dirichlet mixtures) about expected patterns in individual motif positions. extensive experiments are reported which support the claim that {meme} benefits from, but does not require, background knowledge. the experiments use seven previously studied {dna} and protein sequence families and 75 of the protein families documented in the prosite database of sites and patterns, release 11.1.\"\"{genemania} (http://www.genemania.org) is a flexible, user-friendly web interface for generating hypotheses about gene function, analyzing gene lists and prioritizing genes for functional assays. given a query list, {genemania} extends the list with functionally similar genes that it identifies using available genomics and proteomics data. {genemania} also reports weights that indicate the predictive value of each selected data set for the query. six organisms are currently supported (arabidopsis thaliana, caenorhabditis elegans, drosophila melanogaster, mus musculus, homo sapiens and saccharomyces cerevisiae) and hundreds of data sets have been collected from {geo}, {biogrid}, pathway commons and {i2d}, as well as organism-specific functional genomics data sets. users can select arbitrary subsets of the data sets associated with an organism to perform their analyses and can upload their own data sets to analyze. the {genemania} algorithm performs as well or better than other gene function prediction methods on yeast and mouse benchmarks. the high accuracy of the {genemania} prediction algorithm, an intuitive user interface and large database make {genemania} a useful tool for any biologist.\"information, knowledge and principle: back to metabolism in {kegg}.\",\"in the hierarchy of data, information and knowledge, computational methods play a major role in the initial processing of data to extract information, but they alone become less effective to compile knowledge from information. the kyoto encyclopedia of genes and genomes ({kegg}) resource (http://www.kegg.jp/ or http://www.genome.jp/kegg/) has been developed as a reference knowledge base to assist this latter process. in particular, the {kegg} pathway maps are widely used for biological interpretation of genome sequences and other high-throughput data. the link from genomes to pathways is made through the {kegg} orthology system, a collection of manually defined ortholog groups identified by k numbers. to better automate this interpretation process the {kegg} modules defined by boolean expressions of k numbers have been expanded and improved. once genes in a genome are annotated with k numbers, the {kegg} modules can be computationally evaluated revealing metabolic capacities and other phenotypic features. the reaction modules, which represent chemical units of reactions, have been used to analyze design principles of metabolic networks and also to improve the definition of k numbers and associated annotations. for translational bioinformatics, the {kegg} {medicus} resource has been developed by integrating drug labels (package inserts) used in society.\"corrensstr, 3, 06466 gatersleben, germany. junker@ipk-gatersleben.de\",{vanted}: a system for advanced data analysis and visualization in the context of biological networks.,\"recent advances with high-throughput methods in life-science research have increased the need for automatized data analysis and visual exploration techniques. sophisticated bioinformatics tools are essential to deduct biologically meaningful interpretations from the large amount of experimental data, and help to understand biological processes. we present {vanted}, a tool for the visualization and analysis of networks with related experimental data. data from large-scale biochemical experiments is uploaded into the software via a microsoft excel-based form. then it can be mapped on a network that is either drawn with the tool itself, downloaded from the {kegg} pathway database, or imported using standard network exchange formats. transcript, enzyme, and metabolite data can be presented in the context of their underlying networks, e. g. metabolic pathways or classification hierarchies. visualization and navigation methods support the visual exploration of the data-enriched networks. statistical methods allow analysis and comparison of multiple data sets such as different developmental stages or genetically different lines. correlation networks can be automatically generated from the data and substances can be clustered according to similar behavior over time. as examples, metabolite profiling and enzyme activity data sets have been visualized in different metabolic maps, correlation networks have been generated and similar time patterns detected. some relationships between different metabolites were discovered which are in close accordance with the literature. {vanted} greatly helps researchers in the analysis and interpretation of biochemical data, and thus is a useful tool for modern biological research. {vanted} as a java web start application including a user guide and example data sets is available free of charge at http://vanted.ipk-gatersleben.de.\"\"the human genome encodes the blueprint of life, but the function of the vast majority of its nearly three billion bases is unknown. the encyclopedia of {dna} elements ({encode}) project has systematically mapped regions of transcription, transcription factor association, chromatin structure and histone modification. these data enabled us to assign biochemical functions for 80\\\\% of the genome, in particular outside of the well-studied protein-coding regions. many discovered candidate regulatory elements are physically associated with one another and with expressed genes, providing new insights into the mechanisms of gene regulation. the newly identified elements also show a statistical correspondence to sequence variants linked to human disease, and can thereby guide interpretation of this variation. overall, the project provides new insights into the organization and regulation of our genes and genome, and is an expansive resource of functional annotations for biomedical research.\"summary: arcadia translates text-based descriptions of biological networks ({sbml} files) into standardized diagrams ({sbgn} {pd} maps). users can view the same model from different perspectives and easily alter the layout to emulate traditional textbook representations.\"motivation: the complexity of cancer is prompting researchers to find new ways to synthesize information from diverse data sources and to carry out coordinated research efforts that span multiple institutions. there is a need for standard applications, common data models, and software infrastructure to enable more efficient access to and sharing of distributed computational resources in cancer research. to address this need the national cancer institute ({nci}) has initiated a national-scale effort, called the cancer biomedical informatics grid ({cabig}™), to develop a federation of interoperable research information {systems.results}: at the heart of the {cabig} approach to federated interoperability effort is a grid middleware infrastructure, called {cagrid}. in this paper we describe the {cagrid} framework and its current implementation, {cagrid} version 0.5. {cagrid} is a model-driven and service-oriented architecture that synthesizes and extends a number of technologies to provide a standardized framework for the advertising, discovery, and invocation of data and analytical resources. we expect {cagrid} to greatly facilitate the launch and ongoing management of coordinated cancer research studies involving multiple institutions, to provide the ability to manage and securely share information and analytic resources, and to spur a new generation of research applications that empower researchers to take a more integrative, trans-domain approach to data mining and {analysis.availability}: the {cagrid} version 0.5 release can be downloaded from {https://cabig.nci.nih.gov/workspaces/architecture}/{cagrid}/. the operational test bed grid can be accessed through the client included in the release, or through the {cagrid}-browser web application {http://cagrid-browser.nci.nih.gov.contact}:joel.saltz@osumc.edu\"conservation and condition-specificity of single and alternative transcription start sites in the drosophila genome\",\"{background}:transcription initiation is a key component in the regulation of gene expression. {mrna} 5\\' full-length sequencing techniques have enhanced our understanding of mammalian transcription start sites ({tsss}), revealing different initiation patterns on a genomic {scale.results}:to identify {tsss} in drosophila melanogaster, we applied a hierarchical clustering strategy on available 5\\' expressed sequence tags ({ests}) and identified a high quality set of 5,665 {tsss} for approximately 4,000 genes. we distinguished two initiation patterns: \\'peaked\\' {tsss}, and \\'broad\\' {tss} cluster groups. peaked promoters were found to contain location-specific sequence elements; conversely, broad promoters were associated with non-location-specific elements. in alignments across other drosophila genomes, conservation levels of sequence elements exceeded 90 within the melanogaster subgroup, but dropped considerably for distal species. elements in broad promoters had lower levels of conservation than those in peaked promoters. when characterizing the distributions of {ests}, 64 of {tsss} showed distinct associations to one out of eight different spatiotemporal conditions. available whole-genome tiling array time series data revealed different temporal patterns of embryonic activity across the majority of genes with distinct alternative promoters. many genes with maternally inherited transcripts were found to have alternative promoters utilized later in development. core promoters of maternally inherited transcripts showed differences in motif composition compared to zygotically active {promoters.conclusions}:our study provides a comprehensive map of drosophila {tsss} and the conditions under which they are utilized. distinct differences in motif associations with initiation pattern and spatiotemporal utilization illustrate the complex regulatory code of transcription initiation.\"\"the use of computational modeling to describe and analyze biological systems is at the heart of systems biology. model structures, simulation descriptions and numerical results can be encoded in structured formats, but there is an increasing need to provide an additional semantic layer. semantic information adds meaning to components of structured descriptions to help identify and interpret them unambiguously. ontologies are one of the tools frequently used for this purpose. we describe here three ontologies created specifically to address the needs of the systems biology community. the systems biology ontology ({sbo}) provides semantic information about the model components. the kinetic simulation algorithm ontology ({kisao}) supplies information about existing algorithms available for the simulation of systems biology models, their characterization and interrelationships. the terminology for the description of dynamics ({teddy}) categorizes dynamical features of the simulation results and general systems behavior. the provision of semantic information extends a model\\'s longevity and facilitates its reuse. it provides useful insight into the biology of modeled processes, and may be used to make informed decisions on subsequent simulation experiments.\"visualizing, and analyzing mass spectrometry-based molecular profile data\",\"mass spectrometry ({ms}) coupled with online separation methods is commonly applied for differential and quantitative profiling of biological samples in metabolomic as well as proteomic research. such approaches are used for systems biology, functional genomics, and biomarker discovery, among others. an ongoing challenge of these molecular profiling approaches, however, is the development of better data processing methods. here we introduce a new generation of a popular open-source data processing toolbox, {mzmine} 2. a key concept of the {mzmine} 2 software design is the strict separation of core functionality and data processing modules, with emphasis on easy usability and support for high-resolution spectra processing. data processing modules take advantage of embedded visualization tools, allowing for immediate previews of parameter settings. newly introduced functionality includes the identification of peaks using online databases, {msn} data support, improved isotope pattern support, scatter plot visualization, and a new method for peak list alignment based on the random sample consensus ({ransac}) algorithm. the performance of the {ransac} alignment was evaluated using synthetic datasets as well as actual experimental data, and the results were compared to those obtained using other alignment algorithms. {mzmine} 2 is freely available under a {gnu} {gpl} license and can be obtained from the project website at: http://mzmine.sourceforge.net/. the current version of {mzmine} 2 is suitable for processing large batches of data and has been applied to both targeted and non-targeted metabolomic analyses.\"\"we have previously combined statistical alignment and phylogenetic footprinting to detect conserved functional elements without assuming a fixed alignment. considering a probability-weighted distribution of alignments removes sensitivity to alignment errors, properly accommodates regions of alignment uncertainty, and increases the accuracy of functional element prediction. our method utilized standard dynamic programming hidden markov model algorithms to analyze up to four sequences.\"histone core modifications regulating nucleosome structure and dynamics,\"post-translational modifications of histones regulate all {dna}-templated processes, including replication, transcription and repair. these modifications function as platforms for the recruitment of specific effector proteins, such as transcriptional regulators or chromatin remodellers. recent data suggest that histone modifications also have a direct effect on nucleosomal\"\"motivation: protein-protein interactions ({ppis}), though extremely valuable towards a better understanding of protein functions and cellular processes, do not provide any direct information about the regions/domains within the proteins that mediate the interaction. most often, it is only a fraction of a protein that directly interacts with its biological partners. thus, understanding interaction at the domain level is a critical step towards (i) thorough understanding of {ppi} networks; (ii) precise identification of binding sites; (iii) acquisition of insights into the causes of deleterious mutations at interaction sites; and (iv) most importantly, development of drugs to inhibit pathological protein interactions. in addition, knowledge derived from known domain–domain interactions ({ddis}) can be used to understand binding interfaces, which in turn can help discover unknown {ppis}.\"\"qualitative frameworks, especially those based on the logical discrete formalism, are increasingly used to model regulatory and signalling networks. a major advantage of these frameworks is that they do not require precise quantitative data, and that they are well-suited for studies of large networks. while numerous groups have developed specific computational tools that provide original methods to analyse qualitative models, a standard format to exchange qualitative models has been missing.\"\"the minimum information required in the annotation of models registry (http://www.ebi.ac.uk/miriam) provides unique, perennial and location-independent identifiers for data used in the biomedical domain. at its core is a shared catalogue of data collections, for each of which an individual namespace is created, and extensive metadata recorded. this namespace allows the generation of uniform resource identifiers ({uris}) to uniquely identify any record in a collection. moreover, various services are provided to facilitate the creation and resolution of the identifiers. since its launch in 2005, the system has evolved in terms of the structure of the identifiers provided, the software infrastructure, the number of data collections recorded, as well as the scope of the registry itself. we describe here the new parallel identification scheme and the updated supporting software infrastructure. we also introduce the new identifiers.org service (http://identifiers.org) that is built upon the information stored in the registry and which provides directly resolvable identifiers, in the form of uniform resource locators ({urls}). the flexibility of the identification scheme and resolving system allows its use in many different fields, where unambiguous and perennial identification of data entities are necessary.\"\"most of the published quantitative models in biology are lost for the community because they are either not made available or they are insufficiently characterized to allow them to be reused. the lack of a standard description format, lack of stringent reviewing and authors\\' carelessness are the main causes for incomplete model descriptions. with today\\'s increased interest in detailed biochemical models, it is necessary to define a minimum quality standard for the encoding of those models. we propose a set of rules for curating quantitative models of biological systems. these rules define procedures for encoding and annotating models represented in machine-readable form. we believe their application will enable users to (i) have confidence that curated models are an accurate reflection of their associated reference descriptions, (ii) search collections of curated models with precision, (iii) quickly identify the biological phenomena that a given curated model or model constituent represents and (iv) facilitate model reuse and composition into large subcellular models.\"{cronos}: the cross-reference navigation server.,piscataway, nj 08854-8087, usa. berman@rcsb.rutgers.edu\",the protein data bank,\"the protein data bank ({pdb}; http://www.rcsb.org/pdb/ ) is the single worldwide archive of structural data of biological macromolecules. this paper describes the goals of the {pdb}, the systems in place for data deposition and access, how to obtain further information, and near-term plans for the future development of the resource.\"software for systems biology: from tools to integrated platforms,environmentally induced foregut remodeling by {pha}-{4/foxa} and {daf}-{12/nhr}.,\"growth and development of the caenorhabditis elegans foregut (pharynx) depends on coordinated gene expression, mediated by pharynx defective ({pha})-{4/foxa} in combination with additional, largely unidentified transcription factors. here, we used whole genome analysis to establish clusters of genes expressed in different pharyngeal cell types. we created an expectation maximization algorithm to identify cis-regulatory elements that activate expression within the pharyngeal gene clusters. one of these elements mediates the response to environmental conditions within pharyngeal muscles and is recognized by the nuclear hormone receptor ({nhr}) {daf}-12. our data suggest that {pha}-4 and {daf}-12 endow the pharynx with transcriptional plasticity to respond to diverse developmental and physiological cues. our combination of bioinformatics and in vivo analysis has provided a powerful means for genome-wide investigation of transcriptional control.\"\"metabolomic studies are targeted at identifying and quantifying all metabolites in a given biological context. among the tools used for metabolomic research, mass spectrometry is one of the most powerful tools. however, metabolomics by mass spectrometry always reveals a high number of unknown compounds which complicate in depth mechanistic or biochemical understanding. in principle, mass spectrometry can be utilized within strategies of de novo structure elucidation of small molecules, starting with the computation of the elemental composition of an unknown metabolite using accurate masses with errors <5 ppm (parts per million). however even with very high mass accuracy (<1 ppm) many chemically possible formulae are obtained in higher mass regions. in automatic routines an additional orthogonal filter therefore needs to be applied in order to reduce the number of potential elemental compositions. this report demonstrates the necessity of isotope abundance information by mathematical confirmation of the concept. high mass accuracy (<1 ppm) alone is not enough to exclude enough candidates with complex elemental compositions (c, h, n, s, o, p, and potentially f, cl, br and si). use of isotopic abundance patterns as a single further constraint removes >95\\\\% of false candidates. this orthogonal filter can condense several thousand candidates down to only a small number of molecular formulas. example calculations for 10, 5, 3, 1 and 0.1 ppm mass accuracy are given. corresponding software scripts can be downloaded from http://fiehnlab.ucdavis.edu. a comparison of eight chemical databases revealed that {pubchem} and the dictionary of natural products can be recommended for automatic queries using molecular formulae. more than 1.6 million molecular formulae in the range 0-500 da were generated in an exhaustive manner under strict observation of mathematical and chemical rules. assuming that ion species are fully resolved (either by chromatography or by high resolution mass spectrometry), we conclude that a mass spectrometer capable of 3 ppm mass accuracy and 2\\\\% error for isotopic abundance patterns outperforms mass spectrometers with less than 1 ppm mass accuracy or even hypothetical mass spectrometers with 0.1 ppm mass accuracy that do not include isotope information in the calculation of molecular formulae.\"\"summary: {biodbnet} is an online web resource that provides interconnected access to many types of biological databases. it has integrated many of the most commonly used biological databases and in its current state has 153 database identifiers (nodes) covering all aspects of biology including genes, proteins, pathways and other biological concepts. {biodbnet} offers various ways to work with these databases including conversions, extensive database reports, custom navigation and has various tools to enhance the quality of the results. importantly, the access to {biodbnet} is updated regularly, providing access to the most recent releases of each individual {database.availability}: {http://biodbnet.abcc.ncifcrf.govcontact}: {stephensr@mail.nih.govsupplementary} information: supplementary data are available at bioinformatics online\"\"{background}:the goal of information integration in systems biology is to combine information from a number of databases and data sets, which are obtained from both high and low throughput experiments, under one data management scheme such that the cumulative information provides greater biological insight than is possible with individual information sources considered {separately.results}:here we present {pathsys}, a graph-based system for creating a combined database of networks of interaction for generating integrated view of biological mechanisms. we used {pathsys} to integrate over 14 curated and publicly contributed data sources for the budding yeast (s. cerevisiae) and gene ontology. a number of exploratory questions were formulated as a combination of relational and graph-based queries to the integrated database. thus, {pathsys} is a general-purpose, scalable, graph-data warehouse of biological information, complete with a graph manipulation and a query language, a storage mechanism and a generic data-importing mechanism through {schema-mapping.conclusion}:results from several test studies demonstrate the effectiveness of the approach in retrieving biologically interesting relations between genes and proteins, the networks connecting them, and of the utility of {pathsys} as a scalable graph-based warehouse for interaction-network integration and a hypothesis generator system. the {pathsys}\\'s client software, named {biologicalnetworks}, developed for navigation and analyses of molecular networks, is available as a java web start application at {http://brak.sdsc.edu/pub/biologicalnetworks} webcite.\"wa, usa.\",{bionetbuilder}: automatic integration of biological networks,\"{bionetbuilder} is an open-source client-server cytoscape plugin that offers a user-friendly interface to create biological networks integrated from several databases. users can create networks for ∼1500 organisms, including common model organisms and human. currently supported databases include: {dip}, {bind}, prolinks, {kegg}, {hprd}, the {biogrid} and {go}, among others. the {bionetbuilder} plugin client is available as a java webstart, providing a platform-independent network interface to these public {databases.availability}:{http://err.bio.nyu.edu/cytoscape/bionetbuilder/contact}:iliana\\\\_avila-campillo@merck.com\"\"{jaspar} (http://jaspar.genereg.net) is the largest open-access database of matrix-based nucleotide profiles describing the binding preference of transcription factors from multiple species. the fifth major release greatly expands the heart of {jaspar}-the {jaspar} {core} subcollection, which contains curated, non-redundant profiles-with 135 new curated profiles (74 in vertebrates, 8 in drosophila melanogaster, 10 in caenorhabditis elegans and 43 in arabidopsis thaliana; a 30\\\\% increase in total) and 43 older updated profiles (36 in vertebrates, 3 in d. melanogaster and 4 in a. thaliana; a 9\\\\% update in total). the new and updated profiles are mainly derived from published chromatin immunoprecipitation-seq experimental datasets. in addition, the web interface has been enhanced with advanced capabilities in browsing, searching and subsetting. finally, the new {jaspar} release is accompanied by a new {biopython} package, a new r tool package and a new {r/bioconductor} data package to facilitate access for both manual and automated methods.\"\"the analysis and usage of biological data is hindered by the spread of information across multiple repositories and the difficulties posed by different nomenclature systems and storage formats. in particular, there is an important need for data unification in the study and use of protein-protein interactions. without good integration strategies, it is difficult to analyze the whole set of available data and its properties. we introduce {biana} (biologic interactions and network analysis), a tool for biological information integration and network management. {biana} is a python framework designed to achieve two major goals: i) the integration of multiple sources of biological information, including biological entities and their relationships, and ii) the management of biological information as a network where entities are nodes and relationships are edges. moreover, {biana} uses properties of proteins and genes to infer latent biomolecular relationships by transferring edges to entities sharing similar properties. {biana} is also provided as a plugin for cytoscape, which allows users to visualize and interactively manage the data. a web interface to {biana} providing basic functionalities is also available. the software can be downloaded under {gnu} {gpl} license from {http://sbi.imim.es/web/biana}.php. {biana}\\'s approach to data unification solves many of the nomenclature issues common to systems dealing with biological data. {biana} can easily be extended to handle new specific data repositories and new specific data types. the unification protocol allows {biana} to be a flexible tool suitable for different user requirements: non-expert users can use a suggested unification protocol while expert users can define their own specific unification rules.\"santa cruz, ca 95064, usa. donnak@soe.ucsc.edu\",the {ucsc} genome browser database: 2008 update,\"the university of california, santa cruz, genome browser database ({gbd}) provides integrated sequence and annotation data for a large collection of vertebrate and model organism genomes. seventeen new assemblies have been added to the database in the past year, for a total coverage of 19 vertebrate and 21 invertebrate species as of september 2007. for each assembly, the {gbd} contains a collection of annotation data aligned to the genomic sequence. highlights of this year\\'s additions include a 28-species human-based vertebrate conservation annotation, an enhanced {ucsc} genes set, and more human variation, {mgc}, and {encode} data. the database is optimized for fast interactive performance with a set of web-based tools that may be used to view, manipulate, filter and download the annotation data. new toolset features include the genome graphs tool for displaying genome-wide data sets, session saving and sharing, better custom track management, expanded genome browser configuration options and a genome browser wiki site. the downloadable {gbd} data, the companion genome browser toolset and links to documentation and related information can be found at: http://genome.ucsc.edu/.\"\"{background}:a significant problem in the study of mechanisms of an organism\\'s development is the elucidation of interrelated factors which are making an impact on the different levels of the organism, such as genes, biological molecules, cells, and cell systems. numerous sources of heterogeneous data which exist for these subsystems are still not integrated sufficiently enough to give researchers a straightforward opportunity to analyze them together in the same frame of study. systematic application of data integration methods is also hampered by a multitude of such factors as the orthogonal nature of the integrated data and naming {problems.results}:here we report on a new version of {biologicalnetworks}, a research environment for the integral visualization and analysis of heterogeneous biological data. {biologicalnetworks} can be queried for properties of thousands of different types of biological entities (genes/proteins, promoters, {cogs}, pathways, binding sites, and other) and their relations (interactions, co-expression, co-citations, and other). the system includes the build-pathways infrastructure for molecular interactions/relations and module discovery in high-throughput experiments. also implemented in {biologicalnetworks} are the integrated genome viewer and comparative genomics browser applications, which allow for the search and analysis of gene regulatory regions and their conservation in multiple species in conjunction with molecular pathways/networks, experimental data and functional {annotations.conclusions}:the new release of {biologicalnetworks} together with its back-end database introduces extensive functionality for a more efficient integrated multi-level analysis of microarray, sequence, regulatory, and other data. {biologicalnetworks} is freely available at http://www.biologicalnetworks.org webcite.\"hinxton, cambridge cb10 1sd, uk. apweiler@ebi.ac.uk\",{uniprot}: the universal protein knowledgebase,\"to provide the scientific community with a single, centralized, authoritative resource for protein sequences and functional information, the {swiss‐prot}, {trembl} and {pir} protein database activities have united to form the universal protein knowledgebase ({uniprot}) consortium. our mission is to provide a comprehensive, fully classified, richly and accurately annotated protein sequence knowledgebase, with extensive cross‐references and query interfaces. the central database will have two sections, corresponding to the familiar {swiss‐prot} (fully manually curated entries) and {trembl} (enriched with automated classification, annotation and extensive cross‐references). for convenient sequence searches, {uniprot} also provides several non‐redundant sequence databases. the {uniprot} {nref} ({uniref}) databases provide representative subsets of the knowledgebase suitable for efficient searching. the comprehensive {uniprot} archive ({uniparc}) is updated daily from many public source databases. the {uniprot} databases can be accessed online (http://www.uniprot.org) or downloaded in several formats (ftp://ftp.uniprot.org/pub). the scientific community is encouraged to submit data for inclusion in {uniprot}.\"\"we have used a \"\"perceptron\"\" algorithm to find a weighting function which distinguishes e. coli translational initiation sites from all other sites in a library of over 78,000 nucleotides of {mrna} sequence. the \"\"perceptron\"\" examined sequences as linear representations. the \"\"perceptron\"\" is more successful at finding gene beginnings than our previous searches using \"\"rules\"\" (see previous paper). we note that the weighting function can find translational initiation sites within sequences that were not included in the training set.\"boston, ma 02215, usa. amaral@buphy.bu.edu\",classes of small-world networks,\"we study the statistical properties of a variety of diverse real-world networks. we present evidence of the occurrence of three classes of small-world networks: (a) scale-free networks, characterized by a vertex connectivity distribution that decays as a power law; (b) broad-scale networks, characterized by a connectivity distribution that has a power law regime followed by a sharp cutoff; and (c) single-scale networks, characterized by a connectivity distribution with a fast decaying tail. moreover, we note for the classes of broad-scale and single-scale networks that there are constraints limiting the addition of new links. our results suggest that the nature of such constraints may be the controlling factor for the emergence of different classes of networks.\"90095-1570, usa.\",the database of interacting proteins: 2004 update.,\"the database of interacting proteins (http://dip.doe-mbi.ucla.edu) aims to integrate the diverse body of experimental evidence on protein-protein interactions into a single, easily accessible online database. because the reliability of experimental evidence varies widely, methods of quality assessment have been developed and utilized to identify the most reliable subset of the interactions. this {core} set can be used as a reference when evaluating the reliability of high-throughput protein-protein interaction data sets, for development of prediction methods, as well as in the studies of the properties of protein interaction networks.\"\"motivation: spatio-temporal regulation of gene expression is an indispensable characteristic in the development processes of all animals. \\'master switches\\', a central set of regulatory genes whose states (on/off or activated/deactivated) determine specific developmental fate or cell-fate specification, play a pivotal role for whole developmental processes. in this study on genome-wide integrative network analysis the underlying design principles of developmental gene regulatory networks are examined.\"\"{weblogo} generates sequence logos, graphical representations of the patterns within a multiple sequence alignment. sequence logos provide a richer and more precise description of sequence similarity than consensus sequences and can rapidly reveal significant features of the alignment otherwise difficult to perceive. each logo consists of stacks of letters, one stack for each position in the sequence. the overall height of each stack indicates the sequence conservation at that position (measured in bits), whereas the height of symbols within the stack reflects the relative frequency of the corresponding amino or nucleic acid at that position. {weblogo} has been enhanced recently with additional features and options, to provide a convenient and highly configurable sequence logo generator. a command line interface and the complete, open {weblogo} source code are available for local installation and customization. copyright 2004 cold spring harbor laboratory press\"\"ontologies have proven very useful for capturing knowledge as a hierarchy of terms and their interrelationships. in biology a major challenge has been to construct ontologies of gene function given incomplete biological knowledge and inconsistencies in how this knowledge is manually curated. here we show that large networks of gene and protein interactions in saccharomyces cerevisiae can be used to infer an ontology whose coverage and power are equivalent to those of the manually curated gene ontology ({go}). the network-extracted ontology ({nexo}) contains 4,123 biological terms and 5,766 term-term relations, capturing 58\\\\% of known cellular components. we also explore robust {nexo} terms and term relations that were initially not cataloged in {go}, a number of which have now been added based on our analysis. using quantitative genetic interaction profiling and chemogenomics, we find further support for many of the uncharacterized terms identified by {nexo}, including multisubunit structures related to protein trafficking or mitochondrial function. this work enables a shift from using ontologies to evaluate data to using data to construct and evaluate ontologies.\"csic/usal), 37007 salamanca, spain.\",{apid}: agile protein interaction {dataanalyzer}.,\"agile protein interaction {dataanalyzer} ({apid}) is an interactive bioinformatics web tool developed to integrate and analyze in a unified and comparative platform main currently known information about protein-protein interactions demonstrated by specific small-scale or large-scale experimental methods. at present, the application includes information coming from five main source databases enclosing an unified sever to explore >35 000 different proteins and 111 000 different proven interactions. the web includes search tools to query and browse upon the data, allowing selection of the interaction pairs based in calculated parameters that weight and qualify the reliability of each given protein interaction. such parameters are for the \\'proteins\\': connectivity, cluster coefficient, gene ontology ({go}) functional environment, {go} environment enrichment; and for the \\'interactions\\': number of methods, {go} overlapping, {ipfam} domain-domain interaction. {apid} also includes a graphic interactive tool to visualize selected sub-networks and to navigate on them or along the whole interaction network. the application is available open access at http://bioinfow.dep.usal.es/apid/.\"445 henry mall, madison, wi 53706, usa. ecoli@genetics.wisc.edu\",the complete genome sequence of escherichia coli k-12,\"the 4,639,221–base pair sequence of escherichia {colik}-12 is presented. of 4288 protein-coding genes annotated, 38 percent have no attributed function. comparison with five other sequenced microbes reveals ubiquitous as well as narrowly distributed gene families; many families of similar genes within e. coli are also evident. the largest family of paralogous proteins contains 80 {abc} transporters. the genome as a whole is strikingly organized with respect to the local direction of replication; guanines, oligonucleotides possibly related to replication and recombination, and most genes are so oriented. the genome also contains insertion sequence ({is}) elements, phage remnants, and many other patches of unusual composition indicating genome plasticity through horizontal transfer.\"large sequence-based datasets are often scanned for conserved sequence patterns to extract useful biological information1. sequence logos2 have been developed to visualize conserved patterns in oligonucleotide and protein sequences and rely on shannon\\'s information theory to calculate conservation among all positions in a multiple-sequence alignment.\"{background}:motif discovery aims to detect short, highly conserved patterns in a collection of unaligned {dna} or protein sequences. discriminative motif finding algorithms aim to increase the sensitivity and selectivity of motif discovery by utilizing a second set of sequences, and searching only for patterns that can differentiate the two sets of sequences. potential applications of discriminative motif discovery include discovering transcription factor binding site motifs in {chip}-chip data and finding protein motifs involved in thermal stability using sets of orthologous proteins from thermophilic and mesophilic {organisms.results}:we describe {deme}, a discriminative motif discovery algorithm for use with protein and {dna} sequences. input to {deme} is two sets of sequences; a \"\"positive\"\" set and a \"\"negative\"\" set. {deme} represents motifs using a probabilistic model, and uses a novel combination of global and local search to find the motif that optimally discriminates between the two sets of sequences. {deme} is unique among discriminative motif finders in that it uses an informative bayesian prior on protein motif columns, allowing it to incorporate prior knowledge of residue characteristics. we also introduce four, synthetic, discriminative motif discovery problems that are designed for evaluating discriminative motif finders in various biologically motivated contexts. we test {deme} using these synthetic problems and on two biological problems: finding yeast transcription factor binding motifs in {chip}-chip data, and finding motifs that discriminate between groups of thermophilic and mesophilic orthologous {proteins.conclusion}:using artificial data, we show that {deme} is more effective than a non-discriminative approach when there are \"\"decoy\"\" motifs or when a variant of the motif is present in the \"\"negative\"\" sequences. with real data, we show that {deme} is as good, but not better than non-discriminative algorithms at discovering yeast transcription factor binding motifs. we also show that {deme} can find highly informative thermal-stability protein motifs. binaries for the stand-alone program {deme} is free for academic use and is available at http://bioinformatics.org.au/deme/ webcite\"albany, ny 12201.\",a phylogenetic gibbs sampler that yields centroid solutions for cis-regulatory site prediction,\"motivation: identification of functionally conserved regulatory elements in sequence data from closely related organisms is becoming feasible, due to the rapid growth of public sequence databases. closely related organisms are most likely to have common regulatory motifs; however, the recent speciation of such organisms results in the high degree of correlation in their genome sequences, confounding the detection of functional elements. additionally, alignment algorithms that use optimization techniques are limited to the detection of a single alignment that may not be representative. comparative-genomics studies must be able to address the phylogenetic correlation in the data and efficiently explore the alignment space, in order to make specific and biologically relevant {predictions.results}: we describe here a gibbs sampler that employs a full phylogenetic model and reports an ensemble centroid solution. we describe regulatory motif detection using both simulated and real data, and demonstrate that this approach achieves improved specificity, sensitivity, and positive predictive value over non-phylogenetic algorithms, and over phylogenetic algorithms that report a maximum likelihood {solution.availability}: the software is freely available at {http://bayesweb.wadsworth.org/gibbs/gibbs.htmlcontact}: {william\\\\_thompson\\\\_1}@{brown.edusupplementary} information: supplementary data are available at bioinformatics online.\"university of pittsburgh and department of human genetics, graduate school of public health, and university of pittsburgh cancer institute, school of medicine, university of pittsburgh, pittsburgh, pa, usa.\",{stamp}: a web tool for exploring {dna}-binding motif similarities.,\"{stamp} is a newly developed web server that is designed to support the study of {dna}-binding motifs. {stamp} may be used to query motifs against databases of known motifs; the software aligns input motifs against the chosen database (or alternatively against a user-provided dataset), and lists of the highest-scoring matches are returned. such similarity-search functionality is expected to facilitate the identification of transcription factors that potentially interact with newly discovered motifs. {stamp} also automatically builds multiple alignments, familial binding profiles and similarity trees when more than one motif is inputted. these functions are expected to enable evolutionary studies on sets of related motifs and fixed-order regulatory modules, as well as illustrating similarities and redundancies within the input motif collection. {stamp} is a highly flexible alignment platform, allowing users to \\'mix-and-match\\' between various implemented comparison metrics, alignment methods (local or global, gapped or ungapped), multiple alignment strategies and tree-building methods. motifs may be inputted as frequency matrices (in many of the commonly used formats), consensus sequences, or alignments of known binding sites. {stamp} also directly accepts the output files from 12 supported motif-finders, enabling quick interpretation of motif-discovery analyses. {stamp} is available at http://www.benoslab.pitt.edu/stamp.\"a lightweight cross-platform tool for controlled vocabulary queries\",\"{background}:with the vast amounts of biomedical data being generated by high-throughput analysis methods, controlled vocabularies and ontologies are becoming increasingly important to annotate units of information for ease of search and retrieval. each scientific community tends to create its own locally available ontology. the interfaces to query these ontologies tend to vary from group to group. we saw the need for a centralized location to perform controlled vocabulary queries that would offer both a lightweight web-accessible user interface as well as a consistent, unified {soap} interface for automated {queries.results}:the ontology lookup service ({ols}) was created to integrate publicly available biomedical ontologies into a single database. all modified ontologies are updated daily. a list of currently loaded ontologies is available online. the database can be queried to obtain information on a single term or to browse a complete ontology using {ajax}. auto-completion provides a user-friendly search mechanism. an {ajax}-based ontology viewer is available to browse a complete ontology or subsets of it. a programmatic interface is available to query the webservice using {soap}. the service is described by a {wsdl} descriptor file available online. a sample java client to connect to the webservice using {soap} is available for download from {sourceforge}. all {ols} source code is publicly available under the open source apache {licence.conclusion}:the {ols} provides a user-friendly single entry point for publicly available ontologies in the open biomedical ontology ({obo}) format. it can be accessed interactively or programmatically at http://www.ebi.ac.uk/ontology-lookup/.\"\"recent progress in massively parallel sequencing platforms has enabled genome-wide characterization of {dna}-associated proteins using the combination of chromatin immunoprecipitation and sequencing ({chip}-seq). although a variety of methods exist for analysis of the established alternative {chip} microarray ({chip}-chip), few approaches have been described for processing {chip}-seq data. to fill this gap, we propose an analysis pipeline specifically designed to detect protein-binding positions with high accuracy. using previously reported data sets for three transcription factors, we illustrate methods for improving tag alignment and correcting for background signals. we compare the sensitivity and spatial precision of three peak detection algorithms with published methods, demonstrating gains in spatial precision when an asymmetric distribution of tags on positive and negative strands is considered. we also analyze the relationship between the depth of sequencing and characteristics of the detected binding positions, and provide a method for estimating the sequencing depth necessary for a desired coverage of protein binding sites.\"graph-based analysis and visualization of experimental results with {ondex}.,\"assembling the relevant information needed to interpret the output from high-throughput, genome scale, experiments such as gene expression microarrays is challenging. analysis reveals genes that show statistically significant changes in expression levels, but more information is needed to determine their biological relevance. the challenge is to bring these genes together with biological information distributed across hundreds of databases or buried in the scientific literature (millions of articles). software tools are needed to automate this task which at present is labor-intensive and requires considerable informatics and biological expertise. this article describes {ondex} and how it can be applied to the task of interpreting gene expression results. {ondex} is a database system that combines the features of semantic database integration and text mining with methods for graph-based analysis. an overview of the {ondex} system is presented, concentrating on recently developed features for graph-based analysis and visualization. a case study is used to show how {ondex} can help to identify causal relationships between stress response genes and metabolic pathways from gene expression data. {ondex} also discovered functional annotations for most of the genes that emerged as significant in the microarray experiment, but were previously of unknown function.\"\"chromatin immunoprecipitation experiments followed by sequencing ({chip}–seq) detect {protein–dna} binding events and chemical modifications of histone proteins. challenges in the standard {chip}–seq protocol have motivated recent enhancements in this approach, such as reducing the number of cells that are required and increasing the resolution. complementary experimental approaches — for example, {dnasei} hypersensitive site mapping and analysis of chromatin interactions that are mediated by particular proteins — provide additional information about {dna}-binding proteins and their function. these data are now being used to identify variability in the functions of {dna}-binding proteins across genomes and individuals. in this review, i describe the latest advances in methods to detect and functionally characterize {dna}-bound proteins.\"ihnestr. 73, 14195 berlin, germany.\",predicting transcription factor affinities to {dna} from a biophysical model.,\"theoretical efforts to understand the regulation of gene expression are traditionally centered around the identification of transcription factor binding sites at specific {dna} positions. more recently these efforts have been supplemented by experimental data for relative binding affinities of proteins to longer intergenic sequences. the question arises to what extent these two approaches converge. in this paper, we adopt a physical binding model to predict the relative binding affinity of a transcription factor for a given sequence. we find that a significant fraction of genome-wide binding data in yeast can be accounted for by simple count matrices and a physical model with only two parameters. we demonstrate that our approach is both conceptually and practically more powerful than traditional methods, which require selection of a cutoff. our analysis yields biologically meaningful parameters, suitable for predicting relative binding affinities in the absence of experimental binding data. the c source code for our {trap} program is freely available for non-commercial use at {http://www.molgen.mpg.de/\\\\~{}manke/papers/tfaffinities}/\"five-vertebrate {chip}-seq reveals the evolutionary dynamics of transcription factor binding.,\"transcription factors ({tfs}) direct gene expression by binding to {dna} regulatory regions. to explore the evolution of gene regulation, we used chromatin immunoprecipitation with high-throughput sequencing ({chip}-seq) to determine experimentally the genome-wide occupancy of two {tfs}, {ccaat}/enhancer-binding protein alpha and hepatocyte nuclear factor 4 alpha, in the livers of five vertebrates. although each {tf} displays highly conserved {dna} binding preferences, most binding is species-specific, and aligned binding events present in all five species are rare. regions near genes with expression levels that are dependent on a {tf} are often bound by the {tf} in multiple species yet show no enhanced {dna} sequence constraint. binding divergence between species can be largely explained by sequence changes to the bound motifs. among the binding events lost in one lineage, only half are recovered by another binding event within 10 kilobases. our results reveal large interspecies differences in transcriptional regulation and provide insight into regulatory evolution.\"'),\n",
       " ('beaca02b21b7cad6cb738c5e2682af8d',\n",
       "  'new haven, connecticut 06520, usa.\",annotation transfer between genomes: protein-protein interologs and {protein-dna} regulogs.,\"bioinformatics program, boston university, boston, massachusetts; department of biomedical engineering, boston university, boston, massachusetts\",{zdock}: an initial-stage protein-docking algorithm,\"the development of scoring functions is of great importance to protein docking. here we present a new scoring function for the initial stage of unbound docking. it combines our recently developed pairwise shape complementarity with desolvation and electrostatics. we compare this scoring function with three other functions on a large benchmark of 49 nonredundant test cases and show its superior performance, especially for the antibody-antigen category of test cases. for 44 test cases (90\\\\% of the benchmark), we can retain at least one near-native structure within the top 2000 predictions at the 6° rotational sampling density, with an average of 52 near-native structures per test case. the remaining five difficult test cases can be explained by a combination of poor binding affinity, large backbone conformational changes, and our algorithm\\'s strong tendency for identifying large concave binding pockets. all four scoring functions have been integrated into our fast fourier transform based docking algorithm {zdock}, which is freely available to academic users at http://zlab.bu.edu/∼rong/dock. proteins 2003;52:80–87. {\\\\copyright} 2003 {wiley-liss}, inc.\"\"human protein reference database ({hprd}--http://www.hprd.org/), initially described in 2003, is a database of curated proteomic information pertaining to human proteins. we have recently added a number of new features in {hprd}. these include {phosphomotif} finder, which allows users to find the presence of over 320 experimentally verified phosphorylation motifs in proteins of interest. another new feature is a protein distributed annotation {system--human} proteinpedia (http://www.humanproteinpedia.org/)--through which laboratories can submit their data, which is mapped onto protein entries in {hprd}. over 75 laboratories involved in proteomics research have already participated in this effort by submitting data for over 15,000 human proteins. the submitted data includes mass spectrometry and protein microarray-derived data, among other data types. finally, {hprd} is also linked to a compendium of human signaling pathways developed by our group, {netpath} (http://www.netpath.org/), which currently contains annotations for several cancer and immune signaling pathways. since the last update, more than 5500 new protein sequences have been added, making {hprd} a comprehensive resource for studying the human proteome.\"\"{background}:microarrays revolutionized biological research by enabling gene expression comparisons on a transcriptome-wide scale. microarrays, however, do not estimate absolute expression level accurately. at present, high throughput sequencing is emerging as an alternative methodology for transcriptome studies. although free of many limitations imposed by microarray design, its potential to estimate absolute transcript levels is {unknown.results}:in this study, we evaluate relative accuracy of microarrays and transcriptome sequencing ({rna}-seq) using third methodology: proteomics. we find that {rna}-seq provides a better estimate of absolute expression {levels.conclusion}:our result shows that in terms of overall technical performance, {rna}-seq is the technique of choice for studies that require accurate estimation of absolute transcript levels.\"\"cancers evolve by a reiterative process of clonal expansion, genetic diversification and clonal selection within the adaptive landscapes of tissue ecosystems. the dynamics are complex, with highly variable patterns of genetic diversity and resulting clonal architecture. therapeutic intervention may destroy cancer clones and erode their habitats, but it can also inadvertently provide a potent selective pressure for the expansion of resistant variants. the inherently darwinian character of cancer is the primary reason for this therapeutic failure, but it may also hold the key to more effective control.\"\"intratumoral heterogeneity arises through evolution of genetically diverse subclones during tumor progression. however, whether cells within single genetic clones are functionally equivalent remains unknown. by combining {dna} copy number alteration ({cna}) profiling, sequencing, and lentiviral lineage tracking, we followed the repopulation dynamics of 150 single lentivirus-marked lineages from 10 human colorectal cancers through serial xenograft passages in mice. {cna} and mutational analysis distinguished individual clones and showed that clones remained stable on serial transplantation. despite this stability, the proliferation, persistence, and chemotherapy tolerance of lentivirally marked lineages were variable within each clone. chemotherapy promoted dominance of previously minor or dormant lineages. thus, apart from genetic diversity, tumor cells display inherent functional variability in tumor propagation potential, a mechanism that contributes both to cancer growth and therapy tolerance.\"\"recent advances in functional genomics have helped generate large-scale high-throughput protein interaction data. such networks, though extremely valuable towards molecular level understanding of cells, do not provide any direct information about the regions (domains) in the proteins that mediate the interaction. here, we performed co-evolutionary analysis of domains in interacting proteins in order to understand the degree of co-evolution of interacting and non-interacting domains. using a combination of sequence and structural analysis, we analyzed protein–protein interactions in {f1-atpase}, {sec23p/sec24p}, {dna}-directed {rna} polymerase and nuclear pore complexes, and found that interacting domain pair(s) for a given interaction exhibits higher level of co-evolution than the non-interacting domain pairs. motivated by this finding, we developed a computational method to test the generality of the observed trend, and to predict large-scale domain–domain interactions. given a protein–protein interaction, the proposed method predicts the domain pair(s) that is most likely to mediate the protein interaction. we applied this method on the yeast interactome to predict domain–domain interactions, and used known domain–domain interactions found in {pdb} crystal structures to validate our predictions. our results show that the prediction accuracy of the proposed method is statistically significant. comparison of our prediction results with those from two other methods reveals that only a fraction of predictions are shared by all the three methods, indicating that the proposed method can detect known interactions missed by other methods. we believe that the proposed method can be used with other methods to help identify previously unrecognized domain–domain interactions on a genome scale, and could potentially help reduce the search space for identifying interaction sites.\"boston, ma 02215.\",\"structure, function, and evolution of transient and obligate protein–protein interactions\",\"recent analyses of high-throughput protein interaction data coupled with large-scale investigations of evolutionary properties of interaction networks have left some unanswered questions. to what extent do protein interactions act as constraints during evolution of the protein sequence? how does the type of interaction, specifically transient or obligate, play into these constraints? are the mutations in the binding site of an interacting protein correlated with mutations in the binding site of its partner? we address these and other questions by relying on a carefully curated dataset of protein complex structures. results point to the importance of distinguishing between transient and obligate interactions. we conclude that residues in the interfaces of obligate complexes tend to evolve at a relatively slower rate, allowing them to coevolve with their interacting partners. in contrast, the plasticity inherent in transient interactions leads to an increased rate of substitution for the interface residues and leaves little or no evidence of correlated mutations across the interface.\"\"motivation: structural genomics projects are beginning to produce protein structures with unknown function, therefore, accurate, automated predictors of protein function are required if all these structures are to be properly annotated in reasonable time. identifying the interface between two interacting proteins provides important clues to the function of a protein and can reduce the search space required by docking algorithms to predict the structures of complexes.\"\"biomolecular structure and modelling unit, department of biochemistry and molecular biology, university college london, london, united kingdom\",scoring residue conservation,\"the importance of a residue for maintaining the structure and function of a protein can usually be inferred from how conserved it appears in a multiple sequence alignment of that protein and its homologues. a reliable metric for quantifying residue conservation is desirable. over the last two decades many such scores have been proposed, but none has emerged as a generally accepted standard. this work surveys the range of scores that biologists, biochemists, and, more recently, bioinformatics workers have developed, and reviews the intrinsic problems associated with developing and evaluating such a score. a general formula is proposed that may be used to compare the properties of different particular conservation scores or as a measure of conservation in its own right. proteins 2002;48:227–241. {\\\\copyright} 2002 {wiley-liss}, inc.\"state university of new york at buffalo, 124 sherman hall, buffalo, ny 14214, usa.\",protein binding site prediction using an empirical scoring function,\"most biological processes are mediated by interactions between proteins and their interacting partners including proteins, nucleic acids and small molecules. this work establishes a method called {pinup} for binding site prediction of monomeric proteins. with only two weight parameters to optimize, {pinup} produces not only 42.2\\\\% coverage of actual interfaces (percentage of correctly predicted interface residues in actual interface residues) but also 44.5\\\\% accuracy in predicted interfaces (percentage of correctly predicted interface residues in the predicted interface residues) in a cross validation using a 57-protein dataset. by comparison, the expected accuracy via random prediction (percentage of actual interface residues in surface residues) is only 15\\\\%. the binding sites of the 57-protein set are found to be easier to predict than that of an independent test set of 68 proteins. the average coverage and accuracy for this independent test set are 30.5 and 29.4\\\\%, respectively. the significant gain of {pinup} over expected random prediction is attributed to (i) effective residue-energy score and accessible-surface-area-dependent interface-propensity, (ii) isolation of functional constraints contained in the conservation score from the structural constraints through the combination of residue-energy score (for structural constraints) and conservation score and (iii) a consensus region built on top-ranked initial patches.\"gower street, london, wc1e 6bt, england.\",prediction of protein-protein interaction sites using patch analysis,\"a method for defining and analysing a series of residue patches on the surface of protein structures is used to predict the location of protein-protein interaction sites. each residue patch is analysed for six parameters; solvation potential, residue interface propensity, hydrophobicity, planarity, protrusion and accessible surface area. the method involves the calculation of a relative combined score that gives the probability of a surface patch forming protein-protein interactions. predictions are made for the known structures of protomers from 28 homo-dimers, large protomers from 11 hetero-complexes, small protomers from 14 hetero-complexes, and antigens from six antibody-antigen complexes. the predictions are successful for 66\\\\% (39/59) of the structures and the remainder can usually be rationalized in terms of additional interaction sites.\"\"the sequence of the human genome encodes the genetic instructions for human physiology, as well as rich information about human evolution. in 2001, the international human genome sequencing consortium reported a draft sequence of the euchromatic portion of the human genome. since then, the international collaboration has worked to convert this draft into a genome sequence with high accuracy and nearly complete coverage. here, we report the result of this finishing process. the current genome sequence (build 35) contains 2.85 billion nucleotides interrupted by only 341 gaps. it covers \\\\~{}99\\\\% of the euchromatic genome and is accurate to an error rate of \\\\~{}1 event per 100,000 bases. many of the remaining euchromatic gaps are associated with segmental duplications and will require focused work with new methods. the near-complete sequence, the first for a vertebrate, greatly improves the precision of biological analyses of the human genome including studies of gene number, birth and death. notably, the human genome seems to encode only 20,000–25,000 protein-coding genes. the genome sequence reported here should serve as a firm foundation for biomedical research in the decades ahead.\"lab of biophysical chemistry, university of groningen, nijenborgh 4, 9747 ag groningen, the netherlands; department of computer science, university of groningen, groningen, the netherlands\",{lincs}: a linear constraint solver for molecular simulations,\"in this article, we present a new {linear} constraint solver ({lincs}) for molecular simulations with bond constraints. the algorithm is inherently stable, as the constraints themselves are reset instead of derivatives of the constraints, thereby eliminating drift. although the derivation of the algorithm is presented in terms of matrices, no matrix matrix multiplications are needed and only the nonzero matrix elements have to be stored, making the method useful for very large molecules. at the same accuracy, the {lincs} algorithm is three to four times faster than the {shake} algorithm. parallelization of the algorithm is straightforward.\\xa0{\\\\copyright} 1997 john wiley \\\\& sons, inc.\\xa0j comput chem18: 1463–1472, 1997\"distant metastasis occurs late during the genetic evolution of pancreatic cancer,exploring the genomes of cancer cells: progress and promise.,exome sequencing identifies frequent mutation of the {swi}/{snf} complex gene {pbrm1} in renal carcinoma,\"the genetics of renal cancer is dominated by inactivation of the {vhl} tumour suppressor gene in clear cell carcinoma ({ccrcc}), the commonest histological subtype. a recent large-scale screen of ∼3,500 genes by {pcr}-based exon re-sequencing identified several new cancer genes in {ccrcc} including {utx} (also known as {kdm6a}), {jarid1c} (also known as {kdm5c}) and {setd2} (ref. 2). these genes encode enzymes that demethylate ({utx}, {jarid1c}) or methylate ({setd2}) key lysine residues of histone h3. modification of the methylation state of these lysine residues of histone h3 regulates chromatin structure and is implicated in transcriptional control. however, together these mutations are present in fewer than 15\\\\% of {ccrcc}, suggesting the existence of additional, currently unidentified cancer genes. here, we have sequenced the protein coding exome in a series of primary {ccrcc} and report the identification of the {swi}/{snf} chromatin remodelling complex gene {pbrm1} (ref. 4) as a second major {ccrcc} cancer gene, with truncating mutations in 41\\\\% (92/227) of cases. these data further elucidate the somatic genetic architecture of {ccrcc} and emphasize the marked contribution of aberrant chromatin biology.\"florida state university, tallahassee, florida 32306, usa.\",{meta-ppisp}: a meta web server for protein-protein interaction site prediction,\"summary: a number of complementary methods have been developed for predicting protein-protein interaction sites. we sought to increase prediction robustness and accuracy by combining results from different predictors, and report here a meta web server, {meta-ppisp}, that is built on three individual web servers: {cons-ppisp} (http://pipe.scs.fsu.edu/ppisp.html), promate (http://bioportal.weizmann.ac.il/promate), and {pinup} ({http://sparks.informatics.iupui.edu/pinup}/). a linear regression method, using the raw scores of the three servers as input, was trained on a set of 35 nonhomologous proteins. cross validation showed that {meta-ppisp} outperforms all the three individual servers. at coverages identical to those of the individual methods, the accuracy of {meta-ppisp} is higher by 4.8 to 18.2 percentage points. similar improvements in accuracy are also seen on {capri} and other targets.\"\"it is informative to detect highly conserved positions in proteins and nucleic acid sequence/structure since they are often indicative of structural and/or functional importance. {consurf} (http://consurf.tau.ac.il) and {conseq} (http://conseq.tau.ac.il) are two well-established web servers for calculating the evolutionary conservation of amino acid positions in proteins using an empirical bayesian inference, starting from protein structure and sequence, respectively. here, we present the new version of the {consurf} web server that combines the two independent servers, providing an easier and more intuitive step-by-step interface, while offering the user more flexibility during the process. in addition, the new version of {consurf} calculates the evolutionary rates for nucleic acid sequences. the new version is freely available at: http://consurf.tau.ac.il/.\"rumelifeneri yolu, 34450 sariyer istanbul, turkey; basic research program, saic-frederick, inc., center for cancer research nanobiology program, nci-frederick, frederick, maryland 21702; and sackler institute of molecular medicine, department of human genetics and molecular medicine, sackler school of medicine, tel aviv university, tel aviv 69978, israel\",principles of {protein−protein} interactions: what are the preferred ways for proteins to interact?,pmid: 18355092\"10.1093/bioinformatics/btm215 motivation: mutual information can be used to explore covarying positions in biological sequences. in the past, it has been successfully used to infer {rna} secondary structure conformations from multiple sequence alignments. in this study, we show that the same principles allow the discovery of transcription factor amino acids that are coevolving with nucleotides in their {dna}-binding {targets.results}: given an alignment of transcription factor binding domains, and a separate alignment of their {dna} target motifs, we demonstrate that mutually covarying base-amino acid positions may indicate possible {protein\\\\^{a}\\x80\\x93dna} contacts. examples explored in this study include {c2h2} zinc finger, homeodomain and {bhlh} {dna}-binding motif families, where a number of known base-amino acid contacting positions are identified. mutual information analyses may aid the prediction of base-amino acid contacting pairs for particular transcription factor families, thereby yielding structural insights from sequence information alone. such inference of {protein\\\\^{a}\\x80\\x93dna} contacting positions may guide future experimental studies of {dna} {recognition.contact}: shaun.mahony@ccbb.pitt.edu or benos@pitt.edu\"unil, ch-101 lausanne, conway institute, university college dublin, belfield, dublin 4, ireland and information g\\\\\\'{e}nomique et structurale, cnrs upr2589, institute for structural biology and microbiology (ibsm), parc scientifique de luminy, 163 avenue de luminy, fr- 13288, marseille cedex 09, france.\",the {m-coffee} web server: a meta-method for computing multiple sequence alignments by combining alternative alignment methods.,\"the {m-coffee} server is a web server that makes it possible to compute multiple sequence alignments ({msas}) by running several {msa} methods and combining their output into one single model. this allows the user to simultaneously run all his methods of choice without having to arbitrarily choose one of them. the {msa} is delivered along with a local estimation of its consistency with the individual {msas} it was derived from. the computation of the consensus multiple alignment is carried out using a special mode of the {t-coffee} package [notredame, higgins and heringa ({t-coffee}: a novel method for fast and accurate multiple sequence alignment. j. mol. biol. 2000; 302: 205-217); wallace, {o\\'sullivan}, higgins and notredame ({m-coffee}: combining multiple sequence alignment methods with {t-coffee}. nucleic acids res. 2006; 34: 1692-1699)] given a set of sequences ({dna} or proteins) in {fasta} format, {m-coffee} delivers a multiple alignment in the most common formats. {m-coffee} is a freeware open source package distributed under a {gpl} license and it is available either as a standalone package or as a web service from www.tcoffee.org.\"\"protein evolution depends on intramolecular coevolutionary networks whose complexity is proportional to the underlying functional and structural interactions among sites. here we present a novel approach that vastly improves the sensitivity of previous methods for detecting coevolution through a weighted comparison of divergence between amino acid sites. the analysis of the {hiv}-1 gag protein detected convergent adaptive coevolutionary events responsible for the selective variability emerging between subtypes. coevolution analysis and functional data for heat-shock proteins, hsp90 and {groel}, highlight that almost all detected coevolving sites are functionally or structurally important. the results support previous suggestions pinpointing the complex interdomain functional interactions within these proteins and we propose new amino acid sites as important for interdomain functional communication. three-dimensional information sheds light on the functional and structural constraints governing the coevolution between sites. our covariation analyses propose two types of coevolving sites in agreement with previous reports: pairs of sites spatially proximal, where compensatory mutations could maintain the local structure stability, and clusters of distant sites located in functional domains, suggesting a functional dependency between them. all sites detected under adaptive evolution in these proteins belong to coevolution groups, further underlining the importance of testing for coevolution in selective constraints analyses.\"houston, tx 77030, usa.\",rapid detection of similarity in protein structure and function through contact metric distances,\"the characterization of biological function among newly determined protein structures is a central challenge in structural genomics. one class of computational solutions to this problem is based on the similarity of protein structure. here, we implement a simple yet efficient measure of protein structure similarity, the contact metric. even though its computation avoids structural alignments and is therefore nearly instantaneous, we find that small values correlate with geometrical root mean square deviations obtained from structural alignments. to test whether the contact metric detects functional similarity, as defined by gene ontology ({go}) terms, it was compared in large-scale computational experiments to four other measures of structural similarity, including alignment algorithms as well as alignment independent approaches. the contact metric was the fastest method and its sensitivity, at any given specificity level, was a close second only to fast alignment and search tool—a structural alignment method that is slower by three orders of magnitude. critically, nearly 40\\\\% of correct functional inferences by the contact metric were not identified by any other approach, which shows that the contact metric is complementary and computationally efficient in detecting functional relationships between proteins. a public \\'contact metric internet server\\' is provided.\"69117 heidelberg, germany.\",protein complexes: structure prediction challenges for the 21st century,\"only a tiny fraction of the many hundreds of known protein complexes are also of known three-dimensional structure. the experimental difficulties surrounding structure determination of complexes make methods that are able to predict structures paramount. the challenge of predicting complex structures is daunting and raises many issues that need to be addressed. to produce the best models, new prediction methods have to somehow combine partial structures with a mixed bag of experimental data, including interactions and low-resolution electron microscopy images.\"1775 north ursula street, mail stop b140, po box 6511, denver, co, 80045, usa.\",{genespeed}: protein domain organization of the transcriptome.,\"the {genespeed} database (http://genespeed.uchsc.edu/) is an online database and resource tool facilitating the detailed study of protein domain homology in the transcriptomes of homo sapiens, mus musculus, drosophila melanogaster and caenorhabditis elegans. the population schema for the {genespeed} database takes advantage of {howard} parallel cluster technology (http://www.massivelyparallel.com/) and performs exhaustive {tblastn} searches covering all pre-assigned {pfam} domain classes in all species (currently 7973 domain families) against the respective unigene {est} databases of the selected four transcriptomes. the resulting database provides a complete annotation of presumed protein domain presence for each unigene cluster. to complement this domain annotation we have also performed a custom transcription factor-family curation of all pfam domains, incorporated the gene ontology classifications for these domains as well as integrated the novartis {symatlas2} dataset for both human and mouse which provides rapid and easy access to tissue-based expression analysis. consequently, the {genespeed} database provides the user with the capability to browse or search the database by any of these specialized criteria as well as more traditional means (gene identifier, gene symbol, etc.), thereby enabling a supervised analysis of gene families through a top-down hierarchical basis defined by domain content, all directly linked to an optimized gene expression dataset.\"\"the pyruvate kinase isoforms {pkm1} and {pkm2} are alternatively spliced products of the {pkm2} gene. {pkm2}, but not {pkm1}, alters glucose metabolism in cancer cells and contributes to tumorigenesis by mechanisms that are not explained by its known biochemical activity. we show that {pkm2} gene transcription is activated by hypoxia-inducible factor 1 ({hif}-1). {pkm2} interacts directly with the {hif}-1± subunit and promotes transactivation of {hif}-1 target genes by enhancing {hif}-1 binding and p300 recruitment to hypoxia response elements, whereas {pkm1} fails to regulate {hif}-1 activity. interaction of {pkm2} with prolyl hydroxylase 3 ({phd3}) enhances {pkm2} binding to {hif}-1± and {pkm2} coactivator function. mass spectrometry and anti-hydroxyproline antibody assays demonstrate {pkm2} hydroxylation on proline-403/408. {phd3} knockdown inhibits {pkm2} coactivator function, reduces glucose uptake and lactate production, and increases o2 consumption in cancer cells. thus, {pkm2} participates in a positive feedback loop that promotes {hif}-1 transactivation and reprograms glucose metabolism in cancer cells. º {pkm2} is a direct {hif}-1 target gene º {pkm2} functions as a coactivator for {hif}-1-dependent gene transcription º {pkm2} coactivator function is stimulated by the prolyl hydroxylase {phd3} º {phd3} knockdown reverses the warburg effect in cancer cells.\"\"{background}:the majority of human non-protein-coding {dna} is made up of repetitive sequences, mainly transposable elements ({tes}). it is becoming increasingly apparent that many of these repetitive {dna} sequence elements encode gene regulatory functions. this fact has important evolutionary implications, since repetitive {dna} is the most dynamic part of the genome. we set out to assess the evolutionary rate and pattern of experimentally characterized human transcription factor binding sites ({tfbs}) that are derived from repetitive versus non-repetitive {dna} to test whether repeat-derived {tfbs} are in fact rapidly evolving. we also evaluated the position-specific patterns of variation among {tfbs} to look for signs of functional constraint on {tfbs} derived from repetitive and non-repetitive {dna}.{results}:we found numerous experimentally characterized {tfbs} in the human genome, 7-10\\\\% of all mapped sites, which are derived from repetitive {dna} sequences including simple sequence repeats ({ssrs}) and {tes}. {te}-derived {tfbs} sequences are far less conserved between species than {tfbs} derived from {ssrs} and non-repetitive {dna}. despite their rapid evolution, several lines of evidence indicate that {te}-derived {tfbs} are functionally constrained. first of all, ancient {te} families, such as {mir} and l2, are enriched for {tfbs} relative to younger families like alu and l1. secondly, functionally important positions in {te}-derived {tfbs}, specifically those residues thought to physically interact with their cognate protein binding factors ({tf}), are more evolutionarily conserved than adjacent {tfbs} positions. finally, {te}-derived {tfbs} show position-specific patterns of sequence variation that are highly distinct from random patterns and similar to the variation seen for non-repeat derived sequences of the same {tfbs}.{conclusion}:the abundance of experimentally characterized human {tfbs} that are derived from repetitive {dna} speaks to the substantial regulatory effects that this class of sequence has on the human genome. the unique evolutionary properties of repeat-derived {tfbs} are perhaps even more intriguing. {te}-derived {tfbs} in particular, while clearly functionally constrained, evolve extremely rapidly relative to non-repeat derived sites. such rapidly evolving {tfbs} are likely to confer species-specific regulatory phenotypes, i.e. divergent expression patterns, on the human evolutionary lineage. this result has practical implications with respect to the widespread use of evolutionary conservation as a surrogate for functionally relevant non-coding {dna}. most {te}-derived {tfbs} would be missed using the kinds of sequence conservation-based screens, such as phylogenetic footprinting, that are used to help characterize non-coding {dna}. thus, the very {tfbs} that are most likely to yield human-specific characteristics will be neglected by the comparative genomic techniques that are currently de rigeur for the identification of novel regulatory sites.\"\"comprehensive sequencing of human cancers has identified recurrent mutations in genes encoding chromatin regulatory proteins. for clear cell renal cell carcinoma ({ccrcc}), three of the five commonly mutated genes encode the chromatin regulators {pbrm1}, {setd2}, and {bap1}. how these mutations alter the chromatin landscape and transcriptional program in {ccrcc} or other cancers is not understood. here, we identified alterations in chromatin organization and transcript profiles associated with mutations in chromatin regulators in a large cohort of primary human kidney tumors. by associating variation in chromatin organization with mutations in {setd2}, which encodes the enzyme responsible for {h3k36} trimethylation, we found that changes in chromatin accessibility occurred primarily within actively transcribed genes. this increase in chromatin accessibility was linked with widespread alterations in {rna} processing, including intron retention and aberrant splicing, affecting ∼25\\\\% of all expressed genes. furthermore, decreased nucleosome occupancy proximal to misspliced exons was observed in tumors lacking {h3k36me3}. these results directly link mutations in {setd2} to chromatin accessibility changes and {rna} processing defects in cancer. detecting the functional consequences of specific mutations in chromatin regulatory proteins in primary human samples could ultimately inform the therapeutic application of an emerging class of chromatin-targeted compounds.\"german cancer research center, inf 280, heidelberg, 69120, germany. w.huber@dkfz.de\",variance stabilization applied to microarray data calibration and to the quantification of differential expression.,\"we introduce a statistical model for microarray gene expression data that comprises data calibration, the quantification of differential expression, and the quantification of measurement error. in particular, we derive a transformation h for intensity measurements, and a difference statistic deltah whose variance is approximately constant along the whole intensity range. this forms a basis for statistical inference from microarray data, and provides a rational data pre-processing strategy for multivariate analyses. for the transformation h, the parametric form h(x)=arsinh(a+bx) is derived from a model of the variance-versus-mean dependence for microarray intensity data, using the method of variance stabilizing transformations. for large intensities, h coincides with the logarithmic transformation, and deltah with the log-ratio. the parameters of h together with those of the calibration between experiments are estimated with a robust variant of maximum-likelihood estimation. we demonstrate our approach on data sets from different experimental platforms, including two-colour {cdna} arrays and a series of affymetrix oligonucleotide arrays.\"\"the development of noninvasive methods to detect and monitor tumors continues to be a major challenge in oncology. we used digital polymerase chain reaction-based technologies to evaluate the ability of circulating tumor {dna} ({ctdna}) to detect tumors in 640 patients with various cancer types. we found that {ctdna} was detectable in >75\\\\% of patients with advanced pancreatic, ovarian, colorectal, bladder, gastroesophageal, breast, melanoma, hepatocellular, and head and neck cancers, but in less than 50\\\\% of primary brain, renal, prostate, or thyroid cancers. in patients with localized tumors, {ctdna} was detected in 73, 57, 48, and 50\\\\% of patients with colorectal cancer, gastroesophageal cancer, pancreatic cancer, and breast adenocarcinoma, respectively. {ctdna} was often present in patients without detectable circulating tumor cells, suggesting that these two biomarkers are distinct entities. in a separate panel of 206 patients with metastatic colorectal cancers, we showed that the sensitivity of {ctdna} for detection of clinically relevant {kras} gene mutations was 87.2\\\\% and its specificity was 99.2\\\\%. finally, we assessed whether {ctdna} could provide clues into the mechanisms underlying resistance to epidermal growth factor receptor blockade in 24 patients who objectively responded to therapy but subsequently relapsed. twenty-three (96\\\\%) of these patients developed one or more mutations in genes involved in the mitogen-activated protein kinase pathway. together, these data suggest that {ctdna} is a broadly applicable, sensitive, and specific biomarker that can be used for a variety of clinical and research purposes in patients with multiple different types of cancer.\"baltimore, maryland 21231.\",a genetic model for colorectal tumorigenesis.,identification of computational hot spots in protein interfaces: combining solvent accessibility and inter-residue potentials improves the accuracy.,uji, kyoto 611-0011, japan. kanehisa@kuicr.kyoto-u.ac.jp\",{kegg}: kyoto encyclopedia of genes and genomes.,\"{kegg} (kyoto encyclopedia of genes and genomes) is a knowledge base for systematic analysis of gene functions, linking genomic information with higher order functional information. the genomic information is stored in the {genes} database, which is a collection of gene catalogs for all the completely sequenced genomes and some partial genomes with up-to-date annotation of gene functions. the higher order functional information is stored in the {pathway} database, which contains graphical representations of cellular processes, such as metabolism, membrane transport, signal transduction and cell cycle. the {pathway} database is supplemented by a set of ortholog group tables for the information about conserved subpathways (pathway motifs), which are often encoded by positionally coupled genes on the chromosome and which are especially useful in predicting gene functions. a third database in {kegg} is {ligand} for the information about chemical compounds, enzyme molecules and enzymatic reactions. {kegg} provides java graphics tools for browsing genome maps, comparing two genome maps and manipulating expression maps, as well as computational tools for sequence comparison, graph comparison and path computation. the {kegg} databases are daily updated and made freely available (http://www. genome.ad.jp/kegg/).\"d-38124 braunschweig, germany. ewi@gbf.de\",{transfac}: an integrated system for gene expression regulation,\"{transfac} is a database on transcription factors, their genomic binding sites and {dna}-binding profiles ({http://transfac.gbf.de/transfac}/ ). its content has been enhanced, in particular by information about training sequences used for the construction of nucleotide matrices as well as by data on plant sites and factors. moreover, {transfac} has been extended by two new modules: {pathodb} provides data on pathologically relevant mutations in regulatory regions and transcription factor genes, whereas {s/mart} {db} compiles features of scaffold/matrix attached regions ({s/mars}) and the proteins binding to them. additionally, the databases {transpath}, about signal transduction, and {cytomer}, about organs and cell types, have been extended and are increasingly integrated with the {transfac} data sources.\"\"the heterogeneity of cancer genomes in terms of acquired mutations complicates the identification of genes whose modification may exert a driver role in tumorigenesis. in this study, we present a novel method that integrates expression profiles, mutation effects, and systemic properties of mutated genes to identify novel cancer drivers. we applied our method to ovarian cancer samples and were able to identify putative drivers in the majority of carcinomas without mutations in known cancer genes, thus suggesting that it can be used as a complementary approach to find rare driver mutations that cannot be detected using frequency-based approaches.\"9500 gilman drive, la jolla, ca 92093, usa. nbaker@mccammon.ucsd.edu\",electrostatics of nanosystems: application to microtubules and the ribosome,\"evaluation of the electrostatic properties of biomolecules has become a standard practice in molecular biophysics. foremost among the models used to elucidate the electrostatic potential is the {poisson-boltzmann} equation; however, existing methods for solving this equation have limited the scope of accurate electrostatic calculations to relatively small biomolecular systems. here we present the application of numerical methods to enable the trivially parallel solution of the {poisson-boltzmann} equation for supramolecular structures that are orders of magnitude larger in size. as a demonstration of this methodology, electrostatic potentials have been calculated for large microtubule and ribosome structures. the results point to the likely role of electrostatics in a variety of activities of these structures.\"boston university, boston, ma 02152, usa.\",extracting conserved gene expression motifs from gene expression data.,\"we propose a representation for gene expression data called conserved gene expression motifs or {xmotifs}. a gene\\'s expression level is conserved across a set of samples if the gene is expressed with the same abundance in all the samples. a conserved gene expression motif is a subset of genes that is simultaneously conserved across a subset of samples. we present a computational technique to discover large conserved gene motifs that cover all the samples and classes in the data. when applied to published data sets representing different cancers or disease outcomes, our algorithm constructs {xmotifs} that distinguish between the various classes.\"\"as whole-genome sequencing becomes commoditized and we begin to sequence and analyze personal genomes for clinical and diagnostic purposes, it is necessary to understand what constitutes a complete sequencing experiment for determining genotypes and detecting single-nucleotide variants. here, we show that the current recommendation of ∼30× coverage is not adequate to produce genotype calls across a large fraction of the genome with acceptably low error rates. our results are based on analyses of a clinical sample sequenced on two related illumina platforms, {gaiix} and {hiseq} 2000, to a very high depth (126×). we used these data to establish genotype-calling filters that dramatically increase accuracy. we also empirically determined how the callable portion of the genome varies as a function of the amount of sequence data used. these results help provide a  ” sequencing guide” for future whole-genome sequencing decisions and metrics by which coverage statistics should be reported.\"\"increasing knowledge about the organization of proteins into complexes, systems, and pathways has led to a flowering of theoretical approaches for exploiting this knowledge in order to better learn the functions of proteins and their roles underlying phenotypic traits and diseases. much of this body of theory has been developed and tested in model organisms, relying on their relative simplicity and genetic and biochemical tractability to accelerate the research. in this review, we discuss several of the major approaches for computationally integrating proteomics and genomics observations into integrated protein networks, then applying guilt-by-association in these networks in order to identify genes underlying traits. recent trends in this field include a rising appreciation of the modular network organization of proteins underlying traits or mutational phenotypes, and how to exploit such protein modularity using computational approaches related to the internet search algorithm {pagerank}. many protein network-based predictions have recently been experimentally confirmed in yeast, worms, plants, and mice, and several successful approaches in model organisms have been directly translated to analyze human disease, with notable recent applications to glioma and breast cancer prognosis.\"\"massively parallel approaches to nucleic acid sequencing have matured from proof-of-concept to commercial products during the past 5 years. these technologies are now widely accessible, increasingly affordable, and have already exerted a transformative influence on the study of human cancer. here, we review new features of cancer genomes that are being revealed by large-scale applications of these technologies. we focus on those insights most likely to affect future clinical practice. foremost among these lessons, we summarize the formidable genetic heterogeneity within given cancer types that is appreciable with higher resolution profiling and larger sample sets. we discuss the inherent challenges of defining driving genomic events in a given cancer genome amidst thousands of other somatic events. finally, we explore the organizational, regulatory and societal challenges impeding precision cancer medicine based on genomic profiling from assuming its place as standard-of-care.\",statistical analysis and prediction of protein–protein interfaces,\"predicting protein–protein interfaces from a three-dimensional structure is a key task of computational structural proteomics. in contrast to geometrically distinct small molecule binding sites, protein–protein interface are notoriously difficult to predict. we generated a large nonredundant data set of 1494 true protein–protein interfaces using biological symmetry annotation where necessary. the data set was carefully analyzed and a support vector machine was trained on a combination of a new robust evolutionary conservation signal with the local surface properties to predict protein–protein interfaces. fivefold cross validation verifies the high sensitivity and selectivity of the model. as much as 97\\\\% of the predicted patches had an overlap with the true interface patch while only 22\\\\% of the surface residues were included in an average predicted patch. the model allowed the identification of potential new interfaces and the correction of mislabeled oligomeric states. proteins 2005. {\\\\copyright} 2005 {wiley-liss}, inc.\"swiss federal institute of technology zurich eth zentrum, 8092 zurich, switzerland. barkow@tik.ee.ethz.ch\",{bicat}: a biclustering analysis toolbox.,\"{summary}: besides classical clustering methods such as hierarchical clustering, in recent years biclustering has become a popular approach to analyze biological data sets, e.g. gene expression data. the biclustering analysis toolbox ({bicat}) is a software platform for clustering-based data analysis that integrates various biclustering and clustering techniques in terms of a common graphical user interface. furthermore, {bicat} provides different facilities for data preparation, inspection and postprocessing such as discretization, filtering of biclusters according to specific criteria or gene pair analysis for constructing gene interconnection graphs. the possibility to use different biclustering algorithms inside a single graphical tool allows the user to compare clustering results and choose the algorithm that best fits a specific biological scenario. the toolbox is described in the context of gene expression analysis, but is also applicable to other types of data, e.g. data from proteomics or synthetic lethal experiments. {availability}: the {bicat} toolbox is freely available at http://www.tik.ee.ethz.ch/sop/bicat and runs on all operating systems. the java source code of the program and a developer\\'s guide is provided on the website as well. therefore, users may modify the program and add further algorithms or extensions.\"\"{background}:an important goal in bioinformatics is to unravel the network of transcription factors ({tfs}) and their targets. this is important in the human genome, where many {tfs} are involved in disease progression. here, classification methods are applied to identify new targets for 152 transcriptional regulators using publicly-available targets as training examples. three types of sequence information are used: composition, conservation, and {overrepresentation.results}:starting with 8817 {tf}-target interactions we predict an additional 9333 targets for 152 {tfs}. randomized classifiers make few predictions (\\\\~{}2/18660) indicating that our predictions for many {tfs} are significantly enriched for true targets. an enrichment score is calculated and used to filter new {predictions.two} case-studies for the {tfs} {oct4} and {wt1} illustrate the usefulness of our predictions: * many predicted {oct4} targets fall into the wnt-pathway. this is consistent with known biology as {oct4} is developmentally related and wnt pathway plays a role in early development. * beginning with 15 known targets, 354 predictions are made for {wt1}. {wt1} has a role in formation of wilms\\' tumor. chromosomal regions previously implicated in wilms\\' tumor by cytological evidence are statistically enriched in predicted {wt1} targets. these findings may shed light on wilms\\' tumor progression, suggesting that the tumor progresses either by loss of {wt1} or by loss of regions harbouring its targets. * targets of {wt1} are statistically enriched for cancer related functions including metastasis and apoptosis. among new targets are {bax} and {pde4b}, which may help mediate the established anti-apoptotic effects of {wt1}. * of the thirteen {tfs} found which co-regulate genes with {wt1} (p [less than or equal to] 0.02), 8 have been previously implicated in cancer. the regulatory-network for {wt1} targets in genomic regions relevant to wilms\\' tumor is {provided.conclusion}:we have assembled a set of features for the targets of human {tfs} and used them to develop classifiers for the determination of new regulatory targets. many predicted targets are consistent with the known biology of their regulators, and new targets for the wilms\\' tumor regulator, {wt1}, are proposed. we speculate that wilms\\' tumor development is mediated by chromosomal rearrangements in the location of {wt1} {targets.reviewers}:this article was reviewed by trey ideker, vladimir a. kuznetsov(nominated by frank eisenhaber), and tzachi pilpel.\"\"conserved residues in protein-protein interfaces correlate with residue hot-spots. to obtain insight into their roles, we have studied their mobility. we have performed 39 explicit solvent simulations of 15 complexes and their monomers, with the interfaces varying in size, shape, and function. the dynamic behavior of conserved residues in unbound monomers illustrates significantly lower flexibility as compared to their environment, suggesting that already prior to binding they are constrained in a bound-like configuration. to understand this behavior, we have analyzed the inter- and intra-chain hydrogen-bond residence-time in the interfaces. we find that conserved residues are not involved significantly in hydrogen bonds across the interface as compared to non-conserved. however, the monomer simulations reveal that conserved residues contribute dominantly to hydrogen bond formation prior to binding. packing of conserved residues across the trajectories is significantly higher prior and following the binding, rationalizing their lower mobility. backbone torsional angle distributions show that conserved residues assume restricted regions of space and the most visited conformations in the bound and unbound trajectories are similar, suggesting that conserved residues are pre-organized. combined with previous studies, we conclude that conserved residues, hot spots, anchor and interface-buried residues may be similar residues, fulfilling similar roles.\"\"european molecular biology laboratory, 69012 heidelberg, germany\",knowledge-based protein secondary structure assignment.,\"we have developed an automatic algorithm {stride} for protein secondary structure assignment from atomic coordinates based on the combined use of hydrogen bond energy and statistically derived backbone torsional angle information. parameters of the pattern recognition procedure were optimized using designations provided by the crystallographers as a standard-of-truth. comparison to the currently most widely used technique {dssp} by kabsch and sander (biopolymers 22:2577-2637, 1983) shows that {stride} and {dssp} assign secondary structural states in 58 and 31\\\\% of 226 protein chains in our data sample, respectively, in greater agreement with the specific residue-by-residue definitions provided by the discoverers of the structures while in 11\\\\% of the chains, the assignments are the same. {stride} delineates every 11th helix and every 32nd strand more in accord with published assignments.\"kharagpur, 721 302, india. subhajyoti\\\\_de@yahoo.com\",interaction preferences across protein-protein interfaces of obligatory and non-obligatory components are different.,\"{background}: a polypeptide chain of a protein-protein complex is said to be obligatory if it is bound to another chain throughout its functional lifetime. such a chain might not adopt the native fold in the unbound form. a non-obligatory polypeptide chain associates with another chain and dissociates upon molecular stimulus. although conformational changes at the interaction interface are expected, the overall {3-d} structure of the non-obligatory chain is unaltered. the present study focuses on protein-protein complexes to understand further the differences between obligatory and non-obligatory interfaces. {results}: a non-obligatory chain in a complex of known {3-d} structure is recognized by its stable existence with same fold in the bound and unbound forms. on the contrary, an obligatory chain is detected by its existence only in the bound form with no evidence for the native-like fold of the chain in the unbound form. various interfacial properties of a large number of complexes of known {3-d} structures thus classified are comparatively analyzed with an aim to identify structural descriptors that distinguish these two types of interfaces. we report that the interaction patterns across the interfaces of obligatory and non-obligatory components are different and contacts made by obligatory chains are predominantly non-polar. the obligatory chains have a higher number of contacts per interface (20 +/- 14 contacts per interface) than non-obligatory chains (13 +/- 6 contacts per interface). the involvement of main chain atoms is higher in the case of obligatory chains (16.9 \\\\%) compared to non-obligatory chains (11.2 \\\\%). the beta-sheet formation across the subunits is observed only among obligatory protein chains in the dataset. apart from these, other features like residue preferences and interface area produce marginal differences and they may be considered collectively while distinguishing the two types of interfaces. {conclusion}: these results can be useful in distinguishing the two types of interfaces observed in structures determined in large-scale in the structural genomics initiatives, especially for those multi-component protein assemblies for which the biochemical characterization is incomplete.\"may,2006-02-17 16:55:08,\"bijvoet center for biomolecular research, utrecht university, utrecht, the netherlands.\",{whiscy}: what information does surface conservation yield? application to data-driven docking,\"protein–protein interactions play a key role in biological processes. identifying the interacting residues is a first step toward understanding these interactions at a structural level. in this study, the interface prediction program {whiscy} is presented. it combines surface conservation and structural information to predict protein–protein interfaces. the accuracy of the predictions is more than three times higher than a random prediction. these predictions have been combined with another interface prediction program, {promate} [neuvirth et al. j mol biol 2004;338:181–199], resulting in an even more accurate predictor. the usefulness of the predictions was tested using the data-driven docking program {haddock} [dominguez et al. j am chem soc 2003;125:1731–1737] in an unbound docking experiment, with the goal of generating as many near-native structures as possible. unrefined rigid body docking solutions within 10 \\\\aa{} ligand {rmsd} from the true structure were generated for 22 out of 25 docked complexes. for 18 complexes, more than 100 of the 8000 generated models were correct. our results demonstrates the potential of using interface predictions to drive protein–protein docking. proteins 2006. {\\\\copyright} 2006 {wiley-liss}, inc.\"de boelelaan 1081a, 1081 hv amsterdam, the netherlands. wvanwie@few.vu.nl.\",weighted clustering of called array {cgh} data,\"array comparative genomic hybridization ({acgh}) is a laboratory technique to measure chromosomal copy number changes. a clear biological interpretation of the measurements is obtained by mapping these onto an ordinal scale with categories loss/normal/gain of a copy. the pattern of gains and losses harbors a level of tumor specificity. here, we present {wecca} (weighted clustering of called {acgh} data), a method for weighted clustering of samples on the basis of the ordinal {acgh} data. two similarities to be used in the clustering and particularly suited for ordinal data are proposed, which are generalized to deal with weighted observations. in addition, a new form of linkage, especially suited for ordinal data, is introduced. in a simulation study, we show that the proposed cluster method is competitive to clustering using the continuous data. we illustrate {wecca} using an application to a breast cancer data set, where {wecca} finds a clustering that relates better with survival than the original one.\"\"small cell lung carcinoma ({sclc}) is a highly lethal, smoking-associated cancer with few known targetable genetic alterations. using genome sequencing, we characterized the somatic evolution of a genetically engineered mouse model ({gemm}) of {sclc} initiated by loss of trp53 and rb1. we identified alterations in {dna} copy number and complex genomic rearrangements and demonstrated a low somatic point mutation frequency in the absence of tobacco mutagens. alterations targeting the tumor suppressor pten occurred in the majority of murine {sclc} studied, and engineered pten deletion accelerated murine {sclc} and abrogated loss of chr19 in trp53; rb1; pten compound mutant tumors. finally, we found evidence for polyclonal and sequential metastatic spread of murine {sclc} by comparative sequencing of families of related primary tumors and metastases. we propose a temporal model of {sclc} tumorigenesis with implications for human {sclc} therapeutics and the nature of cancer-genome evolution in {gemms}. \"\"murine {sclcs} acquire few point mutations in the absence of tobacco {mutagens\"\"pten} is recurrently mutated, and engineered deletion accelerates tumor {progression\"\"mycl1} amplifications play an early, central role in {msclc} {tumorigenesis\"\"analysis} of related primary and metastatic {msclc} suggests complex clonal evolution comprehensive genomic analysis in a mouse model of small-cell lung carcinoma delineates metastatic progression in this tumor type, showing that selection for specific cancer mutations can drive large genomic rearrangements and that distant metastases likely arise from a common metastatic seeding step involving the lymph nodes.\"vancouver, bc, canada\",combining probability from independent tests: the weighted z-method is superior to fisher\\'s approach.,\"the most commonly used method in evolutionary biology for combining information across multiple tests of the same null hypothesis is fisher\\'s combined probability test. this note shows that an alternative method called the weighted z-test has more power and more precision than does fisher\\'s test. furthermore, in contrast to some statements in the literature, the weighted z-method is superior to the unweighted z-transform approach. the results in this note show that, when combining p-values from multiple tests of the same hypothesis, the weighted z-method should be preferred.\"\"a major obstacle towards elucidating the molecular basis of transcriptional regulation is the lack of a detailed understanding of the interplay between non-specific and specific {protein-dna} interactions. based on molecular dynamics simulations of {c(2)h}(2) zinc fingers ({zfs}) and engrailed homeodomain transcription factors ({tfs}), we show that each of the studied {dna}-binding domains has a set of highly constrained side chains in preset configurations ready to form hydrogen bonds with the {dna} backbone. interestingly, those domains that bury their recognition helix into the major groove are found to have an electrostatic hot spot for cl(-) ions located on the same binding cavity as the most buried {dna} phosphate. the spot is characterized by three protein hydrogen bond donors, often including two basic side chains. if bound, cl(-) ions, likely mimicking phosphates, steer side chains that end up forming specific contacts with bases into bound-like conformations. these findings are consistent with a multi-step {dna}-binding mechanism in which a pre-organized set of {tf} side chains assist in the desolvation of phosphates into well defined sites, prompting the re-organization of specificity determining side chains into conformations suitable for the recognition of their cognate sequence.\"and department of biochemistry and molecular biology, indiana university medical school, 714 north senate avenue, indianapolis, in 46202, usa. afernan@iupui.edu\",the nonconserved wrapping of conserved protein folds reveals a trend toward increasing connectivity in proteomic networks,\"although protein folding domains are generally conserved for function across distant homologous sequences, one crucial structural feature is not conserved: the wrapping of backbone hydrogen bonds, that is, the extent to which they are intramolecularly desolvated and thereby protected from water attack. extensive data on protein complex interfaces led us to postulate that insufficiently wrapped backbone hydrogen bonds in monomeric domains must be adhesive, and therefore determinants of interactivity, a result that has been experimentally confirmed. here, we show that the wrapping of certain conserved folds becomes progressively poorer as species diverge in some lineages. this trend is thus concurrent with a progressive enhancement of the interactivity of individual domains sharing the conserved fold. such increase in interactivity is predicted to impose an  ” evolutionary brake” on the overall speed of sequence divergence. this phenomenon follows when more and more residues become engaged in protein associations and thus become functionally indispensable. for complete proteomes for which statistically significant structural data are available, scale-free network statistics based solely on the distribution of folding domains, catalogued by their number of wrapping defects, best describe the proteomic connectivity. thus, the intermolecular connectivity may be effectively used as a measure of species complexity. our results might contribute to explaining how interactome complexity may be achieved without a dramatic increase in genome size.\"evolution of human {bcr}-{abl1} lymphoblastic leukaemia-initiating cells,\"many tumours are composed of genetically diverse cells; however, little is known about how diversity evolves or the impact that diversity has on functional properties. here, using xenografting and {dna} copy number alteration ({cna}) profiling of human {bcr}-{abl1} lymphoblastic leukaemia, we demonstrate that genetic diversity occurs in functionally defined leukaemia-initiating cells and that many diagnostic patient samples contain multiple genetically distinct leukaemia-initiating cell subclones. reconstructing the subclonal genetic ancestry of several samples by {cna} profiling demonstrated a branching multi-clonal evolution model of leukaemogenesis, rather than linear succession. for some patient samples, the predominant diagnostic clone repopulated xenografts, whereas in others it was outcompeted by minor subclones. reconstitution with the predominant diagnosis clone was associated with more aggressive growth properties in xenografts, deletion of {cdkn2a} and {cdkn2b}, and a trend towards poorer patient outcome. our findings link clonal diversity with leukaemia-initiating-cell function and underscore the importance of developing therapies that eradicate all intratumoral subclones.\"santa cruz, 1156 high st, santa cruz, ca 95064, usa.\",{footprinter}: a program designed for phylogenetic footprinting,\"phylogenetic footprinting is a method for the discovery of regulatory elements in a set of homologous regulatory regions, usually collected from multiple species. it does so by identifying the best conserved motifs in those homologous regions. this note describes web software that has been designed specifically for this purpose, making use of the phylogenetic relationships among the homologous sequences in order to make more accurate predictions. the software is called {footprinter} and is available at http://bio.cs.washington.edu/software.html.\"\"the basic region leucine zipper ({bzip}) proteins form one of the largest families of transcription factors in eukaryotic cells. despite relatively high homology between the amino acid sequences of the {bzip} motifs, these proteins recognize diverse {dna} sequences. here we report the 2.0 \\\\aa{} resolution crystal structure of the {bzip} motif of one such transcription factor, {pap1}, a fission yeast {ap}-1-like transcription factor that binds {dna} containing the novel consensus sequence {ttacgtaa}. the structure reveals how the pap1-specific residues of the {bzip} basic region recognize the target sequence and shows that the side chain of the invariant asn in the {bzip} motif adopts an alternative conformation in pap1. this conformation, which is stabilized by a pap1-specific residue and its associated water molecule, recognizes a different base in the target sequence from that in other {bzip} subfamilies.\"convergent loss of {pten} leads to clinical resistance to a {pi}({3)k}[agr] inhibitor,\"broad and deep tumour genome sequencing has shed new light on tumour heterogeneity and provided important insights into the evolution of metastases arising from different clones. there is an additional layer of complexity, in that tumour evolution may be influenced by selective pressure provided by therapy, in a similar fashion to that occurring in infectious diseases. here we studied tumour genomic evolution in a patient (index patient) with metastatic breast cancer bearing an activating {pik3ca} (phosphatidylinositol-4,5-bisphosphate 3-kinase, catalytic subunit alpha, {pi}({3)k}\\\\&\\\\#177;) mutation. the patient was treated with the {pi}({3)k}\\\\&\\\\#177; inhibitor {byl719}, which achieved a lasting clinical response, but the patient eventually became resistant to this drug (emergence of lung metastases) and died shortly thereafter. a rapid autopsy was performed and material from a total of 14 metastatic sites was collected and sequenced. all metastatic lesions, when compared to the pre-treatment tumour, had a copy loss of {pten} (phosphatase and tensin homolog) and those lesions that became refractory to {byl719} had additional and different {pten} genetic alterations, resulting in the loss of {pten} expression. to put these results in context, we examined six other patients also treated with {byl719}. acquired bi-allelic loss of {pten} was found in one of these patients, whereas in two others {pik3ca} mutations present in the primary tumour were no longer detected at the time of progression. to characterize our findings functionally, we examined the effects of {pten} knockdown in several preclinical models (both in cell lines intrinsically sensitive to {byl719} and in {pten}-null xenografts derived from our index patient), which we found resulted in resistance to {byl719}, whereas simultaneous {pi}({3)k} p110\\\\&\\\\#178; blockade reverted this resistance phenotype. we conclude that parallel genetic evolution of separate metastatic sites with different {pten} genomic alterations leads to a convergent {pten}-null phenotype resistant to {pi}({3)k}\\\\&\\\\#177; inhibition.\"\"researchers have been studying metastasis for more than 100 years, and only recently have we gained insight into the mechanisms by which metastatic cells arise from primary tumours and the reasons that certain tumour types tend to metastasize to specific organs. stephen paget\\'s 1889 proposal that metastasis depends on cross-talk between selected cancer cells (the \\'seeds\\') and specific organ microenvironments (the \\'soil\\') still holds forth today. it is now known that the potential of a tumour cell to metastasize depends on its interactions with the homeostatic factors that promote tumour-cell growth, survival, angiogenesis, invasion and metastasis. how has this field developed over the past century, and what major breakthroughs are most likely to lead to effective therapeutic approaches?\"712 medical center drive, rockville, maryland 20850, usa. johnq@tigr.org\",computational analysis of microarray data,\"{background}:comparison of metabolic networks across species is a key to understanding how evolutionary pressures shape these networks. by selecting taxa representative of different lineages or lifestyles and using a comprehensive set of descriptors of the structure and complexity of their metabolic networks, one can highlight both qualitative and quantitative differences in the metabolic organization of species subject to distinct evolutionary paths or environmental {constraints.results}:we used a novel representation of metabolic networks, termed network of interacting pathways or {nip}, to focus on the modular, high-level organization of the metabolic capabilities of the cell. using machine learning techniques we identified the most relevant aspects of cellular organization that change under evolutionary pressures. we considered the transitions from prokarya to eukarya (with a focus on the transitions among the archaea, bacteria and eukarya), from unicellular to multicellular eukarya, from free living to host-associated bacteria, from anaerobic to aerobic, as well as the acquisition of cell motility or growth in an environment of various levels of salinity or temperature. intuitively, we expect organisms with more complex lifestyles to have more complex and robust metabolic networks. here we demonstrate for the first time that such organisms are not only characterized by larger, denser networks of metabolic pathways but also have more efficiently organized cross communications, as revealed by subtle changes in network topology. these changes are unevenly distributed among metabolic pathways, with specific categories of pathways being promoted to more central locations as an answer to environmental {constraints.conclusions}:combining methods from graph theory and machine learning, we have shown here that evolutionary pressures not only affects gene and protein sequences, but also specific details of the complex wiring of functional modules in the cell. this approach allows the identification and quantification of those changes, and provides an overview of the evolution of intracellular systems.\"\"we have presented a new gapped {pwm} model for variable length {dna} binding sites that is not too restrictive nor over-parameterised. our comparison with existing tools shows that on average it does not have better predictive accuracy than existing methods. however, it does provide more interpretable models of motifs of variable structure that are suitable for follow-up structural studies. to our knowledge, we are the first to apply variable length motif models to eukaryotic {chip}-seq data sets and consequently the first to show their value in this domain. the results include a novel motif for the ubiquitous transcription factor sp1.\"\"motivation: although metabolic reactions are unquestionably shaped by evolutionary processes, the degree to which the overall structure and complexity of their interconnections are linked to the phylogeny of species has not been evaluated in depth. here, we apply an original metabolome representation, termed network of interacting pathways or {nip}, with a combination of graph theoretical and machine learning strategies, to address this question. {nips} compress the information of the metabolic network exhibited by a species into much smaller networks of overlapping metabolic pathways, where nodes are pathways and links are the metabolites they exchange.\"44 binney st, boston, ma 02115, usa. rgentlem@jimmy.harvard.edu\",bioconductor: open software development for computational biology and bioinformatics.,\"the bioconductor project is an initiative for the collaborative creation of extensible software for computational biology and bioinformatics. the goals of the project include: fostering collaborative development and widespread use of innovative software, reducing barriers to entry into interdisciplinary scientific research, and promoting the achievement of remote reproducibility of research results. we describe details of our aims and methods, identify current challenges, compare bioconductor to other open bioinformatics projects, and provide working examples.\"\"although genomewide {rna} expression analysis has become a routine tool in biomedical research, extracting biological insight from such information remains a major challenge. here, we describe a powerful analytical method called gene set enrichment analysis ({gsea}) for interpreting gene expression data. the method derives its power by focusing on gene sets, that is, groups of genes that share common biological function, chromosomal location, or regulation. we demonstrate how {gsea} yields insights into several cancer-related data sets, including leukemia and lung cancer. notably, where single-gene analysis finds little similarity between two independent studies of patient survival in lung cancer, {gsea} reveals many biological pathways in common. the {gsea} method is embodied in a freely available software package, together with an initial database of 1,325 biologically defined gene sets.\"\"the naked mole-rat is the longest living rodent with a maximum lifespan exceeding 28 years. in addition to its longevity, naked mole-rats have an extraordinary resistance to cancer as tumors have never been observed in these rodents. furthermore, we show that a combination of activated ras and {sv40} {lt} fails to induce robust anchorage-independent growth in naked mole-rat cells, while it readily transforms mouse fibroblasts. the mechanisms responsible for the cancer resistance of naked mole-rats were unknown. here we show that naked mole-rat fibroblasts display hypersensitivity to contact inhibition, a phenomenon we termed \"\"early contact inhibition.\"\" contact inhibition is a key anticancer mechanism that arrests cell division when cells reach a high density. in cell culture, naked mole-rat fibroblasts arrest at a much lower density than those from a mouse. we demonstrate that early contact inhibition requires the activity of p53 and {prb} tumor suppressor pathways. inactivation of both p53 and {prb} attenuates early contact inhibition. contact inhibition in human and mouse is triggered by the induction of {p27(kip1}). in contrast, early contact inhibition in naked mole-rat is associated with the induction of {p16(ink4a}). furthermore, we show that the roles of {p16(ink4a}) and {p27(kip1}) in the control of contact inhibition became temporally separated in this species: the early contact inhibition is controlled by {p16(ink4a}), and regular contact inhibition is controlled by {p27(kip1}). we propose that the additional layer of protection conferred by two-tiered contact inhibition contributes to the remarkable tumor resistance of the naked mole-rat.\"\"genetic sequence alignment is the basis of many evolutionary and comparative studies, and errors in alignments lead to errors in the interpretation of evolutionary information in genomes. traditional multiple sequence alignment methods disregard the phylogenetic implications of gap patterns that they create and infer systematically biased alignments with excess deletions and substitutions, too few insertions, and implausible insertion-deletion–event histories. we present a method that prevents these systematic errors by recognizing insertions and deletions as distinct evolutionary events. we show theoretically and practically that this improves the quality of sequence alignments and downstream analyses over a wide range of realistic alignment problems. these results suggest that insertions and sequence turnover are more common than is currently thought and challenge the conventional picture of sequence evolution and mechanisms of functional and structural changes.\"\"in molecular dynamics ({md}) simulations the need often arises to maintain such parameters as temperature or pressure rather than energy and volume, or to impose gradients for studying transport properties in nonequilibrium {md}. a method is described to realize coupling to an external bath with constant temperature or pressure with adjustable time constants for the coupling. the method is easily extendable to other variables and to gradients, and can be applied also to polyatomic molecules involving internal constraints. the influence of coupling time constants on dynamical variables is evaluated. a leap‐frog algorithm is presented for the general case involving constraints with coupling to both a constant temperature and a constant pressure bath.\"illinois 60637, usa. ariel@uchicago.edu\",dehydron: a structurally encoded signal for protein interaction.,\"we introduce a quantifiable structural motif, called dehydron, that is shown to be central to protein-protein interactions. a dehydron is a defectively packed backbone hydrogen bond suggesting preformed monomeric structure whose coulomb energy is highly sensitive to binding-induced water exclusion. such preformed hydrogen bonds are effectively adhesive, since water removal from their vicinity contributes to their stability. at the structural level, a significant correlation is established between dehydrons and sites for protein complexation, with the {hiv}-1 capsid protein p24 complexed with antibody light-chain {fab25}.3 providing the most dramatic correlation. furthermore, the number of dehydrons in homologous similar-fold proteins from different species is shown to be a signature of proteomic complexity. the techniques are then applied to higher levels of organization: the formation of the capsid and its organization in picornaviruses correlates strongly with the distribution of dehydrons on the rim of the virus unit. furthermore, antibody contacts and crystal contacts may be assigned to dehydrons still prevalent after the capsid has been assembled. the implications of the dehydron as an encoded signal in proteomics, bioinformatics, and inhibitor drug design are emphasized.\"\"although a few cancer genes are mutated in a high proportion of tumours of a given type (>20\\\\%), most are mutated at intermediate frequencies (2-20\\\\%). to explore the feasibility of creating a comprehensive catalogue of cancer genes, we analysed somatic point mutations in exome sequences from 4,742 human cancers and their matched normal-tissue samples across 21 cancer types. we found that large-scale genomic analysis can identify nearly all known cancer genes in these tumour types. our analysis also identified 33 genes that were not previously known to be significantly mutated in cancer, including genes related to proliferation, apoptosis, genome stability, chromatin regulation, immune evasion, {rna} processing and protein homeostasis. down-sampling analysis indicates that larger sample sizes will reveal many more genes mutated at clinically important frequencies. we estimate that near-saturation may be achieved with 600-5,000 samples per tumour type, depending on background mutation frequency. the results may help to guide the next stage of cancer genomics.\"\"we describe a statistical method for the characterization of genomic aberrations in single nucleotide polymorphism microarray data acquired from cancer genomes. our approach allows us to model the joint effect of polyploidy, normal {dna} contamination and intra-tumour heterogeneity within a single unified bayesian framework. we demonstrate the efficacy of our method on numerous datasets including laboratory generated mixtures of normal-cancer cell lines and real primary tumours.\"\"crosstalk and complexity within signaling pathways and their perturbation by oncogenes limit component-by-component approaches to understanding human disease. network analysis of how normal and oncogenic signaling can be rewired by drugs may provide opportunities to target tumors with high specificity and efficacy. using targeted inhibition of oncogenic signaling pathways, combined with {dna}-damaging chemotherapy, we report that time-staggered {egfr} inhibition, but not simultaneous coadministration, dramatically sensitizes a subset of triple-negative breast cancer cells to genotoxic drugs. systems-level analysis-using high-density time-dependent measurements of signaling networks, gene expression profiles, and cell phenotypic responses in combination with mathematical modeling-revealed an approach for altering the intrinsic state of the cell through dynamic rewiring of oncogenic signaling pathways. this process converts these cells to a less tumorigenic state that is more susceptible to {dna} damage-induced cell death by reactivation of an extrinsic apoptotic pathway whose function is suppressed in the oncogene-addicted state. copyright {\\\\copyright} 2012 elsevier inc. all rights reserved.\"emergence of {kras} mutations and acquired resistance to {anti-egfr} therapy in colorectal cancer.,\"a main limitation of therapies that selectively target kinase signalling pathways is the emergence of secondary drug resistance. cetuximab, a monoclonal antibody that binds the extracellular domain of epidermal growth factor receptor ({egfr}), is effective in a subset of {kras} wild-type metastatic colorectal cancers. after an initial response, secondary resistance invariably ensues, thereby limiting the clinical benefit of this drug. the molecular bases of secondary resistance to cetuximab in colorectal cancer are poorly understood. here we show that molecular alterations (in most instances point mutations) of {kras} are causally associated with the onset of acquired resistance to {anti-egfr} treatment in colorectal cancers. expression of mutant {kras} under the control of its endogenous gene promoter was sufficient to confer cetuximab resistance, but resistant cells remained sensitive to combinatorial inhibition of {egfr} and mitogen-activated protein-kinase kinase ({mek}). analysis of metastases from patients who developed resistance to cetuximab or panitumumab showed the emergence of {kras} amplification in one sample and acquisition of secondary {kras} mutations in 60\\\\% (6 out of 10) of the cases. {kras} mutant alleles were detectable in the blood of cetuximab-treated patients as early as 10 months before radiographic documentation of disease progression. in summary, the results identify {kras} mutations as frequent drivers of acquired resistance to cetuximab in colorectal cancers, indicate that the emergence of {kras} mutant clones can be detected non-invasively months before radiographic progression and suggest early initiation of a {mek} inhibitor as a rational strategy for delaying or reversing drug resistance.\"\"intra-tumour genetic heterogeneity is the result of ongoing evolutionary change within each cancer. the expansion of genetically distinct sub-clonal populations may explain the emergence of drug resistance, and if so, would have prognostic and predictive utility. however, methods for objectively quantifying tumour heterogeneity have been missing and are particularly difficult to establish in cancers where predominant copy number variation prevents accurate phylogenetic reconstruction owing to horizontal dependencies caused by long and cascading genomic rearrangements. to address these challenges, we present {medicc}, a method for phylogenetic reconstruction and heterogeneity quantification based on a minimum event distance for intra-tumour copy-number comparisons. using a transducer-based pairwise comparison function, we determine optimal phasing of major and minor alleles, as well as evolutionary distances between samples, and are able to reconstruct ancestral genomes. rigorous simulations and an extensive clinical study show the power of our method, which outperforms state-of-the-art competitors in reconstruction accuracy, and additionally allows unbiased numerical quantification of tumour heterogeneity. accurate quantification and evolutionary inference are essential to understand the functional consequences of tumour heterogeneity. the {medicc} algorithms are independent of the experimental techniques used and are applicable to both next-generation sequencing and array {cgh} data. cancer is a disease of random mutation and selection within the cellular genomes of an organism. as a result, when advanced disease is diagnosed, the cells comprising the tumour show a great amount of variability on the genomic level, a phenomenon termed intra-tumour genetic heterogeneity. heterogeneity is thought to be one of the main reasons why tumors become resistant to therapy, and thus hinders personalised medicine approaches. if we want to understand tumour heterogeneity and its connection to resistance development we need to quantify it, which implies reconstructing the evolutionary history of cancer within the patient. unfortunately, so far, methods for accurate reconstructions of these particular evolutionary trees and for quantification of heterogeneity have been missing. we here present {medicc}, a method that uses a minimum evolution criterion to compare cancer genomes based on genomic profiles of {dna} content (copy-number profiles). it enables accurate reconstruction of the history of the disease and quantifies heterogeneity. it is specifically designed to deal with diploid human genomes, in that it disentangles genomic events on both parental alleles and includes a variety of accompanying algorithms to test for shapes of the evolutionary trees as well as the rate at which the cancer evolves.\"boston, ma 02115, usa. yizong.cheng@uc.edu\",biclustering of expression data.,\"an efficient node-deletion algorithm is introduced to find submatrices in expression data that have low mean squared residue scores and it is shown to perform well in finding co-regulation patterns in yeast and human. this introduces \"\"biclustering\"\", or simultaneous clustering of both genes and conditions, to knowledge discovery from expression data. this approach overcomes some problems associated with traditional clustering methods, by allowing automatic discovery of similarity based on a subset of attributes, simultaneous clustering of genes and conditions, and overlapped grouping that provides a better representation for genes with multiple functions or regulated by many factors.\"new york, new york, usa\",theory and limitations of genetic network inference from microarray data.,\"since the advent of gene expression microarray technology more than 10 years ago, many computational approaches have been developed aimed at using statistical associations between {mrna} abundance profiles to predict transcriptional regulatory interactions. the ultimate goal is to develop causal network models describing the transcriptional influences that genes exert on each other (via their protein products), which can be used to predict network disruptions (e.g., mutations) leading to a disease phenotype, as well as the appropriate therapeutic intervention. however, microarray data measure only a small component of the interacting variables in a genetic regulatory network, as cells are known to regulate gene expression via many diverse mechanisms. although many researchers have acknowledged the questionable interpretation of statistical dependencies between {mrna} profiles, very little work has been done on theoretically characterizing the nature of inferred dependencies using models that account for unobserved interacting variables. in this work, we review the theory behind reverse engineering algorithms derived from three separate disciplines-system control theory, graphical models, and information theory-and highlight several mathematical relationships between the various methods. we then apply recent theoretical work on constructing graphical models with latent variables to the context of reverse engineering genetic networks. we demonstrate that even the addition of simple latent variables induces statistical dependencies between non-directly interacting (e.g., co-regulated) genes that cannot be eliminated by conditioning on any observed variables.\"\"high-throughput sequencing platforms are generating massive amounts of genetic variation data for diverse genomes, but it remains a challenge to pinpoint a small subset of functionally important variants. to fill these unmet needs, we developed the {annovar} tool to annotate single nucleotide variants ({snvs}) and insertions/deletions, such as examining their functional consequence on genes, inferring cytogenetic bands, reporting functional importance scores, finding variants in conserved regions, or identifying variants reported in the 1000 genomes project and {dbsnp}. {annovar} can utilize annotation databases from the {ucsc} genome browser or any annotation data set conforming to generic feature format version 3 ({gff3}). we also illustrate a \\'variants reduction\\' protocol on 4.7\\u2009million {snvs} and indels from a human genome, including two causal mutations for miller syndrome, a rare recessive disease. through a stepwise procedure, we excluded variants that are unlikely to be causal, and identified 20 candidate genes including the causal gene. using a desktop computer, {annovar} requires ∼4\\u2009min to perform gene-based annotation and ∼15\\u2009min to perform variants reduction on 4.7\\u2009million variants, making it practical to handle hundreds of human genomes in a day. {annovar} is freely available at http://www.openbioinformatics.org/annovar/.\"\"the characterization of protein interactions is essential for understanding biological systems. while genome-scale methods are available for identifying interacting proteins, they do not pinpoint the interacting motifs (e.g., a domain, sequence segments, a binding site, or a set of residues). here, we develop and apply a method for delineating the interacting motifs of hub proteins (i.e., highly connected proteins). the method relies on the observation that proteins with common interaction partners tend to interact with these partners through a common interacting motif. the sole input for the method are binary protein interactions; neither sequence nor structure information is needed. the approach is evaluated by comparing the inferred interacting motifs with domain families defined for 368 proteins in the structural classification of proteins ({scop}). the positive predictive value of the method for detecting proteins with common {scop} families is 75\\\\% at sensitivity of 10\\\\%. most of the inferred interacting motifs were significantly associated with sequence patterns, which could be responsible for the common interactions. we find that yeast hubs with multiple interacting motifs are more likely to be essential than hubs with one or two interacting motifs, thus rationalizing the previously observed correlation between essentiality and the number of interacting partners of a protein. we also find that yeast hubs with multiple interacting motifs evolve slower than the average protein, contrary to the hubs with one or two interacting motifs. the proposed method will help us discover unknown interacting motifs and provide biological insights about protein hubs and their roles in interaction networks.\"\"{background}: biclustering of gene expression data searches for local patterns of gene expression. a bicluster (or a two-way cluster) is defined as a set of genes whose expression profiles are mutually similar within a subset of experimental conditions/samples. although several biclustering algorithms have been studied, few are based on rigorous statistical models. {results}: we developed a bayesian biclustering model ({bbc}), and implemented a gibbs sampling procedure for its statistical inference. we showed that bayesian biclustering model can correctly identify multiple clusters of gene expression data. using simulated data both from the model and with realistic characters, we demonstrated the {bbc} algorithm outperforms other methods in both robustness and accuracy. we also showed that the model is stable for two normalization methods, the interquartile range normalization and the smallest quartile range normalization. applying the {bbc} algorithm to the yeast expression data, we observed that majority of the biclusters we found are supported by significant biological evidences, such as enrichments of gene functions and transcription factor binding sites in the corresponding promoter sequences. {conclusions}: the {bbc} algorithm is shown to be a robust model-based biclustering method that can discover biologically significant gene-condition clusters in microarray data. the {bbc} model can easily handle missing data via monte carlo imputation and has the potential to be extended to integrated study of gene transcription networks.\"\"{background}: the arrangement of regulatory motifs in gene promoters, or promoter architecture, is the result of mutation and selection processes that have operated over many millions of years. in mammals, tissue-specific transcriptional regulation is related to the presence of specific protein-interacting {dna} motifs in gene promoters. however, little is known about the relative location and spacing of these motifs. to fill this gap, we have performed a systematic search for motifs that show significant bias at specific promoter locations in a large collection of housekeeping and tissue-specific genes. {results}: we observe that promoters driving housekeeping gene expression are enriched in particular motifs with strong positional bias, such as {yy1}, which are of little relevance in promoters driving tissue-specific expression. we also identify a large number of motifs that show positional bias in genes expressed in a highly tissue-specific manner. they include well-known tissue-specific motifs, such as {hnf1} and {hnf4} motifs in liver, kidney and small intestine, or {rfx} motifs in testis, as well as many potentially novel regulatory motifs. based on this analysis, we provide predictions for 559 tissue-specific motifs in mouse gene promoters. {conclusion}: the study shows that motif positional bias is an important feature of mammalian proximal promoters and that it affects both general and tissue-specific motifs. motif positional constraints define very distinct promoter architectures depending on breadth of expression and type of tissue.\"\"understanding how genes are expressed and regulated in different tissues is a fundamental and challenging question. however, most of currently available biological databases do not focus on tissue-specific gene regulation.\"\"we propose a novel approach to predict domain-domain interactions from a protein-protein interaction network. in our method we apply a parsimony-driven explanation of the network, where the domain interactions are inferred using linear programming optimization, and false positives in the protein network are handled by a probabilistic construction. this method outperforms previous approaches by a considerable margin. the results indicate that the parsimony principle provides a correct approach for detecting domain-domain contacts.\"philadelphia, pennsylvania 19104, usa. hxzhou@einstein.drexel.edu\",prediction of protein interaction sites from sequence profile and residue neighbor list.,\"protein-protein interaction sites are predicted from a neural network with sequence profiles of neighboring residues and solvent exposure as input. the network was trained on 615 pairs of nonhomologous complex-forming proteins. tested on a different set of 129 pairs of nonhomologous complex-forming proteins, 70\\\\% of the 11,004 predicted interface residues are actually located in the interfaces. these 7732 correctly predicted residues account for 65\\\\% of the 11,805 residues making up the 129 interfaces. the main strength of the network predictor lies in the fact that neighbor lists and solvent exposure are relatively insensitive to structural changes accompanying complex formation. as such, it performs equally well with bound or unbound structures of the proteins. for a set of 35 test proteins, when the input was calculated from the bound and unbound structures, the correct fractions of the predicted interface residues were 69 and 70\\\\%, respectively.\"\"next-generation {dna} sequencing technologies are enabling genome-wide measurements of somatic mutations in large numbers of cancer patients. a major challenge in the interpretation of these data is to distinguish functional \"\"driver mutations\"\" important for cancer development from random \"\"passenger mutations.\"\" a common approach for identifying driver mutations is to find genes that are mutated at significant frequency in a large cohort of cancer genomes. this approach is confounded by the observation that driver mutations target multiple cellular signaling and regulatory pathways. thus, each cancer patient may exhibit a different combination of mutations that are sufficient to perturb these pathways. this mutational heterogeneity presents a problem for predicting driver mutations solely from their frequency of occurrence. we introduce two combinatorial properties, coverage and exclusivity, that distinguish driver pathways, or groups of genes containing driver mutations, from groups of genes with passenger mutations. we derive two algorithms, called dendrix, to find driver pathways de novo from somatic mutation data. we apply dendrix to analyze somatic mutation data from 623 genes in 188 lung adenocarcinoma patients, 601 genes in 84 glioblastoma patients, and 238 known mutations in 1000 patients with various cancers. in all data sets, we find groups of genes that are mutated in large subsets of patients and whose mutations are approximately exclusive. our dendrix algorithms scale to whole-genome analysis of thousands of patients and thus will prove useful for larger data sets to come from the cancer genome atlas ({tcga}) and other large-scale cancer genome sequencing projects.\"\"cancer evolves dynamically as clonal expansions supersede one another driven by shifting selective pressures, mutational processes, and disrupted cancer genes. these processes mark the genome, such that a cancer\\'s life history is encrypted in the somatic mutations present. we developed algorithms to decipher this narrative and applied them to 21 breast cancers. mutational processes evolve across a cancer\\'s lifespan, with many emerging late but contributing extensive genetic variation. subclonal diversification is prominent, and most mutations are found in just a fraction of tumor cells. every tumor has a dominant subclonal lineage, representing more than 50\\\\% of tumor cells. minimal expansion of these subclones occurs until many hundreds to thousands of mutations have accumulated, implying the existence of long-lived, quiescent cell lineages capable of substantial proliferation upon acquisition of enabling genomic changes. expansion of the dominant subclone to an appreciable mass may therefore represent the final rate-limiting step in a breast cancer\\'s development, triggering diagnosis. \\\\^{a}\\x96º genome-wide analyses of mutations emerging through time in 21 breast cancers \\\\^{a}\\x96º minimal expansion of subclones occurs until thousands of mutations have accumulated \\\\^{a}\\x96º cancer-specific signatures of point mutations and genomic instability emerge late \\\\^{a}\\x96º {erbb2} amplification begins early but continues to evolve over long molecular time\"\"spatial and temporal dissection of the genomic changes occurring during the evolution of human non–small cell lung cancer ({nsclc}) may help elucidate the basis for its dismal prognosis. we sequenced 25 spatially distinct regions from seven operable {nsclcs} and found evidence of branched evolution, with driver mutations arising before and after subclonal diversification. there was pronounced intratumor heterogeneity in copy number alterations, translocations, and mutations associated with {apobec} cytidine deaminase activity. despite maintained carcinogen exposure, tumors from smokers showed a relative decrease in smoking-related mutations over time, accompanied by an increase in {apobec}-associated mutations. in tumors from former smokers, genome-doubling occurred within a smoking-signature context before subclonal diversification, which suggested that a long period of tumor latency had preceded clinical detection. the regionally separated driver mutations, coupled with the relentless and heterogeneous nature of the genome instability processes, are likely to confound treatment success in {nsclc}.\"\"somatic alterations in cellular {dna} underlie almost all human cancers. the prospect of targeted therapies and the development of high-resolution, genome-wide approaches are now spurring systematic efforts to characterize cancer genomes. here we report a large-scale project to characterize copy-number alterations in primary lung adenocarcinomas. by analysis of a large collection of tumours (n = 371) using dense single nucleotide polymorphism arrays, we identify a total of 57 significantly recurrent events. we find that 26 of 39 autosomal chromosome arms show consistent large-scale copy-number gain or loss, of which only a handful have been linked to a specific gene. we also identify 31 recurrent focal events, including 24 amplifications and 7 homozygous deletions. only six of these focal events are currently associated with known mutations in lung carcinomas. the most common event, amplification of chromosome 14q13.3, is found in approximately 12\\\\% of samples. on the basis of genomic and functional analyses, we identify {nkx2}-1 ({nk2} homeobox 1, also called {titf1}), which lies in the minimal 14q13.3 amplification interval and encodes a lineage-specific transcription factor, as a novel candidate proto-oncogene involved in a significant fraction of lung adenocarcinomas. more generally, our results indicate that many of the genes that are involved in lung adenocarcinoma remain to be discovered.\"\"the elucidation of the human genome sequence has made it possible to identify genetic alterations in cancers in unprecedented detail. to begin a systematic analysis of such alterations, we determined the sequence of well-annotated human protein-coding genes in two common tumor types. analysis of 13,023 genes in 11 breast and 11 colorectal cancers revealed that individual tumors accumulate an average of ∼90 mutant genes but that only a subset of these contribute to the neoplastic process. using stringent criteria to delineate this subset, we identified 189 genes (average of 11 per tumor) that were mutated at significant frequency. the vast majority of these genes were not known to be genetically altered in tumors and are predicted to affect a wide range of cellular functions, including transcription, adhesion, and invasion. these data define the genetic landscape of two human cancer types, provide new targets for diagnostic and therapeutic intervention, and open fertile avenues for basic research in tumor biology.\"university park, pa 16802, usa. djmiller@engr.psu.edu\",emergent unsupervised clustering paradigms with potential application to bioinformatics.,\"in recent years, there has been a great upsurge in the application of data clustering, statistical classification, and related machine learning techniques to the field of molecular biology, in particular analysis of {dna} microarray expression data. clustering methods can be used to group co-expressed genes, shedding light on gene function and co-regulation. alternatively, they can group samples or conditions to identify phenotypical groups, disease subgroups, or to help identify disease pathways. a rich variety of unsupervised techniques have been applied, including partitional, hierarchical, graph-based, model-based, and biclustering methods. while a number of machine learning problems and tools have found mainstream applications in bioinformatics, in this article we identify some challenging problems which, though clearly relevant to bioinformatics, have not been extensively investigated in this domain. these include i) unsupervised clustering with unsupervised feature selection, ii) semisupervised learning, iii) unsupervised learning (and supervised learning) in the presence of confounding variables, and iv) stability of clustering solutions. we review recent methods which address these problems and take the position that these methods are well-suited to addressing some common scenarios that occur in bioinformatics.\"understanding the contribution of synonymous mutations to human disease,\"synonymous mutations - sometimes called \\'silent\\' mutations - are now widely acknowledged to be able to cause changes in protein expression, conformation and function. the recent increase in knowledge about the association of genetic variants with disease, particularly through genome-wide association studies, has revealed a substantial contribution of synonymous {snps} to human disease risk and other complex traits. here we review current understanding of the extent to which synonymous mutations influence disease, the various molecular mechanisms that underlie these effects and the implications for future research and biomedical applications.\"united kingdom. elevy@mrc-lmb.cam.ac.uk\",{3d} complex: a structural classification of protein complexes,\"most of the proteins in a cell assemble into complexes to carry out their function. it is therefore crucial to understand the physicochemical properties as well as the evolution of interactions between proteins. the protein data bank represents an important source of information for such studies, because more than half of the structures are homo- or heteromeric protein complexes. here we propose the first hierarchical classification of whole protein complexes of known {3-d} structure, based on representing their fundamental structural features as a graph. this classification provides the first overview of all the complexes in the protein data bank and allows nonredundant sets to be derived at different levels of detail. this reveals that between one-half and two-thirds of known structures are multimeric, depending on the level of redundancy accepted. we also analyse the structures in terms of the topological arrangement of their subunits and find that they form a small number of arrangements compared with all theoretically possible ones. this is because most complexes contain four subunits or less, and the large majority are homomeric. in addition, there is a strong tendency for symmetry in complexes, even for heteromeric complexes. finally, through comparison of biological units in the protein data bank with the protein quaternary structure database, we identified many possible errors in quaternary structure assignments. our classification, available as a database and web server at {http://www.3dcomplex}.org, will be a starting point for future work aimed at understanding the structure and evolution of protein complexes.\"\"a catalogue of molecular aberrations that cause ovarian cancer is critical for developing and deploying therapies that will improve patients\\' lives. the cancer genome atlas project has analysed messenger {rna} expression, {microrna} expression, promoter methylation and {dna} copy number in 489 high-grade serous ovarian adenocarcinomas and the {dna} sequences of exons from coding genes in 316 of these tumours. here we report that high-grade serous ovarian cancer is characterized by {tp53} mutations in almost all tumours (96\\\\%); low prevalence but statistically recurrent somatic mutations in nine further genes including {nf1}, {brca1}, {brca2}, {rb1} and {cdk12}; 113 significant focal {dna} copy number aberrations; and promoter methylation events involving 168 genes. analyses delineated four ovarian cancer transcriptional subtypes, three {microrna} subtypes, four promoter methylation subtypes and a transcriptional signature associated with survival duration, and shed new light on the impact that tumours with {brca1}/2 ({brca1} or {brca2}) and {ccne1} aberrations have on survival. pathway analyses suggested that homologous recombination is defective in about half of the tumours analysed, and that {notch} and {foxm1} signalling are involved in serous ovarian cancer pathophysiology.\"\"many cancers have substantial genomic heterogeneity within a given tumor, and to fully understand that diversity requires the ability to perform single cell analysis. we performed targeted sequencing of a panel of single nucleotide variants ({snvs}), deletions, and {igh} sequences in 1,479 single tumor cells from six acute lymphoblastic leukemia ({all}) patients. by accurately segregating groups of cooccurring mutations into distinct clonal populations, we identified codominant clones in the majority of patients. evaluation of intraclonal mutation patterns identified clone-specific punctuated cytosine mutagenesis events, showed that most structural variants are acquired before {snvs}, determined that {kras} mutations occur late in disease development but are not sufficient for clonal dominance, and identified clones within the same patient that are arrested at varied stages in b-cell development. taken together, these data order the sequence of genetic events that underlie childhood {all} and provide a framework for understanding the development of the disease at single-cell resolution.\"\"darwinian evolution favours genotypes with high replication rates, a process called \\'survival of the fittest\\'. however, knowing the replication rate of each individual genotype may not suffice to predict the eventual survivor, even in an asexual population. according to quasi-species theory, selection favours the cloud of genotypes, interconnected by mutation, whose average replication rate is highest1, 2, 3, 4, 5. here we confirm this prediction using digital organisms that self-replicate, mutate and evolve6, 7, 8, 9. forty pairs of populations were derived from 40 different ancestors in identical selective environments, except that one of each pair experienced a 4-fold higher mutation rate. in 12 cases, the dominant genotype that evolved at the lower mutation rate achieved a replication rate >1.5-fold faster than its counterpart. we allowed each of these disparate pairs to compete across a range of mutation rates. in each case, as mutation rate was increased, the\\xa0outcome of competition switched to favour the genotype with the lower replication rate. these genotypes, although they occupied lower fitness peaks, were located in flatter regions of the fitness surface and were therefore more robust with respect to mutations.\"\"advances in structural genomics and protein structure prediction require the design of automatic, fast, objective, and well benchmarked methods capable of comparing and assessing the similarity of low-resolution three-dimensional structures, via experimental or theoretical approaches. here, a new method for sequence-independent structural alignment is presented that allows comparison of an experimental protein structure with an arbitrary low-resolution protein tertiary model. the heuristic algorithm is given and then used to show that it can describe random structural alignments of proteins with different folds with good accuracy by an extreme value distribution. from this observation, a structural similarity score between two proteins or two different conformations of the same protein is derived from the likelihood of obtaining a given structural alignment by chance. the performance of the derived score is then compared with well established, consensus manual-based scores and data sets. we found that the new approach correlates better than other tools with the gold standard provided by a human evaluator. timings indicate that the algorithm is fast enough for routine use with large databases of protein models. overall, our results indicate that the new program ({mammoth}) will be a good tool for protein structure comparisons in structural genomics applications. {mammoth} is available from our web site at http://physbio.mssm.edu/\\\\~{}ortizg/.\"\"experiment and simulation are now conspiring to give atomic-level descriptions of protein folding relevant to folding, misfolding, trafficking, and degradation in the cell. we are on the threshold of predicting those protein folding events using simulation that has been carefully benchmarked by experiment.\"lyngby, denmark. gorm@cbs.dtu.dk\",the biology of eukaryotic promoter prediction--a review.,\"computational prediction of eukaryotic promoters from the nucleotide sequence is one of the most attractive problems in sequence analysis today, but it is also a very difficult one. thus, current methods predict in the order of one promoter per kilobase in human {dna}, while the average distance between functional promoters has been estimated to be in the range of 30-40 kilobases. although it is conceivable that some of these predicted promoters correspond to cryptic initiation sites that are used in vivo, it is likely that most are false positives. this suggests that it is important to carefully reconsider the biological data that forms the basis of current algorithms, and we here present a review of data that may be useful in this regard. the review covers the following topics: (1) basal transcription and core promoters, (2) activated transcription and transcription factor binding sites, (3) {cpg} islands and {dna} methylation, (4) chromosomal structure and nucleosome modification, and (5) chromosomal domains and domain boundaries. we discuss the possible lessons that may be learned, especially with respect to the wealth of information about epigenetic regulation of transcription that has been appearing in recent years.\"\"aligning and comparing genomic sequences enables the identification of conserved sequence signatures and can enrich for coding and noncoding functional regions. in vertebrates, the comparison of human and rodent genomes and the comparison of evolutionarily distant genomes, such as human and pufferfish, have identified specific sets of \\'ultraconserved\\' sequence elements associated with the control of early development. however, is this just the tip of a \\'conservation iceberg\\' or do these sequences represent a specific class of regulatory element? studies on the zebrafish phox2b gene region and the {encode} project suggest that many regulatory elements are not highly conserved, posing intriguing questions about the relationship between noncoding sequence conservation and function and the evolution of regulatory sequences.\"\"plasma of cancer patients contains cell-free tumor {dna} that carries information on tumor mutations and tumor burden. individual mutations have been probed using allele-specific assays, but sequencing of entire genes to detect cancer mutations in circulating {dna} has not been demonstrated. we developed a method for tagged-amplicon deep sequencing ({tam}-seq) and screened 5995 genomic bases for low-frequency mutations. using this method, we identified cancer mutations present in circulating {dna} at allele frequencies as low as 2\\\\%, with sensitivity and specificity of >97\\\\%. we identified mutations throughout the tumor suppressor gene {tp53} in circulating {dna} from 46 plasma samples of advanced ovarian cancer patients. we demonstrated use of {tam}-seq to noninvasively identify the origin of metastatic relapse in a patient with multiple primary tumors. in another case, we identified in plasma an {egfr} mutation not found in an initial ovarian biopsy. we further used {tam}-seq to monitor tumor dynamics, and tracked 10 concomitant mutations in plasma of a metastatic breast cancer patient over 16 months. this low-cost, high-throughput method could facilitate analysis of circulating {dna} as a noninvasive  ” liquid biopsy” for personalized cancer genomics.\"\"structural bioinformatics, biotec tu dresden, tatzberg 47-51, 01307 dresden, germany\",characterization of interfacial solvent in protein complexes and contribution of <i>wet {spots</i}> to the interface description,\"water networks in protein interfaces can complement direct interactions contributing significantly to molecular recognition, function, and stability of protein association. thus, water can be seen as an extension or addition of protein structural features, which may add plenty of information to protein interfacial definition. however, solvent is frequently neglected in protein interaction studies. analysis of the interfacial information contained in the {pdb} is essential to achieve more accurate descriptions of protein interfaces. with this aim, we have used the {scowlp} database () and applied computational geometry methods to extract and analyze interfacial information of a high-resolution nonredundant dataset of 176 protein complexes containing obligate and transient interfaces. we have identified all interfacial residues and characterized them in terms of temperature factors, secondary structure, residue composition, and pairing preferences to understand their contribution to the interface description. we have paid special attention to water-bridged residues; focusing on those that interact only mediated by a water molecule called wet spots. our results show that 40.1\\\\% of the interfacial residues are interacting through water and that wet spots represent a 14.5\\\\% of the total, emphasizing the importance of the inclusion of solvent in protein interaction studies, and the contribution of wet spots to interfacial description. wet spots present similar characteristics to residues binding buried water molecules in the core or cavities of proteins; being preferably located in nonregular secondary structures and establishing hydrogen bonds by their main-chains. we observe that obligate and transient interfaces present a comparable amount of solvent. moreover, the role of solvent in both complex types differs according to the different nature of their interfaces. the information obtained in our studies will assist in the process of accomplishing more accurate descriptions of protein interfaces and may be helpful to improve comparison of protein family interfaces, to facilitate rational ligand design, and to guide protein docking. proteins 2007. {\\\\copyright} 2007 {wiley-liss}, inc.\"\"the identification of protein–protein interaction sites is an essential intermediate step for mutant design and the prediction of protein networks. in recent years a significant number of methods have been developed to predict these interface residues and here we review the current status of the field. progress in this area requires a clear view of the methodology applied, the data sets used for training and testing the systems, and the evaluation procedures. we have analysed the impact of a representative set of features and algorithms and highlighted the problems inherent in generating reliable protein data sets and in the posterior analysis of the results. although it is clear that there have been some improvements in methods for predicting interacting sites, several major bottlenecks remain. proteins in complexes are still under-represented in the structural databases and in particular many proteins involved in transient complexes are still to be crystallized. we provide suggestions for effective feature selection, and make it clear that community standards for testing, training and performance measures are necessary for progress in the field.\"p-1/12 cit scheme viim, calcutta 700 054, india.\",conservation and relative importance of residues across protein-protein interfaces,\"a core region surrounded by a rim characterizes biological interfaces. we ascertain the importance of the core by showing the sequence entropies of the residues comprising the core to be smaller than those in the rim. such a distinction is not seen in the 2-fold-related, nonphysiological interfaces formed in crystal lattices of monomeric proteins, thereby providing a procedure for characterizing the oligomeric state from crystal structures of protein molecules. this method is better than those that rely on the comparison of the sequence entropies in the interface and the rest of the protein surface, especially in cases where the surface harbors additional binding sites. to a good approximation there is a correlation between the accessible surface area lost because of complexation and {δδg} values obtained through alanine-scanning mutagenesis (26-38 cal per \\\\aa{}2 of the surface buried) for residues located in the core, a relationship that is not discernable for rim residues. if, however, a residue participates in hydrogen bonding across the interface, the extent of stabilization is 52 cal/mol per 1 \\\\aa{}2 of the nonpolar surface area buried by the residue. as opposed to an amino acid classification used earlier, an environment-based grouping of residues yields a better discrimination in the sequence entropy between the core and the rim.\"co-adaptation and interactions.\",\"co-evolution has an important function in the evolution of species and it is clearly manifested in certain scenarios such as host-parasite and predator-prey interactions, symbiosis and mutualism. the extrapolation of the concepts and methodologies developed for the study of species co-evolution at the molecular level has prompted the development of a variety of computational methods able to predict protein interactions through the characteristics of co-evolution. particularly successful have been those methods that predict interactions at the genomic level based on the detection of pairs of protein families with similar evolutionary histories (similarity of phylogenetic trees: mirrortree). future advances in this field will require a better understanding of the molecular basis of the co-evolution of protein families. thus, it will be important to decipher the molecular mechanisms underlying the similarity observed in phylogenetic trees of interacting proteins, distinguishing direct specific molecular interactions from other general functional constraints. in particular, it will be important to separate the effects of physical interactions within protein complexes (\\'co-adaptation\\') from other forces that, in a less specific way, can also create general patterns of co-evolution.\"\"humans differ from other primates by marked differences in cognitive abilities and a significantly larger brain. these differences correlate with metabolic changes, as evidenced by the relative up-regulation of energy-related genes and metabolites in human brain. while the mechanisms underlying these evolutionary changes have not been elucidated, altered activities of key transcription factors ({tfs}) could play a pivotal role. to assess this possibility, we analyzed microarray data from five tissues from humans and chimpanzees. we identified 90 {tf} genes with significantly different expression levels in human and chimpanzee brain among which the rapidly evolving {krab}-zinc finger genes are markedly over-represented. the differentially expressed {tfs} cluster within a robust regulatory network consisting of two distinct but interlinked modules, one strongly associated with energy metabolism functions, and the other with transcription, vesicular transport, and ubiquitination. our results suggest that concerted changes in a relatively small number of interacting {tfs} may coordinate major gene expression differences in human and chimpanzee brain.\"\"the recognition of transcription factor binding sites ({tfbss}) is the first step on the way to deciphering the {dna} regulatory code. there is a large variety of experimental approaches providing information on {tfbs} location in genomic sequences. many computational approaches to {tfbs} recognition based on the experimental data obtained are available, each having its own advantages and shortcomings. this article provides short review of approaches to computational recognition of {tfbs} in genomic sequences and methods of experimental verification of predicted sites. we also present a case study of the interplay between experimental and theoretical approaches to the successful prediction of steroidogenic factor 1 ({sf1}).\"\"breast cancer is the most common cancer and the leading cause of cancer-related death in women worldwide.1 metastatic breast cancer remains an incurable disease but is treatable by means of serial administration of endocrine, cytotoxic, or biologic therapies. the monitoring of treatment response is essential to avoid continuing ineffective therapies, to prevent unnecessary side effects, and to determine the benefit of new therapeutics. treatment response is generally assessed with the use of serial imaging, but radiographic measurements often fail to detect changes in tumor burden. therefore, there is an urgent need for biomarkers that measure tumor burden with high sensitivity .\\xa0.\\xa0.\"genomic architecture and evolution of clear cell renal cell carcinomas defined by multiregion sequencing,\"clear cell renal carcinomas ({ccrccs}) can display intratumor heterogeneity ({ith}). we applied multiregion exome sequencing (m-seq) to resolve the genetic architecture and evolutionary histories of ten {ccrccs}. ultra-deep sequencing identified {ith} in all cases. we found that 73-75\\\\% of identified {ccrcc} driver aberrations were subclonal, confounding estimates of driver mutation prevalence. {ith} increased with the number of biopsies analyzed, without evidence of saturation in most tumors. chromosome 3p loss and {vhl} aberrations were the only ubiquitous events. the proportion of {c>t} transitions at {cpg} sites increased during tumor progression. m-seq permits the temporal resolution of {ccrcc} evolution and refines mutational signatures occurring during tumor development.\"aneuploidy drives genomic instability in yeast.,\"aneuploidy decreases cellular fitness, yet it is also associated with cancer, a disease of enhanced proliferative capacity. to investigate one mechanism by which aneuploidy could contribute to tumorigenesis, we examined the effects of aneuploidy on genomic stability. we analyzed 13 budding yeast strains that carry extra copies of single chromosomes and found that all aneuploid strains exhibited one or more forms of genomic instability. most strains displayed increased chromosome loss and mitotic recombination, as well as defective {dna} damage repair. aneuploid fission yeast strains also exhibited defects in mitotic recombination. aneuploidy-induced genomic instability could facilitate the development of genetic alterations that drive malignant growth in cancer.\"\"proteins that can interact with multiple partners play central roles in the network of protein–protein interactions. they are called hub proteins, and recently it was suggested that an abundance of intrinsically disordered regions on their surfaces facilitates their binding to multiple partners. however, in those studies, the hub proteins were identified as proteins with multiple partners, regardless of whether the interactions were transient or permanent. as a result, a certain number of hub proteins are subunits of stable multi-subunit proteins, such as supramolecules. it is well known that stable complexes and transient complexes have different structural features, and thus the statistics based on the current definition of hub proteins will hide the true nature of hub proteins. therefore, in this paper, we first describe a new approach to identify proteins with multiple partners dynamically, using the protein data bank, and then we performed statistical analyses of the structural features of these proteins. we refer to the proteins as transient hub proteins or sociable proteins, to clarify the difference with hub proteins. as a result, we found that the main difference between sociable and nonsociable proteins is not the abundance of disordered regions, in contrast to the previous studies, but rather the structural flexibility of the entire protein. we also found greater predominance of charged and polar residues in sociable proteins than previously reported.\"\"domains are the building blocks of proteins and play a crucial role in protein\\\\&\\\\#8211;protein interactions. here, we propose a new approach for the analysis and prediction of domain\\\\&\\\\#8211;domain interfaces. our method, which relies on the representation of domains as residue-interacting networks, finds an optimal decomposition of domain structures into modules. the resulting modules comprise highly cooperative residues, which exhibit few connections with other modules. we found that non-overlapping binding sites in a domain, involved in different domain\\\\&\\\\#8211;domain interactions, are generally contained in different modules. this observation indicates that our modular decomposition is able to separate protein domains into regions with specialized functions. our results show that modules with high modularity values identify binding site regions, demonstrating the predictive character of modularity. furthermore, the combination of modularity with other characteristics, such as sequence conservation or surface patches, was found to improve our predictions. in an attempt to give a physical interpretation to the modular architecture of domains, we analyzed in detail six examples of protein domains with available experimental binding data. the modular configuration of the {tem1}-\\\\&\\\\#946;-lactamase binding site illustrates the energetic independence of hotspots located in different modules and the cooperativity of those sited within the same modules. the energetic and structural cooperativity between intramodular residues is also clearly shown in the example of the chymotrypsin inhibitor, where non\\\\&\\\\#8211;binding site residues have a synergistic effect on binding. interestingly, the binding site of the t cell receptor \\\\&\\\\#946; chain variable domain 2.1 is contained in one module, which includes structurally distant hot regions displaying positive cooperativity. these findings support the idea that modules possess certain functional and energetic independence. a modular organization of binding sites confers robustness and flexibility to the performance of the functional activity, and facilitates the evolution of protein interactions.\"\"division of biomedical informatics, children\\'s hospital research foundation, cincinnati, ohio 45229; department of informatics, nicholas copernicus university, 87-100 toru\\\\&nacute;, poland\",prediction-based fingerprints of protein-protein interactions,\"the recognition of protein interaction sites is an important intermediate step toward identification of functionally relevant residues and understanding protein function, facilitating experimental efforts in that regard. toward that goal, the authors propose a novel representation for the recognition of protein-protein interaction sites that integrates enhanced relative solvent accessibility ({rsa}) predictions with high resolution structural data. an observation that {rsa} predictions are biased toward the level of surface exposure consistent with protein complexes led the authors to investigate the difference between the predicted and actual (i.e., observed in an unbound structure) {rsa} of an amino acid residue as a fingerprint of interaction sites. the authors demonstrate that {rsa} prediction-based fingerprints of protein interactions significantly improve the discrimination between interacting and noninteracting sites, compared with evolutionary conservation, physicochemical characteristics, structure-derived and other features considered before. on the basis of these observations, the authors developed a new method for the prediction of protein-protein interaction sites, using machine learning approaches to combine the most informative features into the final predictor. for training and validation, the authors used several large sets of protein complexes and derived from them nonredundant representative chains, with interaction sites mapped from multiple complexes. alternative machine learning techniques are used, including support vector machines and neural networks, so as to evaluate the relative effects of the choice of a representation and a specific learning algorithm. the effects of induced fit and uncertainty of the negative (noninteracting) class assignment are also evaluated. several representative methods from the literature are reimplemented to enable direct comparison of the results. using rigorous validation protocols, the authors estimated that the new method yields the overall classification accuracy of about 74\\\\% and matthews correlation coefficients of 0.42, as opposed to up to 70\\\\% classification accuracy and up to 0.3 matthews correlation coefficient for methods that do not utilize {rsa} prediction-based fingerprints. the new method is available at . proteins 2007. {\\\\copyright} 2006 {wiley-liss}, inc.\"\"clonal evolution is a key feature of cancer progression and relapse. we studied intratumoral heterogeneity in 149 chronic lymphocytic leukemia ({cll}) cases by integrating whole-exome sequence and copy number to measure the fraction of cancer cells harboring each somatic mutation. we identified driver mutations as predominantly clonal (e.g., {myd88}, trisomy 12, and del(13q)) or subclonal (e.g., {sf3b1} and {tp53}), corresponding to earlier and later events in {cll} evolution. we sampled leukemia cells from 18 patients at two time points. ten of twelve {cll} cases treated with chemotherapy (but only one of six without treatment) underwent clonal evolution, predominantly involving subclones with driver mutations (e.g., {sf3b1} and {tp53}) that expanded over time. furthermore, presence of a subclonal driver mutation was an independent risk factor for rapid disease progression. our study thus uncovers patterns of clonal evolution in {cll}, providing insights into its stepwise transformation, and links the presence of subclones with adverse clinical outcomes. copyright {\\\\copyright} 2013 elsevier inc. all rights reserved.\"\"aneuploidy has been recognized as a hallmark of cancer for more than 100 years, yet no general theory to explain the recurring patterns of aneuploidy in cancer has emerged. here, we develop tumor suppressor and oncogene ({tuson}) explorer, a computational method that analyzes the patterns of mutational signatures in tumors and predicts the likelihood that any individual gene functions as a tumor suppressor ({tsg}) or oncogene ({og}). by analyzing >8,200 tumor-normal pairs, we provide statistical evidence suggesting that many more genes possess cancer driver properties than anticipated, forming a continuum of oncogenic potential. integrating our driver predictions with information on somatic copy number alterations, we find that the distribution and potency of {tsgs} ({stop} genes), {ogs}, and essential genes ({go} genes) on chromosomes can predict the complex patterns of aneuploidy and copy number variation characteristic of cancer genomes. we propose that the cancer genome is shaped through a process of cumulative haploinsufficiency and triplosensitivity. copyright {\\\\copyright} 2013 elsevier inc. all rights reserved.\"iowa city, ia 52242-1109, usa. adrain-elcock@uiowa.edu\",identification of protein oligomerization states by analysis of interface conservation,\"the discrimination of true oligomeric protein–protein contacts from nonspecific crystal contacts remains problematic. criteria that have been used previously base the assignment of oligomeric state on consideration of the area of the interface and/or the results of scoring functions based on statistical potentials. both techniques have a high success rate but fail in more than 10\\\\% of cases. more importantly, the oligomeric states of several proteins are incorrectly assigned by both methods. here we test the hypothesis that true oligomeric contacts should be identifiable on the basis of an increased degree of conservation of the residues involved in the interface. by quantifying the degree of conservation of the interface and comparing it with that of the remainder of the protein surface, we develop a new criterion that provides a highly effective complement to existing methods.\"national institutes of health, bethesda, md 20894, usa. altschul@ncbi.nlm.nih.gov\",gapped {blast} and {psi}-{blast}: a new generation of protein database search programs.,\"the {blast} programs are widely used tools for searching protein and {dna} databases for sequence similarities. for protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the {blast} programs to be decreased substantially while enhancing their sensitivity to weak similarities. a new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped {blast} program that runs at approximately three times the speed of the original. in addition, a method is introduced for automatically combining statistically significant alignments produced by {blast} into a position-specific score matrix, and searching the database using this matrix. the resulting {position-specific} iterated {blast} ({psi}-{blast}) program runs at approximately the same speed per iteration as gapped {blast}, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. {psi}-{blast} is used to uncover several new and interesting members of the {brct} superfamily.\"california, usa. cherry@stanford.edu\",gene ontology: tool for the unification of biology. the gene ontology consortium.,\"genomic sequencing has made it clear that a large fraction of the genes specifying the core biological functions are shared by all eukaryotes. knowledge of the biological role of such shared proteins in one organism can often be transferred to other organisms. the goal of the gene ontology consortium is to produce a dynamic, controlled vocabulary that can be applied to all eukaryotes even as knowledge of gene and protein roles in cells is accumulating and changing. to this end, three independent ontologies accessible on the {world-wide} web (http://www.geneontology.org) are being constructed: biological process, molecular function and cellular component.\"the university of manchester, michael smith building, oxford road, manchester m13 9pt, uk.\",protein interactions from complexes: a structural perspective.,san diego, ca 92121, usa.\",a gene atlas of the mouse and human protein-encoding transcriptomes,\"the tissue-specific pattern of {mrna} expression can indicate important clues about gene function. high-density oligonucleotide arrays offer the opportunity to examine patterns of gene expression on a genome scale. toward this end, we have designed custom arrays that interrogate the expression of the vast majority of protein-encoding human and mouse genes and have used them to profile a panel of 79 human and 61 mouse tissues. the resulting data set provides the expression patterns for thousands of predicted genes, as well as known and poorly characterized genes, from mice and humans. we have explored this data set for global trends in gene expression, evaluated commonly used lines of evidence in gene prediction methodologies, and investigated patterns indicative of chromosomal organization of transcription. we describe hundreds of regions of correlated transcription and show that some are subject to both tissue and parental allele-specific expression, suggesting a link between spatial expression and imprinting.\"\"exhaustive methods of sequence alignment are accurate but slow, whereas heuristic approaches run quickly, but their complexity makes them more difficult to implement. we introduce bounded sparse dynamic programming ({bsdp}) to allow rapid approximation to exhaustive alignment. this is used within a framework whereby the alignment algorithms are described in terms of their underlying model, to allow automated development of efficient heuristic implementations which may be applied to a general set of sequence comparison problems. the speed and accuracy of this approach compares favourably with existing methods. examples of its use in the context of genome annotation are given. this system allows rapid implementation of heuristics approximating to many complex alignment models, and has been incorporated into the freely available sequence alignment program, exonerate.\"and college of engineering, rumelifeneri yolu, 34450 sariyer istanbul, turkey. okeskin@ku.edu.tr\",hot regions in protein--protein interactions: the organization and contribution of structurally conserved hot spot residues.,seattle 98195, usa.\",assembly of protein tertiary structures from fragments with similar local sequences using simulated annealing and bayesian scoring functions,\"we explore the ability of a simple simulated annealing procedure to assemble native-like structures from fragments of unrelated protein structures with similar local sequences using bayesian scoring functions. environment and residue pair specific contributions to the scoring functions appear as the first two terms in a series expansion for the residue probability distributions in the protein database; the decoupling of the distance and environment dependencies of the distributions resolves the major problems with current database-derived scoring functions noted by thomas and dill. the simulated annealing procedure rapidly and frequently generates native-like structures for small helical proteins and better than random structures for small \\\\^{i}² sheet containing proteins. most of the simulated structures have native-like solvent accessibility and secondary structure patterns, and thus ensembles of these structures provide a particularly challenging set of decoys for evaluating scoring functions. we investigate the effects of multiple sequence information and different types of conformational constraints on the overall performance of the method, and the ability of a variety of recently developed scoring functions to recognize the native-like conformations in the ensembles of simulated structures.\"london, u.k.\",protein-protein interactions: a review of protein dimer structures.,rehovot 76100 israel.\",{promate}: a structure based prediction program to identify the location of protein-protein binding sites.,se-10691 stockholm, sweden\",\"{gromacs} 4:\\u2009 algorithms for highly efficient, {load-balanced}, and scalable molecular simulation\",\"molecular simulation is an extremely useful, but computationally very expensive tool for studies of chemical and biomolecular systems. here, we present a new implementation of our molecular simulation toolkit {gromacs} which now both achieves extremely high performance on single processors from algorithmic optimizations and hand-coded routines and simultaneously scales very well on parallel machines. the code encompasses a minimal-communication domain decomposition algorithm, full dynamic load balancing, a state-of-the-art parallel constraint solver, and efficient virtual site algorithms that allow removal of hydrogen atom degrees of freedom to enable integration time steps up to 5 fs for atomistic simulations also in parallel. to improve the scaling properties of the common particle mesh ewald electrostatics algorithms, we have in addition used a {multiple-program}, {multiple-data} approach, with separate node domains responsible for direct and reciprocal space interactions. not only does this combination of algorithms enable extremely long simulations of large systems but also it provides that simulation performance on quite modest numbers of standard cluster nodes.\"000 human genes?\",10.1126/science.1058969\"vertebrate {cpg} islands ({cgis}) are short interspersed {dna} sequences that deviate significantly from the average genomic pattern by being {gc}-rich, {cpg}-rich, and predominantly nonmethylated. most, perhaps all, {cgis} are sites of transcription initiation, including thousands that are remote from currently annotated promoters. shared {dna} sequence features adapt {cgis} for promoter function by destabilizing nucleosomes and attracting proteins that create a transcriptionally permissive chromatin state. silencing of {cgi} promoters is achieved through dense {cpg} methylation or polycomb recruitment, again using their distinctive {dna} sequence composition. {cgis} are therefore generically equipped to influence local chromatin structure and simplify regulation of gene activity.\"replication stress links structural and numerical cancer chromosomal instability,\"essentially all the biological functions of {dna} depend on site‐specific {dna}‐binding proteins finding their targets, and therefore \\'searching\\' through megabases of non‐target {dna}. in this article, we review current understanding of how this sequence searching is done. we review how simple diffusion through solution may be unable to account for the rapid rates of association observed in experiments on some model systems, primarily the lac repressor. we then present a simplified version of the \\'facilitated diffusion\\' model of berg, winter and von hippel, showing how non‐specific {dna}–protein interactions may account for accelerated targeting, by permitting the protein to sample many binding sites per {dna} encounter. we discuss the 1‐dimensional \\'sliding\\' motion of protein along non‐specific {dna}, often proposed to be the mechanism of this multiple site sampling, and we discuss the role of short‐range diffusive \\'hopping\\' motions. we then derive the optimal range of sliding for a few physical situations, including simple models of chromosomes in vivo, showing that a sliding range of ∼100\\xa0bp before dissociation optimizes targeting in vivo. going beyond first‐order binding kinetics, we discuss how processivity, the interaction of a protein with two or more targets on the same {dna}, can reveal the extent of sliding and we review recent experiments studying processivity using the restriction enzyme {ecorv}. finally, we discuss how single molecule techniques might be used to study the dynamics of {dna} site‐specific targeting of proteins.\"a comprehensive transcriptional portrait of human cancer cell lines,hinxton, cambridge cb10 1sd, uk.\",inference of macromolecular assemblies from crystalline state.,\"we discuss basic physical-chemical principles underlying the formation of stable macromolecular complexes, which in many cases are likely to be the biological units performing a certain physiological function. we also consider available theoretical approaches to the calculation of macromolecular affinity and entropy of complexation. the latter is shown to play an important role and make a major effect on complex size and symmetry. we develop a new method, based on chemical thermodynamics, for automatic detection of macromolecular assemblies in the protein data bank ({pdb}) entries that are the results of x-ray diffraction experiments. as found, biological units may be recovered at 80-90\\\\% success rate, which makes x-ray crystallography an important source of experimental data on macromolecular complexes and protein-protein interactions. the method is implemented as a public {www} service.\"repitools: an r package for the analysis of enrichment-based epigenomic data.,,estimates of worldwide burden of cancer in 2008: {globocan} 2008,\"estimates of the worldwide incidence and mortality from 27 cancers in 2008 have been prepared for 182 countries as part of the {globocan} series published by the international agency for research on cancer. in this article, we present the results for 20 world regions, summarizing the global patterns for the eight most common cancers. overall, an estimated 12.7 million new cancer cases and 7.6 million cancer deaths occur in 2008, with 56\\\\% of new cancer cases and 63\\\\% of the cancer deaths occurring in the less developed regions of the world. the most commonly diagnosed cancers worldwide are lung (1.61 million, 12.7\\\\% of the total), breast (1.38 million, 10.9\\\\%) and colorectal cancers (1.23 million, 9.7\\\\%). the most common causes of cancer death are lung cancer (1.38 million, 18.2\\\\% of the total), stomach cancer (738,000 deaths, 9.7\\\\%) and liver cancer (696,000 deaths, 9.2\\\\%). cancer is neither rare anywhere in the world, nor mainly confined to high-resource countries. striking differences in the patterns of cancer from region to region are observed.\"intra-tumour heterogeneity: a looking glass for cancer?,\"cancers arise owing to mutations in a subset of genes that confer growth advantage. the availability of the human genome sequence led us to propose that systematic resequencing of cancer genomes for mutations would lead to the discovery of many additional cancer genes. here we report more than 1,000 somatic mutations found in 274\\u2009megabases (mb) of {dna} corresponding to the coding exons of 518 protein kinase genes in 210 diverse human cancers. there was substantial variation in the number and pattern of mutations in individual cancers reflecting different exposures, {dna} repair defects and cellular origins. most somatic mutations are likely to be \\'passengers\\' that do not contribute to oncogenesis. however, there was evidence for \\'driver\\' mutations contributing to the development of the cancers studied in approximately 120 genes. systematic sequencing of cancer genomes therefore reveals the evolutionary diversity of cancers and implicates a larger repertoire of cancer genes than previously anticipated.\"\"{formoleculardynamicssimulationsofhydratedproteinsasimpleyetreliablemodelfortheintermolecularpotentialforwaterisrequired.suchamodelmustbeaneffectivepairpotentialvalidforliquiddensitiesthattakesaveragemany}—{bodyinteractionsintoaccount.wehavedevelopedathree}—{pointchargemodel(onhydrogenandoxygenpositions)withalennard}—{jones6—12potentialontheoxygenpositionsonly.parametersforthemodelweredeterminedfrom12moleculardynamicsrunscoveringthetwo}—{dimensionalparameterspaceofchargeandoxygenrepulsion.bothpotentialenergyandpressurewererequiredtocoincidewithexperimentalvalues}.{themodelhasverysatisfactoryproperties,iseasilyincorporatedintoprotein—waterpotentials,andrequiresonly0.25seccomputertimeperdynamicsstep(for216molecules)onacray}—1computer.\"\"author {summarya} historically difficult problem in computational biology is the identification of transcription factor binding sites ({tfbs}) in the promoters of co-regulated genes. with increasing emphasis on research in transcriptional regulation, this problem is also uniquely relevant to emerging results from recent experiments in high-throughput and systems biology. despite extensive research in the area, recent evaluations of previously published techniques show much room for improvement. in this paper, we introduce a fundamentally new approach to the identification of {tfbs}. first, we start by representing nucleotides in promoters as an undirected, weighted graph. given this representation of a binding site graph ({bsg}), we employ relatively simple graph clustering techniques to identify functional {tfbs}. we show that {bsg} predictions significantly outperform all previously evaluated methods in nearly every performance measure using a standardized assessment benchmark. we also find that this approach is more robust than traditional gibbs sampling to selection of input promoters, and thus more likely to perform well under noisy experimental conditions. finally, {bsgs} are very good at predicting specificity determining nucleotides. using {bsg} predictions, we were able to confirm recent experimental results on binding specificity of e-box {tfs} {cbf1} and {pho4} and predict novel specificity determining nucleotides for {tye7}.\"\"while genetic mutation is a hallmark of cancer, many cancers also acquire epigenetic alterations during tumorigenesis including aberrant {dna} hypermethylation of tumor suppressors, as well as changes in chromatin modifications as caused by genetic mutations of the chromatin-modifying machinery. however, the extent of epigenetic alterations in cancer cells has not been fully characterized. here, we describe complete methylome maps at single nucleotide resolution of a low-passage breast cancer cell line and primary human mammary epithelial cells. we find widespread {dna} hypomethylation in the cancer cell, primarily at partially methylated domains ({pmds}) in normal breast cells. unexpectedly, genes within these regions are largely silenced in cancer cells. the loss of {dna} methylation in these regions is accompanied by formation of repressive chromatin, with a significant fraction displaying allelic {dna} methylation where one allele is {dna} methylated while the other allele is occupied by histone modifications {h3k9me3} or {h3k27me3}. our results show a mutually exclusive relationship between {dna} methylation and {h3k9me3} or {h3k27me3}. these results suggest that global {dna} hypomethylation in breast cancer is tightly linked to the formation of repressive chromatin domains and gene silencing, thus identifying a potential epigenetic pathway for gene regulation in cancer cells.\"\"regulation of gene expression at the transcriptional level is a fundamental mechanism that is well conserved in all cellular systems. due to advances in large-scale experimental analyses, we now have a wealth of information on gene regulation such as {mrna} expression level across multiple conditions, genome-wide location data of transcription factors and data on transcription factor binding sites. this knowledge can be used to reconstruct transcriptional regulatory networks. such networks are usually represented as directed graphs where regulatory interactions are depicted as directed edges from the transcription factor nodes to the target gene nodes. this abstract representation allows us to apply graph theory to study transcriptional regulation at global and local levels, to predict regulatory motifs and regulatory modules such as regulons and to compare the regulatory network of different genomes. here we review some of the available computational methodologies for studying transcriptional regulatory networks as well as their interpretation.\"germany.\",statistical modeling of transcription factor binding affinities predicts regulatory interactions,\"recent experimental and theoretical efforts have highlighted the fact that binding of transcription factors to {dna} can be more accurately described by continuous measures of their binding affinities, rather than a discrete description in terms of binding sites. while the binding affinities can be predicted from a physical model, it is often desirable to know the distribution of binding affinities for specific sequence backgrounds. in this paper, we present a statistical approach to derive the exact distribution for sequence models with fixed {gc} content. we demonstrate that the affinity distribution of almost all known transcription factors can be effectively parametrized by a class of generalized extreme value distributions. moreover, this parameterization also describes the affinity distribution for sequence backgrounds with variable {gc} content, such as human promoter sequences. our approach is applicable to arbitrary sequences and all transcription factors with known binding preferences that can be described in terms of a motif matrix. the statistical treatment also provides a proper framework to directly compare transcription factors with very different affinity distributions. this is illustrated by our analysis of human promoters with known binding sites, for many of which we could identify the known regulators as those with the highest affinity. the combination of physical model and statistical normalization provides a quantitative measure which ranks transcription factors for a given sequence, and which can be compared directly with large-scale binding data. its successful application to human promoter sequences serves as an encouraging example of how the method can be applied to other sequences. the binding of proteins to {dna} is a key molecular mechanism, which can regulate the expression of genes in response to different cellular and environmental conditions. the extensive research on gene regulation has generated binding models for many transcription factors, but the prediction of new binding sites is still challenging and difficult to improve in any systematic way. recent experimental advances, notably high throughput binding assays, have shifted the theoretical focus from the prediction of new binding sites towards more quantitative models for the binding affinities of transcription factors, which can now be measured across whole genomes. therefore we have developed a biophysical model which accounts for much of the observed variation in binding strength. here we extend this framework to model not just the binding affinity, but also its distribution in various sequence backgrounds. this enables us to compare predicted affinities from different transcription factors, and to rank them according to their normalized affinity. what are the biological implications of such a ranking? we have demonstrated that many known associations between transcription factors and their respective targets appear as strong interactions. this provides a rationale to predict, for any given promoter region, those transcription factors which are most likely to be involved in its regulation.\"assessing phylogenetic motif models for predicting transcription factor binding sites.,stanford, ca 94305, usa.\",repeated observation of breast tumor subtypes in independent gene expression data sets,\"characteristic patterns of gene expression measured by {dna} microarrays have been used to classify tumors into clinically relevant subgroups. in this study, we have refined the previously defined subtypes of breast tumors that could be distinguished by their distinct patterns of gene expression. a total of 115 malignant breast tumors were analyzed by hierarchical clustering based on patterns of expression of 534  ” intrinsic” genes and shown to subdivide into one basal-like, one {erbb2}-overexpressing, two luminal-like, and one normal breast tissue-like subgroup. the genes used for classification were selected based on their similar expression levels between pairs of consecutive samples taken from the same tumor separated by 15 weeks of neoadjuvant treatment. similar cluster analyses of two published, independent data sets representing different patient cohorts from different laboratories, uncovered some of the same breast cancer subtypes. in the one data set that included information on time to development of distant metastasis, subtypes were associated with significant differences in this clinical feature. by including a group of tumors from {brca1} carriers in the analysis, we found that this genotype predisposes to the basal tumor subtype. our results strongly support the idea that many of these breast tumor subtypes represent biologically distinct disease entities.\"\"germline {brca1} mutations predispose to breast cancer. to identify genetic modifiers of this risk, we performed a genome-wide association study in 1,193 individuals with {brca1} mutations who were diagnosed with invasive breast cancer under age 40 and 1,190 {brca1} carriers without breast cancer diagnosis over age 35. we took forward 96 {snps} for replication in another 5,986 {brca1} carriers (2,974 individuals with breast cancer and 3,012 unaffected individuals). five {snps} on 19p13 were associated with breast cancer risk (p(trend) = 2.3 × 10⁻⁹ to p(trend) = 3.9 × 10⁻⁷), two of which showed independent associations (rs8170, hazard ratio ({hr}) = 1.26, 95\\\\% {ci} 1.17-1.35; rs2363956 {hr} = 0.84, 95\\\\% {ci} 0.80-0.89). genotyping these {snps} in 6,800 population-based breast cancer cases and 6,613 controls identified a similar association with estrogen receptor-negative breast cancer (rs2363956 per-allele odds ratio ({or}) = 0.83, 95\\\\% {ci} 0.75-0.92, p(trend) = 0.0003) and an association with estrogen receptor-positive disease in the opposite direction ({or} = 1.07, 95\\\\% {ci} 1.01-1.14, p(trend) = 0.016). the five {snps} were also associated with triple-negative breast cancer in a separate study of 2,301 triple-negative cases and 3,949 controls (p(trend) = 1 × 10⁻⁷) to p(trend) = 8 × 10⁻⁵; rs2363956 per-allele {or} = 0.80, 95\\\\% {ci} 0.74-0.87, p(trend) = 1.1 × 10⁻⁷\"baltimore, md 21231, usa.\",the genomic landscapes of human breast and colorectal cancers.,\"human cancer is caused by the accumulation of mutations in oncogenes and tumor suppressor genes. to catalog the genetic changes that occur during tumorigenesis, we isolated {dna} from 11 breast and 11 colorectal tumors and determined the sequences of the genes in the reference sequence database in these samples. based on analysis of exons representing 20,857 transcripts from 18,191 genes, we conclude that the genomic landscapes of breast and colorectal cancers are composed of a handful of commonly mutated gene \"\"mountains\"\" and a much larger number of gene \"\"hills\"\" that are mutated at low frequency. we describe statistical and bioinformatic tools that may help identify mutations with a role in tumorigenesis. these results have implications for understanding the nature and heterogeneity of human cancers and for using personal genomics for tumor diagnosis and therapy.\"non-cell-autonomous driving of tumour growth supports sub-clonal heterogeneity,ibbmc, cnrs umr 8619, bat. 430, universit\\\\\\'{e} paris-sud, 91405-orsay, france.\",\"the third {capri} assessment meeting toronto, canada, april 20-21, 2007.\",\"{capri} is a community-wide experiment to test protein-protein docking methods in blind predictions. the toronto meeting assessed structure predictions made from 2005-2007 on nine target protein-protein complexes or homodimers, and reported new developments in functions used to score predicted interactions, in treatment of conformational flexibility, and in taking nonstructural information into account in the predictions.\"\"protein sequences evolve through random mutagenesis with selection for optimal fitness1. cooperative folding into a stable tertiary structure is one aspect of fitness, but evolutionary selection ultimately operates on function, not on structure. in the accompanying paper2, we proposed a model for the evolutionary constraint on a small protein interaction module (the {ww} domain) through application of the {sca}, a statistical analysis of multiple sequence alignments3, 4. construction of artificial protein sequences directed only by the {sca} showed that the information extracted by this analysis is sufficient to engineer the {ww} fold at atomic resolution. here, we demonstrate that these artificial {ww} sequences function like their natural counterparts, showing class-specific recognition of proline-containing target peptides5, 6, 7, 8. consistent with {sca} predictions, a distributed network of residues mediates functional specificity in {ww} domains. the ability to recapitulate natural-like function in designed sequences shows that a relatively small quantity of sequence information is sufficient to specify the global energetics of amino acid interactions.\"ramat-aviv, tel-aviv, 69978, israel. amos@tau.ac.il\",discovering statistically significant biclusters in gene expression data,\"in gene expression data, a bicluster is a subset of the genes exhibiting consistent patterns over a subset of the conditions. we propose a new method to detect significant biclusters in large expression datasets. our approach is graph theoretic coupled with statistical modelling of the data. under plausible assumptions, our algorithm is polynomial and is guaranteed to find the most significant biclusters. we tested our method on a collection of yeast expression profiles and on a human cancer dataset. cross validation results show high specificity in assigning function to genes based on their biclusters, and we are able to annotate in this way 196 uncharacterized yeast genes. we also demonstrate how the biclusters lead to detecting new concrete biological associations. in cancer data we are able to detect and relate finer tissue types than was previously possible. we also show that the method outperforms the biclustering algorithm of cheng and church (2000).\"\"multiple somatic rearrangements are often found in cancer genomes; however, the underlying processes of rearrangement and their contribution to cancer development are poorly characterized. here we use a paired-end sequencing strategy to identify somatic rearrangements in breast cancer genomes. there are more rearrangements in some breast cancers than previously appreciated. rearrangements are more frequent over gene footprints and most are intrachromosomal. multiple rearrangement architectures are present, but tandem duplications are particularly common in some cancers, perhaps reflecting a specific defect in {dna} maintenance. short overlapping sequences at most rearrangement junctions indicate that these have been mediated by non-homologous end-joining {dna} repair, although varying sequence patterns indicate that multiple processes of this type are operative. several expressed in-frame fusion genes were identified but none was recurrent. the study provides a new perspective on cancer genomes, highlighting the diversity of somatic rearrangements and their potential contribution to cancer development.\"\"prostate cancer is the second most common cause of male cancer deaths in the united states. however, the full range of prostate cancer genomic alterations is incompletely characterized. here we present the complete sequence of seven primary human prostate cancers and their paired normal counterparts. several tumours contained complex chains of balanced (that is, \\'copy-neutral\\') rearrangements that occurred within or adjacent to known cancer genes. rearrangement breakpoints were enriched near open chromatin, androgen receptor and {erg} {dna} binding sites in the setting of the {ets} gene fusion {tmprss2}-{erg}, but inversely correlated with these regions in tumours lacking {ets} fusions. this observation suggests a link between chromatin or transcriptional regulation and the genesis of genomic aberrations. three tumours contained rearrangements that disrupted {cadm2}, and four harboured events disrupting either {pten} (unbalanced events), a prostate tumour suppressor, or {magi2} (balanced events), a {pten} interacting protein not previously implicated in prostate tumorigenesis. thus, genomic rearrangements may arise from transcriptional or chromatin aberrancies and engage prostate tumorigenic mechanisms.\"\"{background}:assays of multiple tumor samples frequently reveal recurrent genomic aberrations, including point mutations and copy-number alterations, that affect individual genes. analyses that extend beyond single genes are often restricted to examining pathways, interactions and functional modules that are already {known.methods}:we present a method that identifies functional modules without any information other than patterns of recurrent and mutually exclusive aberrations ({rme} patterns) that arise due to positive selection for key cancer phenotypes. our algorithm efficiently constructs and searches networks of potential interactions and identifies significant modules ({rme} modules) by using the algorithmic significance {test.results}:we apply the method to the {tcga} collection of 145 glioblastoma samples, resulting in extension of known pathways and discovery of new functional modules. the method predicts a role for {ep300} that was previously unknown in glioblastoma. we demonstrate the clinical relevance of these results by validating that expression of {ep300} is prognostic, predicting survival independent of age at diagnosis and tumor {grade.conclusions}:we have developed a sensitive, simple, and fast method for automatically detecting functional modules in tumors based solely on patterns of recurrent genomic aberration. due to its ability to analyze very large amounts of diverse data, we expect it to be increasingly useful when applied to the many tumor panels scheduled to be assayed in the near future.\"saic-frederick, inc., nci-frederick, frederick, md 21702, usa. guna@ncifcrf.gov\",analysis of ordered and disordered protein complexes reveals structural features discriminating between stable and unstable monomers,\"most proteins exist in the cell as multi-component assemblies. however, which proteins need to be present simultaneously in order to perform a given function is frequently unknown. the first step toward this goal would be to predict proteins that can function only when in a complexed form. here, we propose a scheme to distinguish whether the protein components are ordered (stable) or disordered when separated from their complexed partners. we analyze structural characteristics of several types of complexes, such as natively unstructured proteins, ribosomal proteins, two-state and three-state complexes, and crystal-packing dimers. our analysis makes use of the fact that natively unstructured proteins, which undergo a disorder-to-order transition upon binding their partner, and stable monomeric proteins, which exist as dimers only in their crystal form, provide examples of two vastly different scenarios. we find that ordered monomers can be distinguished from disordered monomers on the basis of the per-residue surface and interface areas, which are significantly smaller for ordered proteins. with this scale, two-state dimers (where the monomers unfold upon dimer separation) and ribosomal proteins are shown to resemble disordered proteins. on the other hand, crystal-packing dimers, whose monomers are stable in solution, fall into the ordered protein category. while there should be a continuum in the distributions, nevertheless, the per-residue scale measures the confidence in the determination of whether a protein can exist as a stable monomer. further analysis, focusing on the chemical and contact preferences at the interface, interior and exposed surface areas, reveals that disordered proteins lack a strong hydrophobic core and are composed of highly polar surface area. we discuss the implication of our results for de novo design of stable monomeric proteins and peptides.\"\"this protocol explains how to discover functional signals in genomic sequences by detecting over- or under-represented oligonucleotides (words) or spaced pairs thereof (dyads) with the regulatory sequence analysis tools (http://rsat.ulb.ac.be/rsat/). two typical applications are presented: (i) predicting transcription factor-binding motifs in promoters of coregulated genes and (ii) discovering phylogenetic footprints in promoters of orthologous genes. the steps of this protocol include purging genomic sequences to discard redundant fragments, discovering over-represented patterns and assembling them to obtain degenerate motifs, scanning sequences and drawing feature maps. the main strength of the method is its statistical ground: the binomial significance provides an efficient control on the rate of false positives. in contrast with optimization-based pattern discovery algorithms, the method supports the detection of under- as well as over-represented motifs. computation times vary from seconds (gene clusters) to minutes (whole genomes). the execution of the whole protocol should take approximately 1 h.\"\"{\\\\^{a}€{\\\\oe}phylogenetic} profiling\\\\^{a}€� is based on the hypothesis that during evolution functionally or physically interacting genes are likely to be inherited or eliminated in a codependent manner. creating presence\\\\^{a}€ ” absence profiles of orthologous genes is now a common and powerful way of identifying functionally associated genes. in this approach, correctly determining orthology, as a means of identifying functional equivalence between two genes, is a critical and nontrivial step and largely explains why previous work in this area has mainly focused on using presence\\\\^{a}€ ” absence profiles in prokaryotic species. here, we demonstrate that eukaryotic genomes have a high proportion of multigene families whose phylogenetic profile distributions are poor in presence\\\\^{a}€ ” absence information content. this feature makes them prone to orthology mis-assignment and unsuited to standard profile-based prediction methods. using {cath} structural domain assignments from the {gene3d} database for 13 complete eukaryotic genomes, we have developed a novel modification of the phylogenetic profiling method that uses genome copy number of each domain superfamily to predict functional relationships. in our approach, superfamilies are subclustered at ten levels of sequence identity\\\\^{a}€”from 30\\\\% to 100\\\\%\\\\^{a}€”and phylogenetic profiles built at each level. all the profiles are compared using normalised euclidean distances to identify those with correlated changes in their domain copy number. we demonstrate that two protein families will \\\\^{a}€{\\\\oe}auto-tune\\\\^{a}€� with strong co-evolutionary signals when their profiles are compared at the similarity levels that capture their functional relationship. our method finds functional relationships that are not detectable by the conventional presence\\\\^{a}€ ” absence profile comparisons, and it does not require a priori any fixed criteria to define orthologous genes.\",algorithms for detecting significantly mutated pathways in cancer.,\"recent genome sequencing studies have shown that the somatic mutations that drive cancer development are distributed across a large number of genes. this mutational heterogeneity complicates efforts to distinguish functional mutations from sporadic, passenger mutations. since cancer mutations are hypothesized to target a relatively small number of cellular signaling and regulatory pathways, a common practice is to assess whether known pathways are enriched for mutated genes. we introduce an alternative approach that examines mutated genes in the context of a genome-scale gene interaction network. we present a computationally efficient strategy for de novo identification of subnetworks in an interaction network that are mutated in a statistically significant number of patients. this framework includes two major components. first, we use a diffusion process on the interaction network to define a local neighborhood of \"\"influence\"\" for each mutated gene in the network. second, we derive a two-stage multiple hypothesis test to bound the false discovery rate ({fdr}) associated with the identified subnetworks. we test these algorithms on a large human protein-protein interaction network using somatic mutation data from glioblastoma and lung adenocarcinoma samples. we successfully recover pathways that are known to be important in these cancers and also identify additional pathways that have been implicated in other cancers but not previously reported as mutated in these samples. we anticipate that our approach will find increasing use as cancer genome studies increase in size and scope.\"\"large-scale sequencing of cancer genomes has uncovered thousands of {dna} alterations, but the functional relevance of the majority of these mutations to tumorigenesis is unknown. we have developed a computational method, called cancer-specific high-throughput annotation of somatic mutations ({chasm}), to identify and prioritize those missense mutations most likely to generate functional changes that enhance tumor cell proliferation. the method has high sensitivity and specificity when discriminating between known driver missense mutations and randomly generated missense mutations (area under receiver operating characteristic curve, >0.91; area under {precision-recall} curve, >0.79). {chasm} substantially outperformed previously described missense mutation function prediction methods at discriminating known oncogenic mutations in p53 and the tyrosine kinase epidermal growth factor receptor. we applied the method to 607 missense mutations found in a recent glioblastoma multiforme sequencing study. based on a model that assumed the glioblastoma multiforme mutations are a mixture of drivers and passengers, we estimate that 8\\\\% of these mutations are drivers, causally contributing to tumorigenesis.\"600 n. wolfe street, baltimore, md 21287, usa.\",computational analysis of tissue-specific combinatorial gene regulation: predicting interaction between transcription factors in human tissues,\"tissue-specific gene expression is generally regulated by more than a single transcription factor ({tf}). multiple {tfs} work in concert to achieve tissue specificity. in order to explore these complex {tf} interaction networks, we performed a large-scale analysis of {tf} interactions for 30 human tissues. we first identified tissue-specific genes for 30 tissues based on gene expression databases. we then evaluated the relationships between {tfs} using the relative position and co-occurrence of their binding sites in the promoters of tissue-specific genes. the predicted {tf}–{tf} interactions were validated by both known protein–protein interactions and co-expression of their target genes. we found that our predictions are enriched in known protein–protein interactions (>80 times that of random expectation). in addition, we found that the target genes show the highest co-expression in the tissue of interest. our findings demonstrate that non-tissue specific {tfs} play a large role in regulation of tissue-specific genes. furthermore, they show that individual {tfs} can contribute to tissue specificity in different tissues by interacting with distinct {tf} partners. lastly, we identified several tissue-specific {tf} clusters that may play important roles in tissue-specific gene regulation.\"\"we present an integrated method called chromia for the genome-wide identification of functional target loci of transcription factors. designed to capture the characteristic patterns of transcription factor binding motif occurrences and the histone profiles associated with regulatory elements such as promoters and enhancers, chromia significantly outperforms other methods in the identification of 13 transcription factor binding sites in mouse embryonic stem cells, evaluated by both binding ({chip}-seq) and functional ({rna} interference knockdown) experiments.\"\"the formation of specific protein interactions plays a crucial role in most, if not all, biological processes, including signal transduction, cell regulation, the immune response and others. recent advances in our understanding of the molecular architecture of protein-protein binding sites, which facilitates such diversity in binding affinity and specificity, are enabling us to address key questions. what is the amino acid composition of binding sites? what are interface hotspots? how are binding sites organized? what are the differences between tight and weak interacting complexes? how does water contribute to binding? can the knowledge gained be translated into protein design? and does a universal code for binding exist, or is it the architecture and chemistry of the interface that enable diverse but specific binding solutions?\"\"substantial evidence supports the concept that cancers are organized in a cellular hierarchy with cancer stem cells ({csc}) at the apex. to date, the primary evidence for {cscs} derives from transplantation assays, which have known limitations. in particular, they are unable to report on the fate of cells within the original human tumor. because of the difficulty in measuring tumor characteristics in patients, cellular organization and other aspects of cancer dynamics have not been quantified directly, although they likely play a fundamental role in tumor progression and therapy response. as such, new approaches to study {cscs} in patient-derived tumor specimens are needed. in this study, we exploited ultradeep single-molecule genomic data derived from multiple microdissected colorectal cancer glands per tumor, along with a novel quantitative approach to measure tumor characteristics, define patient-specific tumor profiles, and infer tumor ancestral trees. we show that each cancer is unique in terms of its cellular organization, molecular heterogeneity, time from malignant transformation, and rate of mutation and apoptosis. importantly, we estimate {csc} fractions between 0.5\\\\% and 4\\\\%, indicative of a hierarchical organization responsible for long-lived {csc} lineages, with variable rates of symmetric cell division. we also observed extensive molecular heterogeneity, both between and within individual cancer glands, suggesting a complex hierarchy of mitotic clones. our framework enables the measurement of clinically relevant patient-specific characteristics in vivo, providing insight into the cellular organization and dynamics of tumor growth, with implications for personalized patient care. cancer res; 73(1); 41–49. {\\\\copyright}2012 {aacr}.\"1275 york avenue, new york, ny 10021, usa.\",a faster circular binary segmentation algorithm for the analysis of array {cgh} data,\"motivation: array {cgh} technologies enable the simultaneous measurement of {dna} copy number for thousands of sites on a genome. we developed the circular binary segmentation ({cbs}) algorithm to divide the genome into regions of equal copy number. the algorithm tests for change-points using a maximal t-statistic with a permutation reference distribution to obtain the corresponding p-value. the number of computations required for the maximal test statistic is {o(n2}), where n is the number of markers. this makes the full permutation approach computationally prohibitive for the newer arrays that contain tens of thousands markers and highlights the need for a faster algorithm.\"\"to characterize somatic alterations in colorectal carcinoma, we conducted a genome-scale analysis of 276 samples, analysing exome sequence, {dna} copy number, promoter methylation and messenger {rna} and {microrna} expression. a subset of these samples (97) underwent low-depth-of-coverage whole-genome sequencing. in total, 16\\\\% of colorectal carcinomas were found to be hypermutated: three-quarters of these had the expected high microsatellite instability, usually with hypermethylation and {mlh1} silencing, and one-quarter had somatic mismatch-repair gene and polymerase ε ({pole}) mutations. excluding the hypermutated cancers, colon and rectum cancers were found to have considerably similar patterns of genomic alteration. twenty-four genes were significantly mutated, and in addition to the expected {apc}, {tp53}, {smad4}, {pik3ca} and {kras} mutations, we found frequent mutations in {arid1a}, {sox9} and {fam123b}. recurrent copy-number alterations include potentially drug-targetable amplifications of {erbb2} and newly discovered amplification of {igf2}. recurrent chromosomal translocations include the fusion of {nav2} and {wnt} pathway member {tcf7l1}. integrative analyses suggest new markers for aggressive colorectal carcinoma and an important role for {myc}-directed transcriptional activation and repression.\"\"most proteins interact with only a few other proteins while a small number of proteins (hubs) have many interaction partners. hub proteins and non-hub proteins differ in several respects; however, understanding is not complete about what properties characterize the hubs and set them apart from proteins of low connectivity. therefore, we have investigated what differentiates hubs from non-hubs and static hubs (party hubs) from dynamic hubs (date hubs) in the protein-protein interaction network of saccharomyces cerevisiae. the many interactions of hub proteins can only partly be explained by bindings to similar proteins or domains. it is evident that domain repeats, which are associated with binding, are enriched in hubs. moreover, there is an over representation of multi-domain proteins and long proteins among the hubs. in addition, there are clear differences between party hubs and date hubs. fewer of the party hubs contain long disordered regions compared to date hubs, indicating that these regions are important for flexible binding but less so for static interactions. furthermore, party hubs interact to a large extent with each other, supporting the idea of party hubs as the cores of highly clustered functional modules. in addition, hub proteins, and in particular party hubs, are more often ancient. finally, the more recent paralogs of party hubs are underrepresented. our results indicate that multiple and repeated domains are enriched in hub proteins and, further, that long disordered regions, which are common in date hubs, are particularly important for flexible binding.\"{rhvdm}: an r package to predict the activity and targets of a transcription factor.,\"{summary}: highly parallel genomic platforms like microarrays often present researchers with long lists of differentially expressed genes but contain little or no information on how these genes are regulated. {rhvdm} is a novel r package which uses gene expression time course data to predict the activity and targets of a transcription factor. in the first step, {rhvdm} uses a small number of known targets to derive the activity profile of a given transcription factor. then, in a subsequent step, this activity profile is used to predict other putative targets of that transcription factor. a dynamic and mechanistic model of gene expression is at the heart of the technique. measurement error is taken into account during the process, which allows an objective assessment of the robustness of fit and, therefore, the quality of the predictions. the package relies on efficient algorithms and vectorization to accomplish potentially time consuming tasks including optimization and differential equation integration. we demonstrate the efficiency and accuracy of {rhvdm} by examining the activity of the tumour-suppressing transcription factor, p53. {availability}: the version of the package presented here (1.8.1) is freely available from the bioconductor web site ({http://bioconductor.org/packages/2.3/bioc/html/rhvdm}.html).\"\"we describe a bioinformatic tool, tumor aberration prediction suite ({taps}), for the identification of allele-specific copy numbers in tumor samples using data from affymetrix {snp} arrays. it includes detailed visualization of genomic segment characteristics and iterative pattern recognition for copy number identification, and does not require patient-matched normal samples. {taps} can be used to identify chromosomal aberrations with high sensitivity even when the proportion of tumor cells is as low as 30\\\\%. analysis of cancer samples indicates that {taps} is well suited to investigate samples with aneuploidy and tumor heterogeneity, which is commonly found in many types of solid tumors.\"\"we present a novel method that combines protein structure information with protein interaction data to identify residues that form part of an interaction interface. our prediction method can retrieve interaction hotspots with an accuracy of 60\\\\% (at a 20\\\\% false positive rate). the method was applied to all mutations in the online mendelian inheritance in man ({omim}) database, predicting 1,428 mutations to be related to an interaction defect. combining predicted and hand-curated sets, we discuss how mutations affect protein interactions in general.\"\"cancer results from genetic alterations that disturb the normal cooperative behavior of cells. recent high-throughput genomic studies of cancer cells have shown that the mutational landscape of cancer is complex and that individual cancers may evolve through mutations in as many as 20 different cancer-associated genes. we use data published by sj\\\\\"\"{o}blom et al. (2006) to develop a new mathematical model for the somatic evolution of colorectal cancers. we employ the {wright-fisher} process for exploring the basic parameters of this evolutionary process and derive an analytical approximation for the expected waiting time to the cancer phenotype. our results highlight the relative importance of selection over both the size of the cell population at risk and the mutation rate. the model predicts that the observed genetic diversity of cancer genomes can arise under a normal mutation rate if the average selective advantage per mutation is on the order of 1\\\\%. increased mutation rates due to genetic instability would allow even smaller selective advantages during tumorigenesis. the complexity of cancer progression can be understood as the result of multiple sequential mutations, each of which has a relatively small but positive effect on net cell growth. cancer is a disease of multicellular organisms that is characterized by a breakdown of cooperation between individual cells. the progression of cancer proceeds from a single genetically altered cell to billions of invasive cells through a series of clonal expansions. during tumorigenesis the cancer cells undergo replication and mutation, thereby increasing the size and invasiveness of the tumor. recent sequencing projects of cancer cells suggest that mutations in up to 20 different genes might be responsible for driving an individual tumor\\'s development. this insight contrasts with most mathematical models of cancer progression, which assume that the cancer phenotype is driven by mutations in only a few genes. we present a new mathematical model in which tumorigenesis is driven by mutations in many genes, most of which confer only a small selective advantage. specifically, the progression of a benign tumor of the colon (adenoma) to a malignant tumor (carcinoma) is described by a {wright-fisher} process with growing population size. we explore the basic parameters of the model that are consistent with observed data. we also derive an analytical formula for the expected waiting time for the progression from benign to maligant tumor in terms of the population size, the mutation rate, the selective advantage, and the number of susceptible genes.\"cnb-csic, cantoblanco, 28049, madrid, spain. valencia@cnb.uam.es\",computational methods for the prediction of protein interactions.,\"establishing protein interaction networks is crucial for understanding cellular operations. detailed knowledge of the \\'interactome\\', the full network of protein-protein interactions, in model cellular systems should provide new insights into the structure and properties of these systems. parallel to the first massive application of experimental techniques to the determination of protein interaction networks and protein complexes, the first computational methods, based on sequence and genomic information, have emerged.\"non-invasive analysis of acquired resistance to cancer therapy by sequencing of plasma {dna},philadelphia, pennsylvania 19104, usa;\",{penncnv}: an integrated hidden markov model designed for high-resolution copy number variation detection in whole-genome {snp} genotyping data.,\"comprehensive identification and cataloging of copy number variations ({cnvs}) is required to provide a complete view of human genetic variation. the resolution of {cnv} detection in previous experimental designs has been limited to tens or hundreds of kilobases. here we present {penncnv}, a hidden markov model ({hmm}) based approach, for kilobase-resolution detection of {cnvs} from illumina high-density {snp} genotyping data. this algorithm incorporates multiple sources of information, including total signal intensity and allelic intensity ratio at each {snp} marker, the distance between neighboring {snps}, the allele frequency of {snps}, and the pedigree information where available. we applied {penncnv} to genotyping data generated for 112 {hapmap} individuals; on average, we detected approximately 27 {cnvs} for each individual with a median size of approximately 12 kb. excluding common rearrangements in lymphoblastoid cell lines, the fraction of {cnvs} in offspring not detected in parents ({cnv}-{ndps}) was 3.3\\\\%. our results demonstrate the feasibility of whole-genome fine-mapping of {cnvs} via high-density {snp} genotyping.\"1, d-69117, heidelberg, germany. aloy@embl.de\",{interprets}: protein interaction prediction through tertiary structure,\"summary: {interprets} (interaction prediction through tertiary structure) is a web-based version of our method for predicting protein–protein interactions (aloy and russell, 2002, proc. natl acad. sci. {usa}, 99, 5896–5901). given a pair of query sequences, we first search for homologues in a database of interacting domains ({dbid}) of known three-dimensional complex structures. pairs of sequences homologous to a known interacting pair are scored for how well they preserve the atomic contacts at the interaction interface. {interprets} includes a useful interface for visualising molecular details of any predicted interaction.\"cambridge cb2 0qh, united kingdom. elevy@mrc-lmb.cam.ac.uk\",{piqsi}: protein quaternary structure investigation,\"the smooth particle mesh ewald summation method is widely used to efficiently compute long-range electrostatic force terms in molecular dynamics simulations, and there has been considerable work in developing optimized implementations for a variety of parallel computer architectures. we describe an implementation for nvidia graphical processing units ({gpus}) which are general purpose computing devices with a high degree of intrinsic parallelism and arithmetic performance. we find that, for typical biomolecular simulations (e.g., {dhfr}, {26k} atoms), a single {gpu} equipped workstation is able to provide sufficient performance to permit simulation rates of ≈50 ns/day when used in conjunction with the {acemd} molecular dynamics package(1) and exhibits an accuracy comparable to that of a reference double-precision {cpu} implementation.\"\"motivation: promoter prediction is an important task in genome annotation projects, and during the past years many new promoter prediction programs ({ppps}) have emerged. however, many of these programs are compared inadequately to other programs. in most cases, only a small portion of the genome is used to evaluate the program, which is not a realistic setting for whole genome annotation projects. in addition, a common evaluation design to properly compare {ppps} is still {lacking.results}: we present a large-scale benchmarking study of 17 state-of-the-art {ppps}. a multi-faceted evaluation strategy is proposed that can be used as a gold standard for promoter prediction evaluation, allowing authors of promoter prediction software to compare their method to existing methods in a proper way. this evaluation strategy is subsequently used to compare the chosen promoter predictors, and an in-depth analysis on predictive performance, promoter class specificity, overlap between predictors and positional bias of the predictions is {conducted.availability}: we provide the implementations of the four protocols, as well as the datasets required to perform the benchmarks to the academic community free of charge on {request.contact}: {yves.vandepeer@psb.ugent.besupplementary} information: supplementary data are available at bioinformatics online.\"\"the tumor suppressor p53 exerts antiproliferation effects through its ability to function as a sequence-specific {dna}-binding transcription factor. here, we demonstrate that p53 can be modified by acetylation both in vivo and in vitro. remarkably, the site of p53 that is acetylated by its coactivator, p300, resides in a c-terminal domain known to be critical for the regulation of p53 {dna} binding. furthermore, the acetylation of p53 can dramatically stimulate its sequence-specific {dna}-binding activity, possibly as a result of an acetylation-induced conformational change. these observations clearly indicate a novel pathway for p53 activation and, importantly, provide an example of an acetylation-mediated change in the function of a nonhistone regulatory protein. these results have significant implications regarding the molecular mechanisms of various acetyltransferase-containing transcriptional coactivators whose primary targets have been presumed to be histones.\"genetic variegation of clonal architecture and propagating cells in leukaemia.,\"little is known of the genetic architecture of cancer at the subclonal and single-cell level or in the cells responsible for cancer clone maintenance and propagation. here we have examined this issue in childhood acute lymphoblastic leukaemia in which the {etv6}-{runx1} gene fusion is an early or initiating genetic lesion followed by a modest number of recurrent or \\'driver\\' copy number alterations. by multiplexing fluorescence in situ hybridization probes for these mutations, up to eight genetic abnormalities can be detected in single cells, a genetic signature of subclones identified and a composite picture of subclonal architecture and putative ancestral trees assembled. subclones in acute lymphoblastic leukaemia have variegated genetics and complex, nonlinear or branching evolutionary histories. copy number alterations are independently and reiteratively acquired in subclones of individual patients, and in no preferential order. clonal architecture is dynamic and is subject to change in the lead-up to a diagnosis and in relapse. leukaemia propagating cells, assayed by serial transplantation in {nod}/{scid} {il2rγ}(null) mice, are also genetically variegated, mirroring subclonal patterns, and vary in competitive regenerative capacity in vivo. these data have implications for cancer genomics and for the targeted therapy of cancer.\"tel aviv 69978, israel.\",revealing modularity and organization in the yeast molecular network by integrated analysis of highly heterogeneous genomewide data,\"the dissection of complex biological systems is a challenging task, made difficult by the size of the underlying molecular network and the heterogeneous nature of the control mechanisms involved. novel high-throughput techniques are generating massive data sets on various aspects of such systems. here, we perform analysis of a highly diverse collection of genomewide data sets, including gene expression, protein interactions, growth phenotype data, and transcription factor binding, to reveal the modular organization of the yeast system. by integrating experimental data of heterogeneous sources and types, we are able to perform analysis on a much broader scope than previous studies. at the core of our methodology is the ability to identify modules, namely, groups of genes with statistically significant correlated behavior across diverse data sources. numerous biological processes are revealed through these modules, which also obey global hierarchical organization. we use the identified modules to study the yeast transcriptional network and predict the function of >800 uncharacterized genes. our analysis framework, samba ({statistical-algorithmic} method for bicluster analysis), enables the processing of current and future sources of biological information and is readily extendable to experimental techniques and higher organisms.\"\"colorectal cancer ({crc}) is a heterogeneous disease in which unique subtypes are characterized by distinct genetic and epigenetic alterations. here we performed comprehensive genome-scale {dna} methylation profiling of 125 colorectal tumors and 29 adjacent normal tissues. we identified four {dna} methylation–based subgroups of {crc} using model-based cluster analyses. each subtype shows characteristic genetic and clinical features, indicating that they represent biologically distinct subgroups. a {cimp}-high ({cimp}-h) subgroup, which exhibits an exceptionally high frequency of cancer-specific {dna} hypermethylation, is strongly associated with {mlh1} {dna} hypermethylation and the {brafv600e} mutation. a {cimp}-low ({cimp}-l) subgroup is enriched for {kras} mutations and characterized by {dna} hypermethylation of a subset of {cimp}-h-associated markers rather than a unique group of {cpg} islands. {non-cimp} tumors are separated into two distinct clusters. one {non-cimp} subgroup is distinguished by a significantly higher frequency of {tp53} mutations and frequent occurrence in the distal colon, while the tumors that belong to the fourth group exhibit a low frequency of both cancer-specific {dna} hypermethylation and gene mutations and are significantly enriched for rectal tumors. furthermore, we identified 112 genes that were down-regulated more than twofold in {cimp}-h tumors together with promoter {dna} hypermethylation. these represent ∼7\\\\% of genes that acquired promoter {dna} methylation in {cimp}-h tumors. intriguingly, 48/112 genes were also transcriptionally down-regulated in {non-cimp} subgroups, but this was not attributable to promoter {dna} hypermethylation. together, we identified four distinct {dna} methylation subgroups of {crc} and provided novel insight regarding the role of {cimp}-specific {dna} hypermethylation in gene silencing.\"{elife} - open access to the most promising advances in sciencecancer genomics: from discovery science to personalized medicine.,london, nw7 1aa, uk. cedric.notredame@europe.com\",t-coffee: a novel method for fast and accurate multiple sequence alignment,\"we describe a new method ({t-coffee}) for multiple sequence alignment that provides a dramatic improvement in accuracy with a modest sacrifice in speed as compared to the most commonly used alternatives. the method is broadly based on the popular progressive approach to multiple alignment but avoids the most serious pitfalls caused by the greedy nature of this algorithm. with {t-coffee} we pre-process a data set of all pair-wise alignments between the sequences. this provides us with a library of alignment information that can be used to guide the progressive alignment. intermediate alignments are then based not only on the sequences to be aligned next but also on how all of the sequences align with each other. this alignment information can be derived from heterogeneous sources such as a mixture of alignment programs and/or structure superposition. here, we illustrate the power of the approach by using a combination of local and global pair-wise alignments to generate the library. the resulting alignments are significantly more reliable, as determined by comparison with a set of 141 test cases, than any of the popular alternatives that we tried. the improvement, especially clear with the more difficult test cases, is always visible, regardless of the phylogenetic spread of the sequences in the tests.\"kanazawa university, kanazawa 920-0934, japan. titolab@kenroku.kanazawa-u.ac.jp\",a comprehensive two-hybrid analysis to explore the yeast protein interactome,\"protein–protein interactions play crucial roles in the execution of various biological functions. accordingly, their comprehensive description would contribute considerably to the functional interpretation of fully sequenced genomes, which are flooded with novel genes of unpredictable functions. we previously developed a system to examine two-hybrid interactions in all possible combinations between the ≈6,000 proteins of the budding yeast saccharomyces cerevisiae. here we have completed the comprehensive analysis using this system to identify 4,549 two-hybrid interactions among 3,278 proteins. unexpectedly, these data do not largely overlap with those obtained by the other project [uetz, p., et al. (2000) nature (london) 403, 623–627] and hence have substantially expanded our knowledge on the protein interaction space or interactome of the yeast. cumulative connection of these binary interactions generates a single huge network linking the vast majority of the proteins. bioinformatics-aided selection of biologically relevant interactions highlights various intriguing subnetworks. they include, for instance, the one that had successfully foreseen the involvement of a novel protein in spindle pole body function as well as the one that may uncover a hitherto unidentified multiprotein complex potentially participating in the process of vesicular transport. our data would thus significantly expand and improve the protein interaction map for the exploration of genome functions that eventually leads to thorough understanding of the cell as a molecular system.\"\"the ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. it is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. it is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. the ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. the ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.\"\"motivation: in this work, we aim to develop a computational approach for predicting {dna}-binding sites in proteins from amino acid sequences. to avoid over-fitting with this method, all available {dna}-binding proteins from the protein data bank ({pdb}) are used to construct the models. the random forest ({rf}) algorithm is used because it is fast and has robust performance for different parameter values. a novel hybrid feature is presented which incorporates evolutionary information of the amino acid sequence, secondary structure ({ss}) information and orthogonal binary vector ({obv}) information which reflects the characteristics of 20 kinds of amino acids for two physical-chemical properties (dipoles and volumes of the side chains). the numbers of binding and non-binding residues in proteins are highly unbalanced, so a novel scheme is proposed to deal with the problem of imbalanced datasets by downsizing the majority class.  results: the results show that the {rf} model achieves 91.41\\\\% overall accuracy with matthew\\'s correlation coefficient of 0.70 and an area under the receiver operating characteristic curve ({auc}) of 0.913. to our knowledge, the {rf} method using the hybrid feature is currently the computationally optimal approach for predicting {dna}-binding sites in proteins from amino acid sequences without using three-dimensional structural information. we have demonstrated that the prediction results are useful for understanding {protein-dna} interactions.  availability: {dbindr} web-server implementation is freely available at {http://www.cbi.seu.edu.cn/dbindr}/{dbindr}.htm.  contact: xsun@seu.edu.cn 10.1093/bioinformatics/btn583\"university college london, uk.\",{cath}--a hierarchic classification of protein domain structures.,\"protein evolution gives rise to families of structurally related proteins, within which sequence identities can be extremely low. as a result, structure-based classifications can be effective at identifying unanticipated relationships in known structures and in optimal cases function can also be assigned. the ever increasing number of known protein structures is too large to classify all proteins manually, therefore, automatic methods are needed for fast evaluation of protein structures. we present a semi-automatic procedure for deriving a novel hierarchical classification of protein domain structures ({cath}). the four main levels of our classification are protein class (c), architecture (a), topology (t) and homologous superfamily (h). class is the simplest level, and it essentially describes the secondary structure composition of each domain. in contrast, architecture summarises the shape revealed by the orientations of the secondary structure units, such as barrels and sandwiches. at the topology level, sequential connectivity is considered, such that members of the same architecture might have quite different topologies. when structures belonging to the same t-level have suitably high similarities combined with similar functions, the proteins are assumed to be evolutionarily related and put into the same homologous superfamily. analysis of the structural families generated by {cath} reveals the prominent features of protein structure space. we find that nearly a third of the homologous superfamilies (h-levels) belong to ten major t-levels, which we call superfolds, and furthermore that nearly two-thirds of these h-levels cluster into nine simple architectures. a database of well-characterised protein structure families, such as {cath}, will facilitate the assignment of structure-function/evolution relationships to both known and newly determined protein structures.\"national institutes of health, bethesda, md 20894.\",basic local alignment search tool.,\"a new approach to rapid sequence comparison, basic local alignment search tool ({blast}), directly approximates alignments that optimize a measure of local similarity, the maximal segment pair ({msp}) score. recent mathematical results on the stochastic properties of {msp} scores allow an analysis of the performance of this method as well as the statistical significance of alignments it generates. the basic algorithm is simple and robust; it can be implemented in a number of ways and applied in a variety of contexts including straightforward {dna} and protein sequence database searches, motif searches, gene identification searches, and in the analysis of multiple regions of similarity in long {dna} sequences. in addition to its flexibility and tractability to mathematical analysis, {blast} is an order of magnitude faster than existing sequence comparison tools of comparable sensitivity.\"rockville, maryland 20850, usa. johnq@tigr.org\",microarray data normalization and transformation,\"over the past decade, comprehensive sequencing efforts have revealed the genomic landscapes of common forms of human cancer. for most cancer types, this landscape consists of a small number of  ” mountains” (genes altered in a high percentage of tumors) and a much larger number of  ” hills” (genes altered infrequently). to date, these studies have revealed \\\\~{}140 genes that, when altered by intragenic mutations, can promote or  ” drive” tumorigenesis. a typical tumor contains two to eight of these  ” driver gene” mutations; the remaining mutations are passengers that confer no selective growth advantage. driver genes can be classified into 12 signaling pathways that regulate three core cellular processes: cell fate, cell survival, and genome maintenance. a better understanding of these pathways is one of the most pressing needs in basic cancer research. even now, however, our knowledge of cancer genomes is sufficient to guide the development of more effective approaches for reducing cancer morbidity and mortality.\"\"neoplasms are microcosms of evolution. within a neoplasm, a mosaic of mutant cells compete for space and resources, evade predation by the immune system and can even cooperate to disperse and colonize new organs. the evolution of neoplastic cells explains both why we get cancer and why it has been so difficult to cure. the tools of evolutionary biology and ecology are providing new insights into neoplastic progression and the clinical control of cancer.\"michael smith building, oxford road, manchester, m13 9pt, united kingdom.\",specificity in protein interactions and its relationship with sequence diversity and coevolution.,\"studies of interacting proteins have found correlated evolution of the sequences of binding partners, apparently as a result of compensating mutations to maintain specificity (i.e., molecular coevolution). here, we analyze the coevolution of interacting proteins in yeast and demonstrate correlated evolution of binding partners in eukaryotes. detailed investigation of this apparent coevolution, focusing on the proteins\\' surface and binding interface, surprisingly leads to no improvement in the correlation. we conclude that true coevolution, as characterized by compensatory mutations between binding partners, is unlikely to be chiefly responsible for the apparent correlated evolution. we postulate that the correlation between sequence alignments is simply due to interacting proteins being subject to similar constraints on their evolutionary rate. because gene expression has a strong influence on evolutionary rate, and interacting proteins will tend to have similar levels of expression, we investigated this particular constraint. we found that the absolute expression level outperformed correlated evolution for predicting interacting protein partners. a correlation between sequence alignments could also be identified not only between pairs of proteins that physically interact but also between those that are merely functionally related (i.e., within the same protein complex). this indicates that the observed correlated evolution of interacting proteins is due to similar constraints on evolutionary rate and not coevolution.\"center for computational biology and bioinformatics and college of engineering, rumelifeneri yolu, 34450 sariyer istanbul, turkey.\",similar binding sites and different partners: implications to shared proteins in cellular pathways.,transcripts and proteins\",\"{ncbi}\\'s reference sequence ({refseq}) database ({http://www.ncbi.nlm.nih.gov/refseq}/) is a curated non-redundant collection of sequences representing genomes, transcripts and proteins. the database includes 3774 organisms spanning prokaryotes, eukaryotes and viruses, and has records for 2 879 860 proteins ({refseq} release 19). {refseq} records integrate information from multiple sources, when additional data are available from those sources and therefore represent a current description of the sequence and its features. annotations include coding regions, conserved domains, {trnas}, sequence tagged sites ({sts}), variation, references, gene and protein product names, and database cross-references. sequence is reviewed and features are added using a combined approach of collaboration and other input from the scientific community, prediction, propagation from {genbank} and curation by {ncbi} staff. the format of all {refseq} records is validated, and an increasing number of tests are being applied to evaluate the quality of sequence and annotation, especially in the context of complete genomic sequence.\"saic frederick, nci-fcrdc, md 21702-1201, usa.\",hydrogen bonds and salt bridges across protein-protein interfaces.,\"to understand further, and to utilize, the interactions across protein-protein interfaces, we carried out an analysis of the hydrogen bonds and of the salt bridges in a collection of 319 non-redundant protein-protein interfaces derived from high-quality x-ray structures. we found that the geometry of the hydrogen bonds across protein interfaces is generally less optimal and has a wider distribution than typically observed within the chains. this difference originates from the more hydrophilic side chains buried in the binding interface than in the folded monomer interior. protein folding differs from protein binding. whereas in folding practically all degrees of freedom are available to the chain to attain its optimal configuration, this is not the case for rigid binding, where the protein molecules are already folded, with only six degrees of translational and rotational freedom available to the chains to achieve their most favorable bound configuration. these constraints enforce many polar/charged residues buried in the interface to form weak hydrogen bonds with protein atoms, rather than strongly hydrogen bonding to the solvent. since interfacial hydrogen bonds are weaker than the intra-chain ones to compete with the binding of water, more water molecules are involved in bridging hydrogen bond networks across the protein interface than in the protein interior. interfacial water molecules both mediate non-complementary donor-donor or acceptor-acceptor pairs, and connect non-optimally oriented donor-acceptor pairs. these differences between the interfacial hydrogen bonding patterns and the intra-chain ones further substantiate the notion that protein complexes formed by rigid binding may be far away from the global minimum conformations. moreover, we summarize the pattern of charge complementarity and of the conservation of hydrogen bond network across binding interfaces. we further illustrate the utility of this study in understanding the specificity of protein-protein associations, and hence in docking prediction and molecular (inhibitor) design.\"\"bioinformatics program, boston university, boston, massachusetts 02215; department of cell biology, harvard medical school, boston, massachusetts 02115; yeast structural genomics, ibbmc universit\\\\\\'{e} paris-sud, cnrs umr 8619, 91405-orsay, france; department of biomedical engineering, boston university, boston, massachusetts 02215; program in bioinformatics and integrative biology, university of massachusetts medical school, worcester, massachusetts 01605\",protein–protein docking benchmark version 3.0,\"we present version 3.0 of our publicly available protein–protein docking benchmark. this update includes 40 new test cases, representing a 48\\\\% increase from benchmark 2.0. for all of the new cases, the crystal structures of both binding partners are available. as with benchmark 2.0, structural classification of proteins (murzin et al., j mol biol 1995;247:536–540) was used to remove redundant test cases. the 124 unbound-unbound test cases in benchmark 3.0 are classified into 88 rigid-body cases, 19 medium-difficulty cases, and 17 difficult cases, based on the degree of conformational change at the interface upon complex formation. in addition to providing the community with more test cases for evaluating docking methods, the expansion of benchmark 3.0 will facilitate the development of new algorithms that require a large number of training examples. benchmark 3.0 is available to the public at http://zlab.bu.edu/benchmark. proteins 2008. {\\\\copyright} 2008 {wiley-liss}, inc.\"\"multiple sequence alignments are often used to reveal functionally important residues within a protein family. they can be particularly useful for the identification of key residues that determine functional differences between protein subfamilies. we present a new entropy-based method, sequence harmony ({sh}) that accurately detects subfamily-specific positions from a multiple sequence alignment. the {sh} algorithm implements a novel formula, able to score compositional differences between subfamilies, without imposing conservation, in a simple manner on an intuitive scale. we compare our method with the most important published methods, i.e. {amas}, {treedet} and {sdp}-pred, using three well-studied protein families: the receptor-binding domain ({mh2}) of the smad family of transcription factors, the ras-superfamily of small {gtpases} and the {mip}-family of integral membrane transporters. we demonstrate that {sh} accurately selects known functional sites with higher coverage than the other methods for these test-cases. this shows that compositional differences between protein subfamilies provide sufficient basis for identification of functional sites. in addition, {sh} selects a number of sites of unknown function that could be interesting candidates for further experimental investigation.\"\"why do proteins evolve at different rates? advances in systems biology and genomics have facilitated a move from studying individual proteins to characterizing global cellular factors. systematic surveys indicate that protein evolution is not determined exclusively by selection on protein structure and function, but is also affected by the genomic position of the encoding genes, their expression patterns, their position in biological networks and possibly their robustness to mistranslation. recent work has allowed insights into the relative importance of these factors. we discuss the status of a much-needed coherent view that integrates studies on protein evolution with biochemistry and functional and structural genomics.\"\"bayesian statistics allow scientists to easily incorporate prior knowledge into their data analysis. nonetheless, the sheer amount of computational power that is required for bayesian statistical analyses has previously limited their use in genetics. these computational constraints have now largely been overcome and the underlying advantages of bayesian approaches are putting them at the forefront of genetic data analysis in an increasing number of areas.\"kyoto university, uji, kyoto 611-0011, japan. kanehisa@kuicr.kyoto-u.ac.jp\",the {kegg} resource for deciphering the genome,\"a grand challenge in the post‐genomic era is a complete computer representation of the cell and the organism, which will enable computational prediction of higher‐level complexity of cellular processes and organism behavior from genomic information. toward this end we have been developing a knowledge‐based approach for network prediction, which is to predict, given a complete set of genes in the genome, the protein interaction networks that are responsible for various cellular processes. {kegg} at http://www.genome.ad.jp/kegg/ is the reference knowledge base that integrates current knowledge on molecular interaction networks such as pathways and complexes ({pathway} database), information about genes and proteins generated by genome projects ({genes}/{ssdb}/{ko} databases) and information about biochemical compounds and reactions ({compound}/{glycan}/{reaction} databases). these three types of database actually represent three graph objects, called the protein network, the gene universe and the chemical universe. new efforts are being made to abstract knowledge, both computationally and manually, about ortholog clusters in the {ko} ({kegg} orthology) database, and to collect and analyze carbohydrate structures in the {glycan} database.\"model-based clustering of array {cgh} data.,the ridgeway, mill hill, london nw7 1aa, uk. ffranca@nimr.mrc.ac.uk\",parameter optimized surfaces ({pops}): analysis of key interactions and conformational changes in the ribosome.,\"we present a new method for the calculation of solvent accessible surface areas at the atomic and residue levels, which we call parameter optimized surfaces ({pops}-a and {pops}-r ). atomic and residue areas (the latter simulated with a single sphere centered at the c(alpha)s atom for amino acids and at the p atom for nucleotides) have been optimized versus accurate all-atoms methods. we concentrated on an analytical formula for the approximation of solvent accessibilities. the formula is simple, easily derivable and fast to compute, therefore it is practical for use in molecular dynamics simulations as an approximation to the first solvation shell. the residue based approach {pops}-r has been derived as a useful tool for the analysis of large macromolecular assemblies like the ribosome, and is especially suited for use in refinement of low resolution structures. the structures of the {70s}, {50s} and {30s} ribosomes have been analyzed in detail and most of the interactions within the subunits and at their interfaces were clearly identified. some interesting differences between {30s} alone and within the {70s} have been highlighted. owing to the presence of the {p-trna} in the {70s} ribosome, localized conformational rearrangements occur within the subunits, exposing arg and lys residues to negatively charged binding sites of {p-trna}. {pops}-r also allows for estimates of the loss of free energy of solvation upon complex formation, particularly useful in designing new {protein-rna} complexes and in suggesting more focused experimental work.\"\"the diverse range of cellular functions is performed by a limited number of protein folds existing in nature. one may similarly expect that cellular functional diversity would be covered by a limited number of protein-protein interface architectures. here, we present 8205 interface clusters, each representing a unique interface architecture. this data set of protein-protein interfaces is analyzed and compared with older data sets. we observe that the number of both biological and crystal interfaces increases significantly compared to the number of protein data bank entries. furthermore, we find that the number of distinct interface architectures grows at a much faster rate than the number of folds and is yet to level off. we further analyze the growth trend of the functional coverage by constructing functional interaction networks from interfaces. the functional coverage is also found to steadily increase. interestingly, we also observe that despite the diversity of interface architectures, some are more favorable and frequently used, and of particular interest, are the ones that are also preferred in single chains.\"\"high-throughput genotyping microarrays assess both total {dna} copy number and allelic composition, which makes them a tool of choice for copy number studies in cancer, including total copy number and loss of heterozygosity ({loh}) analyses. even after state of the art preprocessing methods, allelic signal estimates from genotyping arrays still suffer from systematic effects that make them difficult to use effectively for such downstream analyses. we propose a method, {tumorboost}, for normalizing allelic estimates of one tumor sample based on estimates from a single matched normal. the method applies to any paired tumor-normal estimates from any microarray-based technology, combined with any preprocessing method. we demonstrate that it increases the signal-to-noise ratio of allelic signals, making it significantly easier to detect allelic imbalances. {tumorboost} increases the power to detect somatic copy-number events (including copy-neutral {loh}) in the tumor from allelic signals of affymetrix or illumina origin. we also conclude that high-precision allelic estimates can be obtained from a single pair of tumor-normal hybridizations, if {tumorboost} is combined with single-array preprocessing methods such as (allele-specific) {crma} v2 for affymetrix or {beadstudio}\\'s (proprietary) {xy}-normalization method for illumina. a bounded-memory implementation is available in the open-source and cross-platform r package aroma.cn, which is part of the aroma project (http://www.aroma-project.org/).\"\"the kyoto encyclopedia of genes and genomes ({kegg}) is the primary database resource of the japanese {genomenet} service (http://www.genome.ad.jp/) for understanding higher order functional meanings and utilities of the cell or the organism from its genome information. {kegg} consists of the {pathway} database for the computerized knowledge on molecular interaction networks such as pathways and complexes, the {genes} database for the information about genes and proteins generated by genome sequencing projects, and the {ligand} database for the information about chemical compounds and chemical reactions that are relevant to cellular processes. in addition to these three main databases, limited amounts of experimental data for microarray gene expression profiles and yeast two-hybrid systems are stored in the {expression} and {brite} databases, respectively. furthermore, a new database, named {ssdb}, is available for exploring the universe of all protein coding genes in the complete genomes and for identifying functional links and ortholog groups. the data objects in the {kegg} databases are all represented as graphs and various computational methods are developed to detect graph features that can be related to biological functions. for example, the correlated clusters are graph similarities which can be used to predict a set of genes coding for a pathway or a complex, as summarized in the ortholog group tables, and the cliques in the {ssdb} graph are used to annotate genes. the {kegg} databases are updated daily and made freely available (http://www.genome.ad.jp/kegg/).\"single-cell dissection of transcriptional heterogeneity in human colon tumors,\"the complementarity of gene expression and {protein-dna} interaction data led to several successful models of biological systems. however, recent studies in multiple species raise doubts about the relationship between these two datasets. these studies show that the overwhelming majority of genes bound by a particular transcription factor ({tf}) are not affected when that factor is knocked out. here, we show that this surprising result can be partially explained by considering the broader cellular context in which {tfs} operate. factors whose functions are not backed up by redundant paralogs show a fourfold increase in the agreement between their bound targets and the expression levels of those targets. in addition, we show that incorporating protein interaction networks provides physical explanations for knockout effects. new double knockout experiments support our conclusions. our results highlight the robustness provided by redundant {tfs} and indicate that in the context of diverse cellular systems, binding is still largely functional.\"p.o. box 19498, arlington, tx 76019, usa. michalak@uta.edu\",\"coexpression, coregulation, and cofunctionality of neighboring genes in eukaryotic genomes\",\"accumulating evidence indicates that gene order in eukaryotic genomes is not completely random and that genes with similar expression levels tend to be clustered within the same genomic neighborhoods. the mechanism behind these gene coexpression clusters is as yet unclear. in this article, plausible biochemical, genetic, evolutionary, and technological determinants of this pattern are briefly reviewed.\"\"all cancers carry somatic mutations. the patterns of mutation in cancer genomes reflect the {dna} damage and repair processes to which cancer cells and their precursors have been exposed. to explore these mechanisms further, we generated catalogs of somatic mutation from 21 breast cancers and applied mathematical methods to extract mutational signatures of the underlying processes. multiple distinct single- and double-nucleotide substitution signatures were discernible. cancers with {brca1} or {brca2} mutations exhibited a characteristic combination of substitution mutation signatures and a distinctive profile of deletions. complex relationships between somatic mutation prevalence and transcription were detected. a remarkable phenomenon of localized hypermutation, termed \"\"kataegis,\"\" was observed. regions of kataegis differed between cancers but usually colocalized with somatic rearrangements. base substitutions in these regions were almost exclusively of cytosine at {tpc} dinucleotides. the mechanisms underlying most of these mutational signatures are unknown. however, a role for the {apobec} family of cytidine deaminases is proposed. copyright {\\\\copyright} 2012 elsevier inc. all rights reserved.\"national institutes of health bethesda, md 20894, usa.\",predicting protein–protein interaction by searching evolutionary tree automorphism space,\"motivation: uncovering the protein–protein interaction network is a fundamental step in the quest to understand the molecular machinery of a cell. this motivates the search for efficient computational methods for predicting such interactions. among the available predictors are those that are based on the co-evolution hypothesis  ” evolutionary trees of protein families (that are known to interact) are expected to have similar topologies”. many of these methods are limited by the fact that they can handle only a small number of protein sequences. also, details on evolutionary tree topology are missing as they use similarity matrices in lieu of the {trees.results}: we introduce {morph}, a new algorithm for predicting protein interaction partners between members of two protein families that are known to interact. our approach can also be seen as a new method for searching the best superposition of the corresponding evolutionary trees based on tree automorphism group. we discuss relevant facts related to the predictability of protein–protein interaction based on their co-evolution. when compared with related computational approaches, our method reduces the search space by ∼3 × 105-fold and at the same time increases the accuracy of predicting correct binding {partners.contact}: przytyck@mail.nih.gov\"10550 north torrey pines road, la jolla, ca 92037, usa.\",comparative study of several algorithms for flexible ligand docking.,\"we have performed a comparative assessment of several programs for flexible molecular docking: {dock} 4.0, {flexx} 1.8, {autodock} 3.0, {gold} 1.2 and {icm} 2.8. this was accomplished using two different studies: docking experiments on a data set of 37 protein-ligand complexes and screening a library containing 10,037 entries against 11 different proteins. the docking accuracy of the methods was judged based on the corresponding rank-one solutions. we have found that the fraction of molecules docked with acceptable accuracy is 0.47, 0.31, 0.35, 0.52 and 0.93 for, respectively, {autodock}, {dock}, {flexx}, {gold} and {icm}. thus {icm} provided the highest accuracy in ligand docking against these receptors. the results from the other programs are found to be less accurate and of approximately the same quality. a speed comparison demonstrated that {flexx} was the fastest and {autodock} was the slowest among the tested docking programs. the database screening was performed using {dock}, {flexx} and {icm}. {icm} was able to identify the original ligands within the top 1\\\\% of the total library in 17 cases. the corresponding number for {dock} and {flexx} was 7 and 8, respectively. we have estimated that in virtual database screening, 50\\\\% of the potentially active compounds will be found among approximately 1.5\\\\% of the top scoring solutions found with {icm} and among approximately 9\\\\% of the top scoring solutions produced by {dock} and {flexx}.\"los angeles, ca, 90095-1570, usa.\",three-dimensional cluster analysis identifies interfaces and functional residue clusters in proteins.,national centre for biotechnology (cnb-csic), madrid, spain.\",prediction of protein interaction based on similarity of phylogenetic trees.,\"computational methods for predicting protein interaction partners are becoming increasingly popular. many of them are mature enough to be widely used by molecular biologists who can look for proteins related to the protein of interest in order to infer information about its context in the cell. in this chapter we describe the use of the mirrortree set of programs and related software for predicting protein interactions. they are all based on the idea that interacting or functionally related proteins tend to show similar phylogenetic trees due to coevolution. the basic mirrortree program can be used to calculate the similarity between the phylogenetic trees implicit in the multiple sequence alignments of two protein families. the {ecid} database contains protein interactions and relationships from different computational and experimental sources for the model organism escherichia coli, including the ones generated with mirrortree. finally, the {tsema} server uses the concept of tree similarity between interacting families to look for the best mapping between two families of interacting proteins: which member in one family interacts with which member in the other.\"inc., laboratory of experimental and computational biology, national cancer institute, frederick, md 21702, usa.\",protein–protein interactions: structurally conserved residues distinguish between binding sites and exposed protein surfaces,\"polar residue hot spots have been observed at protein–protein binding sites. here we show that hot spots occur predominantly at the interfaces of macromolecular complexes, distinguishing binding sites from the remainder of the surface. consequently, hot spots can be used to define binding epitopes. we further show a correspondence between energy hot spots and structurally conserved residues. the number of structurally conserved residues, particularly of high ranking energy hot spots, increases with the binding site contact size. this finding may suggest that effectively dispersing hot spots within a large contact area, rather than compactly clustering them, may be a strategy to sustain essential key interactions while still allowing certain protein flexibility at the interface. thus, most conserved polar residues at the binding interfaces confer rigidity to minimize the entropic cost on binding, whereas surrounding residues form a flexible cushion. furthermore, our finding that similar residue hot spots occur across different protein families suggests that affinity and specificity are not necessarily coupled: higher affinity does not directly imply greater specificity. conservation of trp on the protein surface indicates a highly likely binding site. to a lesser extent, conservation of phe and met also imply a binding site. for all three residues, there is a significant conservation in binding sites, whereas there is no conservation on the exposed surface. a hybrid strategy, mapping sequence alignment onto a single structure illustrates the possibility of binding site identification around these three residues.\"\"motivated by genetic expression data, we introduce plaid models.  these are a form of two-sided cluster analysis that allows clusters to  overlap. using these models we nd interpretable structure in some  yeast {dna} data, as well as in some nutrition data and some foreign  exchange data.  key words: clustering, {dna} microarray, information retrieval  1 introduction  this article introduces the plaid model, a tool for exploratory analysis of multivariate data. the motivating application is the search for interpretable biological structure in gene expression microarray data. eisen, spellman, brown \\\\&amp; botstein (1998) is an early and inuential paper advocating the use of cluster methods to identify groups of co-regulated genes from microarray data. we present the model and illustrate it on gene expression and other data. the plaid model allows a gene to be in more than one cluster, or in none at all. it also allows a cluster of genes to be dened with respect to only a subset of samples, no...\"\"the {sv40} transcriptional enhancer is composed of separate 15 to 20 base-pair-(bp)-long enhancer elements that cooperate with one another or duplicates of themselves to enhance transcription. these elements are bipartite, being composed of subunits, called enhansons, that can be duplicated or interchanged to create new enhancer elements. enhansons differ from the enhancer elements because they are very sensitive to changes in spacing. this prototypic enhancer, therefore, contains two distinct levels of organization, each of which requires redundancy to be effective.\"mutational heterogeneity in cancer and the search for new cancer-associated genes,\"major international projects are underway that are aimed at creating a comprehensive catalogue of all the genes responsible for the initiation and progression of cancer. these studies involve the sequencing of matched tumour-normal samples followed by mathematical analysis to identify those genes in which mutations occur more frequently than expected by random chance. here we describe a fundamental problem with cancer genome studies: as the sample size increases, the list of putatively significant genes produced by current analytical methods burgeons into the hundreds. the list includes many implausible genes (such as those encoding olfactory receptors and the muscle protein titin), suggesting extensive false-positive findings that overshadow true driver events. we show that this problem stems largely from mutational heterogeneity and provide a novel analytical methodology, {mutsigcv}, for resolving the problem. we apply {mutsigcv} to exome sequences from 3,083 tumour-normal pairs and discover extraordinary variation in mutation frequency and spectrum within cancer types, which sheds light on mutational processes and disease aetiology, and in mutation frequency across the genome, which is strongly correlated with {dna} replication timing and also with transcriptional activity. by incorporating mutational heterogeneity into the analyses, {mutsigcv} is able to eliminate most of the apparent artefactual findings and enable the identification of genes truly associated with cancer.\"\"tumorigenesis in humans is thought to be a multistep process where certain mutations confer a selective advantage, allowing lineages derived from the mutated cell to outcompete other cells. although molecular cell biology has substantially advanced cancer research, our understanding of the evolutionary dynamics that govern tumorigenesis is limited. this paper analyzes the computational implications of cancer progression presented by hanahan and weinberg in the hallmarks of cancer. we model the complexities of tumor progression as a small set of underlying rules that govern the transformation of normal cells to tumor cells. the rules are implemented in a stochastic multistep model. the model predicts that (i) early-onset cancers proceed through a different sequence of mutation acquisition than late-onset cancers; (ii) tumor heterogeneity varies with acquisition of genetic instability, mutation pathway, and selective pressures during tumorigenesis; (iii) there exists an optimal initial telomere length which lowers cancer incidence and raises time of cancer onset; and (iv) the ability to initiate angiogenesis is an important stage-setting mutation, which is often exploited by other cells. the model offers insight into how the sequence of acquired mutations affects the timing and cellular makeup of the resulting tumor and how the cellular-level population dynamics drive neoplastic evolution. cancer can be viewed as an ecological system in which cells with different mutations compete for survival. in this work, the authors present a three-dimensional stochastic model of these complex interactions. each cell is represented as an autonomous agent that follows simple rules governing its behavior, where behaviors change as cells gain cancerous mutations. the paper explores the timing of cancer onset, the order in which mutations are acquired, the diversity of tumors, and the competition and cooperation between cells in the tumor microenvironment. one key finding is that early-onset and late-onset tumors take different mutational paths to cancer. the paper provides insight into the early dynamics of tumorigenesis currently inaccessible to experimental investigation.\"modelling vemurafenib resistance in melanoma reveals a strategy to forestall drug resistance.,hinxton, cambridge, united kingdom.\",pfam: a comprehensive database of protein domain families based on seed alignments.,\"databases of multiple sequence alignments are a valuable aid to protein sequence classification and analysis. one of the main challenges when constructing such a database is to simultaneously satisfy the conflicting demands of completeness on the one hand and quality of alignment and domain definitions on the other. the latter properties are best dealt with by manual approaches, whereas completeness in practice is only amenable to automatic methods. herein we present a database based on hidden markov model profiles ({hmms}), which combines high quality and completeness. our database, pfam, consists of parts a and b. {pfam-a} is curated and contains well-characterized protein domain families with high quality alignments, which are maintained by using manually checked seed alignments and {hmms} to find and align all members. {pfam-b} contains sequence families that were generated automatically by applying the domainer algorithm to cluster and align the remaining protein sequences after removal of {pfam-a} domains. by using pfam, a large number of previously unannotated proteins from the caenorhabditis elegans genome project were classified. we have also identified many novel family memberships in known proteins, including new kazal, fibronectin type {iii}, and response regulator receiver domains. {pfam-a} families have permanent accession numbers and form a library of {hmms} available for searching and automatic annotation of new protein sequences.\"interaction-site prediction for protein complexes: a critical assessment.,\"progress in uncovering the protein interaction networks of several species has led to questions of what underlying principles might govern their organization. few studies have tried to determine the impact of protein interaction network evolution on the observed physiological differences between species. using comparative genomics and structural information, we show here that eukaryotic species have rewired their interactomes at a fast rate of approximately 10 \\\\^{a}ˆ\\'5 interactions changed per protein pair, per million years of divergence. for homo sapiens this corresponds to 10 3 interactions changed per million years. additionally we find that the specificity of binding strongly determines the interaction turnover and that different biological processes show significantly different link dynamics. in particular, human proteins involved in immune response, transport, and establishment of localization show signs of positive selection for change of interactions. our analysis suggests that a small degree of molecular divergence can give rise to important changes at the network level. we propose that the power law distribution observed in protein interaction networks could be partly explained by the cell\\'s requirement for different degrees of protein binding specificity.\"\"center for advanced research in biotechnology, university of maryland biotechnology institute, rockville, maryland 20850; genome center, university of california, davis, california 95616; department of biochemistry and mol biophysics, cubic, c2b2, nesg, columbia university, new york, new york 10032; istituto pasteur-fondazione cenci bolognetti, university of rome ldquola sapienzardquo, p.le aldo moro 5, rome 00185, italy\",critical assessment of methods of protein structure {prediction—round} {viii},\"abstract this article is an introduction to the special issue of the journal proteins, dedicated to the eighth {casp} experiment to assess the state of the art in protein structure prediction. the article describes the conduct of the experiment, the categories of prediction included, and outlines the evaluation and assessment procedures. highlights are the first blind assessment of model refinement methods showing that under some circumstances substantial model improvements are possible; improvements in the performance of methods for determining the accuracy of a model; and some progress in the accuracy of comparative models in regions not present in a principal template. against these advances must be stacked the fact that there is no detectable progress in model quality compared with {casp7} in either template-based or template free modeling, using the established {casp} measures. proteins 2009. {\\\\copyright} 2009 {wiley-liss}, inc.\"\"main text {introductionwe} have proposed that six hallmarks of cancer together constitute an organizing principle that provides a logical framework for understanding the remarkable diversity of neoplastic diseases (hanahan and weinberg, 2000). implicit in our discussion was the notion that as normal cells evolve progressively to a neoplastic state, they acquire a succession of these hallmark capabilities, and that the multistep process of human tumor pathogenesis could be rationalized by the need of incipient cancer cells to acquire the traits that enable them to become tumorigenic and ultimately malignant.\"\"bioinformatics program, boston university, boston, massachusetts 02215, usa.\",atomic contact vectors in protein-protein recognition,\"the ability to analyze and compare protein-protein interactions on the structural level is critical to our understanding of various aspects of molecular recognition and the functional interplay of components of biochemical networks. in this study, we introduce atomic contact vectors ({acvs}) as an intuitive way to represent the physico-chemical characteristics of a protein-protein interface as well as a way to compare interfaces to each other. we test the utility of {acvs} in classification by using them to distinguish between homodimers and crystal contacts. our results compare favorably with those reported by other authors. we then apply {acvs} to mine the {pdb} for all known protein-protein complexes and separate transient recognition complexes from permanent oligomeric ones. getting at the basis of this difference is important for our understanding of recognition and we achieved a success rate of 91\\\\% for distinguishing these two classes of complexes. although accessible surface area of the interface is a major discriminating feature, we also show that there are distinct differences in the contact preferences between the two kinds of complexes. illustrating the superiority of {acvs} as a basic comparison measure over a sequence-based approach, we derive a general rule of thumb to determine whether two protein-protein interfaces are redundant. with this method, we arrive at a nonredundant set of 209 recognition complexes—the largest set reported so far. proteins 2003. {\\\\copyright} 2003 {wiley-liss}, inc.\"\"protein-protein interaction networks provide a global picture of cellular function and biological processes. some proteins act as hub proteins, highly connected to others, whereas some others have few interactions. the dysfunction of some interactions causes many diseases, including cancer. proteins interact through their interfaces. therefore, studying the interface properties of cancer-related proteins will help explain their role in the interaction networks. similar or overlapping binding sites should be used repeatedly in single interface hub proteins, making them promiscuous. alternatively, multi-interface hub proteins make use of several distinct binding sites to bind to different partners. we propose a methodology to integrate protein interfaces into cancer interaction networks ({cispin}, cancer structural protein interface network). the interactions in the human protein interaction network are replaced by interfaces, coming from either known or predicted complexes. we provide a detailed analysis of cancer related human protein-protein interfaces and the topological properties of the cancer network. the results reveal that cancer-related proteins have smaller, more planar, more charged and less hydrophobic binding sites than non-cancer proteins, which may indicate low affinity and high specificity of the cancer-related interactions. we also classified the genes in {cispin} according to phenotypes. within phenotypes, for breast cancer, colorectal cancer and leukemia, interface properties were found to be discriminating from non-cancer interfaces with an accuracy of 71\\\\%, 67\\\\%, 61\\\\%, respectively. in addition, cancer-related proteins tend to interact with their partners through distinct interfaces, corresponding mostly to multi-interface hubs, which comprise 56\\\\% of cancer-related proteins, and constituting the nodes with higher essentiality in the network (76\\\\%). we illustrate the interface related affinity properties of two cancer-related hub proteins: erbb3, a multi interface, and raf1, a single interface hub. the results reveal that affinity of interactions of the multi-interface hub tends to be higher than that of the single-interface hub. these findings might be important in obtaining new targets in cancer as well as finding the details of specific binding regions of putative cancer drug candidates.\"d-38304 wolfenb\\\\\"\"{u}ttel, germany. vma@biobase.de\",\"{transfac}: transcriptional regulation, from patterns to profiles.\",an integrated genomic analysis of human glioblastoma multiforme.,\"glioblastoma multiforme ({gbm}) is the most common and lethal type of brain cancer. to identify the genetic alterations in {gbms}, we sequenced 20,661 protein coding genes, determined the presence of amplifications and deletions using high-density oligonucleotide arrays, and performed gene expression analyses using next-generation sequencing technologies in 22 human tumor samples. this comprehensive analysis led to the discovery of a variety of genes that were not known to be altered in {gbms}. most notably, we found recurrent mutations in the active site of isocitrate dehydrogenase 1 ({idh1}) in 12\\\\% of {gbm} patients. mutations in {idh1} occurred in a large fraction of young patients and in most patients with secondary {gbms} and were associated with an increase in overall survival. these studies demonstrate the value of unbiased genomic analyses in the characterization of human brain cancer and identify a potentially useful genetic alteration for the classification and targeted therapy of {gbms}.\"\"summary: phangorn is a package for phylogenetic reconstruction and analysis in the r language. previously it was only possible to estimate phylogenetic trees with distance methods in r. phangorn, now offers the possibility of reconstructing phylogenies with distance based methods, maximum parsimony or maximum likelihood ({ml}) and performing hadamard conjugation. extending the general {ml} framework, this package provides the possibility of estimating mixture and partition models. furthermore, phangorn offers several functions for comparing trees, phylogenetic models or splits, simulating character data and performing congruence analyses.\"\"boston university bioinformatics program, boston, massachusetts 02215, usa.\",protein–protein docking benchmark 2.0: an update,\"we present a new version of the {protein–protein} docking benchmark, reconstructed from the bottom up to include more complexes, particularly focusing on more unbound–unbound test cases. {scop} (structural classification of proteins) was used to assess redundancy between the complexes in this version. the new benchmark consists of 72 unbound–unbound cases, with 52 rigid-body cases, 13 medium-difficulty cases, and 7 high-difficulty cases with substantial conformational change. in addition, we retained 12 antibody–antigen test cases with the antibody structure in the bound form. the new benchmark provides a platform for evaluating the progress of docking methods on a wide variety of targets. the new version of the benchmark is available to the public at http://zlab.bu.edu/benchmark2. proteins 2005;60:214–216. {\\\\copyright} 2005 {wiley-liss}, inc.\"\"{background}:several entropy-based methods have been developed for scoring sequence conservation in protein multiple sequence alignments. high scoring amino acid positions may correlate with structurally or functionally important residues. however, amino acid background frequencies are usually not taken into account in these entropy-based scoring {schemes.results}:we demonstrate that using a relative entropy measure that incorporates amino acid background frequency results in improved performance in identifying functional sites from protein multiple sequence {alignments.conclusion}:our results suggest that the application of appropriate background frequency information may lead to more biologically relevant results in many areas of bioinformatics.\"\"larger organisms have more potentially carcinogenic cells, tend to live longer and require more ontogenic cell divisions. therefore, intuitively one might expect cancer incidence to scale with body size. evidence from mammals, however, suggests that the cancer risk does not correlate with body size. this observation defines  ” peto\\'s paradox.” here, we propose a novel hypothesis to resolve peto\\'s paradox. we suggest that malignant tumors are disadvantaged in larger hosts. in particular, we hypothesize that natural selection acting on competing phenotypes among the cancer cell population will tend to favor aggressive  ” cheaters” that then grow as a tumor on their parent tumor, creating a hypertumor that damages or destroys the original neoplasm. in larger organisms, tumors need more time to reach lethal size, so hypertumors have more time to evolve. so, in large organisms, cancer may be more common and less lethal. we illustrate this hypothesis in silico using a previously published hypertumor model. results from the model predict that malignant neoplasms in larger organisms should be disproportionately necrotic, aggressive, and vascularized than deadly tumors in small mammals. these predictions may serve as the basis on which to test the hypothesis, but to our knowledge, no one has yet performed a systematic investigation of comparative necrosis, histopathology, or vascularization among mammalian cancers.\"\"our results suggest that expression profiles can be used in the computational identification of functional {mirna}-target associations. one can expect a higher chance of finding negatively correlated expression profiles for {targetscan}-predicted interactions than for {mirbase}-predicted ones. with limited experimentally validated {mirna}-target interactions, expression profiles can only serve as a supplementary role in finding interactions between {mirnas} and {mrnas}.\"{peaks}: identification of regulatory motifs by their position in {dna} sequences.,\"many {dna} functional motifs tend to accumulate or cluster at specific gene locations. these locations can be detected, in a group of gene sequences, as high frequency \\'peaks\\' with respect to a reference position, such as the transcription start site ({tss}). we have developed a web tool for the identification of regions containing significant motif peaks. we show, by using different yeast gene datasets, that peak regions are strongly enriched in experimentally-validated motifs and contain potentially important novel motifs. http://genomics.imim.es/peaks\"katholieke universiteit leuven, leuven-heverlee, belgium. qizheng.sheng@esat.kuleuven.ac.be\",biclustering microarray data by gibbs sampling.,\"{motivation}: gibbs sampling has become a method of choice for the discovery of noisy patterns, known as motifs, in {dna} and protein sequences. because handling noise in microarray data presents similar challenges, we have adapted this strategy to the biclustering of discretized microarray data. {results}: in contrast with standard clustering that reveals genes that behave similarly over all the conditions, biclustering groups genes over only a subset of conditions for which those genes have a sharp probability distribution. we have opted for a simple probabilistic model of the biclusters because it has the key advantage of providing a transparent probabilistic interpretation of the biclusters in the form of an easily interpretable fingerprint. furthermore, gibbs sampling does not suffer from the problem of local minima that often characterizes {expectation-maximization}. we demonstrate the effectiveness of our approach on two synthetic data sets as well as a data set from leukemia patients.\"\"ovarian clear cell carcinoma ({occc}) is an aggressive human cancer that is generally resistant to therapy. to explore the genetic origin of {occc}, we determined the exomic sequences of eight tumors after immunoaffinity purification of cancer cells. through comparative analyses of normal cells from the same patients, we identified four genes that were mutated in at least two tumors. {pik3ca}, which encodes a subunit of phosphatidylinositol-3 kinase, and {kras}, which encodes a well-known oncoprotein, had previously been implicated in {occc}. the other two mutated genes were previously unknown to be involved in {occc}: {ppp2r1a} encodes a regulatory subunit of serine/threonine phosphatase 2, and {arid1a} encodes adenine-thymine ({at})–rich interactive domain–containing protein {1a}, which participates in chromatin remodeling. the nature and pattern of the mutations suggest that {ppp2r1a} functions as an oncogene and {arid1a} as a tumor-suppressor gene. in a total of 42 {occcs}, 7\\\\% had mutations in {ppp2r1a} and 57\\\\% had mutations in {arid1a}. these results suggest that aberrant chromatin remodeling contributes to the pathogenesis of {occc}.\"\"protein-protein interactions are critical for cellular functions. recently developed computational approaches for predicting protein-protein interactions utilize co-evolutionary information of the interacting partners, e.g., correlations between distance matrices, where each matrix stores the pairwise distances between a protein and its orthologs from a group of reference genomes. we proposed a novel, simple method to account for some of the intra-matrix correlations in improving the prediction accuracy. specifically, the phylogenetic species tree of the reference genomes is used as a guide tree for hierarchical clustering of the orthologous proteins. the distances between these clusters, derived from the original pairwise distance matrix using the neighbor joining algorithm, form intermediate distance matrices, which are then transformed and concatenated into a super phylogenetic vector. a support vector machine is trained and tested on pairs of proteins, represented as super phylogenetic vectors, whose interactions are known. the performance, measured as {roc} score in cross validation experiments, shows significant improvement of our method ({roc} score 0.8446) over that of using pearson correlations (0.6587). we have shown that the phylogenetic tree can be used as a guide to extract intra-matrix correlations in the distance matrices of orthologous proteins, where these correlations are represented as intermediate distance matrices of the ancestral orthologous proteins. both the unsupervised and supervised learning paradigms benefit from the explicit inclusion of these intermediate distance matrices, and particularly so in the latter case, which offers a better balance between sensitivity and specificity in the prediction of protein-protein interactions.\"frederick, maryland 21702, usa.\",studies of protein-protein interfaces: a statistical analysis of the hydrophobic effect,\"data sets of 362 structurally nonredundant protein-protein interfaces and of 57 symmetry-related oligomeric interfaces have been used to explore whether the hydrophobic effect that guides protein folding is also the main driving force for protein-protein associations. the buried nonpolar surface area has been used to measure the hydrophobic effect. our analysis indicates that, although the hydrophobic effect plays a dominant role in protein-protein binding, it is not as strong as that observed in the interior of protein monomers. comparison of the interiors of the monomers with those of the interfaces reveals that, in general, the hydrophobic amino acids are more frequent in the interior of the monomers than in the interior of the protein-protein interfaces. on the other hand, a higher proportion of charged and polar residues are buried at the interfaces, suggesting that hydrogen bonds and ion pairs contribute more to the stability of protein binding than to that of protein folding. moreover, comparison of the interior of the interfaces to protein surfaces indicates that the interfaces are poorer in polar/charged than the surfaces and are richer in hydrophobic residues. the interior of the interfaces appears to constitute a compromise between the stabilization contributed by the hydrophobic effect on the one hand and avoiding patches on the protein surfaces that are too hydrophobic on the other. such patches would be unfavorable for the unassociated monomers in solution. we conclude that, although the types of interactions are similar between protein-protein interfaces and single-chain proteins overall, the contribution of the hydrophobic effect to protein-protein associations is not as strong as to protein folding. this implies that packing patterns and interatom, or interresidue, pairwise potential functions, derived from monomers, are not ideally suited to predicting and assessing ligand associations or design. these would perform adequately only in cases where the hydrophobic effect at the binding site is substantial.\"germany.\",a structural perspective on protein-protein interactions.,{bivisu}: software tool for bicluster detection and visualization.,\"{bivisu} is an open-source software tool for detecting and visualizing biclusters embedded in a gene expression matrix. through the use of appropriate coherence relations, {bivisu} can detect constant, constant-row, constant-column, additive-related as well as multiplicative-related biclusters. the biclustering results are then visualized under a {2d} setting for easy inspection. in particular, parallel coordinate ({pc}) plots for each bicluster are displayed, from which objective and subjective cluster quality evaluation can be performed. availability: {bivisu} has been developed in matlab and is available at {http://www.eie.polyu.edu.hk/\\\\~{}nflaw/biclustering}/.\"swiss federal institute of technology, eth-h\\\\\"\"{o}nggerberg, 8093 z\\\\\"\"{u}rich, switzerland; groningen biomolecular sciences and biotechnology institute (gbb), department of biophysical chemistry, university of groningen, nijenborgh 4, 9747 ag groningen, the netherlands\",a biomolecular force field based on the free enthalpy of hydration and solvation: the {gromos} force-field parameter sets {53a5} and {53a6},\"successive parameterizations of the {gromos} force field have been used successfully to simulate biomolecular systems over a long period of time. the continuing expansion of computational power with time makes it possible to compute ever more properties for an increasing variety of molecular systems with greater precision. this has led to recurrent parameterizations of the {gromos} force field all aimed at achieving better agreement with experimental data. here we report the results of the latest, extensive reparameterization of the {gromos} force field. in contrast to the parameterization of other biomolecular force fields, this parameterization of the {gromos} force field is based primarily on reproducing the free enthalpies of hydration and apolar solvation for a range of compounds. this approach was chosen because the relative free enthalpy of solvation between polar and apolar environments is a key property in many biomolecular processes of interest, such as protein folding, biomolecular association, membrane formation, and transport over membranes. the newest parameter sets, {53a5} and {53a6}, were optimized by first fitting to reproduce the thermodynamic properties of pure liquids of a range of small polar molecules and the solvation free enthalpies of amino acid analogs in cyclohexane ({53a5}). the partial charges were then adjusted to reproduce the hydration free enthalpies in water ({53a6}). both parameter sets are fully documented, and the differences between these and previous parameter sets are discussed. {\\\\copyright} 2004 wiley periodicals, inc. j comput chem 25: 1656–1676, 2004\"comprehensive molecular portraits of human breast tumours.,\"we analysed primary breast cancers by genomic {dna} copy number arrays, {dna} methylation, exome sequencing, messenger {rna} arrays, {microrna} sequencing and reverse-phase protein arrays. our ability to integrate information across platforms provided key insights into previously defined gene expression subtypes and demonstrated the existence of four main breast cancer classes when combining data from five platforms, each of which shows significant molecular heterogeneity. somatic mutations in only three genes ({tp53}, {pik3ca} and {gata3}) occurred at >10\\\\% incidence across all breast cancers; however, there were numerous subtype-associated and novel gene mutations including the enrichment of specific mutations in {gata3}, {pik3ca} and {map3k1} with the luminal a subtype. we identified two novel protein-expression-defined subgroups, possibly produced by stromal/microenvironmental elements, and integrated analyses identified specific signalling pathways dominant in each molecular subtype including a {her2}/phosphorylated {her2}/{egfr}/phosphorylated {egfr} signature within the {her2}-enriched expression subtype. comparison of basal-like breast tumours with high-grade serous ovarian tumours showed many molecular commonalities, indicating a related aetiology and similar therapeutic opportunities. the biological finding of the four main breast cancer subtypes caused by different subsets of genetic and epigenetic abnormalities raises the hypothesis that much of the clinically observable plasticity and heterogeneity occurs within, and not across, these major biological subtypes of breast cancer.\"\"complex interactions between genes or proteins contribute substantially to phenotypic evolution. we present a probabilistic model and a maximum likelihood approach for cross-species clustering analysis and for identification of conserved as well as species-specific co-expression modules. this model enables a  ” soft” cross-species clustering ({scsc}) approach by encouraging but not enforcing orthologous genes to be grouped into the same cluster. {scsc} is therefore robust to obscure orthologous relationships and can reflect different functional roles of orthologous genes in different species. we generated a time-course gene expression dataset for differentiating mouse embryonic stem ({es}) cells, and compiled a dataset of published gene expression data on differentiating human {es} cells. applying {scsc} to analyze these datasets, we identified conserved and species-specific gene regulatory modules. together with {protein-dna} binding data, an {scsc} cluster specifically induced in murine {es} cells indicated that the {klf2}/4/5 transcription factors, although critical to maintaining the pluripotent phenotype in mouse {es} cells, were decoupled from the {oct4}/{sox2}/{nanog} regulatory module in human {es} cells. two of the target genes of murine {klf2}/4/5, {lin28} and {nodal}, were rewired to be targets of {oct4}/{sox2}/{nanog} in human {es} cells. moreover, by mapping {scsc} clusters onto {kegg} signaling pathways, we identified the signal transduction components that were induced in pluripotent {es} cells in either a conserved or a species-specific manner. these results suggest that the pluripotent cell identity can be established and maintained through more than one gene regulatory network. a major goal in biology is to understand the evolution of complex traits, such as the development of multicellular body plans. to a certain extent, complex traits are governed by regulated gene expression. the comparison expression data between species requires extra considerations than sequence comparison, because gene expression is not static and the level of expression is influenced by external conditions. considering that co-expression patterns are often comparable across species, we developed a statistical model for cross-species clustering analysis. the model allows each species to create its own clusters of the genes but also encourages the species to borrow strength from each others\\' clusters of orthologous genes. the result is a pairing of clusters, one from each species, where the paired clusters share many but not necessarily all orthologous genes. the model-based approach not only reduces subjective influence but also enables effective use of evolutionary dependence. applying this model to analyze human and mouse embryonic stem ({es}) cell data, we identified the transcription factors and the signaling proteins that are specifically expressed in either human or mouse {es} cells. these results suggest that the pluripotent cell identity can be established and maintained through more than one gene regulatory network.\"usa\",biclustering algorithms for biological data analysis: a survey,\"a large number of clustering approaches have been proposed for the analysis of gene expression data obtained from microarray experiments. however, the results from the application of standard clustering methods to genes are limited. this limitation is imposed by the existence of a number of experimental conditions where the activity of genes is uncorrelated. a similar limitation exists when clustering of conditions is performed. for this reason, a number of algorithms that perform simultaneous clustering on the row and column dimensions of the data matrix has been proposed. the goal is to find submatrices, that is, subgroups of genes and subgroups of conditions, where the genes exhibit highly correlated activities for every condition. in this paper, we refer to this class of algorithms as biclustering. biclustering is also referred in the literature as coclustering and direct clustering, among others names, and has also been used in fields such as information retrieval and data mining. in this comprehensive survey, we analyze a large number of existing approaches to biclustering, and classify them in accordance with the type of biclusters they can find, the patterns of biclusters that are discovered, the methods used to perform the search, the approaches used to evaluate the solution, and the target applications.\"\"biclustering extends the traditional clustering techniques by attempting to find (all) subgroups of genes with similar expression patterns under to-be-identified subsets of experimental conditions when applied to gene expression data. still the real power of this clustering strategy is yet to be fully realized due to the lack of effective and efficient algorithms for reliably solving the general biclustering problem. we report a {qualitative} {biclustering} algorithm ({qubic}) that can solve the biclustering problem in a more general form, compared to existing algorithms, through employing a combination of qualitative (or semi-quantitative) measures of gene expression data and a combinatorial optimization technique. one key unique feature of the {qubic} algorithm is that it can identify all statistically significant biclusters including biclusters with the so-called \\'scaling patterns\\', a problem considered to be rather challenging; another key unique feature is that the algorithm solves such general biclustering problems very efficiently, capable of solving biclustering problems with tens of thousands of genes under up to thousands of conditions in a few minutes of the {cpu} time on a desktop computer. we have demonstrated a considerably improved biclustering performance by our algorithm compared to the existing algorithms on various benchmark sets and data sets of our own. {qubic} was written in {ansi} c and tested using {gcc} (version 4.1.2) on linux. its source code is available at: http://csbl.bmb.uga.edu/∼maqin/bicluster. a server version of {qubic} is also available upon request.\"clonal evolution in breast cancer revealed by single nucleus genome sequencing,\"the presence of multiple subclones within tumors mandates understanding of longitudinal and spatial subclonal dynamics. resolving the spatial and temporal heterogeneity of subclones with cancer driver events may offer insight into therapy response, tumor evolutionary histories and clinical trial design.\"\"we used a novel method based on allele-specific quantitative polymerase chain reaction (intplex) for the analysis of circulating cell.free {dna} ({ccfdna}) to compare total {ccfdna} and {kras}- or {braf}-mutated {ccfdna} concentrations in blood samples from mice xenografted with the human {sw620} colorectal cancer ({crc}) cell line and from patients with {crc}. intplex enables single-copy detection of variant alleles down to a sensitivity of ≥0.005 mutant to wild-type ratio. the proportion of mutant allele corresponding to the percentage of tumor-derived {ccfdna} was elevated in xenografted mice with {kras} homozygous mutation and varied highly from 0.13\\\\% to 68.7\\\\% in samples from mutation-positive {crc} patients (n = 38). mutant {ccfdna} alleles were quantified in the plasma of every patient at stages {ii}/{iii} and {iv} with a mean of 8.4\\\\% (median, 8.4\\\\%) and 21.8\\\\% (median, 12.4\\\\%), respectively. twelve of 38 (31.6\\\\%) and 5 of 38 (13.2\\\\%) samples showed a mutation load higher than 25\\\\%and 50\\\\%, respectively. this suggests that an important part of {ccfdna} may originate from tumor cells. in addition, we observed that tumor-derived (mutant) {ccfdna} was more fragmented than {ccfdna} from normal tissues. this observation suggests that the form of tumor-derived and normal {ccfdna} could differ. our approach revealed that allelic dilution is much less pronounced than previously stated, considerably facilitating the noninvasive molecular analysis of tumors.\"baltimore, md 21205, usa. rafa@jhu.edu\",summaries of affymetrix {genechip} probe level data.,\"high density oligonucleotide array technology is widely used in many areas of biomedical research for quantitative and highly parallel measurements of gene expression. affymetrix {genechip} arrays are the most popular. in this technology each gene is typically represented by a set of 11-20 pairs of probes. in order to obtain expression measures it is necessary to summarize the probe level data. using two extensive spike-in studies and a dilution study, we developed a set of tools for assessing the effectiveness of expression measures. we found that the performance of the current version of the default expression measure provided by affymetrix microarray suite can be significantly improved by the use of probe level summaries derived from empirically motivated statistical models. in particular, improvements in the ability to detect differentially expressed genes are demonstrated.\"\"reactome, located at http://www.reactome.org is a curated, peer-reviewed resource of human biological processes. given the genetic makeup of an organism, the complete set of possible reactions constitutes its reactome. the basic unit of the reactome database is a reaction; reactions are then grouped into causal chains to form pathways. the reactome data model allows us to represent many diverse processes in the human system, including the pathways of intermediary metabolism, regulatory pathways, and signal transduction, and high-level processes, such as the cell cycle. reactome provides a qualitative framework, on which quantitative data can be superimposed. tools have been developed to facilitate custom data entry and annotation by expert biologists, and to allow visualization and exploration of the finished dataset as an interactive process map. although our primary curational domain is pathways from homo sapiens, we regularly create electronic projections of human pathways onto other organisms via putative orthologs, thus making reactome relevant to model organism research communities. the database is publicly available under open source terms, which allows both its content and its software infrastructure to be freely used and redistributed.\"\"biophysics department, max planck institute of medical research, 6900 heidelberg, federal republic of germany\",dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features.,\"for a successful analysis of the relation between amino acid sequence and protein structure, an unambiguous and physically meaningful definition of secondary structure is essential. we have developed a set of simple and physically motivated criteria for secondary structure, programmed as a pattern-recognition process of hydrogen-bonded and geometrical features extracted from x-ray coordinates. cooperative secondary structure is recognized as repeats of the elementary hydrogen-bonding patterns  ” turn” and  ” bridge.” repeating turns are  ” helices,” repeating bridges are  ” ladders,” connected ladders are  ” sheets.” geometric structure is defined in terms of the concepts torsion and curvature of differential geometry. local chain  ” chirality” is the torsional handedness of four consecutive cα positions and is positive for right-handed helices and negative for ideal twisted β-sheets. curved pieces are defined as  ” bends.” solvent  ” exposure” is given as the number of water molecules in possible contact with a residue. the end result is a compilation of the primary structure, including {ss} bonds, secondary structure, and solvent exposure of 62 different globular proteins. the presentation is in linear form: strip graphs for an overall view and strip tables for the details of each of 10.925 residues. the dictionary is also available in computer-readable form for protein structure prediction work.\"\"the university of california, santa cruz genome browser (http://genome.ucsc.edu) offers online access to a database of genomic sequence and annotation data for a wide variety of organisms. the browser also has many tools for visualizing, comparing and analyzing both publicly available and user-generated genomic data sets, aligning sequences and uploading user data. among the features released this year are a gene search tool and annotation track drag-reorder functionality as well as support for {bam} and {bigwig}/{bigbed} file formats. new display enhancements include overlay of multiple wiggle tracks through use of transparent coloring, options for displaying transformed wiggle data, a \\'mean+whiskers\\' windowing function for display of wiggle data at high zoom levels, and more color schemes for microarray data. new data highlights include seven new genome assemblies, a neandertal genome data portal, phenotype and disease association data, a human {rna} editing track, and a zebrafish conservation track. we also describe updates to existing tracks.\"national institutes of health, bethesda, md, 20894, usa. sherry@ncbi.nlm.nih.gov\",{dbsnp}: the {ncbi} database of genetic variation.,\"in response to a need for a general catalog of genome variation to address the large-scale sampling designs required by association studies, gene mapping and evolutionary biology, the national center for biotechnology information ({ncbi}) has established the {dbsnp} database [{s.t}.sherry, {m.ward} and k. sirotkin (1999) genome res., 9, 677-679]. submissions to {dbsnp} will be integrated with other sources of information at {ncbi} such as {genbank}, {pubmed}, {locuslink} and the human genome project data. the complete contents of {dbsnp} are available to the public at website: {http://www.ncbi.nlm.nih.gov/snp}. the complete contents of {dbsnp} can also be downloaded in multiple formats via anonymous {ftp} at ftp://ncbi.nlm.nih.gov/snp/.\"houston, texas 77030, usa.\",a switch from high-fidelity to error-prone {dna} double-strand break repair underlies stress-induced mutation.,\"it has been observed that the evolutionary distances of interacting proteins often display a higher level of similarity than those of noninteracting proteins. this finding indicates that interacting proteins are subject to common evolutionary constraints and constitutes the basis of a method to predict protein interactions known as mirrortree. it has been difficult, however, to identify the direct cause of the observed similarities between evolutionary trees. one possible explanation is the existence of compensatory mutations between partners\\' binding sites to maintain proper binding. this explanation, though, has been recently challenged, and it has been suggested that the signal of correlated evolution uncovered by the mirrortree method is unrelated to any correlated evolution between binding sites. we examine the contribution of binding sites to the correlation between evolutionary trees of interacting domains. we show that binding neighborhoods of interacting proteins have, on average, higher coevolutionary signal compared with the regions outside binding sites; however, when the binding neighborhood is removed, the remaining domain sequence still contains some coevolutionary signal. in conclusion, the correlation between evolutionary trees of interacting domains cannot exclusively be attributed to the correlated evolution of the binding sites or to common evolutionary pressure exerted on the whole protein domain sequence, each of which contributes to the signal measured by the mirrortree approach.\"london, england.\",principles of protein-protein interactions.,\"this review examines protein complexes in the brookhaven protein databank to gain a better understanding of the principles governing the interactions involved in protein-protein recognition. the factors that influence the formation of protein-protein complexes are explored in four different types of protein-protein complexes--homodimeric proteins, heterodimeric proteins, enzyme-inhibitor complexes, and antibody-protein complexes. the comparison between the complexes highlights differences that reflect their biological roles.\"kyoto university, kyoto 606-8502, japan.\",{mafft}: a novel method for rapid multiple sequence alignment based on fast fourier transform,\"a multiple sequence alignment program, {mafft}, has been developed. the {cpu} time is drastically reduced as compared with existing methods. {mafft} includes two novel techniques. (i) homo logous regions are rapidly identified by the fast fourier transform ({fft}), in which an amino acid sequence is converted to a sequence composed of volume and polarity values of each amino acid residue. (ii) we propose a simplified scoring system that performs well for reducing {cpu} time and increasing the accuracy of alignments even for sequences having large insertions or extensions as well as distantly related sequences of similar length. two different heuristics, the progressive method ({fft}‐{ns}‐2) and the iterative refinement method ({fft}‐{ns}‐i), are implemented in {mafft}. the performances of {fft}‐{ns}‐2 and {fft}‐{ns}‐i were compared with other methods by computer simulations and benchmark tests; the {cpu} time of {fft}‐{ns}‐2 is drastically reduced as compared with {clustalw} with comparable accuracy. {fft}‐{ns}‐i is over 100 times faster than {t‐coffee}, when the number of input sequences exceeds 60, without sacrificing the accuracy.\"campus box 8501, 4444 forest park avenue, st louis, missouri 63108, usa. waterston@gs.washington.edu\",initial sequencing and comparative analysis of the mouse genome,\"the sequence of the mouse genome is a key informational tool for understanding the contents of the human genome and a key experimental tool for biomedical research. here, we report the results of an international collaboration to produce a high-quality draft sequence of the mouse genome. we also present an initial comparative analysis of the mouse and human genomes, describing some of the insights that can be gleaned from the two sequences. we discuss topics including the analysis of the evolutionary forces shaping the size, structure and sequence of the genomes; the conservation of large-scale synteny across most of the genomes; the much lower extent of sequence orthology covering less than half of the genomes; the proportions of the genomes under selection; the number of protein-coding genes; the expansion of gene families related to reproduction and immunity; the evolution of proteins; and the identification of intraspecies polymorphism.\"rockefeller university, new york, ny 10021, usa.\",comparative protein structure modeling of genes and genomes,\"▪ abstract\\u2002comparative modeling predicts the three-dimensional structure of a given protein sequence (target) based primarily on its alignment to one or more proteins of known structure (templates). the prediction process consists of fold assignment, target–template alignment, model building, and model evaluation. the number of protein sequences that can be modeled and the accuracy of the predictions are increasing steadily because of the growth in the number of known protein structures and because of the improvements in the modeling software. further advances are necessary in recognizing weak sequence–structure similarities, aligning sequences with structures, modeling of rigid body shifts, distortions, loops and side chains, as well as detecting errors in a model. despite these problems, it is currently possible to model with useful accuracy significant parts of approximately one third of all known protein sequences. the use of individual comparative models in biology is already rewarding and increasingly widespread. a major new challenge for comparative modeling is the integration of it with the torrents of data from genome sequencing projects as well as from functional and structural genomics. in particular, there is a need to develop an automated, rapid, robust, sensitive, and accurate comparative modeling pipeline applicable to whole genomes. such large-scale modeling is likely to encourage new kinds of applications for the many resulting models, based on their large number and completeness at the level of the family, organism, or functional network.\"\"the identification of mutations that are present in a small fraction of {dna} templates is essential for progress in several areas of biomedical research. although massively parallel sequencing instruments are in principle well suited to this task, the error rates in such instruments are generally too high to allow confident identification of rare variants. we here describe an approach that can substantially increase the sensitivity of massively parallel sequencing instruments for this purpose. the keys to this approach, called the {safe-sequencing} system ( ” {safe-seqs}”), are (i) assignment of a unique identifier ({uid}) to each template molecule, (ii) amplification of each uniquely tagged template molecule to create {uid} families, and (iii) redundant sequencing of the amplification products. {pcr} fragments with the same {uid} are considered mutant ( ” supermutants”) only if ≥95\\\\% of them contain the identical mutation. we illustrate the utility of this approach for determining the fidelity of a polymerase, the accuracy of oligonucleotides synthesized in vitro, and the prevalence of mutations in the nuclear and mitochondrial genomes of normal cells.\"\"synonymous mutations change the sequence of a\\xa0gene without directly altering the sequence of the encoded protein. here, we present evidence that these ?silent? mutations frequently contribute to human cancer. selection on synonymous mutations in\\xa0oncogenes is cancer-type specific, and although the functional consequences of cancer-associated synonymous mutations may be diverse, they recurrently alter exonic motifs that regulate splicing and are associated with changes in oncogene splicing in tumors. the p53 tumor suppressor ({tp53}) also has recurrent synonymous mutations, but, in contrast to those in oncogenes, these are adjacent to splice sites and inactivate them. we estimate that between one in two and\\xa0one in five silent mutations in oncogenes have been selected, equating to ?6\\\\%? 8\\\\% of all selected single-nucleotide changes in these genes. in addition, our analyses suggest that dosage-sensitive oncogenes have selected mutations in their 3?\\xa0{utrs}.\"\"the recognition of specific {dna}-binding sites by transcription factors is a critical yet poorly understood step in the control of gene expression. members of the hox family of transcription factors bind {dna} by making nearly identical major groove contacts via the recognition helices of their homeodomains. in vivo specificity, however, often depends on extended and unstructured regions that link hox homeodomains to a {dna}-bound cofactor, extradenticle (exd). using a combination of structure determination, computational analysis, and in vitro and in vivo assays, we show that hox proteins recognize specific {hox-exd} binding sites via residues located in these extended regions that insert into the minor groove but only when presented with the correct {dna} sequence. our results suggest that these residues, which are conserved in a paralog-specific manner, confer specificity by recognizing a sequence-dependent {dna} structure instead of directly reading a specific {dna} sequence.\"\"motivation: most tumor samples are a heterogeneous mixture of cells, including admixture by normal (non-cancerous) cells and subpopulations of cancerous cells with different complements of somatic aberrations. this intra-tumor heterogeneity complicates the analysis of somatic aberrations in {dna} sequencing data from tumor samples.\"\"identifying interaction sites in proteins provides important clues to the function of a protein and is becoming increasingly relevant in topics such as systems biology and drug discovery. although there are numerous papers on the prediction of interaction sites using information derived from structure, there are only a few case reports on the prediction of interaction residues based solely on protein sequence. here, a sliding window approach is combined with the random forests method to predict protein interaction sites using (i) a combination of sequence- and structure-derived parameters and (ii) sequence information alone. for sequence-based prediction we achieved a precision of 84\\\\% with a 26\\\\% recall and an f-measure of 40\\\\%. when combined with structural information, the prediction performance increases to a precision of 76\\\\% and a recall of 38\\\\% with an f-measure of 51\\\\%. we also present an attempt to rationalize the sliding window size and demonstrate that a nine-residue window is the most suitable for predictor construction. finally, we demonstrate the applicability of our prediction methods by modeling the {ras-raf} complex using predicted interaction sites as target binding interfaces. our results suggest that it is possible to predict protein interaction sites with quite a high accuracy using only sequence information.\"\"the inference of transcriptional networks that regulate transitions into physiological or pathological cellular states remains a central challenge in systems biology. a mesenchymal phenotype is the hallmark of tumour aggressiveness in human malignant glioma, but the regulatory programs responsible for implementing the associated molecular signature are largely unknown. here we show that reverse-engineering and an unbiased interrogation of a glioma-specific regulatory network reveal the transcriptional module that activates expression of mesenchymal genes in malignant glioma. two transcription factors ({c/ebpβ} and {stat3}) emerge as synergistic initiators and master regulators of mesenchymal transformation. ectopic co-expression of {c/ebpβ} and {stat3} reprograms neural stem cells along the aberrant mesenchymal lineage, whereas elimination of the two factors in glioma cells leads to collapse of the mesenchymal signature and reduces tumour aggressiveness. in human glioma, expression of {c/ebpβ} and {stat3} correlates with mesenchymal differentiation and predicts poor clinical outcome. these results show that the activation of a small regulatory module is necessary and sufficient to initiate and maintain an aberrant phenotypic state in cancer cells.\"\"studies of epigenetic modifications would benefit from improved methods for high-throughput methylation profiling. we introduce two complementary approaches that use next-generation sequencing technology to detect cytosine methylation. in the first method, we designed 10,000 bisulfite padlock probes to profile 7,000 {cpg} locations distributed over the {encode} pilot project regions and applied them to human b-lymphocytes, fibroblasts and induced pluripotent stem cells. this unbiased choice of targets takes advantage of existing expression and chromatin immunoprecipitation data and enabled us to observe a pattern of low promoter methylation and high gene-body methylation in highly expressed genes. the second method, methyl-sensitive cut counting, generated nontargeted genome-scale data for 1.4 million {hpaii} sites in the {dna} of b-lymphocytes and confirmed that gene-body methylation in highly expressed genes is a consistent phenomenon throughout the human genome. our observations highlight the usefulness of techniques that are not inherently or intentionally biased towards particular subsets like {cpg} islands or promoter regions.\"tel aviv 69978, israel.\",conservation and evolvability in regulatory networks: the evolution of ribosomal regulation in yeast.,\"transcriptional modules of coregulated genes play a key role in regulatory networks. comparative studies show that modules of coexpressed genes are conserved across taxa. however, little is known about the mechanisms underlying the evolution of module regulation. here, we explore the evolution of cis-regulatory programs associated with conserved modules by integrating expression profiles for two yeast species and sequence data for a total of 17 fungal genomes. we show that although the cis-elements accompanying certain conserved modules are strictly conserved, those of other conserved modules are remarkably diverged. in particular, we infer the evolutionary history of the regulatory program governing ribosomal modules. we show how a cis-element emerged concurrently in dozens of promoters of ribosomal protein genes, followed by the loss of a more ancient cis-element. we suggest that this formation of an intermediate redundant regulatory program allows conserved transcriptional modules to gradually switch from one regulatory mechanism to another while maintaining their functionality. our work provides a general framework for the study of the dynamics of promoter evolution at the level of transcriptional modules and may help in understanding the evolvability and increased redundancy of transcriptional regulation in higher organisms.\"hinxton, cb10 1sd, cambridge, uk.\",structural characterisation and functional significance of transient protein-protein interactions.,\"protein-protein complexes that dissociate and associate readily, often depending on the physiological condition or environment, play an important role in many biological processes. in order to characterise these \"\"transient\"\" protein-protein interactions, two sets of complexes were collected and analysed. the first set consists of 16 experimentally validated \"\"weak\"\" transient homodimers, which are known to exist as monomers and dimers at physiological concentration, with dissociation constants in the micromolar range. a set of 23 functionally validated transient (i.e. intracellular signalling) heterodimers comprise the second set. this set includes complexes that are more stable, with nanomolar binding affinities, and require a molecular trigger to form and break the interaction. in comparison to more stable homodimeric complexes, the weak homodimers demonstrate smaller contact areas between protomers and the interfaces are more planar and polar on average. the physicochemical and geometrical properties of these weak homodimers more closely resemble those of non-obligate hetero-oligomeric complexes, whose components can exist either as monomers or as complexes in vivo. in contrast to the weak transient dimers, \"\"strong\"\" transient dimers often undergo large conformational changes upon association/dissociation and are characterised with larger, less planar and sometimes more hydrophobic interfaces. from sequence alignments we find that the interface residues of the weak transient homodimers are generally more conserved than surface residues, consistent with being constrained to maintain the protein-protein interaction during evolution. protein families that include members with different oligomeric states or structures are identified, and found to exhibit a lower sequence conservation at the interface. the results are discussed in terms of the physiological function and evolution of protein-protein interactions.\"washington university medical school, st. louis, mo 63110, usa. stormo@ural.wustl.edu\",{dna} binding sites: representation and discovery.,\"a public web-based facility to infer, analyse and graphically represent the likely modes of a protein motion, starting from a static structure, is presented. this facility is based on the use of {concoord} to generate an ensemble of feasible protein structures that are subsequently analysed by principal component analysis to identify probable concerted motions. the user is returned the ensemble of feasible structures, together with associated analyses, including animations and graphical representations of both the principal component of the ensemble covariance and indicators of strongly correlated pairwise atomic motions. whilst users are warned that completely reliable inferences about protein motion may be beyond even substantially more rigorous tools for exploring configurational space, it is hoped that the service will allow a much wider community to benefit from the insights that simple dynamic data may offer.\"p-1/12 cit, scheme viim, calcutta 700 054, india.\",a dissection of specific and non-specific protein-protein interfaces.,\"we compare the geometric and physical-chemical properties of interfaces involved in specific and non-specific protein-protein interactions in crystal structures reported in the protein data bank. specific interactions are illustrated by 70 protein-protein complexes and by subunit contacts in 122 homodimeric proteins; non-specific interactions are illustrated by 188 pairs of monomeric proteins making crystal-packing contacts selected to bury more than 800 a2 of protein surface. a majority of these pairs have 2-fold symmetry and form \"\"crystal dimers\"\" that cannot be distinguished from real dimers on the basis of the interface size or symmetry. the chemical and amino acid compositions of the large crystal-packing interfaces resemble the protein solvent-accessible surface. these interfaces are less hydrophobic than in homodimers and contain much fewer fully buried atoms. we develop a residue propensity score and a hydrophobic interaction score to assess preferences seen in the chemical and amino acid compositions of the different types of interfaces, and we derive indexes to evaluate the atomic packing, which we find to be less compact at non-specific than at specific interfaces. we test the capacity of these parameters to identify homodimeric proteins in crystal structures, and show that a simple combination of the non-polar interface area and the fraction of buried interface atoms assigns the quaternary structure of 88\\\\% of the homodimers and 77\\\\% of the monomers in our data set correctly. these success rates increase to 93-95\\\\% when the residue propensity score of the interfaces is taken into consideration.\"domains and functional sites\",\"motivation: {interpro} is a new integrated documentation resource for protein families, domains and functional sites, developed initially as a means of rationalising the complementary efforts of the {prosite}, {prints}, pfam and {prodom} database {projects.results}: merged annotations from {prints}, {prosite} and pfam form the {interpro} core. each combined {interpro} entry includes functional descriptions and literature references, and links are made back to the relevant parent database(s), allowing users to see at a glance whether a particular family or domain has associated patterns, profiles, fingerprints, etc. merged and individual entries (i.e. those that have no counterpart in the companion resources) are assigned unique accession numbers. release 1.2 of {interpro} (june 2000) contains over 3000 entries, representing families, domains, repeats and sites of post-translational modification ({ptms}) encoded by 6581 different regular expressions, profiles, fingerprints and hidden markov models ({hmms}). each {interpro} entry lists all the matches against {swiss}-{prot} and {trembl} (more than 1000000 hits from 264333 different proteins out of 384572 in {swiss}-{prot} and {trembl}).availability: the database is accessible for text- and sequence-based searches at {http://www.ebi.ac.uk/interpro/.contact}: interhelp@ebi.ac.uk\"integrated molecular analysis of clear-cell renal cell carcinoma,\"the performance of gene expression microarrays has been well characterized using controlled reference samples, but the performance on clinical samples remains less clear. we identified sources of technical bias affecting many genes in concert, thus causing spurious correlations in clinical data sets and false associations between genes and clinical variables. we developed a method to correct for technical bias in clinical microarray data, which increased concordance with known biological relationships in multiple data sets.\"\"cancer is a somatic evolutionary process characterized by the accumulation of mutations, which contribute to tumor growth, clinical progression, immune escape, and drug resistance development. evolutionary theory can be used to analyze the dynamics of tumor cell populations and to make inference about the evolutionary history of a tumor from molecular data. we review recent approaches to modeling the evolution of cancer, including population dynamics models of tumor initiation and progression, phylogenetic methods to model the evolutionary relationship between tumor subclones, and probabilistic graphical models to describe dependencies among mutations. evolutionary modeling helps to understand how tumors arise and will also play an increasingly important prognostic role in predicting disease progression and the outcome of medical interventions, such as targeted therapy. {\\\\copyright} the author(s) 2014. published by oxford university press on behalf of the society of systematic biologists.\"the molecular evolution of acquired resistance to targeted {egfr} blockade in colorectal cancers,sydney, new south wales 2006, australia.\",protein-protein interactions in human disease.,room 151, frederick, md 21702, usa.\",\"a new, structurally nonredundant, diverse data set of protein–protein interfaces and its implications\",\"here, we present a diverse, structurally nonredundant data set of two-chain protein–protein interfaces derived from the {pdb}. using a sequence order-independent structural comparison algorithm and hierarchical clustering, 3799 interface clusters are obtained. these yield 103 clusters with at least five nonhomologous members. we divide the clusters into three types. in type i clusters, the global structures of the chains from which the interfaces are derived are also similar. this cluster type is expected because, in general, related proteins associate in similar ways. in type {ii}, the interfaces are similar; however, remarkably, the overall structures and functions of the chains are different. the functional spectrum is broad, from enzymes/inhibitors to immunoglobulins and toxins. the fact that structurally different monomers associate in similar ways, suggests  ” good” binding architectures. this observation extends a paradigm in protein science: it has been well known that proteins with similar structures may have different functions. here, we show that it extends to interfaces. in type {iii} clusters, only one side of the interface is similar across the cluster. this structurally nonredundant data set provides rich data for studies of protein–protein interactions and recognition, cellular networks and drug design. in particular, it may be useful in addressing the difficult question of what are the favorable ways for proteins to interact. (the data set is available at http://protein3d.ncifcrf.gov/∼keskino/ and {http://home.ku.edu.tr/∼okeskin/interface}/{interfaces}.html.)\"university of wisconsin-madison, madison, wi 53706, usa.\",{kfc} server: interactive forecasting of protein interaction hot spots.,\"the {kfc} server is a web-based implementation of the {kfc} (knowledge-based {fade} and contacts) model-a machine learning approach for the prediction of binding hot spots, or the subset of residues that account for most of a protein interface\\'s; binding free energy. the server facilitates the automated analysis of a user submitted protein-protein or {protein-dna} interface and the visualization of its hot spot predictions. for each residue in the interface, the {kfc} server characterizes its local structural environment, compares that environment to the environments of experimentally determined hot spots and predicts if the interface residue is a hot spot. after the computational analysis, the user can visualize the results using an interactive job viewer able to quickly highlight predicted hot spots and surrounding structural features within the protein structure. the {kfc} server is accessible at http://kfc.mitchell-lab.org.\"269 campus drive, center for clinical sciences research 1115, stanford, ca 94305-5151, usa.\",significance analysis of microarrays applied to the ionizing radiation response.,\"microarrays can measure the expression of thousands of genes to identify changes in expression between different biological states. methods are needed to determine the significance of these changes while accounting for the enormous number of genes. we describe a method, significance analysis of microarrays ({sam}), that assigns a score to each gene on the basis of change in gene expression relative to the standard deviation of repeated measurements. for genes with scores greater than an adjustable threshold, {sam} uses permutations of the repeated measurements to estimate the percentage of genes identified by chance, the false discovery rate ({fdr}). when the transcriptional response of human cells to ionizing radiation was measured by microarrays, {sam} identified 34 genes that changed at least 1.5-fold with an estimated {fdr} of 12\\\\%, compared with {fdrs} of 60 and 84\\\\% by using conventional methods of analysis. of the 34 genes, 19 were involved in cell cycle regulation and 3 in apoptosis. surprisingly, four nucleotide excision repair genes were induced, suggesting that this repair pathway for {uv}-damaged {dna} might play a previously unrecognized role in repairing {dna} damaged by ionizing radiation.\"\"intratumor heterogeneity may foster tumor evolution and adaptation and hinder personalized-medicine strategies that depend on results from single tumor-biopsy samples. to examine intratumor heterogeneity, we performed exome sequencing, chromosome aberration analysis, and ploidy profiling on multiple spatially separated samples obtained from primary renal carcinomas and associated metastatic sites. we characterized the consequences of intratumor heterogeneity using immunohistochemical analysis, mutation functional analysis, and profiling of messenger {rna} expression. phylogenetic reconstruction revealed branched evolutionary tumor growth, with 63 to 69\\\\% of all somatic mutations not detectable across every tumor region. intratumor heterogeneity was observed for a mutation within an autoinhibitory domain of the mammalian target of rapamycin ({mtor}) kinase, correlating with s6 and {4ebp} phosphorylation in vivo and constitutive activation of {mtor} kinase activity in vitro. mutational intratumor heterogeneity was seen for multiple tumor-suppressor genes converging on loss of function; {setd2}, {pten}, and {kdm5c} underwent multiple distinct and spatially separated inactivating mutations within a single tumor, suggesting convergent phenotypic evolution. gene-expression signatures of good and poor prognosis were detected in different regions of the same tumor. allelic composition and ploidy profiling analysis revealed extensive intratumor heterogeneity, with 26 of 30 tumor samples from four tumors harboring divergent allelic-imbalance profiles and with ploidy heterogeneity in two of four tumors. intratumor heterogeneity can lead to underestimation of the tumor genomics landscape portrayed from single tumor-biopsy samples and may present major challenges to personalized-medicine and biomarker development. intratumor heterogeneity, associated with heterogeneous protein function, may foster tumor adaptation and therapeutic failure through darwinian selection. (funded by the medical research council and others.).\"\"development requires the establishment of precise patterns of gene expression, which are primarily controlled by transcription factors binding to cis-regulatory modules. although transcription factor occupancy can now be identified at genome-wide scales, decoding this regulatory landscape remains a daunting challenge. here we used a novel approach to predict spatio-temporal cis-regulatory activity based only on in vivo transcription factor binding and enhancer activity data. we generated a high-resolution atlas of cis-regulatory modules describing their temporal and combinatorial occupancy during drosophila mesoderm development. the binding profiles of cis-regulatory modules with characterized expression were used to train support vector machines to predict five spatio-temporal expression patterns. in vivo transgenic reporter assays demonstrate the high accuracy of these predictions and reveal an unanticipated plasticity in transcription factor binding leading to similar expression. this data-driven approach does not require previous knowledge of transcription factor sequence affinity, function or expression, making it widely applicable.\"expression and evolution\",\"transcription factors are key cellular components that control gene expression: their activities determine how cells function and respond to the environment. currently, there is great interest in research into human transcriptional regulation. however, surprisingly little is known about these regulators themselves. for example, how many transcription factors does the human genome contain? how are they expressed in different tissues? are they evolutionarily conserved? here, we present an analysis of 1,391 manually curated sequence-specific {dna}-binding transcription factors, their functions, genomic organization and evolutionary conservation. much remains to be explored, but this study provides a solid foundation for future investigations to elucidate regulatory mechanisms underlying diverse mammalian biological processes.\"evolution of the cancer genome.,\"the advent of massively parallel sequencing technologies has allowed the characterization of cancer genomes at an unprecedented resolution. investigation of the mutational landscape of tumours is providing new insights into cancer genome evolution, laying bare the interplay of somatic mutation, adaptation of clones to their environment and natural selection. these studies have demonstrated the extent of the heterogeneity of cancer genomes, have allowed inferences to be made about the forces that act on nascent cancer clones as they evolve and have shown insight into the mutational processes that generate genetic variation. here we review our emerging understanding of the dynamic evolution of the cancer genome and of the implications for basic cancer biology and the development of antitumour therapy.\"\"{micrornas} ({mirnas}) are endogenous not, vert, similar23 nt {rnas} that play important gene-regulatory roles in animals and plants by pairing to the {mrnas} of protein-coding genes to direct their posttranscriptional repression. this review outlines the current understanding of {mirna} target recognition in animals and discusses the widespread impact of {mirnas} on both the expression and evolution of protein-coding genes.\"vrije universiteit de boelelaan 1081a, 1081hv, amsterdam, the netherlands.\",{popscomp}: an automated interaction analysis of biomolecular complexes.,\"large-scale analysis of biomolecular complexes reveals the functional network within the cell. computational methods are required to extract the essential information from the available data. the {popscomp} server is designed to calculate the interaction surface between all components of a given complex structure consisting of proteins, {dna} or {rna} molecules. the server returns matrices and graphs of surface area burial that can be used to automatically annotate components and residues that are involved in complex formation, to pinpoint conformational changes and to estimate molecular interaction energies. the analysis can be performed on a per-atom level or alternatively on a per-residue level for low-resolution structures. here, we present an analysis of ribosomal structures in complex with various antibiotics to exemplify the potential and limitations of automated complex analysis. the {popscomp} server is accessible at http://ibivu.cs.vu.nl/programs/popscompwww/.\"91904, israel.\",{promateus}—an open research approach to protein-binding sites analysis,\"the development of bioinformatic tools by individual labs results in the abundance of parallel programs for the same task. for example, identification of binding site regions between interacting proteins is done using: {promate}, {whiscy}, {ppi}-pred, {pinup} and others. all servers first identify unique properties of binding sites and then incorporate them into a predictor. obviously, the resulting prediction would improve if the most suitable parameters from each of those predictors would be incorporated into one server. however, because of the variation in methods and databases, this is currently not feasible. here, the protein-binding site prediction server is extended into a general protein-binding sites research tool, {promateus}. this web tool, based on {promate}\\'s infrastructure enables the easy exploration and incorporation of new features and databases by the user, providing an evaluation of the benefit of individual features and their combination within a set framework. this transforms the individual research into a community exercise, bringing out the best from all users for optimized predictions. the analysis is demonstrated on a database of protein protein and {protein-dna} interactions. this approach is basically different from that used in generating meta-servers. the implications of the open-research approach are discussed. {promateus} is available at http://bip.weizmann.ac.il/promate.\"rehovot 76100, israel.\",thousands of samples are needed to generate a robust gene list for predicting outcome in cancer.,\"despite many studies of the likely survival outcome of individual patients with colorectal cancer, our knowledge of this subject remains poor. until recently, we had virtually no understanding of individual responses to therapy, but the discovery of the {kras} mutation as a marker of probable failure of epidermal growth factor receptor ({egfr})-targeted therapy is a first step in the tailoring of treatment to the individual. with the application of molecular analyses, as well as the ability to perform high-throughput screens, there has been an explosive increase in the number of markers thought to be associated with prognosis and treatment outcome in this disease. in this review, we attempt to summarize the sometimes confusing findings, and critically assess those markers already in the public domain.\"dynamics of genomic clones in breast cancer patient xenografts at single-cell resolution,high-accuracy detection of low-frequency variation.\",the clonal and mutational evolution spectrum of primary triple-negative breast cancers,\"primary triple-negative breast cancers ({tnbcs}), a tumour type defined by lack of oestrogen receptor, progesterone receptor and {erbb2} gene amplification, represent approximately 16\\\\% of all breast cancers. here we show in 104 {tnbc} cases that at the time of diagnosis these cancers exhibit a wide and continuous spectrum of genomic evolution, with some having only a handful of coding somatic aberrations in a few pathways, whereas others contain hundreds of coding somatic mutations. high-throughput {rna} sequencing ({rna}-seq) revealed that only approximately 36\\\\% of mutations are expressed. using deep re-sequencing measurements of allelic abundance for 2,414 somatic mutations, we determine for the first time-to our knowledge-in an epithelial tumour subtype, the relative abundance of clonal frequencies among cases representative of the population. we show that {tnbcs} vary widely in their clonal frequencies at the time of diagnosis, with the basal subtype of {tnbc} showing more variation than non-basal {tnbc}. although p53 (also known as {tp53}), {pik3ca} and {pten} somatic mutations seem to be clonally dominant compared to other genes, in some tumours their clonal frequencies are incompatible with founder status. mutations in cytoskeletal, cell shape and motility proteins occurred at lower clonal frequencies, suggesting that they occurred later during tumour progression. taken together, our results show that understanding the biology and therapeutic responses of patients with {tnbc} will require the determination of individual tumour clonal genotypes.\"\"lung cancer is the leading cause of cancer-related death, with non-small cell lung cancer ({nsclc}) being the predominant form of the disease. most lung cancer is caused by the accumulation of genomic alterations due to tobacco exposure. to uncover its mutational landscape, we performed whole-exome sequencing in 31 {nsclcs} and their matched normal tissue samples. we identified both common and unique mutation spectra and pathway activation in lung adenocarcinomas and squamous cell carcinomas, two major histologies in {nsclc}. in addition to identifying previously known lung cancer genes ({tp53}, {kras}, {egfr}, {cdkn2a} and {rb1}), the analysis revealed many genes not previously implicated in this malignancy. notably, a novel gene {csmd3} was identified as the second most frequently mutated gene (next to {tp53}) in lung cancer. we further demonstrated that loss of {csmd3} results in increased proliferation of airway epithelial cells. the study provides unprecedented insights into mutational processes, cellular pathways and gene networks associated with lung cancer. of potential immediate clinical relevance, several highly mutated genes identified in our study are promising druggable targets in cancer therapy including {alk}, {ctnna3}, {dcc}, {mll3}, {pcdhiix}, {pik3c2b}, {pik3cg} and {rock2}.\"\"{background}: the detection of conserved motifs in promoters of orthologous genes (phylogenetic footprints) has become a common strategy to predict cis-acting regulatory elements. several software tools are routinely used to raise hypotheses about regulation. however, these tools are generally used as black boxes, with default parameters. a systematic evaluation of optimal parameters for a footprint discovery strategy can bring a sizeable improvement to the predictions. {results}: we evaluate the performances of a footprint discovery approach based on the detection of over-represented spaced motifs. this method is particularly suitable for (but not restricted to) bacteria, since such motifs are typically bound by factors containing a {helix-turn}-helix domain. we evaluated footprint discovery in 368 escherichia coli k12 genes with annotated sites, under 40 different combinations of parameters (taxonomical level, background model, organism-specific filtering, operon inference). motifs are assessed both at the levels of correctness and significance. we further report a detailed analysis of 181 bacterial orthologs of the {lexa} repressor. distinct motifs are detected at various taxonomical levels, including the 7 previously characterized taxon-specific motifs. in addition, we highlight a significantly stronger conservation of half-motifs in actinobacteria, relative to firmicutes, suggesting an intermediate state in specificity switching between the two gram-positive phyla, and thereby revealing the on-going evolution of {lexa} auto-regulation. {conclusion}: the footprint discovery method proposed here shows excellent results with e. coli and can readily be extended to predict cis-acting regulatory signals and propose testable hypotheses in bacterial genomes for which nothing is known about regulation.\"\"circulating tumor cells ({ctcs}) enter peripheral blood from primary tumors and seed metastases. the genome sequencing of {ctcs} could offer noninvasive prognosis or even diagnosis, but has been hampered by low single-cell genome coverage of scarce {ctcs}. here, we report the use of the recently developed multiple annealing and looping-based amplification cycles for whole-genome amplification of single {ctcs} from lung cancer patients. we observed characteristic cancer-associated single-nucleotide variations and insertions/deletions in exomes of {ctcs}. these mutations provided information needed for individualized therapy, such as drug resistance and phenotypic transition, but were heterogeneous from cell to cell. in contrast, every {ctc} from an individual patient, regardless of the cancer subtypes, exhibited reproducible copy number variation ({cnv}) patterns, similar to those of the metastatic tumor of the same patient. interestingly, different patients with the same lung cancer adenocarcinoma ({adc}) shared similar {cnv} patterns in their {ctcs}. even more interestingly, patients of small-cell lung cancer have {cnv} patterns distinctly different from those of {adc} patients. our finding suggests that {cnvs} at certain genomic loci are selected for the metastasis of cancer. the reproducibility of cancer-specific {cnvs} offers potential for {ctc}-based cancer diagnostics.\"university of bergen, thorm{\\\\o}hlensgate 55, n-5008 bergen, norway, the bioinformatics centre, department of molecular biology \\\\& biotech research and innovation centre, university of copenhagen, ole maal{\\\\o}es vej 5, dk-2100 k{\\\\o}benhavn {\\\\o}, informatics and mathematical modeling, building 321, technical university of denmark, dk-2800 kgs. lyngby, center for comparative genomics, institute of biology, university of copenhagen, universitetsparken 15, dk-2100 copenhagen {\\\\o}, denmark and sars centre for marine molecular biology, university of bergen, thorm{\\\\o}hlensgate 55, n-5008 bergen, norway.\",\"{jaspar}, the open access database of transcription factor-binding profiles: new content and tools in the 2008 update.\",\"{jaspar} is a popular open-access database for matrix models describing {dna}-binding preferences for transcription factors and other {dna} patterns. with its third major release, {jaspar} has been expanded and equipped with additional functions aimed at both casual and power users. the heart of the {jaspar} database-the {jaspar} {core} sub-database-has increased by 12\\\\% in size, and three new specialized sub-databases have been added. new functions include clustering of matrix models by similarity, generation of random matrices by sampling from selected sets of existing models and a language-independent web service applications programming interface for matrix retrieval. {jaspar} is available at http://jaspar.genereg.net.\"emerging patterns of somatic mutations in cancer,\"{background}: site-specific transcription factors ({tfs}) are coordinators of developmental and physiological gene expression programs. their binding to cis-regulatory modules of target genes mediates the precise cell- and context-specific activation and repression of genes. the expression of {tfs} should therefore reflect the core expression program of each cell. {results}: we studied the expression dynamics of about 750 {tfs} using the available genomics resources in drosophila melanogaster. we find that 95\\\\% of these {tfs} are expressed at some point during embryonic development, with a peak roughly between 10 and 12 hours after egg laying, the core stages of organogenesis. we address the differential utilization of {dna}-binding domains in different developmental programs systematically in a spatio-temporal context, and show that the zinc finger class of {tfs} is predominantly early expressed, while homeobox {tfs} exhibit later expression in embryogenesis. {conclusions}: previous work, dissecting cis-regulatory modules during drosophila development, suggests that {tfs} are deployed in groups acting in a cooperative manner. in contrast, we find that there is rapid exchange of co-expressed partners amongst the fly {tfs}, at rates similar to the genome-wide dynamics of co-expression clusters. this suggests there may also be a high level of combinatorial complexity of {tfs} at cis-regulatory modules.\"\"{background}: applications of computational methods for predicting protein functional linkages are increasing. in recent years, several bacteria-specific methods for predicting linkages have been developed. the four major genomic context methods are: gene cluster, gene neighbor, rosetta stone, and phylogenetic profiles. these methods have been shown to be powerful tools and this paper provides guidelines for when each method is appropriate by exploring different features of each method and potential improvements offered by their combination. we also review many previous treatments of these prediction methods, use the latest available annotations, and offer a number of new observations. {results}: using escherichia coli k12 and bacillus subtilis, linkage predictions made by each of these methods were evaluated against three benchmarks: functional categories defined by {cog} and {kegg}, known pathways listed in {ecocyc}, and known operons listed in {regulondb}. each evaluated method had strengths and weaknesses, with no one method dominating all aspects of predictive ability studied. for functional categories, as previous studies have shown, the rosetta stone method was individually best at detecting linkages and predicting functions among proteins with shared {kegg} categories while the phylogenetic profile method was best for linkage detection and function prediction among proteins with common {cog} functions. differences in performance under {cog} versus {kegg} may be attributable to the presence of paralogs. better function prediction was observed when using a weighted combination of linkages based on reliability versus using a simple unweighted union of the linkage sets. for pathway reconstruction, 99 complete metabolic pathways in e. coli k12 (out of the 209 known, non-trivial pathways) and 193 pathways with 50\\\\% of their proteins were covered by linkages from at least one method. gene neighbor was most effective individually on pathway reconstruction, with 48 complete pathways reconstructed. for operon prediction, gene cluster predicted completely 59\\\\% of the known operons in e. coli k12 and 88\\\\% (333/418)in b. subtilis. comparing two versions of the e. coli k12 operon database, many of the unannotated predictions in the earlier version were updated to true predictions in the later version. using only linkages found by both gene cluster and gene neighbor improved the precision of operon predictions. additionally, as previous studies have shown, combining features based on intergenic region and protein function improved the specificity of operon prediction. {conclusion}: a common problem for computational methods is the generation of a large number of false positives that might be caused by an incomplete source of validation. by comparing two versions of a database, we demonstrated the dramatic differences on reported results. we used several benchmarks on which we have shown the comparative effectiveness of each prediction method, as well as provided guidelines as to which method is most appropriate for a given prediction task.\"\"department of molecular biology, scripps research institute, 10550 north torrey pines road, la jolla, california 92037, usa.\",optimal docking area: a new method for predicting protein–protein interaction sites,\"understanding energetics and mechanism of protein–protein association remains one of the biggest theoretical problems in structural biology. it is assumed that desolvation must play an essential role during the association process, and indeed protein–protein interfaces in obligate complexes have been found to be highly hydrophobic. however, the identification of protein interaction sites from surface analysis of proteins involved in non-obligate protein–protein complexes is more challenging. here we present optimal docking area ({oda}), a new fast and accurate method of analyzing a protein surface in search of areas with favorable energy change when buried upon protein–protein association. the method identifies continuous surface patches with optimal docking desolvation energy based on atomic solvation parameters adjusted for protein–protein docking. the procedure has been validated on the unbound structures of a total of 66 non-homologous proteins involved in non-obligate protein–protein hetero-complexes of known structure. optimal docking areas with significant low-docking surface energy were found in around half of the proteins. the \\'{oda} hot spots\\' detected in x-ray unbound structures were correctly located in the known protein–protein binding sites in 80\\\\% of the cases. the role of these low-surface-energy areas during complex formation is discussed. burial of these regions during protein–protein association may favor the complexed configurations with near-native interfaces but otherwise arbitrary orientations, thus driving the formation of an encounter complex. the patch prediction procedure is freely accessible at http://www.molsoft.com/oda and can be easily scaled up for predictions in structural proteomics. proteins 2005. {\\\\copyright} 2004 {wiley-liss}, inc.\"\"a computational method is proposed for inferring protein interactions from genome sequences on the basis of the observation that some pairs of interacting proteins have homologs in another organism fused into a single protein chain. searching sequences from many genomes revealed 6809 such putative protein-protein interactions {inescherichia} coli and 45,502 in yeast. many members of these pairs were confirmed as functionally related; computational filtering further enriches for interactions. some proteins have links to several other proteins; these coupled links appear to represent functional interactions such as complexes or pathways. experimentally confirmed interacting pairs are documented in a database of interacting proteins.\"\"motivation: there are two main areas of difficulty in homology modelling that are particularly important when sequence identity between target and template falls below 50\\\\%: sequence alignment and loop building. these problems become magnified with automatic modelling processes, as there is no human input to correct mistakes. as such we have benchmarked several stand-alone strategies that could be implemented in a workflow for automated high-throughput homology modelling. these include three new sequence-structure alignment programs: {3d}-coffee, staccato and {salign}, plus five homology modelling programs and their respective loop building methods: builder, nest, modeller, {segmod}/{encad} and {swiss-model}. the {sabmark} database provided 123 targets with at least five templates from the same {scop} family and sequence identities [\\\\&le;]50\\\\%. results: when using modeller as the common modelling program, {3d}-coffee outperforms staccato and {salign} using both multiple templates and the best single template, and across the sequence identity range 2050\\\\%. the mean model {rmsd} generated from {3d}-coffee using multiple templates is 15 and 28\\\\% (or using single templates, 3 and 13\\\\%) better than those generated by staccato and salign, respectively. {3d}-coffee gives equivalent modelling accuracy from multiple and single templates, but staccato and {salign} are more successful with single templates, their quality deteriorating as additional lower sequence identity templates are added. evaluating the different homology modelling programs, on average modeller performs marginally better in overall modelling than the others tested. however, on average nest produces the best loops with an 8\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ improvement by mean {rmsd} compared to the loops generated by builder. contact: r.m.jackson@leeds.ac.uk. supplementary information: supplementary data are available at bioinformatics online. 10.1093/bioinformatics/btm262\"\"most of the existing mathematical models for tumour growth and tumour-induced angiogenesis neglect blood flow. this is an important factor on which both nutrient and metabolite supply depend. in this paper we aim to address this shortcoming by developing a mathematical model which shows how blood flow and red blood cell heterogeneity influence the growth of systems of normal and cancerous cells. the model is developed in two stages. first we determine the distribution of oxygen in a native vascular network, incorporating into our model features of blood flow and vascular dynamics such as structural adaptation, complex rheology and red blood cell circulation. once we have calculated the oxygen distribution, we then study the dynamics of a colony of normal and cancerous cells, placed in such a heterogeneous environment. during this second stage, we assume that the vascular network does not evolve and is independent of the dynamics of the surrounding tissue. the cells are considered as elements of a cellular automaton, whose evolution rules are inspired by the different behaviour of normal and cancer cells. our aim is to show that blood flow and red blood cell heterogeneity play major roles in the development of such colonies, even when the red blood cells are flowing through the vasculature of normal, healthy tissue.\"\"this year, 2011, marks the forty-year anniversary of the statistical analysis of retinoblastoma that provided the first evidence that tumorigenesis can be initiated by as few as two mutations. this work provided the foundation for the two-hit hypothesis that explained the role of recessive tumour suppressor genes ({tsgs}) in dominantly inherited cancer susceptibility syndromes. however, four decades later, it is now known that even partial inactivation of tumour suppressors can critically contribute to tumorigenesis. here we analyse this evidence and propose a continuum model of {tsg} function to explain the full range of {tsg} mutations found in cancer.\"\"the initiation of transcription is regulated by transcription factors ({tfs}) binding to {dna} response elements ({res}). how do {tfs} recognize specific binding sites among the many similar ones available in the genome? recent research has illustrated that even a single nucleotide substitution can alter the selective binding of {tfs} to coregulators, that prior binding events can lead to selective {dna} binding, and that selectivity is influenced by the availability of binding sites in the genome. here, we combine structural insights with recent genomics screens to address the problem of {tf}–{dna} interaction specificity. the emerging picture of selective binding site sequence recognition and {tf} activation involves three major factors: the cellular network, protein and {dna} as dynamic conformational ensembles and the tight packing of multiple {tfs} and coregulators on stretches of regulatory {dna}. the classification of {tf} recognition mechanisms based on these factors impacts our understanding of how transcription initiation is regulated.\"\"pancreatic cancer is a disease caused by the accumulation of genetic alterations in specific genes. elucidation of the human genome sequence, in conjunction with technical advances in the ability to perform whole exome sequencing, have provided new insight into the mutational spectra characteristic of this lethal tumour type. most recently, exomic sequencing has been used to clarify the clonal evolution of pancreatic cancer as well as provide time estimates of pancreatic carcinogenesis, indicating that a long window of opportunity may exist for early detection of this disease while in the curative stage. moving forward, these mutational analyses indicate potential targets for personalised diagnostic and therapeutic intervention as well as the optimal timing for intervention based on the natural history of pancreatic carcinogenesis and progression.\"predicting protein interaction sites: binding hot-spots in protein-protein and protein-ligand interfaces.,\"{motivation}: protein assemblies are currently poorly represented in structural databases and their structural elucidation is a key goal in biology. here we analyse clefts in protein surfaces, likely to correspond to binding \\'hot-spots\\', and rank them according to sequence conservation and simple measures of physical properties including hydrophobicity, desolvation, electrostatic and van der waals potentials, to predict which are involved in binding in the native complex. {results}: the resulting differences between predicting binding-sites at protein-protein and protein-ligand interfaces are striking. there is a high level of prediction accuracy (< or =93\\\\%) for protein-ligand interactions, based on the following attributes: van der waals potential, electrostatic potential, desolvation and surface conservation. generally, the prediction accuracy for protein-protein interactions is lower, with the exception of enzymes. our results show that the ease of cleft desolvation is strongly predictive of interfaces and strongly maintained across all classes of protein-binding interface.\"estimating the human mutation rate using autozygosity in a founder population,and college of engineering, rumelifeneri yolu, 34450 sariyer istanbul, turkey. basic research program, saic-frederick, inc., laboratory of experimental and computational biology, nci-frederick, nci-frederick building 469, room 151, frederick, md 21702, usa.\",\"protein–protein interactions: organization, cooperativity and mapping in a bottom-up systems biology approach\",\"understanding and ultimately predicting protein associations is immensely important for functional genomics and drug design. here, we propose that binding sites have preferred organizations. first, the hot spots cluster within densely packed \\'hot regions\\'. within these regions, they form networks of interactions. thus, hot spots located within a hot region contribute cooperatively to the stability of the complex. however, the contributions of separate, independent hot regions are additive. moreover, hot spots are often already pre-organized in the unbound (free) protein states. describing a binding site through independent local hot regions has implications for binding site definition, design and parametrization for prediction. the compactness and cooperativity emphasize the similarity between binding and folding. this proposition is grounded in computation and experiment. it explains why summation of the interactions may over-estimate the stability of the complex. furthermore, statistically, charge-charge coupling of the hot spots is disfavored. however, since within the highly packed regions the solvent is screened, the electrostatic contributions are strengthened. thus, we propose a new description of protein binding sites: a site consists of (one or a few) self-contained cooperative regions. since the residue hot spots are those conserved by evolution, proteins binding multiple partners at the same sites are expected to use all or some combination of these regions.\"\"we noticed that disease-related amyloidogenic proteins and especially cellular prion proteins have the highest proportion of incompletely desolvated backbone h bonds among soluble proteins. such bonds are vulnerable to water attack and thus represent structural weaknesses. we have measured the adsorption of proteins onto phospholipid bilayers and found a strong correlation between the extent of underwrapping of backbone h bonds in the native structure of a protein and its extent of deposition on the bilayer: the less the h bond wrapping, the higher the propensity for protein-bilayer binding. these observations support the proposition that soluble proteins with amyloidogenic propensity and membrane proteins share a pervasive building motif: the underwrapped h bonds. whereas in membrane proteins, this motif does not signal a structural vulnerability, in soluble proteins, it is responsible for their reactivity. 10.1073/pnas.0335642100\"\"computational identification of prognostic biomarkers capable of withstanding follow-up validation efforts is still an open challenge in cancer research. for instance, several gene expression profiles analysis methods have been developed to identify gene signatures that can classify cancer sub-phenotypes associated with poor prognosis. however, signatures originating from independent studies show only minimal overlap and perform poorly when classifying datasets other than the ones they were generated from. in this paper, we propose a computational systems biology approach that can infer robust prognostic markers by identifying upstream master regulators, causally related to the presentation of the phenotype of interest. such a strategy effectively extends and complements other existing methods and may help further elucidate the molecular mechanisms of the observed pathophysiological phenotype. results show that inferred regulators substantially outperform canonical gene signatures both on the original dataset and across distinct datasets.\"whole-genome sequencing of liver cancers identifies etiological influences on mutation patterns and recurrent mutations in chromatin regulators,the patterns and dynamics of genomic instability in metastatic pancreatic cancer,\"the analysis and usage of biological data is hindered by the spread of information across multiple repositories and the difficulties posed by different nomenclature systems and storage formats. in particular, there is an important need for data unification in the study and use of protein-protein interactions. without good integration strategies, it is difficult to analyze the whole set of available data and its properties. we introduce {biana} (biologic interactions and network analysis), a tool for biological information integration and network management. {biana} is a python framework designed to achieve two major goals: i) the integration of multiple sources of biological information, including biological entities and their relationships, and ii) the management of biological information as a network where entities are nodes and relationships are edges. moreover, {biana} uses properties of proteins and genes to infer latent biomolecular relationships by transferring edges to entities sharing similar properties. {biana} is also provided as a plugin for cytoscape, which allows users to visualize and interactively manage the data. a web interface to {biana} providing basic functionalities is also available. the software can be downloaded under {gnu} {gpl} license from {http://sbi.imim.es/web/biana}.php. {biana}\\'s approach to data unification solves many of the nomenclature issues common to systems dealing with biological data. {biana} can easily be extended to handle new specific data repositories and new specific data types. the unification protocol allows {biana} to be a flexible tool suitable for different user requirements: non-expert users can use a suggested unification protocol while expert users can define their own specific unification rules.\"\"previous studies have suggested that nature is restricted to about 1,000 protein folds to perform a great diversity of functions. here, we use protein interaction data from different sources and three-dimensional structures to suggest that the total number of interaction types is also limited, and estimate that most interactions in nature will conform to one of about 10,000 types. we currently know fewer than 2,000, and at the present rate of structure determination, it will be more than 20 years before we know a full representative set.\"england.\",{scop}: a structural classification of proteins database for the investigation of sequences and structures.,\"the 1000 genomes project aims to provide a deep characterization of human genome sequence variation as a foundation for investigating the relationship between genotype and phenotype. here we present results of the pilot phase of the project, designed to develop and compare different strategies for genome-wide sequencing with high-throughput platforms. we undertook three projects: low-coverage whole-genome sequencing of 179 individuals from four populations; high-coverage sequencing of two mother-father-child trios; and exon-targeted sequencing of 697 individuals from seven populations. we describe the location, allele frequency and local haplotype structure of approximately 15 million single nucleotide polymorphisms, 1 million short insertions and deletions, and 20,000 structural variants, most of which were previously undescribed. we show that, because we have catalogued the vast majority of common variation, over 95\\\\% of the currently accessible variants found in any individual are present in this data set. on average, each person is found to carry approximately 250 to 300 loss-of-function variants in annotated genes and 50 to 100 variants previously implicated in inherited disorders. we demonstrate how these results can be used to inform association and functional studies. from the two trios, we directly estimate the rate of de novo germline base substitution mutations to be approximately 10(-8) per base pair per generation. we explore the data with regard to signatures of natural selection, and identify a marked reduction of genetic variation in the neighbourhood of genes, due to selection at linked sites. these methods and public data will support the next phase of human genetic research.\"\"the gene expression omnibus ({geo}) project was initiated in response to the growing demand for a public repository for high-throughput gene expression data. {geo} provides a flexible and open design that facilitates submission, storage and retrieval of heterogeneous data sets from high-throughput gene expression and genomic hybridization experiments. {geo} is not intended to replace in house gene expression databases that benefit from coherent data sets, and which are constructed to facilitate a particular analytic method, but rather complement these by acting as a tertiary, central data distribution hub. the three central data entities of {geo} are platforms, samples and series, and were designed with gene expression and genomic hybridization experiments in mind. a platform is, essentially, a list of probes that define what set of molecules may be detected. a sample describes the set of molecules that are being probed and references a single platform used to generate its molecular abundance data. a series organizes samples into the meaningful data sets which make up an experiment. the {geo} repository is publicly accessible through the world wide web at http://www.ncbi.nlm.nih.gov/geo.\"\"interacting or functionally related protein families tend to have similar phylogenetic trees. based on this observation, techniques have been developed to predict interaction partners. the observed degree of similarity between the phylogenetic trees of two proteins is the result of many different factors besides the actual interaction or functional relationship between them. such factors influence the performance of interaction predictions. one aspect that can influence this similarity is related to the fact that a given protein interacts with many others, and hence it must adapt to all of them. accordingly, the interaction or coadaptation signal within its tree is a composite of the influence of all of the interactors. here, we introduce a new estimator of coevolution to overcome this and other problems. instead of relying on the individual value of tree similarity between two proteins, we use the whole network of similarities between all of the pairs of proteins within a genome to reassess the similarity of that pair, thereby taking into account its coevolutionary context. we show that this approach offers a substantial improvement in interaction prediction performance, providing a degree of accuracy/coverage comparable with, or in some cases better than, that of experimental techniques. moreover, important information on the structure, function, and evolution of macromolecular complexes can be inferred with this methodology.\"the clonal evolution of tumor cell populations.,\"it is proposed that most neoplasms arise from a single cell of origin, and tumor progression results from acquired genetic variability within the original clone allowing sequential selection of more aggressive sublines. tumor cell populations are apparently more genetically unstable than normal cells, perhaps from activation of specific gene loci in the neoplasm, continued presence of carcinogen, or even nutritional deficiencies within the tumor. the acquired genetic insta0ility and associated selection process, most readily recognized cytogenetically, results in advanced human malignancies being highly individual karyotypically and biologically. hence, each patient\\'s cancer may require individual specific therapy, and even this may be thwarted by emergence of a genetically variant subline resistant to the treatment. more research should be directed toward understanding and controlling the evolutionary process in tumors before it reaches the late stage usually seen in clinical cancer.\"faculty of medicine, university of oslo, po box 1112 blindern, 0317 oslo, norway. philippe.collas@medisin.uio.no\",\"chop it, {chip} it, check it: the current status of chromatin immunoprecipitation.\",\"our understanding of the significance of interactions of proteins with {dna} in the context of gene expression, cell differentiation or to some extent disease has immensely been enhanced by the advent of chromatin immunoprecipitation ({chip}). {chip} has been widely used to map the localization of post-translationally modified histones or histone variants on the genome or on a specific gene locus, or to map the association of transcription factors or chromatin modifying enzymes to the genome. in spite of its power, {chip} is a cumbersome procedure and typically requires large numbers of cells. this review outlines variations elaborated on the {chip} assay to shorten the procedure, make it suitable for small cell numbers and unravel the multiplicity of histone modifications on a single locus. in addition, the combination of {chip} assays with {dna} microarray and high-throughput sequencing technologies has in recent years enabled the profiling of histone modifications and transcription factor occupancy sites throughout the genome and in a high-resolution manner throughout a genomic region of interest. we also review applications of {chip} to the mapping of histone modifications or transcription factor binding at the genome-wide level. finally, we speculate on future perspectives opened by the combination of emerging {chip}-related technologies.\"meguro-ku, tokyo 152-8552, japan. ryota.suzuki@is.titech.ac.jp\",pvclust: an r package for assessing the uncertainty in hierarchical clustering,\"summary: pvclust is an add-on package for a statistical software r to assess the uncertainty in hierarchical cluster analysis. pvclust can be used easily for general statistical problems, such as {dna} microarray analysis, to perform the bootstrap analysis of clustering, which has been popular in phylogenetic analysis. pvclust calculates probability values (p-values) for each cluster using bootstrap resampling techniques. two types of p-values are available: approximately unbiased ({au}) p-value and bootstrap probability ({bp}) value. multiscale bootstrap resampling is used for the calculation of {au} p-value, which has superiority in bias over {bp} value calculated by the ordinary bootstrap resampling. in addition the computation time can be enormously decreased with parallel computing option.\"baltimore, maryland 21287, usa.\",development of human protein reference database as an initial platform for approaching systems biology in humans,\"an international, peer-reviewed genome sciences journal featuring outstanding original research that offers novel insights into the biology of all organisms\"via salvador allende, i-84081 baronissi (sa) italy.\",{pops}: a fast algorithm for solvent accessible surface areas at atomic and residue level,\"{pops} (parameter {optimsed} surfaces) is a new method to calculate solvent accessible surface areas, which is based on an empirically parameterisable analytical formula and fast to compute. atomic and residue areas (the latter represented by a single sphere centered on the cα atom of amino acids and at the p atom of nucleotides) have been optimised versus accurate all-atom methods. the parameterisation has been derived from a selected dataset of proteins and nucleic acids of different sizes and topologies. the residue based approach {pops}-r, has been devised as a useful tool for the analysis of large macromolecular assemblies like the ribosome and it is specially suited for the refinement of low resolution structures. {pops}-r also allows for estimates of the loss of free energy of solvation upon complex formation, which should be particularly useful for the design of new protein–protein and protein–nucleic acid complexes. the program {pops} is available at {http://mathbio.nimr.mrc.ac.uk/\\\\~{}ffranca/pops} and at the mirror site http://www.cs.vu.nl/\\\\~{}ibivu/programs/popswww.\"\"laboratoire d\\'enzymologie et biochimie structurales, cnrs, gif-sur-yvette, france; european bioinformatics institute (ebi-embl), wellcome trust genome campus, hinxton, cambridge, united kingdom; center for advanced research in biotechnology, university of maryland biotechnology institute, rockville, maryland; san diego supercomputer center, ucsd 0505, la jolla, california; structural bioinformatics, biochemistry building, department of biological sciences, imperial college, london, united kingdom; biomedical engineering, boston university, boston, massachusetts; bioinformatics laboratory, department of applied mathematics and statistics, suny at stony brook, stony brook, new york; service de conformation de macromol\\\\\\'{e}cules biologiques et bioinformatique, campus plaine-bc6, universit\\\\\\'{e} libre de bruxelles, brusssels, belgium\",{capri}: a critical assessment of {predicted} interactions,\"{capri} is a communitywide experiment to assess the capacity of protein-docking methods to predict protein–protein interactions. nineteen groups participated in rounds 1 and 2 of {capri} and submitted blind structure predictions for seven protein–protein complexes based on the known structure of the component proteins. the predictions were compared to the unpublished x-ray structures of the complexes. we describe here the motivations for launching {capri}, the rules that we applied to select targets and run the experiment, and some conclusions that can already be drawn. the results stress the need for new scoring functions and for methods handling the conformation changes that were observed in some of the target systems. {capri} has already been a powerful drive for the community of computational biologists who development docking algorithms. we hope that this issue of proteins will also be of interest to the community of structural biologists, which we call upon to provide new targets for future rounds of {capri}, and to all molecular biologists who view protein–protein recognition as an essential process. proteins 2003;52:2–9. {\\\\copyright} 2003 {wiley-liss}, inc.\"\"the physical and chemical properties of domain–domain interactions have been analysed in two-domain proteins selected from the protein classification, {cath}. the two-domain structures were divided into those derived from (i) monomeric proteins, or (ii) oligomeric or complexed proteins. the size, polarity, hydrogen bonding and packing of the intra-chain domain interface were calculated for both sets of two-domain structures. the results were compared with inter-chain interface parameters from permanent and non-obligate protein–protein complexes. in general, the intra-chain domain and inter-chain interfaces were remarkably similar. many of the intra-chain interface properties are intermediate between those calculated for permanent and non-obligate inter-chain complexes. residue interface propensities were also found to be very similar, with hydrophobic residues playing a major role, together with positively charged arginine residues. in addition, the residue composition of the domain interfaces were found to be more comparable with domain surfaces than domain cores. the implications of these results for domain swapping and protein folding are discussed.\"gower street, london, wc1e 6bt, england.\",analysis of protein-protein interaction sites using surface patches,\"protein-protein interaction sites in complexes of known structure are characterised using a series of parameters to evaluate what differentiates them from other sites on the protein surface. surface patches are defined in protomers from a data set of 28 homo-dimers, 20 different hetero-complexes (segregated into large and small protomers), and antigens from six antibody-antigen complexes. six parameters (solvation potential, residue interface propensity, hydrophobicity, planarity, protrusion and accessible surface area) are calculated for the observed interface patch and all other surface patches defined on each protein. a ranking of the observed interface, relative to all other possible patches, is calculated. with this approach it becomes possible to analyse the distribution of the rankings of all the observed patches, relative to all other surface patches, for each data set. for each type of complex, none of the parameters were definitive, but the majority showed trends for the observed interface to be distinguished from other surface patches.\"university of california at san francisco, 94143, usa.\",the hallmarks of cancer.,il 60637, usa.\",insufficiently dehydrated hydrogen bonds as determinants of protein interactions,\"the prediction of binding sites and the understanding of interfaces associated with protein complexation remains an open problem in molecular biophysics. this work shows that a crucial factor in predicting and rationalizing protein–protein interfaces can be inferred by assessing the extent of intramolecular desolvation of backbone hydrogen bonds in monomeric structures. our statistical analysis of native structures shows that, in the majority of soluble proteins, most backbone hydrogen bonds are thoroughly wrapped intramolecularly by nonpolar groups except for a few ones. these latter underwrapped hydrogen bonds may be dramatically stabilized by removal of water. this fact implies that packing defects are  ” sticky” in a way that decisively contributes to determining the binding sites for proteins, as an examination of numerous complexes demonstrates.\"urbana 61801, usa.\",{vmd: visual molecular dynamics.},\"{ vmd is a molecular graphics program designed for the display and analysis of molecular assemblies, in particular biopolymers such as proteins and nucleic acids. vmd can simultaneously display any number of structures using a wide variety of rendering styles and coloring methods. molecules are displayed as one or more \"\"representations,\"\" in which each representation embodies a particular rendering method and coloring scheme for a selected subset of atoms. the atoms displayed in each representation are chosen using an extensive atom selection syntax, which includes boolean operators and regular expressions. vmd provides a complete graphical user interface for program control, as well as a text interface using the tcl embeddable parser to allow for complex scripts with variable substitution, control loops, and function calls. full session logging is supported, which produces a vmd command script for later playback. high-resolution raster images of displayed molecules may be produced by generating input scripts for use by a number of photorealistic image-rendering applications. vmd has also been expressly designed with the ability to animate molecular dynamics (md) simulation trajectories, imported either from files or from a direct connection to a running md simulation. vmd is the visualization component of mdscope, a set of tools for interactive problem solving in structural biology, which also includes the parallel md program namd, and the mdcomm software used to connect the visualization and simulation programs. vmd is written in c++, using an object-oriented design; the program, including source code and extensive documentation, is freely available via anonymous ftp and through the world wide web. }\"transcription as a source of genome instability,\"alterations in genome sequence and structure contribute to somatic disease, affect the fitness of subsequent generations and drive evolutionary processes. the crucial roles of highly accurate replication and efficient repair in maintaining overall genome integrity are well-known, but the more localized stability costs that are associated with transcribing {dna} into {rna} molecules are less appreciated. here we review the diverse ways in which the essential process of transcription alters the underlying {dna} template and thereby modifies the genetic landscape.\"\"both {dna} methylation and histone modification are involved in establishing patterns of gene repression during development. certain forms of histone methylation cause local formation of heterochromatin, which is readily reversible, whereas {dna} methylation leads to stable long-term repression. it has recently become apparent that {dna} methylation and histone modification pathways can be dependent on one another, and that this crosstalk can be mediated by biochemical interactions between {set} domain histone methyltransferases and {dna} methyltransferases. relationships between {dna} methylation and histone modification have implications for understanding normal development as well as somatic cell reprogramming and tumorigenesis.\"mapping copy number variation by population-scale genome sequencing.,\"genomic structural variants ({svs}) are abundant in humans, differing from other forms of variation in extent, origin and functional impact. despite progress in {sv} characterization, the nucleotide resolution architecture of most {svs} remains unknown. we constructed a map of unbalanced {svs} (that is, copy number variants) based on whole genome {dna} sequencing data from 185 human genomes, integrating evidence from complementary {sv} discovery approaches with extensive experimental validations. our map encompassed 22,025 deletions and 6,000 additional {svs}, including insertions and tandem duplications. most {svs} (53\\\\%) were mapped to nucleotide resolution, which facilitated analysing their origin and functional impact. we examined numerous whole and partial gene deletions with a genotyping approach and observed a depletion of gene disruptions amongst high frequency deletions. furthermore, we observed differences in the size spectra of {svs} originating from distinct formation mechanisms, and constructed a map of {sv} hotspots formed by common mechanisms. our analytical framework and {sv} map serves as a resource for sequencing-based association studies.\"\"systematic chromatin immunoprecipitation ({chip}-chip) experiments have become a central technique for mapping transcriptional interactions in model organisms and humans. however, measurement of chromatin binding does not necessarily imply regulation, and binding may be difficult to detect if it is condition or cofactor dependent. to address these challenges, we present an approach for reliably assigning transcription factors ({tfs}) to target genes that integrates many lines of direct and indirect evidence into a single probabilistic model. using this approach, we analyze publicly available {chip}-chip binding profiles measured for yeast {tfs} in standard conditions, showing that our model interprets these data with significantly higher accuracy than previous methods. pooling the high-confidence interactions reveals a large network containing 363 significant sets of factors ({tf} modules) that cooperate to regulate common target genes. in addition, the method predicts 980 novel binding interactions with high confidence that are likely to occur in so-far untested conditions. indeed, using new {chip}-chip experiments we show that predicted interactions for the factors rpn4p and pdr1p are observed only after treatment of cells with methyl-methanesulfonate, a {dna}-damaging agent. we outline the first approach for consistently integrating all available evidences for {tf}–target interactions and we comprehensively identify the resulting {tf} module hierarchy. prioritizing experimental conditions for each factor will be especially important as increasing numbers of {chip}-chip assays are performed in complex organisms such as humans, for which  ” standard conditions” are ill defined. transcription factors ({tfs}) bind close to their target genes for regulating transcript levels depending on cellular conditions. each gene may be regulated differently from others through the binding of specific groups of {tfs} ({tf} modules). recently, a wide variety of large-scale measurements about transcriptional networks has become available. here the authors present a framework for consistently integrating all of this evidence to systematically determine the precise set of genes directly regulated by each {tf} (i.e., {tf}–target interactions). the framework is applied to the yeast saccharomyces cerevisiae using seven distinct sources of evidences to score all possible {tf}–target interactions in this organism. subsequently, the authors employ another newly developed algorithm to reveal {tf} modules based on the top 5,000 {tf}–target interactions, yielding more than 300 {tf} modules. the new scoring scheme for {tf}–target interactions allows predicting the binding of {tfs} under so-far untested conditions, which is demonstrated by experimentally verifying interactions for two {tfs} (pdr1p, rpn4p). importantly, the new methods (scoring of {tf}–target interactions and {tf} module identification) are scalable to much larger datasets, making them applicable to future studies in humans, which are thought to have substantially larger numbers of {tf}–target interactions.\"pareto optimality, and the geometry of phenotype space\",\"biological systems that perform multiple tasks face a fundamental trade-off: a given phenotype cannot be optimal at all tasks. here we ask how trade-offs affect the range of phenotypes found in nature. using the pareto front concept from economics and engineering, we find that best-trade-off phenotypes are weighted averages of archetypes--phenotypes specialized for single tasks. for two tasks, phenotypes fall on the line connecting the two archetypes, which could explain linear trait correlations, allometric relationships, as well as bacterial gene-expression patterns. for three tasks, phenotypes fall within a triangle in phenotype space, whose vertices are the archetypes, as evident in morphological studies, including on darwin\\'s finches. tasks can be inferred from measured phenotypes based on the behavior of organisms nearest the archetypes.\"\"most of the studies characterizing {dna} methylation patterns have been restricted to particular genomic loci in a limited number of human samples and pathological conditions. herein, we present a compromise between an extremely comprehensive study of a human sample population with an intermediate level of resolution of {cpgs} at the genomic level. we obtained a {dna} methylation fingerprint of 1628 human samples in which we interrogated 1505 {cpg} sites. the {dna} methylation patterns revealed show this epigenetic mark to be critical in tissue-type definition and stemness, particularly around transcription start sites that are not within a {cpg} island. for disease, the generated {dna} methylation fingerprints show that, during tumorigenesis, human cancer cells underwent a progressive gain of promoter {cpg}-island hypermethylation and a loss of {cpg} methylation in {non-cpg}-island promoters. although transformed cells are those in which {dna} methylation disruption is more obvious, we observed that other common human diseases, such as neurological and autoimmune disorders, had their own distinct {dna} methylation profiles. most importantly, we provide proof of principle that the {dna} methylation fingerprints obtained might be useful for translational purposes by showing that we are able to identify the tumor type origin of cancers of unknown primary origin ({cups}). thus, the {dna} methylation patterns identified across the largest spectrum of samples, tissues, and diseases reported to date constitute a baseline for developing higher-resolution {dna} methylation maps and provide important clues concerning the contribution of {cpg} methylation to tissue identity and its changes in the most prevalent human diseases.\"\"department of biological chemistry, the weizmann institute of science, rehovot 76100, israel; department of physics of complex systems, the weizmann institute of science, rehovot 76100, israel\",cluster conservation as a novel tool for studying protein-protein interactions evolution,\"protein-protein interactions networks has come to be a buzzword associated with nets containing edges that represent a pair of interacting proteins (e.g. hormone-receptor, enzyme-inhibitor, antigen-antibody, and a subset of multichain biological machines). yet, each such interaction composes its own unique network, in which vertices represent amino acid residues, and edges represent atomic contacts. recent studies have shown that analyses of the data encapsulated in these detailed networks may impact predictions of structure-function correlation. here, we study homologous families of protein-protein interfaces, which share the same fold but vary in sequence. in this context, we address what properties of the network are shared among relatives with different sequences (and hence different atomic interactions) and which are not. herein, we develop the general mathematical framework needed to compare the modularity of homologous networks. we then apply this analysis to the structural data of a few interface families, including hemoglobin ?-?, growth hormone-receptor, and serine protease-inhibitor. our results suggest that interface modularity is an evolutionarily conserved property. hence, protein-protein interfaces can be clustered down to a few modules, with the boundaries being evolutionarily conserved along homologous complexes. this suggests that protein engineering of protein-protein binding sites may be simplified by varying each module, but retaining the overall modularity of the interface. proteins 2007. {\\\\copyright} 2007 {wiley-liss}, inc.\"germany.\",why do hubs in the yeast protein interaction network tend to be essential: reexamining the connection between the network topology and essentiality,\"the centrality-lethality rule, which notes that high-degree nodes in a protein interaction network tend to correspond to proteins that are essential, suggests that the topological prominence of a protein in a protein interaction network may be a good predictor of its biological importance. even though the correlation between degree and essentiality was confirmed by many independent studies, the reason for this correlation remains illusive. several hypotheses about putative connections between essentiality of hubs and the topology of protein–protein interaction networks have been proposed, but as we demonstrate, these explanations are not supported by the properties of protein interaction networks. to identify the main topological determinant of essentiality and to provide a biological explanation for the connection between the network topology and essentiality, we performed a rigorous analysis of six variants of the genomewide protein interaction network for saccharomyces cerevisiae obtained using different techniques. we demonstrated that the majority of hubs are essential due to their involvement in essential complex biological modules, a group of densely connected proteins with shared biological function that are enriched in essential proteins. moreover, we rejected two previously proposed explanations for the centrality-lethality rule, one relating the essentiality of hubs to their role in the overall network connectivity and another relying on the recently published essential protein interactions model. analysis of protein interaction networks in the budding yeast saccharomyces cerevisiae has revealed that a small number of proteins, the so-called hubs, interact with a disproportionately large number of other proteins. furthermore, many hub proteins have been shown to be essential for survival of the cell—that is, in optimal conditions, yeast cannot grow and multiply without them. this relation between essentiality and the number of neighbors in the protein–protein interaction network has been termed the centrality-lethality rule. however, why are such hubs essential? jeong and colleagues [1] suggested that overrepresentation of essential proteins among high-degree nodes can be attributed to the central role that hubs play in mediating interactions among numerous, less connected proteins. another view, proposed by he and zhang, suggested that that the majority of proteins are essential due to their involvement in one or more essential protein–protein interactions that are distributed uniformly at random along the network edges [2]. we find that none of the above reasons determines essentiality. instead, the majority of hubs are essential due to their involvement in essential complex biological modules, a group of densely connected proteins with shared biological function that are enriched in essential proteins. this study sheds new light on the topological complexity of protein interaction networks.\"and department of mathematics and computer science, freie universit\\\\\"\"{a}t berlin. sven.rahmann@cebitec.uni-bielefeld.de\",on the power of profiles for transcription factor binding site detection,\"{background}:high-throughput measurement of transcript intensities using affymetrix type oligonucleotide microarrays has produced a massive quantity of data during the last decade. different preprocessing techniques exist to convert the raw signal intensities measured by these chips into gene expression estimates. although these techniques have been widely benchmarked in the context of differential gene expression analysis, there are only few examples where their performance has been assessed in respect to coexpression-based studies such as sample {classification.results}:in the present paper we benchmark the three most used normalization procedures ({mas5}, {rma} and {gcrma}) in the context of inter-array correlation analysis, confirming and extending the finding that {rma} and {gcrma} consistently overestimate sample similarity upon normalization. we determine that median polish summarization is responsible for generating a large proportion of these over-similarity artifacts. furthermore, we show that most affected probesets show also internal signal disagreement, and tend to be composed by individual probes hitting different gene transcripts. we finally provide a correction to the {rma}/{gcrma} summarization procedure that massively reduces inter-array correlation artifacts, without affecting the detection of differentially expressed {genes.conclusions}:we propose {trma} as a modification of {rma} to normalize microarray experiments for correlation-based analysis.\"\"the regulatory sequence analysis tools ({rsat}, http://rsat.ulb.ac.be/rsat/) is a software suite that integrates a wide collection of modular tools for the detection of cis-regulatory elements in genome sequences. the suite includes programs for sequence retrieval, pattern discovery, phylogenetic footprint detection, pattern matching, genome scanning and feature map drawing. random controls can be performed with random gene selections or by generating random sequences according to a variety of background models (bernoulli, markov). beyond the original word-based pattern-discovery tools (oligo-analysis and dyad-analysis), we recently added a battery of tools for matrix-based detection of cis-acting elements, with some original features (adaptive background models, markov-chain estimation of p-values) that do not exist in other matrix-based scanning tools. the web server offers an intuitive interface, where each program can be accessed either separately or connected to the other tools. in addition, the tools are now available as web services, enabling their integration in programmatic workflows. genomes are regularly updated from various genome repositories ({ncbi} and {ensembl}) and 682 organisms are currently supported. since 1998, the tools have been used by several hundreds of researchers from all over the world. several predictions made with {rsat} were validated experimentally and published.\"\"therapeutic proteins such as antibodies constitute the most rapidly growing class of pharmaceuticals for use in diverse clinical settings including cancer, chronic inflammatory diseases, kidney transplantation, cardiovascular medicine, and infectious diseases. unfortunately, they tend to aggregate when stored under the concentrated conditions required in their usage. aggregation leads to a decrease in antibody activity and could elicit an immunological response. using full antibody atomistic molecular dynamics simulations, we identify the antibody regions prone to aggregation by using a technology that we developed called spatial aggregation propensity ({sap}). {sap} identifies the location and size of these aggregation prone regions, and allows us to perform target mutations of those regions to engineer antibodies for stability. we apply this method to therapeutic antibodies and demonstrate the significantly enhanced stability of our mutants compared with the wild type. the technology described here could be used to incorporate developability in a rational way during the screening of antibodies in the discovery phase for several diseases.\"whole-exome sequencing of circulating tumor cells provides a window into metastatic prostate cancer,cantoblanco, e-28049 madrid, spain.\",similarity of phylogenetic trees as indicator of protein-protein interaction.,\"deciphering the network of protein interactions that underlines cellular operations has become one of the main tasks of proteomics and computational biology. recently, a set of bioinformatics approaches has emerged for the prediction of possible interactions by combining sequence and genomic information. even though the initial results are very promising, the current methods are still far from perfect. we propose here a new way of discovering possible protein-protein interactions based on the comparison of the evolutionary distances between the sequences of the associated protein families, an idea based on previous observations of correspondence between the phylogenetic trees of associated proteins in systems such as ligands and receptors. here, we extend the approach to different test sets, including the statistical evaluation of their capacity to predict protein interactions. to demonstrate the possibilities of the system to perform large-scale predictions of interactions, we present the application to a collection of more than 67 000 pairs of e.coli proteins, of which 2742 are predicted to correspond to interacting proteins.\"san francisco 94143-1204.\",cooperativity in protein-folding kinetics.,\"how does a protein find its native state without a globally exhaustive search? we propose the \"\"{hz}\"\" (hydrophobic zipper) hypothesis: hydrophobic contacts act as constraints that bring other contacts into spatial proximity, which then further constrain and zip up the next contacts, etc. in contrast to helix-coil cooperativity, {hz}-heteropolymer collapse cooperativity is driven by nonlocal interactions, causes sheet and irregular conformations in addition to helices, leads to secondary structures concurrently with early hydrophobic core formation, is much more sequence dependent than helix-coil processes, and involves compact intermediate states that have much secondary--but little tertiary--structure. hydrophobic contacts in the 1992 protein data bank have the type of \"\"topological localness\"\" predicted by the hypothesis. the {hz} paths for amino acid sequences that mimic crambin and bovine pancreatic trypsin inhibitor are quickly found by computer; the best configurations thus reached have single hydrophobic cores that are within about 3 kcal/mol of the global minimum. this hypothesis shows how proteins could find globally optimal states without exhaustive search.\"\"transcription factor ({tf}) perturbation experiments give valuable insights into gene regulation. genome-scale evidence from microarray measurements may be used to identify regulatory interactions between {tfs} and targets. recently, hu and colleagues published a comprehensive study covering 269 {tf} knockout mutants for the yeast saccharomyces cerevisiae. however, the information that can be extracted from this valuable dataset is limited by the method employed to process the microarray data. here, we present a reanalysis of the original data using improved statistical techniques freely available from the {bioconductor} project. we identify over 100 000 differentially expressed genes—nine times the total reported by hu et al. we validate the biological significance of these genes by assessing their functions, the occurrence of upstream {tf}-binding sites, and the prevalence of protein–protein interactions. the reanalysed dataset outperforms the original across all measures, indicating that we have uncovered a vastly expanded list of relevant targets. in summary, this work presents a high-quality reanalysis that maximizes the information contained in the hu et al. compendium. the dataset is available from {arrayexpress} (accession: {e-mtab}-109) and it will be invaluable to any scientist interested in the yeast transcriptional regulatory system.\"\"even when there is agreement on what measure a protein multiple structure alignment should be optimizing, finding the optimal alignment is computationally prohibitive. one approach used by many previous methods is aligned fragment pair chaining, where short structural fragments from all the proteins are aligned against each other optimally, and the final alignment chains these together in geometrically consistent ways. ye and godzik have recently suggested that adding geometric flexibility may help better model protein structures in a variety of contexts. we introduce the program matt (multiple alignment with translations and twists), an aligned fragment pair chaining algorithm that, in intermediate steps, allows local flexibility between fragments: small translations and rotations are temporarily allowed to bring sets of aligned fragments closer, even if they are physically impossible under rigid body transformations. after a dynamic programming assembly guided by these \"\"bent\"\" alignments, geometric consistency is restored in the final step before the alignment is output. matt is tested against other recent multiple protein structure alignment programs on the popular homstrad and {sabmark} benchmark datasets. matt\\'s global performance is competitive with the other programs on homstrad, but outperforms the other programs on {sabmark}, a benchmark of multiple structure alignments of proteins with more distant homology. on both datasets, matt demonstrates an ability to better align the ends of alpha-helices and beta-strands, an important characteristic of any structure alignment program intended to help construct a structural template library for threading approaches to the inverse protein-folding problem. the related question of whether matt alignments can be used to distinguish distantly homologous structure pairs from pairs of proteins that are not homologous is also considered. for this purpose, a p-value score based on the length of the common core and average root mean squared deviation ({rmsd}) of matt alignments is shown to largely separate decoys from homologous protein structures in the {sabmark} benchmark dataset. we postulate that matt\\'s strong performance comes from its ability to model proteins in different conformational states and, perhaps even more important, its ability to model backbone distortions in more distantly related proteins.\"\"the evolution of multicellularity required the suppression of cancer. if every cell has some chance of becoming cancerous, large, long-lived organisms should have an increased risk of developing cancer compared with small, short-lived organisms. the lack of correlation between body size and cancer risk is known as peto\\'s paradox. animals with 1000 times more cells than humans do not exhibit an increased cancer risk, suggesting that natural mechanisms can suppress cancer 1000 times more effectively than is done in human cells. because cancer has proven difficult to cure, attention has turned to cancer prevention. in this review, similar to pharmaceutical companies mining natural products, we seek to understand how evolution has suppressed cancer to develop ultimately improved cancer prevention in humans.\"campus ring 1, 28759, bremen, germany, m.zacharias@jacobs-university.de.\",the interface of protein-protein complexes: analysis of contacts and prediction of interactions,\"specific protein-protein interactions are essential for cellular functions. experimentally determined three-dimensional structures of protein-protein complexes offer the possibility to characterize binding interfaces in terms of size, shape and packing density. comparison with crystal-packing interfaces representing nonspecific protein-protein contacts gives insight into how specific binding differs from nonspecific low-affinity binding. an overview is given on empirical structural rules for specific protein-protein recognition derived from known complex structures. although single parameters such as interface size, shape or surface complementary show clear trends for different interface types, each parameter alone is insufficient to fully distinguish between specific versus crystal-packing contacts. a combination of interface parameters is, however, well suited to characterize a specific interface. this knowledge provides us with the essential ingredients that make up a specific protein recognition site. it is also of great value for the prediction of protein binding sites and for the evaluation of predicted complex structures.\"\"center for bioinformatics, the university of kansas, lawrence, kansas; centre de biochimie structurale (cnrs umr 5048, inserm umr u554, umi), montpellier, france; department of molecular biosciences, the university of kansas, lawrence, kansas\",{dockground} system of databases for protein recognition studies: unbound structures for docking,\"computational docking approaches are important as a source of protein-protein complexes structures and as a means to understand the principles of protein association. a key element in designing better docking approaches, including search procedures, potentials, and scoring functions is their validation on experimentally determined structures. thus, the databases of such structures (benchmark sets) are important. the previous, first release of the {dockground} resource (douguet et al., bioinformatics 2006; 22:2612-2618) implemented a comprehensive database of cocrystallized (bound) protein-protein complexes in a relational database of annotated structures. the current release adds important features to the set of bound structures, such as regularly updated downloadable datasets: automatically generated nonredundant set, built according to most common criteria, and a manually curated set that includes only biological nonobligate complexes along with a number of additional useful characteristics. the main focus of the current release is unbound (experimental and simulated) protein-protein complexes. complexes from the bound dataset are used to identify crystallized unbound analogs. if such analogs do not exist, the unbound structures are simulated by rotamer library optimization. thus, the database contains comprehensive sets of complexes suitable for large scale benchmarking of docking algorithms. advanced methodologies for simulating unbound conformations are being explored for the next release. the future releases will include datasets of modeled protein-protein complexes, and systematic sets of docking decoys obtained by different docking algorithms. the growing {dockground} resource is designed to become a comprehensive public environment for developing and validating new docking methodologies. proteins 2007. {\\\\copyright} 2007 {wiley-liss}, inc.\"\"{background}:evolutionary conservation has been used successfully to help identify cis-acting {dna} regions that are important in regulating tissue-specific gene expression. motivated by increasing evidence that some {dna} regulatory regions are not evolutionary conserved, we have developed an approach for cis-regulatory region identification that does not rely upon evolutionary sequence {conservation.results}:the conservation-independent approach is based on an empirical potential energy between interacting transcription factors ({tfs}). in this analysis, the potential energy is defined as a function of the number of {tf} interactions in a genomic region and the strength of the interactions. by identifying sets of interacting {tfs}, the analysis locates regions enriched with the binding sites of these interacting {tfs}. we applied this approach to 30 human tissues and identified 6232 putative cis-regulatory modules ({crms}) regulating 2130 tissue-specific genes. interestingly, some genes appear to be regulated by different {crms} in different tissues. known regulatory regions are highly enriched in our predicted {crms}. in addition, {dnase} i hypersensitive sites, which tend to be associated with active regulatory regions, significantly overlap with the predicted {crms}, but not with more conserved regions. we also find that conserved and non-conserved {crms} regulate distinct gene groups. conserved {crms} control more essential genes and genes involved in fundamental cellular activities such as transcription. in contrast, non-conserved {crms}, in general, regulate more non-essential genes, such as genes related to neural {activity.conclusion}:these results demonstrate that identifying relevant sets of binding motifs can help in the mapping of {dna} regulatory regions, and suggest that non-conserved {crms} play an important role in gene regulation.\"\"in the united states, ovarian cancer ranks as the fifth deadliest cancer among women.1 of the several subtypes of epithelial ovarian cancer, high-grade serous carcinomas are the most common, accounting for approximately 70\\\\% of all cases of epithelial ovarian cancer in north america.2 although ovarian clear-cell carcinoma is the second most common subtype in north america (accounting for 12\\\\% of cases and an even higher percentage in japan3) and is the second leading cause of death from ovarian cancer,2 it is relatively understudied. ovarian clear-cell carcinoma is defined on the basis of histopathological findings, including a predominance of clear .\\xa0.\\xa0.\"\"identifying the interface between two interacting proteins provides important clues to the function of a protein, and is becoming increasing relevant to drug discovery. here, surface patch analysis was combined with a bayesian network to predict protein–protein binding sites with a success rate of 82\\\\% on a benchmark dataset of 180 proteins, improving by 6\\\\% on previous work and well above the 36\\\\% that would be achieved by a random method. a comparable success rate was achieved even when evolutionary information was missing, a further improvement on our previous method which was unable to handle incomplete data automatically. in a case study of the mog1p family, we showed that our bayesian network method can aid the prediction of previously uncharacterised binding sites and provide important clues to protein function. on mog1p itself a putative binding site involved in the {sln1}-{skn7} signal transduction pathway was detected, as was a ran binding site, previously characterized solely by conservation studies, even though our automated method operated without using homologous proteins. on the remaining members of the family (two structural genomics targets, and a protein involved in the photosystem {ii} complex in higher plants) we identified novel binding sites with little correspondence to those on mog1p. these results suggest that members of the mog1p family bind to different proteins and probably have different functions despite sharing the same overall fold. we also demonstrated the applicability of our method to drug discovery efforts by successfully locating a number of binding sites involved in the protein–protein interaction network of papilloma virus infection. in a separate study, we attempted to distinguish between the two types of binding site, obligate and non-obligate, within our dataset using a second bayesian network. this proved difficult although some separation was achieved on the basis of patch size, electrostatic potential and conservation. such was the similarity between the two interacting patch types, we were able to use obligate binding site properties to predict the location of non-obligate binding sites and vice versa.\"a single-array preprocessing method for estimating full-resolution raw copy numbers from all affymetrix genotyping arrays including {genomewidesnp} 5 \\\\& 6.,\"high-resolution copy-number ({cn}) analysis has in recent years gained much attention, not only for the purpose of identifying {cn} aberrations associated with a certain phenotype, but also for identifying {cn} polymorphisms. in order for such studies to be successful and cost effective, the statistical methods have to be optimized. we propose a single-array preprocessing method for estimating full-resolution total {cns}. it is applicable to all affymetrix genotyping arrays, including the recent ones that also contain non-polymorphic probes. a reference signal is only needed at the last step when calculating relative {cns}. as with our method for earlier generations of arrays, this one controls for allelic crosstalk, probe affinities and {pcr} fragment-length effects. additionally, it also corrects for probe sequence effects and co-hybridization of fragments digested by multiple enzymes that takes place on the latest chips. we compare our method with affymetrix\\'s {cn5} method and the {dchip} method by assessing how well they differentiate between various {cn} states at the full resolution and various amounts of smoothing. although {crma} v2 is a single-array method, we observe that it performs as well as or better than alternative methods that use data from all arrays for their preprocessing. this shows that it is possible to do online analysis in large-scale projects where additional arrays are introduced over time.\"66123 saarbr\\\\\"\"{u}cken, germany. hzhu@mpi-sb.mpg.de\",{noxclass}: prediction of protein-protein interaction types,\"{background}:structural models determined by x-ray crystallography play a central role in understanding protein-protein interactions at the molecular level. interpretation of these models requires the distinction between non-specific crystal packing contacts and biologically relevant interactions. this has been investigated previously and classification approaches have been proposed. however, less attention has been devoted to distinguishing different types of biological interactions. these interactions are classified as obligate and non-obligate according to the effect of the complex formation on the stability of the protomers. so far no automatic classification methods for distinguishing obligate, non-obligate and crystal packing interactions have been made {available.results}:six interface properties have been investigated on a dataset of 243 protein interactions. the six properties have been combined using a support vector machine algorithm, resulting in {noxclass}, a classifier for distinguishing obligate, non-obligate and crystal packing interactions. we achieve an accuracy of 91.8\\\\% for the classification of these three types of interactions using a leave-one-out cross-validation {procedure.conclusion}:{noxclass} allows the interpretation and analysis of protein quaternary structures. in particular, it generates testable hypotheses regarding the nature of protein-protein interactions, when experimental results are not available. we expect this server will benefit the users of protein structural models, as well as protein crystallographers and {nmr} spectroscopists. a web server based on the method and the datasets used in this study are available at http://noxclass.bioinf.mpi-inf.mpg.de/.\"brigham and women\\'s hospital and harvard medical school, new research building, 77 avenue louis pasteur, boston, ma 02115, usa. mlbulyk@rascal.med.harvard.edu\",computational prediction of transcription-factor binding site locations,\"identifying genomic locations of transcription-factor binding sites, particularly in higher eukaryotic genomes, has been an enormous challenge. various experimental and computational approaches have been used to detect these sites; methods involving computational comparisons of related genomes have been particularly successful.\"\"motivation: high-throughput data is providing a comprehensive view of the molecular changes in cancer tissues. new technologies allow for the simultaneous genome-wide assay of the state of genome copy number variation, gene expression, {dna} methylation and epigenetics of tumor samples and cancer cell lines.\"\"we developed a computational method to characterize aneuploidy in tumor samples based on coordinated aberrations in expression of genes localized to each chromosomal region. we summarized the total level of chromosomal aberration in a given tumor in a univariate measure termed total functional aneuploidy. we identified a signature of chromosomal instability from specific genes whose expression was consistently correlated with total functional aneuploidy in several cancer types. net overexpression of this signature was predictive of poor clinical outcome in 12 cancer data sets representing six cancer types. also, the signature of chromosomal instability was higher in metastasis samples than in primary tumors and was able to stratify grade 1 and grade 2 breast tumors according to clinical outcome. these results provide a means to assess the potential role of chromosomal instability in determining malignant potential over a broad range of tumors.\"1205 ave dr penfield, montreal, qc h3a 1b1, canada;  community and conservation ecology group, university of groningen, haren, the netherlands;  department of biology, university of oslo, oslo, norway;  department of ecology and evolutionary biology, university of michigan, ann arbor, mi, usa;  department of statistics, university of auckland, auckland, new zealand;  gatty marine laboratory, university of st andrews, fife, scotland;  department of ecology and evolutionary biology, university of arizona, tucson, az, usa;  school of natural sciences, university of california merced, merced, ca, usa;  department of renewable natural resources, university of alberta, edmonton, alberta, canada;  national center for ecological analysis and synthesis, university of california santa barbara, santa barbara, ca, usa;  center for advanced studies in ecology \\\\& biodiversity (caseb), departamento de ecologa, facultad de ciencias biolgicas, pontificia universidad catlica de chile, alameda 340, santiago, chile;  instituto de ecologa y biodiversidad (ieb), departamento de ciencias ecolgicas. facultad de ciencias, universidad de chile. casilla 653, santiago, chile;  department of fisheries and wildlife, michigan state university, east lansing, mi, usa;  school of life sciences, arizona state university, tempe, az, usa\",species abundance distributions: moving beyond single prediction theories to integration within an ecological framework,\"species abundance distributions ({sads}) follow one of ecology\\'s oldest and most universal laws – every community shows a hollow curve or hyperbolic shape on a histogram with many rare species and just a few common species. here, we review theoretical, empirical and statistical developments in the study of {sads}. several key points emerge. (i) literally dozens of models have been proposed to explain the hollow curve. unfortunately, very few models are ever rejected, primarily because few theories make any predictions beyond the hollow-curve {sad} itself. (ii) interesting work has been performed both empirically and theoretically, which goes beyond the hollow-curve prediction to provide a rich variety of information about how {sads} behave. these include the study of {sads} along environmental gradients and theories that integrate {sads} with other biodiversity patterns. central to this body of work is an effort to move beyond treating the {sad} in isolation and to integrate the {sad} into its ecological context to enable making many predictions. (iii) moving forward will entail understanding how sampling and scale affect {sads} and developing statistical tools for describing and comparing {sads}. we are optimistic that {sads} can provide significant insights into basic and applied ecological science.\"rehovot 76100, israel.\",the modular architecture of protein-protein binding interfaces.,\"the human genome is peppered with mobile repetitive elements called long interspersed nuclear element–1 (l1) retrotransposons. propagating through {rna} and {cdna} intermediates, these molecular parasites copy and insert themselves throughout the genome, with potentially disruptive effects on neighboring genes or regulatory sequences. in the germ line, unique sequence downstream of l1 elements can also be retrotransposed if transcription continues beyond the repeat, a process known as 3′ transduction. there has been growing interest in retrotransposition and 3′ transduction as a possible source of somatic mutations during tumorigenesis.\"\"multiple myeloma is an incurable malignancy of plasma cells, and its pathogenesis is poorly understood. here we report the massively parallel sequencing of 38 tumour genomes and their comparison to matched normal {dnas}. several new and unexpected oncogenic mechanisms were suggested by the pattern of somatic mutation across the data set. these include the mutation of genes involved in protein translation (seen in nearly half of the patients), genes involved in histone methylation, and genes involved in blood coagulation. in addition, a broader than anticipated role of {nf}-{κb} signalling was indicated by mutations in 11 members of the {nf}-{κb} pathway. of potential immediate clinical relevance, activating mutations of the kinase {braf} were observed in 4\\\\% of patients, suggesting the evaluation of {braf} inhibitors in multiple myeloma clinical trials. these results indicate that cancer genome sequencing of large collections of samples will yield new insights into cancer not anticipated by existing knowledge.\"\"we evaluate {3d} models of human nucleoside diphosphate kinase, mouse cellular retinoic acid binding protein i, and human eosinophil neurotoxin that were calculated by {modeller}, a program for comparative protein modeling by satisfaction of spatial restraints. the models have good stereochemistry and are at least as similar to the crystallographic structures as the closest template structures. the largest errors occur in the regions that were not aligned correctly or where the template structures are not similar to the correct structure. these regions correspond predominantly to exposed loops, insertions of any length, and non-conserved side chains. when a template structure with more than 40\\\\% sequence identity to the target protein is available, the model is likely to have about 90\\\\% of the mainchain atoms modeled with an rms deviation from the x-ray structure of approximately 1 a, in large part because the templates are likely to be that similar to the x-ray structure of the target. this rms deviation is comparable to the overall differences between refined {nmr} and x-ray crystallography structures of the same protein.\"massachusetts institute of technology, nine cambridge center, cambridge, ma 02142, usa.\",\"rictor, a novel binding partner of {mtor}, defines a {rapamycin-insensitive} and {raptor-independent} pathway that regulates the cytoskeleton\",\"the mammalian {tor} ({mtor}) pathway integrates nutrient- and growth factor-derived signals to regulate growth, the process whereby cells accumulate mass and increase in size. {mtor} is a large protein kinase and the target of rapamycin, an immunosuppressant that also blocks vessel restenosis and has potential anticancer applications. {mtor} interacts with the raptor and {gβl} proteins [1], [2] and [3] to form a complex that is the target of rapamycin. here, we demonstrate that {mtor} is also part of a distinct complex defined by the novel protein rictor (rapamycin-insensitive companion of {mtor}). rictor shares homology with the previously described pianissimo from d. discoidieum [4], {ste20p} from s. pombe [5], and {avo3p} from s. cerevisiae [6] and [7]. interestingly, {avo3p} is part of a rapamycin-insensitive {tor} complex that does not contain the yeast homolog of raptor and signals to the actin cytoskeleton through {pkc1} [6]. consistent with this finding, the rictor-containing {mtor} complex contains {gβl} but not raptor and it neither regulates the {mtor} effector {s6k1} nor is it bound by {fkbp12}-rapamycin. we find that the {rictor-mtor} complex modulates the phosphorylation of protein kinase c α ({pkcα}) and the actin cytoskeleton, suggesting that this aspect of {tor} signaling is conserved between yeast and mammals.\"usa\",a mathematical theory of communication,an abstract is not available.dense genotyping and resequencing data\",\"summary: the {protorp} server analyses protein-protein associations in {3d} structures. the server calculates a series of physical and chemical parameters of the protein interaction sites that contribute to the binding energy of the association. these parameters include, size and shape, intermolecular bonding, residue and atom composition and secondary structure contributions. the server is flexible, in that it allows users to analyse individual protein associations or large datasets of associations deposited in the {pdb}, or upload and analyse proprietary files. the properties calculated can be compared with parameter distributions for non-homologous datasets of different classes of protein associations provided on the server website. the server provides an efficient way of characterizing protein-protein associations of new or existing proteins, and a means of putting these values in the context of previously observed associations.  availability: http://www.bioinformatics.sussex.ac.uk/protorp  contact: s.jones@sussex.ac.uk 10.1093/bioinformatics/btn584\"'),\n",
       " ('dacb9d5d7b6e1b8090aa2d4cf6542ea1',\n",
       "  'wellcome trust genome campus, hinxton, cambridge, cb10 1sd, uk.\",the ontology lookup service: more data and better tools for controlled vocabulary queries,\"the ontology lookup service ({ols}) (http://www.ebi.ac.uk/ols) provides interactive and programmatic interfaces to query, browse and navigate an ever increasing number of biomedical ontologies and controlled vocabularies. the volume of data available for querying has more than quadrupled since it went into production and {ols} functionality has been integrated into several high-usage databases and data entry tools. improvements have been made to both {ols} query interfaces, based on user feedback and requirements, to improve usability and service interoperability and provide novel ways to perform queries.\"hinxton, cambridge cb10 1sd and department of genetics, university of cambridge, cambridge cb2 3eh, uk.\",{chebi}: a database and ontology for chemical entities of biological interest,\"chemical entities of biological interest ({chebi}) is a freely available dictionary of molecular entities focused on \\'small\\' chemical compounds. the molecular entities in question are either natural products or synthetic products used to intervene in the processes of living organisms. genome-encoded macromolecules (nucleic acids, proteins and peptides derived from proteins by cleavage) are not as a rule included in {chebi}. in addition to molecular entities, {chebi} contains groups (parts of molecular entities) and classes of entities. {chebi} includes an ontological classification, whereby the relationships between molecular entities or classes of entities and their parents and/or children are specified. {chebi} is available online at http://www.ebi.ac.uk/chebi/\"large-scale prediction and testing of drug activity on side-effect targets,\"{background}:recently there has been an explosion of new data sources about genes, proteins, genetic variations, chemical compounds, diseases and drugs. integration of these data sources and the identification of patterns that go across them is of critical interest. initiatives such as {bio2rdf} and {lodd} have tackled the problem of linking biological data and drug data respectively using {rdf}. thus far, the inclusion of chemogenomic and systems chemical biology information that crosses the domains of chemistry and biology has been very {limitedresults}:we have created a single repository called {chem2bio2rdf} by aggregating data from multiple chemogenomics repositories that is cross-linked into {bio2rdf} and {lodd}. we have also created a linked-path generation tool to facilitate {sparql} query generation, and have created extended {sparql} functions to address specific chemical/biological search needs. we demonstrate the utility of {chem2bio2rdf} in investigating polypharmacology, identification of potential multiple pathway inhibitors, and the association of pathways with adverse drug {reactions.conclusions}:we have created a new semantic systems chemical biology resource, and have demonstrated its potential usefulness in specific examples of polypharmacology, multiple pathway inhibition and adverse drug reaction - pathway mapping. we have also demonstrated the usefulness of extending {sparql} with cheminformatics and bioinformatics functionality.\"\"the field of bioinformatics and computational biology has gone through a number of transformations during the past 15 years, establishing itself as a key component of new biology. this spectacular growth has been challenged by a number of disruptive changes in science and technology. despite the apparent fatigue of the linguistic use of the term itself, bioinformatics has grown perhaps to a point beyond recognition. we explore both historical aspects and future trends and argue that as the field expands, key questions remain unanswered and acquire new meaning while at the same time the range of applications is widening to cover an ever increasing number of biological disciplines. these trends appear to be pointing to a redefinition of certain objectives, milestones, and possibly the field itself.\"\"the research blog has become a popular mechanism for the quick discussion of scholarly information. however, unlike peer-reviewed journals, the characteristics of this form of scientific discourse are not well understood, for example in terms of the spread of blogger levels of education, gender and institutional affiliations. in this paper we fill this gap by analyzing a sample of blog posts discussing science via an aggregator called {researchblogging}.org ({rb}). {researchblogging}.org aggregates posts based on peer-reviewed research and allows bloggers to cite their sources in a scholarly manner. we studied the bloggers, blog posts and referenced journals of bloggers who posted at least 20 items. we found that {rb} bloggers show a preference for papers from high-impact journals and blog mostly about research in the life and behavioral sciences. the most frequently referenced journal sources in the sample were: science, nature, {pnas} and {plos} one. most of the bloggers in our sample had active twitter accounts connected with their blogs, and at least 90\\\\% of these accounts connect to at least one other {rb}-related twitter account. the average {rb} blogger in our sample is male, either a graduate student or has been awarded a {phd} and blogs under his own name.\"\"despite its long existence and international acceptance, network theory and analysis is a practically unknown approach in documentation, both theoretically and methodologically speaking. fortunately, this trend is changing, inasmuch as network theory and analysis may mean a quantitative and qualitative leap forward in the representation and analysis of the structure of all types of scientific domains, whether geographic, thematic or institutional. the extraordinary advances that have taken place in recent years in the study and analysis of complex networks have been made possible by a number of parallel developments. first of all, with computerized data acquisition and handling, large databases can be managed, leading to the emergence of different real network topologies. secondly, the increase in computing power has made it possible to explore networks with millions of nodes. thirdly, there is the slow but sure breakdown of boundaries between disciplines. this can be seen by researchers because of their ability to access and use databases that facilitate an understanding of the generic properties of complex networks\"\"tens of thousands of biomedical journals exist, and the deluge of new articles in the biomedical sciences is leading to information overload. hence, there is much interest in text mining, the use of computational tools to enhance the human ability to parse and understand complex text.\"theoretical foundations of information visualization,\"the field of information visualization, being related to many other diverse disciplines (for example, engineering, graphics, statistical modeling) suffers from not being based on a clear underlying theory. the absence of a framework for information visualization makes the significance of achievements in this area difficult to describe, validate and defend. drawing on theories within associated disciplines, three different approaches to theoretical foundations of information visualization are presented here: data-centric predictive theory, information theory, and scientific modeling. definitions from linguistic theory are used to provide an over-arching framework for these three approaches.\"\"this report reviews some of the extensive literature in health literacy, much of it focused on the intersection of low literacy and the understanding of basic health care information. several articles describe methods for assessing health literacy as well as methods for assessing the readability of texts, although generally these latter have not been developed with health materials in mind. other studies have looked more closely at the mismatch between patients\\' literacy levels and the readability of materials intended for use by those patients. a number of studies have investigated the phenomenon of literacy from the perspective of patients\\' interactions in the health care setting, the disenfranchisement of some patients because of their low literacy skills, the difficulty some patients have in navigating the health care system, the quality of the communication between doctors and their patients including the cultural overlay of such exchanges, and ultimately the effect of low literacy on health outcomes. finally, the impact of new information technologies has been studied by a number of investigators. there remain many opportunities for conducting further research to gain a better understanding of the complex interactions between general literacy, health literacy, information technologies, and the existing health care infrastructure.\"comprehensiveness, and compatibility of pathway databases\",it is necessary to analyze microarray experiments together with biological information to make better biological inferences. we investigate the adequacy of current biological databases to address this need.usa\",systematic review of factors influencing the adoption of information and communication technologies by healthcare professionals,\"this systematic review of mixed methods studies focuses on factors that can facilitate or limit the implementation of information and communication technologies ({icts}) in clinical settings. systematic searches of relevant bibliographic databases identified studies about interventions promoting {ict} adoption by healthcare professionals. content analysis was performed by two reviewers using a specific grid. one hundred and one (101) studies were included in the review. perception of the benefits of the innovation (system usefulness) was the most common facilitating factor, followed by ease of use. issues regarding design, technical concerns, familiarity with {ict}, and time were the most frequent limiting factors identified. our results suggest strategies that could effectively promote the successful adoption of {ict} in healthcare professional practices.\"\"information literacy ({il}) is recognised internationally as an essential competence for participation in education, employment and society. communities and organisations need strategies to ensure their members are efficient and effective information users. an investigation of formal {il} strategies in {uk} universities was initiated to examine their content and presentation. the study breaks new ground in undertaking an in-depth qualitative analysis of 10 institutional cases, evaluating {il} practice from a strategic management perspective and discussing how corporate strategy concepts and models could increase effectiveness in this emerging area of professional practice. its insights and suggestions contribute to the development of {il} and related strategies at both conceptual and practical levels. the study found that all the strategies aimed to integrate {il} into subject curricula by engaging stakeholders in collaborative partnerships. common approaches included the adoption of professional standards and development of new methods of delivery, including e-learning. the majority of strategies covered {il} of academic and other staff in addition to students. most strategy documents provided extensive contextualisation, demonstrating the relevance of {il} to corporate concerns; many included case studies of good practice. few documents conformed to strategic planning norms: none provided mission or vision statements and several contained poorly specified objectives. the study concluded that corporate strategy tools, such as stakeholder mapping, portfolio analysis and customisation models, could strengthen {il} strategies. future research could test the use of such analytical techniques to advance {il} strategies in higher education and other sectors.\"\"to comprehend the results of a randomized controlled trial ({rct}), readers must understand its design, conduct, analysis, and interpretation. that goal can be achieved only through complete transparency from authors. despite several decades of educational efforts, the reporting of {rcts} needs improvement. investigators and editors developed the original {consort} (consolidated standards of reporting trials) statement to help authors improve reporting by using a checklist and flow diagram. the revised {consort} statement presented in this article incorporates new evidence and addresses some criticisms of the original statement.\"competence and performance in the context of assessments in healthcare – deciphering the terminology\",\"background: the definitions of performance, competence and competency are not very clear in the literature. the assessment of performance and the selection of tools for this purpose depend upon a deep understanding of each of the above terms and the factors influencing performance. aim: in this article, we distinguish between competence and competency and explain the relationship of competence and performance in the light of the dreyfus model of skills acquisition. we briefly critique the application of the principles described by miller to the modern assessment tools and distinguish between assessment of actual performance in workplace settings and the observed performance, demonstrated by the candidates in the workplace or simulated settings. results: we describe a modification of the dreyfus model applicable to assessments in healthcare and propose a new model for the assessment of performance and performance rating scale ({prs}) based on this model. conclusion: we propose that the use of adapted versions of this {prs} will result in benchmarking of performance and allowing the candidates to track their progression of skills in various areas of clinical practice.\"genes and diseases.\",\"the scientific literature represents a rich source for retrieval of knowledge on associations between biomedical concepts such as genes, diseases and cellular processes. a commonly used method to establish relationships between biomedical concepts from literature is co-occurrence. apart from its use in knowledge retrieval, the co-occurrence method is also well-suited to discover new, hidden relationships between biomedical concepts following a simple {abc}-principle, in which a and c have no direct relationship, but are connected via shared b-intermediates. in this paper we describe {copub} discovery, a tool that mines the literature for new relationships between biomedical concepts. statistical analysis using {roc} curves showed that {copub} discovery performed well over a wide range of settings and keyword thesauri. we subsequently used {copub} discovery to search for new relationships between genes, drugs, pathways and diseases. several of the newly found relationships were validated using independent literature sources. in addition, new predicted relationships between compounds and cell proliferation were validated and confirmed experimentally in an in vitro cell proliferation assay. the results show that {copub} discovery is able to identify novel associations between genes, drugs, pathways and diseases that have a high probability of being biologically valid. this makes {copub} discovery a useful tool to unravel the mechanisms behind disease, to find novel drug targets, or to find novel applications for existing drugs.\"{iphace}: integrative navigation in pharmacological space.,\"{background}:in the past few years there has been an ongoing debate as to whether the proliferation of open access ({oa}) publishing would damage the peer review system and put the quality of scientific journal publishing at risk. our aim was to inform this debate by comparing the scientific impact of {oa} journals with subscription journals, controlling for journal age, the country of the publisher, discipline and (for {oa} publishers) their business {model.methods}:the 2-year impact factors (the average number of citations to the articles in a journal) were used as a proxy for scientific impact. the directory of open access journals ({doaj}) was used to identify {oa} journals as well as their business model. journal age and discipline were obtained from the ulrich\\'s periodicals directory. comparisons were performed on the journal level as well as on the article level where the results were weighted by the number of articles published in a journal. a total of 610 {oa} journals were compared with 7,609 subscription journals using web of science citation data while an overlapping set of 1,327 {oa} journals were compared with 11,124 subscription journals using scopus {data.results}:overall, average citation rates, both unweighted and weighted for the number of articles per journal, were about 30\\\\% higher for subscription journals. however, after controlling for discipline (medicine and health versus other), age of the journal (three time periods) and the location of the publisher (four largest publishing countries versus other countries) the differences largely disappeared in most subcategories except for journals that had been launched prior to 1996. {oa} journals that fund publishing with article processing charges ({apcs}) are on average cited more than other {oa} journals. in medicine and health, {oa} journals founded in the last 10 years are receiving about as many citations as subscription journals launched during the same {period.conclusions}:our results indicate that {oa} journals indexed in web of science and/or scopus are approaching the same scientific impact and quality as subscription journals, particularly in biomedicine and for journals funded by article processing charges.\"\"background medical literature searches provide critical information for clinicians. however, the best strategy for identifying relevant high-quality literature is unknown. objectives we compared search results using {pubmed} and google scholar on four clinical questions and analysed these results with respect to article relevance and quality. methods abstracts from the first 20 citations for each search were classified into three relevance categories. we used the weighted kappa statistic to analyse reviewer agreement and nonparametric rank tests to compare the number of citations for each article and the corresponding journals\\' impact factors. results reviewers ranked 67.6\\\\% of {pubmed} articles and 80\\\\% of google scholar articles as at least possibly relevant (p\\xa0=\\xa00.116) with high agreement (all kappa p-values\\xa0<\\xa00.01). google scholar articles had a higher median number of citations (34 vs. 1.5, p\\xa0<\\xa00.0001) and came from higher impact factor journals (5.17 vs. 3.55, p\\xa0=\\xa00.036). conclusions {pubmed} searches and google scholar searches often identify different articles. in this study, google scholar articles were more likely to be classified as relevant, had higher numbers of citations and were published in higher impact factor journals. the identification of frequently cited articles using google scholar for searches probably has value for initial literature searches.\"\"linda logdberg, a medical ghostwriter for 11 years, provides a personal view of her former work and what she believes should be done about the problem of fraud in authorship.\"\"the study of science-making is a growing discipline that builds largely on online publication and citation databases, while prepublication processes remain hidden. here, we report on results from a large-scale survey of the submission process, covering 923 scientific journals from the biological sciences in years 2006 to 2008. manuscript flows among journals revealed a modular submission network, with high-impact journals preferentially attracting submissions. however, about 75\\\\% of published articles were submitted first to the journal that would publish them, and high-impact journals published proportionally more articles that had been resubmitted from another journal. submission history affected post-publication impact: resubmissions from other journals received significantly more citations than first-intent submissions, and resubmissions between different journal communities received significantly fewer citations.\"\"evidence-based medicine ({ebm}) is arguably the most important contemporary initiative committed to reshaping biomedical reason and practice. the move to establish scientific research as a fundamental ground of medical decision making has met with an enthusiastic reception within academic medicine, but has also generated considerable controversy. {ebm} and the broader forms of evidence-based decision making it has occasioned raise provocative questions about the relation of scientific knowledge to social action across a variety of domains. social science inquiry about {ebm} has not yet reached the scale one might expect, given the breadth and significance of the phenomenon. this paper contributes reflections, critique and analysis aimed at helping to build a more robust social science investigation of {ebm}. the paper begins with a  ” diagnostics” of the existing social science literature on {ebm}, emphasizing the possibilities and limitations of its two central organizing analytic perspectives: political economy and humanism. we further explore emerging trends in the literature including a turn to original empirical investigation and the embrace of  ” newer” theoretical resources such as postmodern critique. we argue for the need to move the social inquiry of {ebm} beyond concerns about rationalization and the potential erasure of the patient and, to this end, suggest new avenues of exploration. the latter include analysis of clinical epidemiology and clinical reason as the discursive preconditions of {ebm}, the role of the patient as a site for the production of evidence, and the textually mediated character of {ebm}.\"\"this article presents the findings of a small-scale preliminary survey of one cohort of students studying towards a {diploma/bsc} in nursing. the survey sought to establish student characteristics and indicate their confidence levels using identified key library facilities. from questionnaires (n\\xa0=\\xa064), the data confirmed the \\'typicality\\' of the student group illuminating a breadth of prior experiences in terms of students\\' prior learning and perceived confidence in using library resources. whilst a number of respondents indicated confidence using identified library resources, a significant number of students (typically over one third n\\xa0=\\xa014+) indicated that they lacked confidence in and did not utilise library facilities. this suggests that they may not be using the resources to full advantage. over half of the respondents (53\\\\% n\\xa0=\\xa023) had not attended library skills training within the last two years and 9\\\\% (n\\xa0=\\xa04) had not used the library although they were completing assignments. this survey points to gaps in student confidence, and by implication, use of key library skills, particularly those involving electronic resources. at a time of widening participation and the explosion of information technology, this survey is a timely reminder of the need to revisit key skill development for nursing students studying in higher education.\"\"background differential diagnosis ({ddx}) generators are computer programs that generate a {ddx} based on various clinical data. objective we identified evaluation criteria through consensus, applied these criteria to describe the features of {ddx} generators, and tested performance using cases from the new england journal of medicine ({nejm}{\\\\copyright}) and the medical knowledge self assessment program ({mksap}{\\\\copyright}). methods we first identified evaluation criteria by consensus. then we performed google® and pubmed searches to identify {ddx} generators. to be included, {ddx} generators had to do the following: generate a list of potential diagnoses rather than text or article references; rank or indicate critical diagnoses that need to be considered or eliminated; accept at least two signs, symptoms or disease characteristics; provide the ability to compare the clinical presentations of diagnoses; and provide diagnoses in general medicine. the evaluation criteria were then applied to the included {ddx} generators. lastly, the performance of the {ddx} generators was tested with findings from 20 test cases. each case performance was scored one through five, with a score of five indicating presence of the exact diagnosis. mean scores and confidence intervals were calculated. key results twenty three programs were initially identified and four met the inclusion criteria. these four programs were evaluated using the consensus criteria, which included the following: input method; mobile access; filtering and refinement; lab values, medications, and geography as diagnostic factors; evidence based medicine ({ebm}) content; references; and drug information content source. the mean scores (95\\\\% confidence interval) from performance testing on a five-point scale were isabel{\\\\copyright} 3.45 (2.53, 4.37), {dxplain}® 3.45 (2.63–4.27), diagnosis pro® 2.65 (1.75–3.55) and {pepid}™ 1.70 (0.71–2.69). the number of exact matches paralleled the mean score finding. conclusions consensus criteria for {ddx} generator evaluation were developed. application of these criteria as well as performance testing supports the use of {dxplain}® and isabel{\\\\copyright} over the other currently available {ddx} generators.\"\"the applicability, validity and usefulness of karl popper\\'s  three worlds\\' epistemology is examined, with specific  reference to health-care information and knowledge. it is concluded that popper\\'s ideas provide a valuable way of understanding this domain and that, in particular, popper\\'s controversial  world   3\\' of objective knowledge is a valid concept. 10.1177/016555150202800106\"\"a fully-integrated, self-populating, and potentially internet-based {cds} tool could contribute to improved global {cvd} risk management in australian primary health care. the findings from this study will inform a large-scale trial intervention.\"\"objectives:\\u2002 this study surveyed web 2.0 application in three types of selected health or medical-related organisations such as university medical libraries, hospitals and non-profit medical-related organisations. methods:\\u2002 thirty organisations participated in an online survey on the perceived purposes, benefits and difficulties in using web 2.0. a phone interview was further conducted with eight organisations (26.7\\\\%) to collect information on the use of web 2.0. data were analysed using both quantitative and qualitative approaches. results:\\u2002 results showed that knowledge and information sharing and the provision of a better communication platform were rated as the main purposes of using web 2.0. time constraints and low staff engagement were the most highly rated difficulties. in addition, most participants found web 2.0 to be beneficial to their organisations. conclusions:\\u2002 medical-related organisations that adopted web 2.0 technologies have found them useful, with benefits outweighing the difficulties in the long run. the implications of this study are discussed to help medical-related organisations make decisions regarding the use of web 2.0 technologies.\"maastricht, the netherlands. p.a.d.clerq@tue.nl\",approaches for creating computer-interpretable guidelines that facilitate decision support.,\"combination therapies are often needed for effective clinical outcomes in the management of complex diseases, but presently they are generally based on empirical clinical experience. here we suggest a novel application of search algorithms -- originally developed for digital communication -- modified to optimize combinations of therapeutic interventions. in biological experiments measuring the restoration of the decline with age in heart function and exercise capacity in drosophila melanogaster, we found that search algorithms correctly identified optimal combinations of four drugs using only one-third of the tests performed in a fully factorial search. in experiments identifying combinations of three doses of up to six drugs for selective killing of human cancer cells, search algorithms resulted in a highly significant enrichment of selective combinations compared with random searches. in simulations using a network model of cell death, we found that the search algorithms identified the optimal combinations of 6-9 interventions in 80-90\\\\% of tests, compared with 15-30\\\\% for an equivalent random search. these findings suggest that modified search algorithms from information theory have the potential to enhance the discovery of novel therapeutic drug combinations. this report also helps to frame a biomedical problem that will benefit from an interdisciplinary effort and suggests a general strategy for its solution.\"\"objective\\u2002 this paper aims to summarise the evidence supporting the role of experience-based, non-analytic reasoning ({nar}) or pattern recognition as a central feature of expert medical diagnosis. methods\\u2002 the authors examine a series of studies, primarily from their own research programme at {mcmaster} university, that demonstrate that expert and novice diagnostic problem solving is based, to some degree, on similarity to a prior specific exemplar in the memory. results\\u2002 the studies reviewed have shown {nar} to be a component of diagnostic reasoning at all levels from novice to subspecialist, and in dermatology, electrocardiography and psychiatry. the retrieval process is rapid and is not available to retrospection. it may be based on visual similarity, but can also be present in verbal descriptions. some evidence exists that the process is unlikely to be available to introspection. further, early hypotheses based on {nar} can result in the reinterpretaton of critical clinical findings. conclusions\\u2002 non-analytic reasoning is a central component of diagnostic expertise at all levels. clinical teaching should recognise the centrality of this process, and aim to both enhance the process through the learning of multiple examples and to supplement the process with analytical de-biasing strategies.\"\"clear, transparent, and sufficiently detailed abstracts of conferences and journal articles related to randomized controlled trials ({rcts}) are important, because readers often base their assessment of a trial solely on information in the abstract. here, we extend the {consort} (consolidated standards of reporting trials) statement to develop a minimum list of essential items, which authors should consider when reporting the results of a {rct} in any journal or conference abstract. we generated a list of items from existing quality assessment tools and empirical evidence. a three-round, {modified-delphi} process was used to select items. in all, 109 participants were invited to participate in an electronic survey; the response rate was 61\\\\%. survey results were presented at a meeting of the {consort} group in montebello, canada, january 2007, involving 26 participants, including clinical trialists, statisticians, epidemiologists, and biomedical editors. checklist items were discussed for eligibility into the final checklist. the checklist was then revised to ensure that it reflected discussions held during and subsequent to the meeting. {consort} for abstracts recommends that abstracts relating to {rcts} have a structured format. items should include details of trial objectives; trial design (e.g., method of allocation, blinding/masking); trial participants (i.e., description, numbers randomized, and number analyzed); interventions intended for each randomized group and their impact on primary efficacy outcomes and harms; trial conclusions; trial registration name and number; and source of funding. we recommend the checklist be used in conjunction with this explanatory document, which includes examples of good reporting, rationale, and evidence, when available, for the inclusion of each item. {consort} for abstracts aims to improve reporting of abstracts of {rcts} published in journal articles and conference proceedings. it will help authors of abstracts of these trials provide the detail and clarity needed by readers wishing to assess a trial\\'s validity and the applicability of its results.\"actions and support requirements\",\"as bioinformatics becomes increasingly central to research in the molecular life sciences, the need to train non-bioinformaticians to make the most of bioinformatics resources is growing. here, we review the key challenges and pitfalls to providing effective training for users of bioinformatics services, and discuss successful training strategies shared by a diverse set of bioinformatics trainers. we also identify steps that trainers in bioinformatics could take together to advance the state of the art in current training practices. the ideas presented in this article derive from the first trainer networking session held under the auspices of the {eu}-funded {sling} integrating activity, which took place in november 2009.\"where to publish and find ontologies? a survey of ontology libraries,\"one of the key promises of the semantic web is its potential to enable and facilitate data interoperability. the ability of data providers and application developers to share and reuse ontologies is a critical component of this data interoperability: if different applications and data sources use the same set of well defined terms for describing their domain and data, it will be much easier for them to  ” talk” to one another. ontology libraries are the systems that collect ontologies from different sources and facilitate the tasks of finding, exploring, and using these ontologies. thus ontology libraries can serve as a link in enabling diverse users and applications to discover, evaluate, use, and publish ontologies. in this paper, we provide a survey of the growing—and surprisingly diverse—landscape of ontology libraries. we highlight how the varying scope and intended use of the libraries affects their features, content, and potential exploitation in applications. from reviewing 11 ontology libraries, we identify a core set of questions that ontology practitioners and users should consider in choosing an ontology library for finding ontologies or publishing their own. we also discuss the research challenges that emerge from this survey, for the developers of ontology libraries to address.\"\"choosing good problems is essential for being a good scientist. but what is a good problem, and how do you choose one? the subject is not usually discussed explicitly within our profession. scientists are expected to be smart enough to figure it out on their own and through the observation of their teachers. this lack of explicit discussion leaves a vacuum that can lead to approaches such as choosing problems that can give results that merit publication in valued journals, resulting in a job and tenure.\"usa\",why web 2.0 is good for learning and for research: principles and prototypes,\"the term \"\"web 2.0\"\" is used to describe applications that distinguish themselves from previous generations of software by a number of principles. existing work shows that web 2.0 applications can be successfully exploited for technology-enhance learning. however, in-depth analyses of the relationship between web 2.0 technology on the one hand and teaching and learning on the other hand are still rare. in this article, we will analyze the technological principles of the web 2.0 and describe their pedagogical implications on learning. we will furthermore show that web 2.0 is not only well suited for learning but also for research on learning: the wealth of services that is available and their openness regarding {api} and data allow to assemble prototypes of technology-supported learning applications in amazingly small amount of time. these prototypes can be used to evaluate research hypotheses quickly. we will present two example prototypes and discuss the lessons we learned from building and using these prototypes.\"national institutes of health, department of health and human services, 8600 rockville pike, bethesda, md 20894, usa. tcr@nlm.nih.gov\",the interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text,\"interpretation of semantic propositions in free-text documents such as {medline} citations would provide valuable support for biomedical applications, and several approaches to semantic interpretation are being pursued in the biomedical informatics community. in this paper, we describe a methodology for interpreting linguistic structures that encode hypernymic propositions, in which a more specific concept is in a taxonomic relationship with a more general concept. in order to effectively process these constructions, we exploit underspecified syntactic analysis and structured domain knowledge from the unified medical language system ({umls}). after introducing the syntactic processing on which our system depends, we focus on the {umls} knowledge that supports interpretation of hypernymic propositions. we first use semantic groups from the semantic network to ensure that the two concepts involved are compatible; hierarchical information in the metathesaurus then determines which concept is more general and which more specific. a preliminary evaluation of a sample based on the semantic group chemicals and drugs provides 83\\\\% precision. an error analysis was conducted and potential solutions to the problems encountered are presented. the research discussed here serves as a paradigm for investigating the interaction between domain knowledge and linguistic structure in natural language processing, and could also make a contribution to research on automatic processing of discourse structure. additional implications of the system we present include its integration in advanced semantic interpretation processors for biomedical text and its use for information extraction in specific domains. the approach has the potential to support a range of applications, including information retrieval and ontology engineering.\"blogs and podcasts: a new generation of web-based tools for virtual collaborative clinical practice and education\",\"{background}:we have witnessed a rapid increase in the use of web-based \\'collaborationware\\' in recent years. these web 2.0 applications, particularly wikis, blogs and podcasts, have been increasingly adopted by many online health-related professional and educational services. because of their ease of use and rapidity of deployment, they offer the opportunity for powerful information sharing and ease of collaboration. wikis are web sites that can be edited by anyone who has access to them. the word \\'blog\\' is a contraction of \\'web log\\' - an online web journal that can offer a resource rich multimedia environment. podcasts are repositories of audio and video materials that can be \"\"pushed\"\" to subscribers, even without user intervention. these audio and video files can be downloaded to portable media players that can be taken anywhere, providing the potential for \"\"anytime, anywhere\"\" learning experiences (mobile {learning).discussion}:wikis, blogs and podcasts are all relatively easy to use, which partly accounts for their proliferation. the fact that there are many free and open source versions of these tools may also be responsible for their explosive growth. thus it would be relatively easy to implement any or all within a health professions\\' educational environment. paradoxically, some of their disadvantages also relate to their openness and ease of use. with virtually anybody able to alter, edit or otherwise contribute to the collaborative web pages, it can be problematic to gauge the reliability and accuracy of such resources. while arguably, the very process of collaboration leads to a darwinian type \\'survival of the fittest\\' content within a web page, the veracity of these resources can be assured through careful monitoring, moderation, and operation of the collaborationware in a closed and secure digital environment. empirical research is still needed to build our pedagogic evidence base about the different aspects of these tools in the context of medical/health {education.summary} {and} {conclusion}:if effectively deployed, wikis, blogs and podcasts could offer a way to enhance students\\', clinicians\\' and patients\\' learning experiences, and deepen levels of learners\\' engagement and collaboration within digital learning environments. therefore, research should be conducted to determine the best ways to integrate these tools into existing {e-learning} programmes for students, health professionals and patients, taking into account the different, but also overlapping, needs of these three audience classes and the opportunities of virtual collaboration between them. of particular importance is research into novel integrative applications, to serve as the \"\"glue\"\" to bind the different forms of web-based collaborationware synergistically in order to provide a coherent wholesome learning experience.\"current applications in radiology and future prospects.\",the university of chicago, 5841 south maryland avenue, chicago, il 60637, usa. k-doi@uchicago.edu\",\"computer-aided diagnosis in medical imaging: historical review, current status and future potential\",\"for more than a decade, reports from expert panels have called for improvements in science education. there is general agreement that science courses consisting of traditional lectures and cookbook laboratory exercises need to be changed. what is required instead is \"\"scientific teaching,\"\" teaching that mirrors science at its best-experimental, rigorous, and based on evidence. this policy forum explores the reasons for the slow pace of change in the way science is taught at research universities and offers recommendations for faculty, staff, and administrators at research universities, funding agencies, and professional organizations in order to accelerate the reform of science education. to help faculty initiate change in their own classrooms, this forum includes extensive resources to guide the transition to tested, effective instructional methods, which include group-learning in lectures, inquiry-based laboratories, and interactive computer modules.\"525 e. 68 street, box 298, new york, ny 10021, usa. gkuperman@nyp.org\",medication-related clinical decision support in computerized provider order entry systems: a review,\"while medications can improve patients\\' health, the process of prescribing them is complex and error prone, and medication errors cause many preventable injuries. computer provider order entry ({cpoe}) with clinical decision support ({cds}), can improve patient safety and lower medication-related costs.\"\"this early stage demonstration highlights critical directions to follow that will enable translational pharmacotherapeutic research. the uniform application of semantic web methodology to problems in data integration, knowledge representation, and analysis provides an efficient and potentially powerful means to allow mining of drug action and disease mechanism relationships. further improvements in semantic representation of mechanistic relationships will provide a fertile basis for accelerated drug repositioning, reasoning, and discovery across the spectrum of human disease.\"and department of veterans affairs medical center, birmingham, al 35233, usa. terry.shaneyfelt@med.va.gov\",instruments for evaluating education in evidence-based practice: a systematic review.,\"evidence-based practice ({ebp}) is the integration of the best research evidence with patients\\' values and clinical circumstances in clinical decision making. teaching of {ebp} should be evaluated and guided by evidence of its own effectiveness. to appraise, summarize, and describe currently available {ebp} teaching evaluation {instruments.data} sources and we searched the {medline}, {embase}, {cinahl}, {hapi}, and {eric} databases; reference lists of retrieved articles; {ebp} internet sites; and 8 education journals from 1980 through april 2006. for inclusion, studies had to report an instrument evaluating {ebp}, contain sufficient description to permit analysis, and present quantitative results of administering the instrument. two raters independently abstracted information on the development, format, learner levels, evaluation domains, feasibility, reliability, and validity of the {ebp} evaluation instruments from each article. we defined 3 levels of instruments based on the type, extent, methods, and results of psychometric testing and suitability for different evaluation purposes. of 347 articles identified, 115 were included, representing 104 unique instruments. the instruments were most commonly administered to medical students and postgraduate trainees and evaluated {ebp} skills. among {ebp} skills, acquiring evidence and appraising evidence were most commonly evaluated, but newer instruments evaluated asking answerable questions and applying evidence to individual patients. most behavior instruments measured the performance of {ebp} steps in practice but newer instruments documented the performance of evidence-based clinical maneuvers or patient-level outcomes. at least 1 type of validity evidence was demonstrated for 53\\\\% of instruments, but 3 or more types of validity evidence were established for only 10\\\\%. high-quality instruments were identified for evaluating the {ebp} competence of individual trainees, determining the effectiveness of {ebp} curricula, and assessing {ebp} behaviors with objective outcome measures. instruments with reasonable validity are available for evaluating some domains of {ebp} and may be targeted to different evaluation needs. further development and testing is required to evaluate {ebp} attitudes, behaviors, and more recently articulated {ebp} skills.\"ny, usa\",information foraging theory: adaptive interaction with information,\"although much of the hubris and hyperbole surrounding the 1990\\'s internet has softened to a reasonable level, the inexorable momentum of information growth continues unabated. this wealth of information provides resources for adapting to the problems posed by our increasingly complex world, but the simple availability of more information does not guarantee its successful transformation into valuable knowledge that shapes, guides, and improves our activity. when faced with something like the analysis of sense-making behavior on the web, traditional research models tell us a lot about learning and performance with browser operations, but very little about how people will actively navigate and search through information structures, what information they will choose to consume, and what conceptual models they will induce about the landscape of cyberspace. thus, it is fortunate that a new field of research, adaptive information interaction ({aii}), is becoming possible. {aii} centers on the problems of understanding and improving human-information interaction. it is about how people will best shape themselves to their information environments, and how information environments can best be shaped to people. its roots lie in human-computer interaction ({hci}), information retrieval, and the behavioral and social sciences. this book is about information foraging theory ({ift}), a new theory in adaptive information interaction that is one example of a recent flourish of theories in adaptationist psychology that draw upon evolutionary-ecological theory in biology. {ift} assumes that people (indeed, all organisms) are ecologically rational, and that human information-seeking mechanisms and strategies adapt the structure of the information environments in which they operate. its main aim is to create technology that is better shaped to users. information foraging theory will be of interest to student and professional researchers in {hci} and cognitive psychology.\"\"the value of any kind of data is greatly enhanced when it exists in a form that allows it to be integrated with other data. one approach to integration is through the annotation of multiple bodies of data using common controlled vocabularies or \\'ontologies\\'. unfortunately, the very success of this approach has led to a proliferation of ontologies, which itself creates obstacles to integration. the open biomedical ontologies ({obo}) consortium is pursuing a strategy to overcome this problem. existing {obo} ontologies, including the gene ontology, are undergoing coordinated reform, and new ontologies are being created on the basis of an evolving set of shared principles governing ontology development. the result is an expanding family of ontologies designed to be interoperable and logically well formed and to incorporate accurate representations of biological reality. we describe this {obo} foundry initiative and provide guidelines for those who might wish to become involved.\"\"for over thirty years, there have been predictions that the widespread clinical use of computers was imminent. yet the  ” wave” has never broken. in this article, two broad time periods are examined: the 1960\\'s to the 1980\\'s and the 1980\\'s to the present. technology immaturity, health administrator focus on financial systems, application  ” unfriendliness,” and physician resistance were all barriers to acceptance during the early time period. although these factors persist, changes in clinicians\\' economics, more computer literacy in the general population, and, most importantly, changes in government policies and increased support for clinical computing suggest that the wave may break in the next decade.\"\"systems pharmacology is an emerging area of pharmacology which utilizes network analysis of drug action as one of its approaches. by considering drug actions and side effects in the context of the regulatory networks within which the drug targets and disease gene products function, network analysis promises to greatly increase our knowledge of the mechanisms underlying the multiple actions of drugs. systems pharmacology can provide new approaches for drug discovery for complex diseases. the integrated approach used in systems pharmacology can allow for drug action to be considered in the context of the whole genome. network-based studies are becoming an increasingly important tool in understanding the relationships between drug action and disease susceptibility genes. this review discusses how analysis of biological networks has contributed to the genesis of systems pharmacology and how these studies have improved global understanding of drug targets, suggested new targets and approaches for therapeutics, and provided a deeper understanding of the effects of drugs. taken together, these types of analyses can lead to new therapeutic options while improving the safety and efficacy of existing medications.\"\"{background}:high-throughput screening ({hts}) is one of the main strategies to identify novel entry points for the development of small molecule chemical probes and drugs and is now commonly accessible to public sector research. large amounts of data generated in {hts} campaigns are submitted to public repositories such as {pubchem}, which is growing at an exponential rate. the diversity and quantity of available {hts} assays and screening results pose enormous challenges to organizing, standardizing, integrating, and analyzing the datasets and thus to maximize the scientific and ultimately the public health impact of the huge investments made to implement public sector {hts} capabilities. novel approaches to organize, standardize and access {hts} data are required to address these {challenges.results}:we developed the first ontology to describe {hts} experiments and screening results using expressive description logic. the {bioassay} ontology ({bao}) serves as a foundation for the standardization of {hts} assays and data and as a semantic knowledge model. in this paper we show important examples of formalizing {hts} domain knowledge and we point out the advantages of this approach. the ontology is available online at the {ncbo} bioportal http://bioportal.bioontology.org/ontologies/44531 {webcite.conclusions}:after a large manual curation effort, we loaded {bao}-mapped data triples into a {rdf} database store and used a reasoner in several case studies to demonstrate the benefits of formalized domain knowledge representation in {bao}. the examples illustrate semantic querying capabilities where {bao} enables the retrieval of inferred search results that are relevant to a given query, but are not explicitly defined. {bao} thus opens new functionality for annotating, querying, and analyzing {hts} datasets and the potential for discovering new knowledge by means of inference.\"\"{ehealth}, the use of information technology to improve or enable health and health care, has recently been high on the health care development agenda. given the vivid interest in {ehealth}, little reference has been made to the use of these technologies in the promotion of health. the aim of this present study was to conduct a review on recent uses of information technology in health promotion through looking at research articles published in peer-reviewed journals. fifteen relevant journals with issues published between 2003 and june 2005 yielded altogether 1352 articles, 56 of which contained content related to the use of information technology in the context of health promotion. as reflected by this rather small proportion, research on the role of information technology is only starting to emerge. four broad thematic application areas within health promotion were identified: use of information technology as an intervention medium, use of information technology as a research focus, use of information technology as a research instrument and use of information technology for professional development. in line with this rather instrumental focus, the concepts  {epromotion} of health\\' or  health {epromotion}\\' would come close to describing the role of information technology in health promotion. 10.1093/her/cym001\"self-organization, and the growth of international collaboration in science\",\"different approaches have been used to analyse international collaboration in science but none can fully explain its rapid growth. using international co-authorships, we test the hypothesis that international collaboration is a self-organising network. applying tools from network analysis, the paper shows that the growth of international co-authorships can be explained based on the organising principle of preferential attachment, although the attachment mechanism deviates from an ideal power-law. several explanations for the deviation are explored, including that of the influence of institutional constraints on the mechanism of self-organisation.\"388 papers\",\"social tagging and controlled indexing both facilitate access to information resources. given the increasing popularity of social tagging and the limitations of controlled indexing (primarily cost and scalability), it is reasonable to investigate to what degree social tagging could substitute for controlled indexing. in this study, we compared {citeulike} tags to medical subject headings ({mesh}) terms for 231,388 citations indexed in {medline}. in addition to descriptive analyses of the data sets, we present a paper-by-paper analysis of tags and {mesh} terms: the number of common annotations, jaccard similarity, and coverage ratio. in the analysis, we apply three increasingly progressive levels of text processing, ranging from normalization to stemming, to reduce the impact of lexical differences. annotations of our corpus consisted of over 76,968 distinct tags and 21,129 distinct {mesh} terms. the top 20 {tags/mesh} terms showed little direct overlap. on a paper-by-paper basis, the number of common annotations ranged from 0.29 to 0.5 and the jaccard similarity from 2.12\\\\% to 3.3\\\\% using increased levels of text processing. at most, 77,834 citations (33.6\\\\%) shared at least one annotation. our results show that {citeulike} tags and {mesh} terms are quite distinct lexically, reflecting different viewpoints/processes between social tagging and controlled indexing.\"\"interaction between drug substances may yield excessive risk of adverse drug reactions ({adrs}) when two drugs are taken in combination. collections of individual case safety reports ({icsrs}) related to suspected {adr} incidents in clinical practice have proven to be very useful in post-marketing surveillance for pairwise {drug--adr} associations, but have yet to reach their full potential for drug-drug interaction surveillance. in this paper, we implement and evaluate a shrinkage observed-to-expected ratio for exploratory analysis of suspected drug-drug interaction in {icsr} data, based on comparison with an additive risk model. we argue that the limited success of previously proposed methods for drug-drug interaction detection based on {icsr} data may be due to an underlying assumption that the absence of interaction is equivalent to having multiplicative risk factors. we provide empirical examples of established drug-drug interaction highlighted with our proposed approach that go undetected with logistic regression. a database wide screen for suspected drug-drug interaction in the entire {who} database is carried out to demonstrate the feasibility of the proposed approach. as always in the analysis of {icsrs}, the clinical validity of hypotheses raised with the proposed method must be further reviewed and evaluated by subject matter experts.\"\"with the increasing amount of data made available in the chemical field, there is a strong need for systems capable of comparing and classifying chemical compounds in an efficient and effective way. the best approaches existing today are based on the structure-activity relationship premise, which states that biological activity of a molecule is strongly related to its structural or physicochemical properties. this work presents a novel approach to the automatic classification of chemical compounds by integrating semantic similarity with existing structural comparison methods. our approach was assessed based on the matthews correlation coefficient for the prediction, and achieved values of 0.810 when used as a prediction of blood-brain barrier permeability, 0.694 for p-glycoprotein substrate, and 0.673 for estrogen receptor binding activity. these results expose a significant improvement over the currently existing methods, whose best performances were 0.628, 0.591, and 0.647 respectively. it was demonstrated that the integration of semantic similarity is a feasible and effective way to improve existing chemical compound classification systems. among other possible uses, this tool helps the study of the evolution of metabolic pathways, the study of the correlation of metabolic networks with properties of those networks, or the improvement of ontologies that represent chemical information.\"\"interest in developing ways to assess information literacy has been growing for several years. many librarians have developed their own tools to assess aspects of information literacy and have written articles to share their experiences. this article reviews the literature and offers readers a flavour of the methods being used for assessment: those which are popular within the field and also illustrative examples from some of the case studies found, particularly where they show how the reliability and validity of the methods have been considered. it does not aim to be an exhaustive list of case studies or methods, but a representative sample to act as a `jumping off point\\' for librarians considering introducing assessment of information literacy into their own institutions.\"\"automatic text summarization for a biomedical concept can help researchers to get the key points of a certain topic from large amount of biomedical literature efficiently. in this paper, we present a method for generating text summary for a given biomedical concept, e.g., {h1n1} disease, from multiple documents based on semantic relation extraction. our approach includes three stages: 1) we extract semantic relations in each sentence using the semantic knowledge representation tool {semrep}. 2) we develop a relation-level retrieval method to select the relations most relevant to each query concept and visualize them in a graphic representation. 3) for relations in the relevant set, we extract informative sentences that can interpret them from the document collection to generate text summary using an information retrieval based method. our major focus in this work is to investigate the contribution of semantic relation extraction to the task of biomedical text summarization. the experimental results on summarization for a set of diseases show that the introduction of semantic knowledge improves the performance and our results are better than the {mead} system, a well-known tool for text summarization.\"\"motivation: identifying drug–drug interactions ({ddis}) is a critical process in drug administration and drug development. clinical support tools often provide comprehensive lists of {ddis}, but they usually lack the supporting scientific evidences and different tools can return inconsistent results. in this article, we propose a novel approach that integrates text mining and automated reasoning to derive {ddis}. through the extraction of various facts of drug metabolism, not only the {ddis} that are explicitly mentioned in text can be extracted but also the potential interactions that can be inferred by reasoning.\"\"the 2009 annual conference of the asia pacific bioinformatics network ({apbionet}), asia\\'s oldest bioinformatics organisation from 1998, was organized as the 8th international conference on bioinformatics ({incob}), sept. 9-11, 2009 at biopolis, singapore. {incob} has actively engaged researchers from the area of life sciences, systems biology and clinicians, to facilitate greater synergy between these groups. to encourage bioinformatics students and new researchers, tutorials and student symposium, the singapore symposium on computational biology ({symbio}) were organized, along with the workshop on education in bioinformatics and computational biology ({webcb}) and the clinical bioinformatics ({cbas}) symposium. however, to many students and young researchers, pursuing a career in a multi-disciplinary area such as bioinformatics poses a himalayan challenge. a collection to tips is presented here to provide signposts on the road to a career in bioinformatics. an overview of the application of bioinformatics to traditional and emerging areas, published in this supplement, is also presented to provide possible future avenues of bioinformatics investigation. a case study on the application of e-learning tools in undergraduate bioinformatics curriculum provides information on how to go impart targeted education, to sustain bioinformatics in the {asia-pacific} region. the next {incob} is scheduled to be held in tokyo, japan, sept. 26-28, 2010.\"\"electronic slideshow presentations are often faulted anecdotally, but little empirical work has documented their faults. in study 1 we found that eight psychological principles are often violated in {powerpoint}(®) slideshows, and are violated to similar extents across different fields - for example, academic research slideshows generally were no better or worse than business slideshows. in study 2 we found that respondents reported having noticed, and having been annoyed by, specific problems in presentations arising from violations of particular psychological principles. finally, in study 3 we showed that observers are not highly accurate in recognizing when particular slides violated a specific psychological rule. furthermore, even when they correctly identified the violation, they often could not explain the nature of the problem. in sum, the psychological foundations for effective slideshow presentation design are neither obvious nor necessarily intuitive, and presentation designers in all fields, from education to business to government, could benefit from explicit instruction in relevant aspects of psychology.\"\"although competency-based medical education ({cbme}) has attracted renewed interest in recent years among educators and policy-makers in the health care professions, there is little agreement on many aspects of this paradigm. we convened a unique partnership ? the international {cbme} collaborators ? to examine conceptual issues and current debates in {cbme}. we engaged in a multi-stage group process and held a consensus conference with the aim of reviewing the scholarly literature of competency-based medical education, identifying controversies in need of clarification, proposing definitions and concepts that could be useful to educators across many jurisdictions, and exploring future directions for this approach to preparing health professionals. in this paper, we describe the evolution of {cbme} from the outcomes movement in the 20th century to a renewed approach that, focused on accountability and curricular outcomes and organized around competencies, promotes greater learner-centredness and de-emphasizes time-based curricular design. in this paradigm, competence and related terms are redefined to emphasize their multi-dimensional, dynamic, developmental, and contextual nature. {cbme} therefore has significant implications for the planning of medical curricula and will have an important impact in reshaping the enterprise of medical education. we elaborate on this emerging {cbme} approach and its related concepts, and invite medical educators everywhere to enter into further dialogue about the promise and the potential perils of competency-based medical curricula for the 21st century.\"\"the purpose of this study was to consider the efficacy and popularity of \"\"virtual lectures\"\" (text-based, structured electronic courseware with information presented in manageable \"\"chunks\"\", interaction and multimedia) and \"\"{e-lectures}\"\" (on-screen synchrony of {powerpoint} slides and recorded voice) as alternatives to traditional lectures. we considered how three modes of delivery compare when increasingly deeper forms of learning are assessed and also student reaction to electronic delivery. fifty-eight students in three groups took three topics of a human genetics module, one in each delivery style. results indicated no overall greater efficacy of either delivery style when all question types were taken into account but significantly different delivery-specific results depending on which level of bloom\\'s taxonomy was assessed. that is, overall, questions assessing knowledge consistently achieved the highest marks followed by analysis, comprehension, evaluation and application. students receiving traditional lectures scored significantly lower marks for comprehension questions. students receiving virtual lectures scored high for knowledge, comprehension and application but significantly lower for analysis and evaluation questions. the {e-lectures} scored high for knowledge questions and were the median for all question types except application. questionnaire analysis revealed a preference for traditional lectures over computer-based but nevertheless an appreciation of the advantages offered by them.\",\"the literature of bibliometrics, scientometrics, and informetrics\",\"since vassily v. nalimov coined the term \\'scientometrics\\' in the 1960s, this term has grown in popularity and is used to describe the study of science: growth, structure, interrelationships and productivity. scientometrics is related to and has overlapping interests with bibliometrics and informetrics. the terms bibliometrics, scientometrics, and informetrics refer to component fields related to the study of the dynamics of disciplines as reflected in the production of their literature. areas of study range from charting changes in the output of a scholarly field through time and across countries, to the library collection problem of maintaining control of the output, and to the low publication productivity of most researchers. these terms are used to describe similar and overlapping methodologies. the origins and historical survey of the development of each of these terms are presented. profiles of the usage of each of these terms over time are presented, using an appropriate subject category of databases on the {dialog} information service. various definitions of each of the terms are provided from an examination of the literature. the size of the overall literature of these fields is determined and the growth and stabilisation of both the dissertation and non-dissertation literature are shown. a listing of the top journals in the three fields are given, as well as a list of the major reviews and bibliographies that have been published over the years.\"\"the paper reviews recent studies that evaluate the impact of free access (open access) on the behavior of scientists as authors, readers, and citers in developed and developing nations. it also examines the extent to which the biomedical literature is used by the general public. the paper is a critical review of the literature, with systematic description of key studies. researchers report that their access to the scientific literature is generally good and improving. for authors, the access status of a journal is not an important consideration when deciding where to publish. there is clear evidence that free access increases the number of article downloads, although its impact on article citations is not clear. recent studies indicate that large citation advantages are simply artifacts of the failure to adequately control for confounding variables. the effect of free access on the general public\\'s use of the primary medical literature has not been thoroughly evaluated. recent studies provide little evidence to support the idea that there is a crisis in access to the scholarly literature. further research is needed to investigate whether free access is making a difference in non-research contexts and to better understand the dissemination of scientific literature through peer-to-peer networks and other informal mechanisms.\"\"the journal impact factor ({if}) is generally accepted to be a good measurement of the relevance/quality of articles that a journal publishes. in spite of an, apparently, homogenous peer-review process for a given journal, we hypothesize that the country affiliation of authors from developing latin american ({la}) countries affects the {if} of a journal detrimentally. seven prestigious international journals, one multidisciplinary journal and six serving specific branches of science, were examined in terms of their {if} in the web of science. two subsets of each journal were then selected to evaluate the influence of author\\'s affiliation on the {if}. they comprised contributions (i) with authorship from four latin american ({la}) countries (argentina, brazil, chile and mexico) and (ii) with authorship from five developed countries (england, france, germany, japan and {usa}). both subsets were further subdivided into two groups: articles with authorship from one country only and collaborative articles with authorship from other countries. articles from the five developed countries had {if} close to the overall {if} of the journals and the influence of collaboration on this value was minor. in the case of {la} articles the effect of collaboration (virtually all with developed countries) was significant. the {ifs} for non-collaborative articles averaged 66\\\\% of the overall {if} of the journals whereas the articles in collaboration raised the {ifs} to values close to the overall {if}. the study shows a significantly lower {if} in the group of the subsets of non-collaborative {la} articles and thus that country affiliation of authors from non-developed {la} countries does affect the {if} of a journal detrimentally. there are no data to indicate whether the lower {ifs} of {la} articles were due to their inherent inferior quality/relevance or psycho-social trend towards under-citation of articles from these countries. however, further study is required since there are foreseeable consequences of this trend as it may stimulate strategies by editors to turn down articles that tend to be under-cited.\"\"we have developed {netpath} as a resource of curated human signaling pathways. as an initial step, {netpath} provides detailed maps of a number of immune signaling pathways, which include approximately 1,600 reactions annotated from the literature and more than 2,800 instances of transcriptionally regulated genes - all linked to over 5,500 published articles. we anticipate {netpath} to become a consolidated resource for human signaling pathways that should enable systems biology approaches.\"greece;department of medicine, tufts university school of medicine, boston, massachusetts, usa; andinstitute of continuing medical education of ioannina, ioannina, greece.\",\"comparison of {pubmed}, scopus, web of science, and google scholar: strengths and weaknesses.\",\"the evolution of the electronic age has led to the development of numerous medical databases on the world wide web, offering search facilities on a particular subject and the ability to perform citation analysis. we compared the content coverage and practical utility of {pubmed}, scopus, web of science, and google scholar. the official web pages of the databases were used to extract information on the range of journals covered, search facilities and restrictions, and update frequency. we used the example of a keyword search to evaluate the usefulness of these databases in biomedical information retrieval and a specific published article to evaluate their utility in performing citation analysis. all databases were practical in use and offered numerous search facilities. {pubmed} and google scholar are accessed for free. the keyword search with {pubmed} offers optimal update frequency and includes online early articles; other databases can rate articles by number of citations, as an index of importance. for citation analysis, scopus offers about 20\\\\% more coverage than web of science, whereas google scholar offers results of inconsistent accuracy. {pubmed} remains an optimal tool in biomedical electronic research. scopus covers a wider journal range, of help both in keyword searching and citation analysis, but it is currently limited to recent articles (published after 1995) compared with web of science. google scholar, as for the web in general, can help in the retrieval of even the most obscure information but its use is marred by inadequate, less often updated, citation information.\"\"many scientists now manage the bulk of their bibliographic information electronically, thereby organizing their publications and citation material from digital libraries. however, a library has been described as \"\"thought in cold storage,\"\" and unfortunately many digital libraries can be cold, impersonal, isolated, and inaccessible places. in this review, we discuss the current chilly state of digital libraries for the computational biologist, including {pubmed}, {ieee} xplore, the {acm} digital library, {isi} web of knowledge, scopus, citeseer, {arxiv}, {dblp}, and google scholar. we illustrate the current process of using these libraries with a typical workflow, and highlight problems with managing data and metadata using {uris}. we then examine a range of new applications such as zotero, mendeley, mekentosj papers, {myncbi}, {citeulike}, connotea, and {hubmed} that exploit the web to make these digital libraries more personal, sociable, integrated, and accessible places. we conclude with how these applications may begin to help achieve a digital defrost, and discuss some of the issues that will help or hinder this in terms of making libraries on the web warmer places in the future, becoming resources that are considerably more useful to both humans and machines.\"enhanced display of scientific articles using extended metadata,\"{although the web has transformed science publishing, scientific papers themselves are still essentially ” black boxes”, with much of their content intended for human readers only. typically, computer-readable metadata associated with an article is limited to bibliographic details. by expanding article metadata to include taxonomic names, identifiers for cited material (e.g., publications, sequences, specimens, and other data), and geographical coordinates, publishers could greatly increase the scientific value of their digital content. at the same time this will provide novel ways for users to discover and navigate through this content, beyond the relatively limited linkage provided by bibliographic citation. as a proof of concept, my entry in the elsevier grand challenge extracted extended metadata from a set of articles from the journal molecular phylogeny and evolution and used it to populate a entity-attribute-value database. a simple web interface to this database enables an enhanced display of the content of an article, including a map of localities mentioned either explicitly or implicitly (through links to geotagged data), taxonomic coverage, and both data and citation links. metadata extraction was limited to information listed in tables in the articles, such as genbank sequences and specimen codes. the body of the article wasn\\'t used, a restriction that was deliberate to demonstrate that making extended metadata available doesn\\'t require a journal\\'s publisher to make the full-text freely available (although this is desirable for other reasons).}\"\"the semantic web offers an ideal platform for representing and linking biomedical information, which is a prerequisite for the development and application of analytical tools to address problems in data-intensive areas such as systems biology and translational medicine. as for any new paradigm, the adoption of the semantic web offers opportunities and poses questions and challenges to the life sciences scientific community: which technologies in the semantic web stack will be more beneficial for the life sciences? is biomedical information too complex to benefit from simple interlinked representations? what are the implications of adopting a new paradigm for knowledge representation? what are the incentives for the adoption of the semantic web, and who are the facilitators? is there going to be a semantic web revolution in the life {sciences?we} report here a few reflections on these questions, following discussions at the {swat4ls} (semantic web applications and tools for life sciences) workshop series, of which this journal of biomedical semantics special issue presents selected papers from the 2009 edition, held in amsterdam on november 20th.\"\"the benefit of {powerpoint}™ is continuously debated, but both supporters and detractors have insufficient empirical evidence. its use in university lectures has influenced investigations of {powerpoint}\\'s effects on student performance (e.g., overall quiz/exam scores) in comparison to lectures based on overhead projectors, traditional lectures (e.g.,  ” chalk-and-talk”), and online lectures. thus far, comparisons of overall exam scores have yielded mixed results. the present study decomposes overall quiz scores into auditory, graphic, and alphanumeric scores to reveal new insights into effects of {powerpoint} presentations on student performance. analyses considered retention of lecture information presented to students without the presence of {powerpoint} (i.e., traditional lecture), auditory information in the presence of {powerpoint}, and visual (i.e., graphic and alphanumeric) information displayed on {powerpoint} slides. data were collected from 62 students via quiz and questionnaire. students retained 15\\\\% less information delivered verbally by the lecturer during {powerpoint} presentations, but they preferred {powerpoint} presentations over traditional presentations.\"\"a network of disorders and disease genes linked by known disorder-gene associations offers a platform to explore in a single graph-theoretic framework all known phenotype and disease gene associations, indicating the common genetic origin of many diseases. genes associated with similar disorders show both higher likelihood of physical interactions between their products and higher expression profiling similarity for their transcripts, supporting the existence of distinct disease-specific functional modules. we find that essential human genes are likely to encode hub proteins and are expressed widely in most tissues. this suggests that disease genes also would play a central role in the human interactome. in contrast, we find that the vast majority of disease genes are nonessential and show no tendency to encode hub proteins, and their expression pattern indicates that they are localized in the functional periphery of the network. a selection-based model explains the observed difference between essential and disease genes and also suggests that diseases caused by somatic mutations should not be peripheral, a prediction we confirm for cancer genes.\"\"motivated group members experience a full sense of choice: of doing what one wants. such behavior shows high performance, is enjoyable, and enhances innovation. this essay describes principles of building a motivated research group. copyright 2010 elsevier inc. all rights reserved.\"discovery, {problem-based}, experiential, and {inquiry-based} teaching\",\"evidence for the superiority of guided instruction is explained in the context of our knowledge of human cognitive architecture, expert?novice differences, and cognitive load. although unguided or minimally guided instructional approaches are very popular and intuitively appealing, the point is made that these approaches ignore both the structures that constitute human cognitive architecture and evidence from empirical studies over the past half-century that consistently indicate that minimally guided instruction is less effective and less efficient than instructional approaches that place a strong emphasis on guidance of the student learning process. the advantage of guidance begins to recede only when learners have sufficiently high prior knowledge to provide \"\"internal\"\" guidance. recent developments in instructional research and instructional design models that support guidance during instruction are briefly described.\"classification models for the prediction of clinicians\\' information needs.,the results suggest that classification models based on infobutton usage data are a promising method for the prediction of content topics that a clinician would choose to answer patient care questions while using an {emr} system.\"in this paper we utilize methods of hyperdimensional computing to mediate the identification of therapeutically useful connections for the purpose of literature-based discovery. our approach, named predication-based semantic indexing, is utilized to identify empirically sequences of relationships known as  ” discovery patterns”, such as  ” drug x {inhibits} substance y, substance y {causes} disease z” that link pharmaceutical substances to diseases they are known to treat. these sequences are derived from semantic predications extracted from the biomedical literature by the {semrep} system, and subsequently utilized to direct the search for known treatments for a held out set of diseases. rapid and efficient inference is accomplished through the application of geometric operators in {psi} space, allowing for both the derivation of discovery patterns from a large set of known {treats} relationships, and the application of these discovered patterns to constrain search for therapeutic relationships at scale. our results include the rediscovery of discovery patterns that have been constructed manually by other authors in previous research, as well as the discovery of a set of previously unrecognized patterns. the application of these patterns to direct search through {psi} space results in better recovery of therapeutic relationships than is accomplished with models based on distributional statistics alone. these results demonstrate the utility of efficient approximate inference in geometric space as a means to identify therapeutic relationships, suggesting a role of these methods in drug repurposing efforts. in addition, the results provide strong support for the utility of the discovery pattern approach pioneered by hristovski and his colleagues. \\\\^{a}\\x96º {psi} represents concepts and relations in hyperdimensional space. \\\\^{a}\\x96º {psi} is used to infer discovery patterns from known therapeutic relationships. \\\\^{a}\\x96º these patterns are used to recover therapeutic relationships for a held-out disease set. \\\\^{a}\\x96º {psi} outperforms a co-occurrence based approach in this regard. \\\\^{a}\\x96º {psi} searches efficiently across large networks of relevant relationships..\"\"although there is some evidence that online videos are increasingly used by academics for informal scholarly communication and teaching, the extent to which they are used in published academic research is unknown. this article explores the extent to which {youtube} videos are cited in academic publications and whether there are significant broad disciplinary differences in this practice. to investigate, we extracted the {url} citations to {youtube} videos from academic publications indexed by scopus. a total of 1,808 scopus publications cited at least one {youtube} video, and there was a steady upward growth in citing online videos within scholarly publications from 2006 to 2011, with {youtube} citations being most common within arts and humanities (0.3\\\\%) and the social sciences (0.2\\\\%). a content analysis of 551 {youtube} videos cited by research articles indicated that in science (78\\\\%) and in medicine and health sciences (77\\\\%), over three fourths of the cited videos had either direct scientific (e.g., laboratory experiments) or scientific-related contents (e.g., academic lectures or education) whereas in the arts and humanities, about 80\\\\% of the {youtube} videos had art, culture, or history themes, and in the social sciences, about 63\\\\% of the videos were related to news, politics, advertisements, and documentaries. this shows both the disciplinary differences and the wide variety of innovative research communication uses found for videos within the different subject areas.\"\"a new breed of networking applications offers scientists much more than typical social networking sites, but how useful are they? amy maxmen reports.\"\"{motivation}:in the biological sciences, the need to analyse vast amounts of information has become commonplace. such large-scale analyses often involve drawing together data from a variety of different databases, held remotely on the internet or locally on in-house servers. supporting these tasks are ad hoc collections of data-manipulation tools, scripting languages and visualisation software, which are often combined in arcane ways to create cumbersome systems that have been customised for a particular purpose, and are consequently not readily adaptable to other uses. for many day-to-day bioinformatics tasks, the sizes of current databases, and the scale of the analyses necessary, now demand increasing levels of automation; nevertheless, the unique experience and intuition of human researchers is still required to interpret the end results in any meaningful biological way. putting humans in the loop requires tools to support real-time interaction with these vast and complex data-sets. numerous tools do exist for this purpose, but many do not have optimal interfaces, most are effectively isolated from other tools and databases owing to incompatible data formats, and many have limited real-time performance when applied to realistically large data-sets: much of the user\\'s cognitive capacity is therefore focused on controlling the software and manipulating esoteric file formats rather than on performing the {research.methods}:to confront these issues, harnessing expertise in human-computer interaction ({hci}), high-performance rendering and distributed systems, and guided by bioinformaticians and end-user biologists, we are building reusable software components that, together, create a toolkit that is both architecturally sound from a computing point of view, and addresses both user and developer requirements. key to the system\\'s usability is its direct exploitation of semantics, which, crucially, gives individual components knowledge of their own functionality and allows them to interoperate seamlessly, removing many of the existing barriers and bottlenecks from standard bioinformatics {tasks.results}:the toolkit, named utopia, is freely available from http://utopia.cs.man.ac.uk/ webcite.\"systematic reviews and {meta-analysis},objective: to determine the suitability of insurance claims information for use in clinical outcomes research in ischemic heart disease.\"speculation on the implications of increased use of information and communication technologies in scientific research suggests that use of databases may change the processes and the outcomes of knowledge production. most attention focuses on databases as a large-scale means of communicating research, but they can also be used on a much smaller scale as research tools. this paper presents an ethnographic study of the development of a mouse genome mapping resource organized around a database. through an examination of the natural, social and digital orderings that arise in the construction of the resource, it argues that the use of databases in science, at least in this kind of project, is unlikely to produce wholesale change. such changes as do occur in work practices, communication regimes and knowledge outcomes are dependent on the orderings that each database embodies and is embedded within. instead of imposing its own computer logic, the database provides a focus for specifying and tying together particular natural and social orderings. the database does not act as an independent agent of change, but is an emergent structure that needs to be embedded in an appropriate set of work practices.\"\"the united states ({us}) food and drug administration ({fda}) approves new drugs based on sponsor-submitted clinical trials. the publication status of these trials in the medical literature and factors associated with publication have not been evaluated. we sought to determine the proportion of trials submitted to the {fda} in support of newly approved drugs that are published in biomedical journals that a typical clinician, consumer, or policy maker living in the {us} would reasonably search. we conducted a cohort study of trials supporting new drugs approved between 1998 and 2000, as described in {fda} medical and statistical review documents and the {fda} approved drug label. we determined publication status and time from approval to full publication in the medical literature at 2 and 5 y by searching {pubmed} and other databases through 01 august 2006. we then evaluated trial characteristics associated with publication. we identified 909 trials supporting 90 approved drugs in the {fda} reviews, of which 43\\\\% (394/909) were published. among the subset of trials described in the {fda}-approved drug label and classified as  ” pivotal trials” for our analysis, 76\\\\% (257/340) were published. in multivariable logistic regression for all trials 5 y postapproval, likelihood of publication correlated with statistically significant results (odds ratio [{or}] 3.03, 95\\\\% confidence interval [{ci}] 1.78–5.17); larger sample sizes ({or} 1.33 per 2-fold increase in sample size, 95\\\\% {ci} 1.17–1.52); and pivotal status ({or} 5.31, 95\\\\% {ci} 3.30–8.55). in multivariable logistic regression for only the pivotal trials 5 y postapproval, likelihood of publication correlated with statistically significant results ({or} 2.96, 95\\\\% {ci} 1.24–7.06) and larger sample sizes ({or} 1.47 per 2-fold increase in sample size, 95\\\\% {ci} 1.15–1.88). statistically significant results and larger sample sizes were also predictive of publication at 2 y postapproval and in multivariable cox proportional models for all trials and the subset of pivotal trials. over half of all supporting trials for {fda}-approved drugs remained unpublished ≥ 5 y after approval. pivotal trials and trials with statistically significant results and larger sample sizes are more likely to be published. selective reporting of trial results exists for commonly marketed drugs. our data provide a baseline for evaluating publication bias as the new {fda} amendments act comes into force mandating basic results reporting of clinical trials.\"wellesley, ma; division of general medicine, brigham \\\\& women\\'s hospital, boston, ma; harvard medical school, boston, ma.\",automated acquisition of disease drug knowledge from biomedical and clinical documents: an initial study.,\"this paper presents a method for acquiring disease-specific knowledge and a feasibility study of the method. the method is based on applying a combination of {nlp} and statistical techniques to both biomedical and clinical documents. the approach enabled extraction of knowledge about the drugs clinicians are using for patients with specific diseases based on the patient record, while it is also acquired knowledge of drugs frequently involved in controlled trials for those same diseases. in comparing the disease-drug associations, we found the results to be appropriate: the two text sources contained consistent as well as complementary knowledge, and manual review of the top five disease-drug associations by a medical expert supported their correctness across the diseases.\"open access, and scholarly communication: a study of conflicting paradigms\",\"the open access movement of the past decade, and institutional repositories developed by universities and academic libraries as a part of that movement, have openly challenged the traditional scholarly communication system. this article examines the growth of repositories around the world, and summarizes a growing body of evidence of the response of academics to institutional repositories. it reports the findings of a national survey of academics which highlights the conflict between the principles and rewards of the traditional scholarly communication system, and the benefits of open access. the article concludes by suggesting ways in which academic libraries can alleviate the conflict between these two paradigms.\"\"{background}:assessing the risk of bias in individual studies in a systematic review can be done using individual components or by summarizing the study quality in an overall {score.methods}:we examined the instructions to authors of the 50 cochrane review groups that focus on clinical interventions for recommendations on methodological quality assessment of {studies.results}:forty-one of the review groups (82\\\\%) recommended quality assessment using components and nine using a scale. all groups recommending components recommended to assess concealment of allocation, compared to only two of the groups recommending scales (p < 0.0001). thirty-five groups (70\\\\%) recommended assessment of sequence generation and 21 groups (42\\\\%) recommended assessment of intention-to-treat analysis. only 28 groups (56\\\\%) had specific recommendations for using the quality assessment of studies analytically in reviews, with sensitivity analysis, quality as an inclusion threshold and subgroup analysis being the most commonly recommended methods. the scales recommended had problems in the individual items and some of the groups recommending components recommended items not related to bias in their quality {assessment.conclusion}:we found that recommendations by some groups were not based on empirical evidence and many groups had no recommendations on how to use the quality assessment in reviews. we suggest that all cochrane review groups refer to the cochrane handbook for systematic reviews of interventions, which is evidence-based, in their instructions to authors and that their own guidelines are kept to a minimum and describe only how methodological topics that are specific to their fields should be handled.\"\"network tools and applications in biology ({nettab}) workshops are a series of meetings focused on the most promising and innovative {ict} tools and to their usefulness in bioinformatics. the {nettab} 2011 workshop, held in pavia, italy, in october 2011 was aimed at presenting some of the most relevant methods, tools and infrastructures that are nowadays available for clinical bioinformatics ({cbi}), the research field that deals with clinical applications of bioinformatics.\"10.1001/jama.2010.1262milwaukee 53226, usa.\",management of secondary peritonitis.,\"sepsis represents the host\\'s systemic inflammatory response to bacterial peritonitis. to improve results, both the initiator and the biologic consequences of the peritoneal infective-inflammatory process should be addressed. the initiator may be better controlled in severe forms of peritonitis by aggressive surgical methods, whereas the search for methods to abort its systemic consequences is continuing.\"\"the enormous amount of data available in public gene expression repositories such as gene expression omnibus ({geo}) offers an inestimable resource to explore gene expression programs across several organisms and conditions. this information can be used to discover experiments that induce similar or opposite gene expression patterns to a given query, which in turn may lead to the discovery of new relationships among diseases, drugs or pathways, as well as the generation of new hypotheses. in this work, we present {marq}, a web-based application that allows researchers to compare a query set of genes, e.g. a set of over- and under-expressed genes, against a signature database built from {geo} datasets for different organisms and platforms. {marq} offers an easy-to-use and integrated environment to mine {geo}, in order to identify conditions that induce similar or opposite gene expression patterns to a given experimental condition. {marq} also includes additional functionalities for the exploration of the results, including a meta-analysis pipeline to find genes that are differentially expressed across different experiments. the application is freely available at http://marq.dacya.ucm.es.\"dartmouth medical school, lebanon, new hampshire; department of biological sciences, dartmouth college, hanover, new hampshire; department of computer science, university of new hampshire, durham, new hampshire; department of computer science, university of vermont, burlington, vermont\",bioinformatics.,for whom and in what circumstances\",\"{background}:educational courses for doctors and medical students are increasingly offered via the internet. despite much research, course developers remain unsure about what (if anything) to offer online and how. prospective learners lack evidence-based guidance on how to choose between the options on offer. we aimed to produce theory driven criteria to guide the development and evaluation of internet-based medical {courses.methods}:realist review - a qualitative systematic review method whose goal is to identify and explain the interaction between context, mechanism and outcome. we searched 15 electronic databases and references of included articles, seeking to identify theoretical models of how the internet might support learning from empirical studies which (a) used the internet to support learning, (b) involved doctors or medical students; and (c) reported a formal evaluation. all study designs and outcomes were considered. using immersion and interpretation, we tested theories by considering how well they explained the different outcomes achieved in different educational {contexts.results}:249 papers met our inclusion criteria. we identified two main theories of the course-in-context that explained variation in learners\\' satisfaction and outcomes: davis\\'s technology acceptance model and laurillard\\'s model of interactive dialogue. learners were more likely to accept a course if it offered a perceived advantage over available {non-internet} alternatives, was easy to use technically, and compatible with their values and norms. \\'interactivity\\' led to effective learning only if learners were able to enter into a dialogue - with a tutor, fellow students or virtual tutorials - and gain formative {feedback.conclusions}:different modes of course delivery suit different learners in different contexts. when designing or choosing an internet-based course, attention must be given to the fit between its technical attributes and learners\\' needs and priorities; and to ways of providing meaningful interaction. we offer a preliminary set of questions to aid course developers and learners consider these issues.\"\"online learning initiatives over the past decade have become increasingly comprehensive in their selection of courses and sophisticated in their presentation, culminating in the recent announcement of a number of consortium and startup activities that promise to make a university education on the internet, free of charge, a real possibility. at this pivotal moment it is appropriate to explore the potential for obtaining comprehensive bioinformatics training with currently existing free video resources. this article presents such a bioinformatics curriculum in the form of a virtual course catalog, together with editorial commentary, and an assessment of strengths, weaknesses, and likely future directions for open online learning in this field.\"hardwick, stockton on tees, uk, ts19 8pe. pwong@doctors.net.uk\",antibiotic regimens for secondary peritonitis of gastrointestinal origin in adults.,\"no specific recommendations can be made for the first line treatment of secondary peritonitis in adults with antibiotics, as all regimens showed equivocal efficacy. other factors such as local guidelines and preferences, ease of administration, costs and availability must therefore be taken into consideration in deciding the antibiotic regimen of choice. future trials should attempt to stratify patients and perform intention-to-treat analysis to allow better external validity.\"ny, usa\",biomedical question answering: a survey.,\"in this survey, we reviewed the current state of the art in biomedical {qa} (question answering), within a broader framework of semantic knowledge-based {qa} approaches, and projected directions for the future research development in this critical area of intersection between artificial intelligence, information retrieval, and biomedical informatics. we devised a conceptual framework within which to categorize current {qa} approaches. in particular, we used \"\"semantic knowledge-based {qa}\"\" as a category under which to subsume {qa} techniques and approaches, both corpus-based and knowledge base ({kb})-based, that utilize semantic knowledge-informed techniques in the {qa} process, and we further classified those approaches into three subcategories: (1) semantics-based, (2) inference-based, and (3) logic-based. based on the framework, we first conducted a survey of open-domain or non-biomedical-domain {qa} approaches that belong to each of the three subcategories. we then conducted an in-depth review of biomedical {qa}, by first noting the characteristics of, and resources available for, biomedical {qa} and then reviewing medical {qa} approaches and biological {qa} approaches, in turn. the research articles reviewed in this paper were found and selected through online searches. our review suggested the following tasks ahead for the future research development in this area: (1) construction of domain-specific typology and taxonomy of questions (biological {qa}), (2) development of more sophisticated techniques for natural language ({nl}) question analysis and classification, (3) development of effective methods for answer generation from potentially conflicting evidences, (4) more extensive and integrated utilization of semantic knowledge throughout the {qa} process, and (5) incorporation of logic and reasoning mechanisms for answer inference. corresponding to the growth of biomedical information, there is a growing need for {qa} systems that can help users better utilize the ever-accumulating information. continued research toward development of more sophisticated techniques for processing {nl} text, for utilizing semantic knowledge, and for incorporating logic and reasoning mechanisms, will lead to more useful {qa} systems. copyright 2009 elsevier ireland ltd. all rights reserved.\"\"analysing the properties of a biological system through in silico experimentation requires a satisfactory mathematical representation of the system including accurate values of the model parameters. fortunately, modern experimental techniques allow obtaining time-series data of appropriate quality which may then be used to estimate unknown parameters. however, in many cases, a subset of those parameters may not be uniquely estimated, independently of the experimental data available or the numerical techniques used for estimation. this lack of identifiability is related to the structure of the model, i.e. the system dynamics plus the observation function. despite the interest in knowing a priori whether there is any chance of uniquely estimating all model unknown parameters, the structural identifiability analysis for general non-linear dynamic models is still an open question. there is no method amenable to every model, thus at some point we have to face the selection of one of the possibilities. this work presents a critical comparison of the currently available techniques. to this end, we perform the structural identifiability analysis of a collection of biological models. the results reveal that the generating series approach, in combination with identifiability tableaus, offers the most advantageous compromise among range of applicability, computational complexity and information provided.\"\"motivation: from the scientific community, a lot of effort has been spent on the correct identification of gene and protein names in text, while less effort has been spent on the correct identification of chemical names. dictionary-based term identification has the power to recognize the diverse representation of chemical information in the literature and map the chemicals to their database {identifiers.results}: we developed a dictionary for the identification of small molecules and drugs in text, combining information from {umls}, {mesh}, {chebi}, {drugbank}, {kegg}, {hmdb} and {chemidplus}. rule-based term filtering, manual check of highly frequent terms and disambiguation rules were applied. we tested the combined dictionary and the dictionaries derived from the individual resources on an annotated corpus, and conclude the following: (i) each of the different processing steps increase precision with a minor loss of recall; (ii) the overall performance of the combined dictionary is acceptable (precision 0.67, recall 0.40 (0.80 for trivial names); (iii) the combined dictionary performed better than the dictionary in the chemical recognizer {oscar3}; (iv) the performance of a dictionary based on {chemidplus} alone is comparable to the performance of the combined {dictionary.availability}: the combined dictionary is freely available as an {xml} file in simple knowledge organization system format on the web site {http://www.biosemantics.org/chemlist.contact}: {k.hettne@erasmusmc.nlsupplementary} information: supplementary data are available at bioinformatics online.\"\"the actions of cell adhesion molecules, in particular, cadherins during embryonic development and morphogenesis more generally, regulate many aspects of cellular interactions, regulation and signaling. often, a gradient of cadherin expression levels drives collective and relative cell motions generating macroscopic cell sorting. computer simulations of cell sorting have focused on the interactions of cells with only a few discrete adhesion levels between cells, ignoring biologically observed continuous variations in expression levels and possible nonlinearities in molecular binding. in this paper, we present three models relating the surface density of cadherins to the net intercellular adhesion and interfacial tension for both discrete and continuous levels of cadherin expression. we then use then the {glazier-graner}-hogeweg ({ggh}) model to investigate how variations in the distribution of the number of cadherins per cell and in the choice of binding model affect cell sorting. we find that an aggregate with a continuous variation in the level of a single type of cadherin molecule sorts more slowly than one with two levels. the rate of sorting increases strongly with the interfacial tension, which depends both on the maximum difference in number of cadherins per cell and on the binding model. our approach helps connect signaling at the molecular level to tissue-level morphogenesis.\"\"this paper is based on a panel discussion held at the artificial intelligence in medicine europe ({aime}) conference in amsterdam, the netherlands, in july 2007. it had been more than 15 years since edward shortliffe gave a talk at {aime} in which he characterized artificial intelligence ({ai}) in medicine as being in its \"\"adolescence\"\" (shortliffe {eh}. the adolescence of {ai} in medicine: will the field come of age in the \\'90s? artificial intelligence in medicine 1993;5:93-106). in this article, the discussants reflect on medical {ai} research during the subsequent years and characterize the maturity and influence that has been achieved to date. participants focus on their personal areas of expertise, ranging from clinical decision-making, reasoning under uncertainty, and knowledge representation to systems integration, translational bioinformatics, and cognitive issues in both the modeling of expertise and the creation of acceptable systems.\"\"{background}:there is growing interest in theory-driven, qualitative and mixed-method approaches to systematic review as an alternative to (or to extend and supplement) conventional cochrane-style reviews. these approaches offer the potential to expand the knowledge base in policy-relevant areas - for example by explaining the success, failure or mixed fortunes of complex interventions. however, the quality of such reviews can be difficult to assess. this study aims to produce methodological guidance, publication standards and training resources for those seeking to use the realist and/or meta-narrative approach to systematic {review.methods}/{design}:we will: [a] collate and summarise existing literature on the principles of good practice in realist and meta-narrative systematic review; [b] consider the extent to which these principles have been followed by published and in-progress reviews, thereby identifying how rigour may be lost and how existing methods could be improved; [c] using an online delphi method with an interdisciplinary panel of experts from academia and policy, produce a draft set of methodological steps and publication standards; [d] produce training materials with learning outcomes linked to these steps; [e] pilot these standards and training materials prospectively on real reviews-in-progress, capturing methodological and other challenges as they arise; [f] synthesise expert input, evidence review and real-time problem analysis into more definitive guidance and standards; [g] disseminate outputs to audiences in academia and policy. the outputs of the study will be threefold:1. quality standards and methodological guidance for realist and meta-narrative reviews for use by researchers, research sponsors, students and supervisors2. a \\'{rameses}\\' (realist and meta-review evidence synthesis: evolving standards) statement (comparable to {consort} or {prisma}) of publication standards for such reviews, published in an open-access academic journal.3. a training module for researchers, including learning outcomes, outline course materials and assessment {criteria.discussion}:realist and meta-narrative review are relatively new approaches to systematic review whose overall place in the secondary research toolkit is not yet fully established. as with all secondary research methods, guidance on quality assurance and uniform reporting is an important step towards improving quality and consistency of studies.\"sackville street, po box 88, manchester m60 1qd, uk. i.spasic@manchester.ac.uk\",text mining and ontologies in biomedicine: making sense of raw text,\"the volume of biomedical literature is increasing at such a rate that it is becoming difficult to locate, retrieve and manage the reported information without text mining, which aims to automatically distill information, extract facts, discover implicit links and generate hypotheses relevant to user needs. ontologies, as conceptual models, provide the necessary framework for semantic representation of textual information. the principal link between text and an ontology is terminology, which maps terms to domain-specific concepts. this paper summarises different approaches in which ontologies have been used for text-mining applications in biomedicine.\"\"identifying interesting relationships between pairs of variables in large data sets is increasingly important. here, we present a measure of dependence for two-variable relationships: the maximal information coefficient ({mic}). {mic} captures a wide range of associations both functional and not, and for functional relationships provides a score that roughly equals the coefficient of determination (r(2)) of the data relative to the regression function. {mic} belongs to a larger class of maximal information-based nonparametric exploration ({mine}) statistics for identifying and classifying relationships. we apply {mic} and {mine} to data sets in global health, gene expression, major-league baseball, and the human gut microbiota and identify known and novel relationships.\"act, australia. rick.mclean@health.gov.au\",the effect of web 2.0 on the future of medical practice and education: darwikinian evolution or folksonomic revolution?,\"what constitutes learning in the 21st century will be contested terrain as our society strives toward post-industrial forms of knowledge acquisition and production without having yet overcome the educational contradictions and failings of the industrial age. educational reformers suggest that the advent of new technologies will radically transform what people learn, how they learn, and where they learn, yet studies of diverse learners\\' use of new media cast doubt on the speed and extent of change. drawing on recent empirical and theoretical work, this essay critically examines beliefs about the nature of digital learning and points to the role of social, culture, and economic factors in shaping and constraining educational transformation in the digital era.\"\"clinical coding and classification processes transform natural language descriptions in clinical text into data that can subsequently be used for clinical care, research, and other purposes. this systematic literature review examined studies that evaluated all types of automated coding and classification systems to determine the performance of such systems. studies indexed in medline or other relevant databases prior to march 2009 were considered. the 113 studies included in this review show that automated tools exist for a variety of coding and classification purposes, focus on various healthcare specialties, and handle a wide variety of clinical document types. automated coding and classification systems themselves are not generalizable, nor are the results of the studies evaluating them. published research shows these systems hold promise, but these data must be considered in context, with performance relative to the complexity of the task and the desired outcome.\"how do you engage faculty in the task of developing student information literacy? this case study offers a model for incorporating information retrieval into a social science research project so that literacy becomes an intrinsic part of the course assignment. students showed significant gains in database search and assessment skills with minimal investment of classroom time.teaching and assessment\",\"in just a few years, e-learning has become part of the mainstream in medical education. while e-learning means many things to many people, at its heart it is concerned with the educational uses of technology. for the purposes of this guide, we consider the many ways that the information revolution has affected and remediated the practice of healthcare teaching and learning. deploying new technologies usually introduces tensions, and e-learning is no exception. some wish to use it merely to perform pre?existing activities more efficiently or faster. others pursue new ways of thinking and working that the use of such technology affords them. simultaneously, while education, not technology, is the prime goal (and for healthcare, better patient outcomes), we are also aware that we cannot always predict outcomes. sometimes, we have to take risks, and ?see what happens.? serendipity often adds to the excitement of teaching. it certainly adds to the excitement of learning. the use of technology in support of education is not, therefore, a causal or engineered set of practices; rather, it requires creativity and adaptability in response to the specific and changing contexts in which it is used. medical education, as with most fields, is grappling with these tensions; the {amee} guide to {e-learning} in medical education hopes to help the reader, whether novice or expert, navigate them. this guide is presented both as an introduction to the novice, and as a resource to more experienced practitioners. it covers a wide range of topics, some in broad outline, and others in more detail. each section is concluded with a brief ?take home message? which serves as a short summary of the section. the guide is divided into two parts. the first part introduces the basic concepts of e-learning, e-teaching, and e-assessment, and then focuses on the day?to?day issues of e-learning, looking both at theoretical concepts and practical implementation issues. the second part examines technical, management, social, design and other broader issues in e-learning, and it ends with a review of emerging forms and directions in e-learning in medical education. ?it is through education that the daughter of a peasant can become a doctor, that the son of a mineworker can become the head of the mine, that the child of farm workers can become the president of a great nation? (nelson mandela, 1994)\"new haven, conn 06520, usa.\",scope and impact of financial conflicts of interest in biomedical research,\"context\\xa0 despite increasing awareness about the potential impact of financial conflicts of interest on biomedical research, no comprehensive synthesis of the body of evidence relating to financial conflicts of interest has been performed.\"implementation specifications, and certification criteria for electronic health record technology. interim final rule.\",\"the department of health and human services ({hhs}) is issuing this interim final rule with a request for comments to adopt an initial set of standards, implementation specifications, and certification criteria, as required by section 3004(b)(1) of the public health service act. this interim final rule represents the first step in an incremental approach to adopting standards, implementation specifications, and certification criteria to enhance the interoperability, functionality, utility, and security of health information technology and to support its meaningful use. the certification criteria adopted in this initial set establish the capabilities and related standards that certified electronic health record ({ehr}) technology will need to include in order to, at a minimum, support the achievement of the proposed meaningful use stage 1 (beginning in 2011) by eligible professionals and eligible hospitals under the medicare and medicaid {ehr} incentive programs.\"\"introduction:\\u2002 systematic reviews have shown that there is limited evidence to demonstrate that the information literacy training health librarians provide is effective in improving clinicians\\' information skills or has an impact on patient care. studies lack measures which demonstrate validity and reliability in evaluating the impact of training. aim:\\u2002 to determine what measures have been used; the extent to which they are valid and reliable; to provide guidance for health librarians who wish to evaluate the impact of their information skills training. methods:\\u2002 data sources: systematic review methodology involved searching seven databases, and personal files. study selection: studies were included if they were about information skills training, used an objective measure to assess outcomes, and occurred in a health setting. results:\\u2002 fifty-four studies were included in the review. most outcome measures used in the studies were not tested for the key criteria of validity and reliability. three tested for validity and reliability are described in more detail. conclusions:\\u2002 selecting an appropriate measure to evaluate the impact of training is a key factor in carrying out any evaluation. this systematic review provides guidance to health librarians by highlighting measures used in various circumstances, and those that demonstrate validity and reliability.\"kuopio, finland. kristiina.hayrinen@uku.fi\",\"definition, structure, content, use and impacts of electronic health records: a review of the research literature.\",\"computational data mining and visualization techniques play a central part in the extraction of structure-activity relationship ({sar}) information from compound sets including high-throughput screening data. standard statistical and classification techniques can be used to organize data sets and evaluate the chemical neighborhood of potent hits; however, such methods are limited in their ability to extract complex {sar} patterns from data sets and make them readily accessible to medicinal chemists. therefore, new approaches and data structures are being developed that explicitly focus on molecular structure and its relationship to biological activity across multiple targets. here, we review standard techniques for compound data analysis and describe new data structures and computational tools for {sar} mining of large compound data sets.\"\"systematic reviews and meta-analyses are essential to summarize evidence relating to efficacy and safety of health care interventions accurately and reliably. the clarity and transparency of these reports, however, is not optimal. poor reporting of systematic reviews diminishes their value to clinicians, policy makers, and other {users.since} the development of the {quorom} ({quality} of reporting of meta-analysis) statement--a reporting guideline published in 1999--there have been several conceptual, methodological, and practical advances regarding the conduct and reporting of systematic reviews and meta-analyses. also, reviews of published systematic reviews have found that key information about these studies is often poorly reported. realizing these issues, an international group that included experienced authors and methodologists developed {prisma} (preferred reporting items for systematic reviews and {meta-analyses}) as an evolution of the original {quorom} guideline for systematic reviews and meta-analyses of evaluations of health care {interventions.the} {prisma} statement consists of a 27-item checklist and a four-phase flow diagram. the checklist includes items deemed essential for transparent reporting of a systematic review. in this explanation and elaboration document, we explain the meaning and rationale for each checklist item. for each item, we include an example of good reporting and, where possible, references to relevant empirical studies and methodological literature. the {prisma} statement, this document, and the associated web site (http://www.prisma-statement.org/) should be helpful resources to improve reporting of systematic reviews and meta-analyses.\"salford, uk. a.brettle@salford.ac.uk\",information skills training: a systematic review of the literature.,\"{pubchem} (http://pubchem.ncbi.nlm.nih.gov) is a public repository for biological properties of small molecules hosted by the {us} national institutes of health ({nih}). {pubchem} {bioassay} database currently contains biological test results for more than 700 000 compounds. the goal of {pubchem} is to make this information easily accessible to biomedical researchers. in this work, we present a set of web servers to facilitate and optimize the utility of biological activity information within {pubchem}. these web-based services provide tools for rapid data retrieval, integration and comparison of biological screening results, exploratory structure-activity analysis, and target selectivity examination. this article reviews these bioactivity analysis tools and discusses their uses. most of the tools described in this work can be directly accessed at http://pubchem.ncbi.nlm.nih.gov/assay/. {urls} for accessing other tools described in this work are specified individually.\"\"although technology-enhanced simulation has widespread appeal, its effectiveness remains uncertain. a comprehensive synthesis of evidence may inform the use of simulation in health professions education. to summarize the outcomes of technology-enhanced simulation training for health professions learners in comparison with no intervention. systematic search of {medline}, {embase}, {cinahl}, {eric}, {psychinfo}, scopus, key journals, and previous review bibliographies through may 2011. original research in any language evaluating simulation compared with no intervention for training practicing and student physicians, nurses, dentists, and other health care professionals. reviewers working in duplicate evaluated quality and abstracted information on learners, instructional design (curricular integration, distributing training over multiple days, feedback, mastery learning, and repetitive practice), and outcomes. we coded skills (performance in a test setting) separately for time, process, and product measures, and similarly classified patient care behaviors. from a pool of 10,903 articles, we identified 609 eligible studies enrolling 35,226 trainees. of these, 137 were randomized studies, 67 were nonrandomized studies with 2 or more groups, and 405 used a single-group pretest-posttest design. we pooled effect sizes using random effects. heterogeneity was large (i(2)>50\\\\%) in all main analyses. in comparison with no intervention, pooled effect sizes were 1.20 (95\\\\% {ci}, 1.04-1.35) for knowledge outcomes (n = 118 studies), 1.14 (95\\\\% {ci}, 1.03-1.25) for time skills (n = 210), 1.09 (95\\\\% {ci}, 1.03-1.16) for process skills (n = 426), 1.18 (95\\\\% {ci}, 0.98-1.37) for product skills (n = 54), 0.79 (95\\\\% {ci}, 0.47-1.10) for time behaviors (n = 20), 0.81 (95\\\\% {ci}, 0.66-0.96) for other behaviors (n = 50), and 0.50 (95\\\\% {ci}, 0.34-0.66) for direct effects on patients (n = 32). subgroup analyses revealed no consistent statistically significant interactions between simulation training and instructional design features or study quality. in comparison with no intervention, technology-enhanced simulation training in health professions education is consistently associated with large effects for outcomes of knowledge, skills, and behaviors and moderate effects for patient-related outcomes.\"computer code, or esperanto?\",\"a public-private partnership to create a network of government and private databases to routinely evaluate and prioritize safety questions is in the public interest. better methods are needed, and a knowledgeable workforce is required to conduct the surveillance and understand how to interpret the results. the international community will benefit from the availability of better methods and more experts.\"montreal, canada. martin.dawes@mcgill.ca\",sicily statement on evidence-based practice.,\"a variety of definitions of evidence-based practice ({ebp}) exist. however, definitions are in themselves insufficient to explain the underlying processes of {ebp} and to differentiate between an evidence-based process and evidence-based outcome. there is a need for a clear statement of what {evidence-based} practice ({ebp}) means, a description of the skills required to practise in an evidence-based manner and a curriculum that outlines the minimum requirements for training health professionals in {ebp}. this consensus statement is based on current literature and incorporating the experience of delegates attending the 2003 conference of {evidence-based} health care teachers and developers (\"\"signposting the future of {ebhc}\"\"). {evidence-based} practice has evolved in both scope and definition. {evidence-based} practice ({ebp}) requires that decisions about health care are based on the best available, current, valid and relevant evidence. these decisions should be made by those receiving care, informed by the tacit and explicit knowledge of those providing care, within the context of available resources. health care professionals must be able to gain, assess, apply and integrate new knowledge and have the ability to adapt to changing circumstances throughout their professional life. curricula to deliver these aptitudes need to be grounded in the five-step model of {ebp}, and informed by ongoing research. core assessment tools for each of the steps should continue to be developed, validated, and made freely available. all health care professionals need to understand the principles of {ebp}, recognise {ebp} in action, implement evidence-based policies, and have a critical attitude to their own practice and to evidence. without these skills, professionals and organisations will find it difficult to provide \\'best practice\\'.\"kelvin grove, queensland, australia. a.barnard@qut.edu.au\",information literacy: developing lifelong skills through nursing education.,,is google scholar useful for bibliometrics? a webometric analysis,\"google scholar, the academic bibliographic database provided free-of-charge by the search engine giant google, has been suggested as an alternative or complementary resource to the commercial citation databases like web of knowledge ({isi}/thomson) or scopus (elsevier). in order to check the usefulness of this database for bibliometric analysis, and especially research evaluation, a novel approach is introduced. instead of names of authors or institutions, a webometric analysis of academic web domains is performed. the bibliographic records for 225 top level web domains ({tld}), 19,240 university and 6,380 research centres institutional web domains have been collected from the google scholar database. about 63.8\\\\% of the records are hosted in generic domains like .com or .org, confirming that most of the scholar data come from large commercial or non-profit sources. considering only institutions with at least one record, one-third of the other items (10.6\\\\% from the global) are hosted by the 10,442 universities, while 3,901 research centres amount for an additional 7.9\\\\% from the total. the individual analysis show that universities from china, brazil, spain, taiwan or indonesia are far better ranked than expected. in some cases, large international or national databases, or repositories are responsible for the high numbers found. however, in many others, the local contents, including papers in low impact journals, popular scientific literature, and unpublished reports or teaching supporting materials are clearly overrepresented. google scholar lacks the quality control needed for its use as a bibliometric tool; the larger coverage it provides consists in some cases of items not comparable with those provided by other similar databases.\"visceral, and vascular surgery, university of the saarland, homburg/saar, germany.\",\"evaluation of procalcitonin for predicting septic multiorgan failure and overall prognosis in secondary peritonitis: a prospective, international multicenter study.\",\"scientific innovation depends on finding, integrating, and re-using the products of previous research. here we explore how recent developments in web technology, particularly those related to the publication of data and metadata, might assist that process by providing semantic enhancements to journal articles within the mainstream process of scholarly journal publishing. we exemplify this by describing semantic enhancements we have made to a recent biomedical research article taken from {plos} neglected tropical diseases, providing enrichment to its content and increased access to datasets within it. these semantic enhancements include provision of live {dois} and hyperlinks; semantic markup of textual terms, with links to relevant third-party information resources; interactive figures; a re-orderable reference list; a document summary containing a study summary, a tag cloud, and a citation analysis; and two novel types of semantic enrichment: the first, a supporting claims tooltip to permit  ” citations in context”, and the second, tag trees that bring together semantically related terms. in addition, we have published downloadable spreadsheets containing data from within tables and figures, have enriched these with provenance information, and have demonstrated various types of data fusion (mashups) with results from other research articles and with google maps. we have also published machine-readable {rdf} metadata both about the article and about the references it cites, for which we developed a citation typing ontology, {cito} (http://purl.org/net/cito/). the enhanced article, which is available at http://dx.doi.org/10.1371/journal.pntd.0\\u200b000228.x001, presents a compelling existence proof of the possibilities of semantic publication. we hope the showcase of examples and ideas it contains, described in this paper, will excite the imaginations of researchers and publishers, stimulating them to explore the possibilities of semantic publishing for their own research articles, and thereby break down present barriers to the discovery and re-use of information within traditional modes of scholarly communication.\"\"in this article, the authors first indicate the range of purposes and the variety of settings in which design experiments have been conducted and then delineate five crosscutting features that collectively differentiate design experiments from other methodologies. design experiments have both a pragmatic bent— ” engineering” particular forms of learning—and a theoretical orientation—developing domain-specific theories by systematically studying those forms of learning and the means of supporting them. the authors clarify what is involved in preparing for and carrying out a design experiment, and in conducting a retrospective analysis of the extensive, longitudinal data sets generated during an experiment. logistical issues, issues of measure, the importance of working through the data systematically, and the need to be explicit about the criteria for making inferences are discussed.\"\"nanoscience and technology ({nst}) is a relatively new interdisciplinary scientific domain, and scholars from a broad range of different disciplines are contributing to it. however, there is an ambiguity in its structure and in the extent of multidisciplinary scientific collaboration of {nst}. this paper investigates the multidisciplinary patterns of iranian research in {nst} based on a selection of 1,120 {isi}—indexed articles published during 1974–2007. using text mining techniques, 96 terms were identified as the main terms of the iranian publications in {nst}. then the scientific structure of the iranian {nst} was mapped through multidimensional scaling, based upon the co-occurrence of the main terms in the academic publications. the results showed that the {nst} domain in iranian publications has a multidisciplinary structure which is composed of different fields, such as pure physics, analytical chemistry, chemistry physics, material science and engineering, polymer science, biochemistry and new emerging topics.\"\"most human diseases are complex multi-factorial diseases resulting from the combination of various genetic and environmental factors. in the {kegg} database resource (http://www.genome.jp/kegg/), diseases are viewed as perturbed states of the molecular system, and drugs as perturbants to the molecular system. disease information is computerized in two forms: pathway maps and gene/molecule lists. the {kegg} {pathway} database contains pathway maps for the molecular systems in both normal and perturbed states. in the {kegg} {disease} database, each disease is represented by a list of known disease genes, any known environmental factors at the molecular level, diagnostic markers and therapeutic drugs, which may reflect the underlying molecular system. the {kegg} {drug} database contains chemical structures and/or chemical components of all drugs in japan, including crude drugs and {tcm} (traditional chinese medicine) formulas, and drugs in the {usa} and europe. this database also captures knowledge about two types of molecular networks: the interaction network with target molecules, metabolizing enzymes, other drugs, etc. and the chemical structure transformation network in the history of drug development. the new disease/drug information resource named {kegg} {medicus} can be used as a reference knowledge base for computational analysis of molecular networks, especially, by integrating large-scale experimental datasets.\"rutgers university, 4 huntington street, new brunswick, nj 08903\",information science,\"this essay is a personal analysis of information science as a field of scientific inquiry and professional practice that has evolved over the past half-century. various sections examine the origin of information science in respect to the problems of information explosion; the social role of the field; the nature of ?information? in information science; the structure of the field in terms of problems addressed; evolutionary trends in information retrieval as a major branch of information science; the relation of information science to other fields, most notably librarianship and computer science; and educational models and issues. conclusions explore some dominant trends affecting the field.\"\"i propose the index h, defined as the number of papers with citation number > or =h, as a useful index to characterize the scientific output of a researcher.\"\"{background}:in order to satisfy different needs, medical terminology systems must have richer structures. this study examines whether a swedish primary health care version of the mono-hierarchical {icd}-10 ({ksh97}-p) may obtain a richer structure using category and chapter mappings from {ksh97}-p to {snomed} {ct} and {snomed} {ct}\\'s structure. manually-built mappings from {ksh97}-p\\'s categories and chapters to {snomed} {ct}\\'s concepts are used as a starting {point.results}:the mappings are manually evaluated using computer-produced information and a small number of mappings are updated. a new and poly-hierarchical chapter division of {ksh97}-p\\'s categories has been created using the category and chapter mappings and {snomed} {ct}\\'s generic structure. in the new chapter division, most categories are included in their original chapters. a considerable number of concepts are included in other chapters than their original chapters. most of these inclusions can be explained by {icd}-10\\'s design. {ksh97}-p\\'s categories are also extended with attributes using the category mappings and {snomed} {ct}\\'s defining attribute relationships. about three-fourths of all concepts receive an attribute of type finding site and about half of all concepts receive an attribute of type associated morphology. other types of attributes are less {common.conclusions}:it is possible to use mappings from {ksh97}-p to {snomed} {ct} and {snomed} {ct}\\'s structure to enrich {ksh97}-p\\'s mono-hierarchical structure with a poly-hierarchical chapter division and attributes of type finding site and associated morphology. the final mappings are available as additional files for this paper.\"exemplar-based visualization of large document corpus ({infovis2009}-1115),\"with the rapid growth of the world wide web and electronic information services, text corpus is becoming available online at an incredible rate. by displaying text data in a logical layout (e.g., color graphs), text visualization presents a direct way to observe the documents as well as understand the relationship between them. in this paper, we propose a novel technique, exemplar-based visualization ({ev}), to visualize an extremely large text corpus. capitalizing on recent advances in matrix approximation and decomposition, {ev} presents a probabilistic multidimensional projection model in the low-rank text subspace with a sound objective function. the probability of each document proportion to the topics is obtained through iterative optimization and embedded to a low dimensional space using parameter embedding. by selecting the representative exemplars, we obtain a compact approximation of the data. this makes the visualization highly efficient and flexible. in addition, the selected exemplars neatly summarize the entire data set and greatly reduce the cognitive overload in the visualization, leading to an easier interpretation of large text corpus. empirically, we demonstrate the superior performance of {ev} through extensive experiments performed on the publicly available text data sets.\"chalmers research group, and children\\'s hospital of eastern ontario research institute, ottawa, ontario, canada. kshojania@ohri.ca\",how quickly do systematic reviews go out of date? a survival analysis.,\"electronic health tools provide little value if the intended users lack the skills to effectively engage them. with nearly half the adult population in the united states and canada having literacy levels below what is needed to fully engage in an information-rich society, the implications for using information technology to promote health and aid in health care, or for {ehealth}, are considerable. engaging with {ehealth} requires a skill set, or literacy, of its own. the concept of {ehealth} literacy is introduced and defined as the ability to seek, find, understand, and appraise health information from electronic sources and apply the knowledge gained to addressing or solving a health problem. in this paper, a model of {ehealth} literacy is introduced, comprised of multiple literacy types, including an outline of a set of fundamental skills consumers require to derive direct benefits from {ehealth}. a profile of each literacy type with examples of the problems patient-clients might present is provided along with a resource list to aid health practitioners in supporting literacy improvement with their patient-clients across each domain. facets of the model are illustrated through a set of clinical cases to demonstrate how health practitioners can address {ehealth} literacy issues in clinical or public health practice. potential future applications of the model are discussed.\"\"we considered all matches played by professional tennis players between 1968 and2010, and, on the basis of this data set, constructed a directed and weighted network of contacts. the resulting graph showed complex features, typical of many real networked systems studied in literature. we developed a diffusion algorithm and applied it to the tennis contact network in order to rank professional players. jimmy connors was identified as the best player in the history of tennis according to our ranking procedure. we performed a complete analysis by determining the best players on specific playing surfaces as well as the best ones in each of the years covered by the data set. the results of our technique were compared to those of two other well established methods. in general, we observed that our ranking method performed better: it had a higher predictive power and did not require the arbitrary introduction of external criteria for the correct assessment of the quality of players. the present work provides novel evidence of the utility of tools and methods of network theory in real applications.\"skills, and resources for pharmacy informatics education.\",\"this paper presents a profile of user behaviour in relation to the use of electronic information services ({eis}), information skills, and the role of training and wider learning experiences in {uk} further education colleges. the research was conducted under the {jisc} user behaviour monitoring and evaluation framework. work was conducted in two strands, by two project teams, {justeis} and {jubilee}. {justeis} profiled the use of {eis} and assessed the availability of {eis}. {jubilee} objectives focussed on understanding the barriers and enablers, with a view to developing success criteria. {justeis} used a multi-stage stratified sampling process, and collected data from 270 respondents from 17 departments in the baseline survey (2001/2002). {jubilee} conducted in-depth fieldwork in five institutions and snapshot fieldwork in 10 institutions, collecting data from 528 respondents. information skills and experience develop across work, home and study. there is a growing use of {eis} in curriculum, but practice varies between institutions and disciplines. tutors express concern about student\\'s ability to evaluate and use the information that they find. assignments can promote {eis} use. the main categories of {eis} used by students are search engines and organisational web sites. search engines are the preferred search tools and search strategies are basic. information skills are acquired through a variety of routes, with peer instruction, surfing and personal experience, instruction from tutors, and {lis} induction and training all making an important contribution. the solutions to improving students\\' information skills may include use of the virtual training suites, but librarians need to adopt different roles in promoting and evaluating use of such tools.\"pharmspresso is a text analysis tool that extracts pharmacogenomic concepts from the literature automatically and thus captures our current understanding of gene-drug interactions in a computable form. we have made pharmspresso available at http://pharmspresso.stanford.edu.\"the growth of electronic publication and informatics archives makes it possible to harvest vast quantities of knowledge about knowledge, or  ” metaknowledge.” we review the expanding scope of metaknowledge research, which uncovers regularities in scientific claims and infers the beliefs, preferences, research tools, and strategies behind those regularities. metaknowledge research also investigates the effect of knowledge context on content. teams and collaboration networks, institutional prestige, and new technologies all shape the substance and direction of research. we argue that as metaknowledge grows in breadth and quality, it will enable researchers to reshape science—to identify areas in need of reexamination, reweight former certainties, and point out new paths that cut across revealed assumptions, heuristics, and disciplinary boundaries.\"10.1136/bmj.326.7384.328\"although controlled biomedical terminologies have been with us for centuries, it is only in the last couple of decades that close attention has been paid to the quality of these terminologies. the result of this attention has been the development of auditing methods that apply formal methods to assessing whether terminologies are complete and accurate. we have performed an extensive literature review to identify published descriptions of these methods and have created a framework for characterizing them. the framework considers manual, systematic and heuristic methods that use knowledge (within or external to the terminology) to measure quality factors of different aspects of the terminology content (terms, semantic classification, and semantic relationships). the quality factors examined included concept orientation, consistency, non-redundancy, soundness and comprehensive coverage. we reviewed 130 studies that were retrieved based on keyword search on publications in {pubmed}, and present our assessment of how they fit into our framework. we also identify which terminologies have been audited with the methods and provide examples to illustrate each part of the framework.\"\"motivation: ontologies provide a structured representation of the concepts of a domain of knowledge as well as the relations between them. attribute ontologies are used to describe the characteristics of the items of a domain, such as the functions of proteins or the signs and symptoms of disease, which opens the possibility of searching a database of items for the best match to a list of observed or desired attributes. however, naive search methods do not perform well on realistic data because of noise in the data, imprecision in typical queries and because individual items may not display all attributes of the category they belong to.\"\"the anatomical therapeutic chemical ({atc}) classification system maintained by the world health organization provides a global standard for the classification of medical substances and serves as a source for drug repurposing research. nevertheless, it lacks several drugs that are major players in the global drug market. in order to establish classifications for yet unclassified drugs, this paper presents a newly developed approach based on a combination of information extraction ({ie}) and machine learning ({ml}) techniques. most of the information about drugs is published in the scientific articles. therefore, an {ie}-based framework is employed to extract terms from free text that express drug?s chemical, pharmacological, therapeutic, and systemic effects. the extracted terms are used as features within a {ml} framework to predict putative {atc} class labels for unclassified drugs. the system was tested on a portion of {atc} containing drugs with an indication on the cardiovascular system. the class prediction turned out to be successful with the best predictive accuracy of 89.47\\\\% validated by a 100-fold bootstrapping of the training set and an accuracy of 77.12\\\\% on an independent test set. the presented concept-based classification system outperformed state-of-the-art classification methods based on chemical structure properties.\"\"digital libraries, e-journal platforms, portals, e-prints and other web-based information systems provide services supporting users to perform intense work tasks that require complex interaction activities. the main components of such services are the users, the offered content and the system on which they are performed. this paper presents a model, which analyses the attributes of the electronic information services\\' components that affect user interaction and correlates them in the usefulness and usability evaluation process. an experimental study traces the relations between usefulness and usability, indicating that these evaluation parameters are interconnected and users do not find discriminating differences between them. the analysis of the content and system attributes suggests that user interaction is affected equally by content and system characteristics. finally, the study illustrates users\\' preference for the attributes that constitute a useful system in contrast to those that support usability. 10.1177/0165551506065934\"\"the idea that a new generation of students is entering the education system has excited recent attention among educators and education commentators. termed \\'digital natives\\' or the \\'net generation\\', these young people are said to have been immersed in technology all their lives, imbuing them with sophisticated technical skills and learning preferences for which traditional education is unprepared. grand claims are being made about the nature of this generational change and about the urgent necessity for educational reform in response. a sense of impending crisis pervades this debate. however, the actual situation is far from clear. in this paper, the authors draw on the fields of education and sociology to analyse the digital natives debate. the paper presents and questions the main claims made about digital natives and analyses the nature of the debate itself. we argue that rather than being empirically and theoretically informed, the debate can be likened to an academic form of a \\'moral panic\\'. we propose that a more measured and disinterested approach is now required to investigate \\'digital natives\\' and their implications for education.\"ottawa, ontario, canada. dmoher@uottawa.ca\",a systematic review identified few methods and strategies describing when and how to update systematic reviews,\"{objective}: systematic reviews ({srs}) are convenient summaries of evidence for health care practitioners. they form a basis for clinical practice guidelines and suggest directions for new research. {srs} are most helpful if they are current; however, most of them are not being updated. this {sr} summarizes strategies and methods describing when and how to update {srs}. {study} {design} {and} {setting}: we searched {medline} (1966 to december 2005), {psycinfo}, the cochrane methodology register, and the 2005 cochrane colloquium proceedings to identify records describing when and how to update {srs} in health care. {results}: four updating strategies, one technique, and two statistical methods were identified. three strategies addressed steps for updating, and one strategy presented a model for assessing the need to update. one technique discussed the use of the \"\"entry date\"\" field in bibliographic searching. the statistical methods were cumulative meta-analysis and a test for detecting outdated meta-analyses with statistically nonsignificant results. {conclusion}: little research has been conducted on when and how to update {srs} in contrast to other methodological areas of conducting {srs} (e.g., publication bias, variance imputation). the feasibility and efficiency of the identified approaches is uncertain. more research is needed to develop pragmatic and efficient methodologies for updating {srs}.\"\"{background}:electronic health records are invaluable for medical research, but much information is stored as free text rather than in a coded form. for example, in the {uk} general practice research database ({gprd}), causes of death and test results are sometimes recorded only in free text. free text can be difficult to use for research if it requires time-consuming manual review. our aim was to develop an automated method for extracting coded information from free text in electronic patient {records.methods}:we reviewed the electronic patient records in {gprd} of a random sample of 3310 patients who died in 2001, to identify the cause of death. we developed a computer program called the freetext matching algorithm ({fma}) to map diagnoses in text to the read clinical terminology. the program uses lookup tables of synonyms and phrase patterns to identify diagnoses, dates and selected test results. we tested it on two random samples of free text from {gprd} (1000 texts associated with death in 2001, and 1000 general texts from cases and controls in a coronary artery disease study), comparing the output to the {u.s}. national library of medicine\\'s {metamap} program and the gold standard of manual {review.results}:among 3310 patients registered in the {gprd} who died in 2001, the cause of death was recorded in coded form in 38.1 of patients, and in the free text alone in 19.4\\\\%. on the 1000 texts associated with death, {fma} coded 683 of the 735 positive diagnoses, with precision (positive predictive value) 98.4\\\\% (95\\\\% confidence interval ({ci}) 97.2, 99.2) and recall (sensitivity) 92.9\\\\% (95\\\\% {ci} 90.8, 94.7). on the general sample, {fma} detected 346 of the 447 positive diagnoses, with precision 91.5\\\\% (95\\\\% {ci} 88.3, 94.1) and recall 77.4\\\\% (95\\\\% {ci} 73.2, 81.2), which was similar to {metamap}.{conclusions}:we have developed an algorithm to extract coded information from free text in {gp} records with good precision. it may facilitate research using free text in electronic patient records, particularly for extracting the cause of death.\"usa\",health information system implementation: a qualitative meta-analysis,\"healthcare information systems ({hiss}) are often implemented to enhance the quality of care and the degree to which it is patient-centered, as well as to improve the efficiency and safety of services. however, the outcomes of {his} implementations have not met expectations. we set out to organize the knowledge gained in qualitative studies performed in association with {his} implementations and to use this knowledge to outline an updated structure for implementation planning. a multi-disciplinary team performed the analyses in order to cover as many aspects of the primary studies as possible. we found that merely implementing an {his} will not automatically increase organizational efficiency. strategic, tactical, and operational actions have to be taken into consideration, including management involvement, integration in healthcare workflow, establishing compatibility between software and hardware and, most importantly, user involvement, education and training. the results should be interpreted as a high-order scheme, and not a predictive theory.\"\"the past decade has witnessed the modern advances of high-throughput technology and rapid growth of research capacity in producing large-scale biological data, both of which were concomitant with an exponential growth of biomedical literature. this wealth of scholarly knowledge is of significant importance for researchers in making scientific discoveries and healthcare professionals in managing health-related matters. however, the acquisition of such information is becoming increasingly difficult due to its large volume and rapid growth. in response, the national center for biotechnology information ({ncbi}) is continuously making changes to its {pubmed} web service for improvement. meanwhile, different entities have devoted themselves to developing web tools for helping users quickly and efficiently search and retrieve relevant publications. these practices, together with maturity in the field of text mining, have led to an increase in the number and quality of various web tools that provide comparable literature search service to {pubmed}. in this study, we review 28 such tools, highlight their respective innovations, compare them to the {pubmed} system and one another, and discuss directions for future development. furthermore, we have built a website dedicated to tracking existing systems and future advances in the field of biomedical literature search. taken together, our work serves information seekers in choosing tools for their needs and service providers and developers in keeping current in the field. database {url}: {http://www.ncbi.nlm.nih.gov/cbbresearch}/lu/search.\"\"the {sage} ({standards-based} active guideline environment) project was formed to create a methodology and infrastructure required to demonstrate integration of decision-support technology for guideline-based care in commercial clinical information systems. this paper describes the development and innovative features of the {sage} guideline model and reports our experience encoding four guidelines. innovations include methods for integrating guideline-based decision support with clinical workflow and employment of enterprise order sets. using {sage}, a clinician informatician can encode computable guideline content as recommendation sets using only standard terminologies and standards-based patient information models. the {sage} model supports encoding large portions of guideline knowledge as re-usable declarative evidence statements and supports querying external knowledge sources.\"\"design and implementation of healthcare information systems affect both computer scientists and health care professionals. in this paper we present our approach to integrate the management of information systems in the education of healthcare professionals and computer scientists alike. we designed a multidisciplinary course for medical and informatics students to provide them with practical experience concerning the design and implementation of medical information systems. this course was implemented in the curriculum of the university of m\\\\\"\"{u}nster in 2009. the key element is a case study that is performed by small teams of medical and informatics students. a practical course on management of information systems can be useful for medical students who want to enhance their knowledge in information systems as well as for informatics students with particular interests in medicine.\"levels of measurement and the \"\"laws\"\" of statistics.\",\"reviewers of research reports frequently criticize the choice of statistical methods. while some of these criticisms are well-founded, frequently the use of various parametric methods such as analysis of variance, regression, correlation are faulted because: (a) the sample size is too small, (b) the data may not be normally distributed, or (c) the data are from likert scales, which are ordinal, so parametric statistics cannot be used. in this paper, i dissect these arguments, and show that many studies, dating back to the 1930s consistently show that parametric statistics are robust with respect to violations of these assumptions. hence, challenges like those above are unfounded, and parametric methods can be utilized without concern for \"\"getting the wrong answer\"\".\"\"the effects of accompanying lectures with computer-mediated {powerpoint} presentations or {powerpoint} generated overheads on students\\' self-efficacy, attitudes, course performance, and class-related behaviors were examined. two introduction to developmental psychology sections were initially taught with lectures accompanied by either overheads or computer-mediated presentations. the teaching format was switched halfway through the semester. students reported higher self-efficacy and more positive attitudes toward the class with computer-mediated presentations. they also claimed that the website was more interesting and useful under these teaching conditions, indicating a halo effect of the computer-mediated presentations. however, the teaching format did not appear to affect course-related behavior, such as performance on exams, class attendance, participation in class discussions, or course website usage.\"\"molecular biology now dominates pharmacology so thoroughly that it is difficult to recall that only a generation ago the field was very different. to understand drug action today, we characterize the targets through which they act and new drug leads are discovered on the basis of target structure and function. until the mid-1980s the information often flowed in reverse: investigators began with organic molecules and sought targets, relating receptors not by sequence or structure but by their ligands. recently, investigators have returned to this chemical view of biology, bringing to it systematic and quantitative methods of relating targets by their ligands. this has allowed the discovery of new targets for established drugs, suggested the bases for their side effects, and predicted the molecular targets underlying phenotypic screens. the bases for these new methods, some of their successes and liabilities, and new opportunities for their use are described.\"\"the small molecule pathway database ({smpdb}) is an interactive, visual database containing more than 350 small-molecule pathways found in humans. more than 2/3 of these pathways (>280) are not found in any other pathway database. {smpdb} is designed specifically to support pathway elucidation and pathway discovery in clinical metabolomics, transcriptomics, proteomics and systems biology. {smpdb} provides exquisitely detailed, hyperlinked diagrams of human metabolic pathways, metabolic disease pathways, metabolite signaling pathways and drug-action pathways. all {smpdb} pathways include information on the relevant organs, organelles, subcellular compartments, protein cofactors, protein locations, metabolite locations, chemical structures and protein quaternary structures. each small molecule is hyperlinked to detailed descriptions contained in the human metabolome database ({hmdb}) or {drugbank} and each protein or enzyme complex is hyperlinked to {uniprot}. all {smpdb} pathways are accompanied with detailed descriptions, providing an overview of the pathway, condition or processes depicted in each diagram. the database is easily browsed and supports full text searching. users may query {smpdb} with lists of metabolite names, drug names, genes/protein names, {swissprot} {ids}, {genbank} {ids}, affymetrix {ids} or agilent microarray {ids}. these queries will produce lists of matching pathways and highlight the matching molecules on each of the pathway diagrams. gene, metabolite and protein concentration data can also be visualized through {smpdb}\\'s mapping interface. all of {smpdb}\\'s images, image maps, descriptions and tables are downloadable. {smpdb} is available at: http://www.smpdb.ca.\"knowledge and clinical behaviour: a controlled trial and before and after study.\",\"the effective development of a computer based assessment ({cba}) depends on students\\' acceptance. the purpose of this study is to build a model that demonstrates the constructs that affect students\\' behavioral intention to use a {cba}. the proposed model, computer based assessment acceptance model ({cbaam}) is based on previous models of technology acceptance such as technology acceptance model ({tam}), theory of planned behavior ({tpb}), and the unified theory of acceptance and usage of technology ({utaut}). constructs from previous models were used such as perceived usefulness, perceived ease of use, computer self efficacy, social influence, facilitating conditions and perceived playfulness. additionally, two new variables, content and goal expectancy, were added to the proposed research model. data were collected from 173 participants in an introductory informatics course using a survey questionnaire. partial least squares ({pls}) was used to test the measurement and the structural model. results indicate that perceived ease of use and perceived playfulness have a direct effect on {cba} use. perceived usefulness, computer self efficacy, social influence, facilitating conditions, content and goal expectancy have only indirect effects. these eight variables explain approximately 50\\\\% of the variance of behavioural intention. \\\\^{a}\\x96º we propose computer based assessment acceptance model ({cbaam}). \\\\^{a}\\x96º {cbaam} uses eight constructs to define behavioural intention to use a {cba}. \\\\^{a}\\x96º behavioural intention is explained directly through playfulness and ease of use. \\\\^{a}\\x96º playfulness is defined by usefulness, content, ease of use and goal expectancy. \\\\^{a}\\x96º computer self efficacy and facilitating conditions define perceived ease of use.\"\"{purpose}: little is known about how often residents encounter unanswered clinical questions in their training. this knowledge would facilitate the development of curricula to help residents practice evidence-based medicine. this study was conducted to determine the frequency, characteristics, and pursuit of residents\\' clinical questions. {subjects} {and} {methods}: residents in a university-based primary care internal medicine program were observed in two hospital-based teaching clinics. residents were interviewed after each patient encounter to determine whether they had any remaining clinical questions. at the end of each clinic session, they recorded their level of agreement with a series of statements about factors that were expected to motivate residents to seek the answers to each question. one week later, residents were contacted to determine if they had pursued these questions. {results}: sixty-four residents were interviewed after 401 (99\\\\%) of 404 patient encounters. they identified 280 new questions, approximately 2 questions for every 3 patients. the most common types of questions were related to therapy (38\\\\%) or diagnosis (27\\\\%). the residents were subsequently contacted about 277 (99\\\\%) of their questions. of these, only 80 (29\\\\%) were pursued, most commonly by consulting textbooks (31\\\\%), original articles (21\\\\%), or attending physicians (17\\\\%). in a multivariable analysis, belief that the patient expected the answer (odds ratio [{or}] = 2.3, 95\\\\% confidence interval [{ci}]: 1.3 to 4.0,  p  = 0.004) and fear of malpractice exposure ({or} = 2.1, 95\\\\% {ci}: 1.0 to 4.3,  p  = 0.05) were associated with information pursuit. lack of time (60\\\\%) and forgetting the question (29\\\\%) were the most frequent reasons for failing to pursue a question. {conclusion}: residents frequently encountered new clinical questions in the outpatient clinic, but infrequently answered them. efforts to demonstrate the feasibility of timely searches, remind them of their questions, and reinforce the exigency (educational if not clinical) of all questions may reclaim missed opportunities for self-directed learning.\"\"biologists rely heavily on the language of information, coding, and transmission that is commonplace in the field of information theory developed by claude shannon, but there is open debate about whether such language is anything more than facile metaphor. philosophers of biology have argued that when biologists talk about information in genes and in evolution, they are not talking about the sort of information that shannon\\'s theory addresses. first, philosophers have suggested that shannon\\'s theory is only useful for developing a shallow notion of correlation, the so-called  causal sense  of information. second, they typically argue that in genetics and evolutionary biology, information language is used in a  semantic sense,  whereas semantics are deliberately omitted from shannon\\'s theory. neither critique is well-founded. here we propose an alternative to the causal and semantic senses of information: a transmission sense of information, in which an object x conveys information if the function of x is to reduce, by virtue of its sequence properties, uncertainty on the part of an agent who observes x. the transmission sense not only captures much of what biologists intend when they talk about information in genes, but also brings shannon\\'s theory back to the fore. by taking the viewpoint of a communications engineer and focusing on the decision problem of how information is to be packaged for transport, this approach resolves several problems that have plagued the information concept in biology, and highlights a number of important features of the way that information is encoded, stored, and transmitted as genetic sequence.\"\"learning is often considered complete when a student can produce the correct answer to a question. in our research, students in one condition learned foreign language vocabulary words in the standard paradigm of repeated study-test trials. in three other conditions, once a student had correctly produced the vocabulary item, it was repeatedly studied but dropped from further testing, repeatedly tested but dropped from further study, or dropped from both study and test. repeated studying after learning had no effect on delayed recall, but repeated testing produced a large positive effect. in addition, students\\' predictions of their performance were uncorrelated with actual performance. the results demonstrate the critical role of retrieval practice in consolidating learning and show that even university students seem unaware of this fact.\",information as thing,1st floor coleridge house, northern general hospital, herries road, sheffield s5 7au, uk. d.newble@sheffield.ac.uk\",techniques for measuring clinical competence: objective structured clinical examinations.,brisbane, q4102, australia. hangwitang@yahoo.com\",googling for a diagnosis—use of google as a diagnostic aid: internet based study,\"increasing evidence that many pharmaceutically relevant compounds elicit their effects through binding to multiple targets, so-called polypharmacology, is beginning to change conventional drug discovery and design strategies. in light of this paradigm shift, we have mined publicly available compound and bioactivity data for promiscuous chemotypes. for this purpose, a hierarchy of active compounds, atomic property based scaffolds, and unique molecular topologies were generated, and activity annotations were analyzed using this framework. starting from ∼35\\u2009000 compounds active against human targets with at least 1 {μm} potency, 33 chemotypes with distinct topology were identified that represented molecules active against at least 3 different target families. network representations were utilized to study scaffold-target family relationships and activity profiles of scaffolds corresponding to promiscuous chemotypes. a subset of promiscuous chemotypes displayed a significant enrichment in drugs over bioactive compounds. a total of 190 drugs were identified that had on average only 2 known target annotations but belonged to the 7 most promiscuous chemotypes that were active against 8-15 target families. these drugs should be attractive candidates for polypharmacological profiling.\"ontologies and information visualization for drug repurposing\",\"the immense growth of {medline} coupled with the realization that a vast amount of biomedical knowledge is recorded in free-text format, has led to the appearance of a large number of literature mining techniques aiming to extract biomedical terms and their inter-relations from the scientific literature. ontologies have been extensively utilized in the biomedical domain either as controlled vocabularies or to provide the framework for mapping relations between concepts in biology and medicine. literature-based approaches and ontologies have been used in the past for the purpose of hypothesis generation in connection with drug discovery. here, we review the application of literature mining and ontology modeling and traversal to the area of drug repurposing ({dr}). in recent years, {dr} has emerged as a noteworthy alternative to the traditional drug development process, in response to the decreased productivity of the biopharmaceutical industry. thus, systematic approaches to {dr} have been developed, involving a variety of in silico, genomic and high-throughput screening technologies. attempts to integrate literature mining with other types of data arising from the use of these technologies as well as visualization tools assisting in the discovery of novel associations between existing drugs and new indications will also be presented.\"\"since its foundation in 1987, the program has undergone several major modifications. from a degree program following medical school it developed into a full-fledged, dedicated 4-year program on medical information sciences training high-school graduates for a master degree. the curriculum has been based from its outset within the university of {amsterdam-faculty} of medicine. this organizational structure leaves ample opportunity for integration of the informatics-oriented components with the medical and health care-oriented components in the program. student-centered approaches are heavily employed in the program, emphasizing students\\' critical appraisal and a style of life-long learning. overall, our program follows the {imia} recommendations with slightly more focus on medicine and health care organization.\"\"a journal may be considered as having dimension-specific prestige when its score, based on a given journal ranking model, exceeds a threshold value. but a journal has multidimensional prestige only if it is a prestigious journal with respect to a number of dimensions—e.g., institute for scientific information impact factor, immediacy index, eigenfactor score, and article influence score. the multidimensional prestige of influential journals takes into account the fact that several prestige indicators should be used for a distinct analysis of the impact of scholarly journals in a subject category. after having identified the multidimensionally influential journals, their prestige scores can be aggregated to produce a summary measure of multidimensional prestige for a subject category, which satisfies numerous properties. using this measure of multidimensional prestige to rank subject categories, we have found the top scientific subject categories of web of knowledge as of 2010.\"visualization, and navigation of large image data sets: one 5000-section {ct} scan can ruin your whole day.\",\"there is an abundance of information about drugs available on the web. data sources range from medicinal chemistry results, over the impact of drugs on gene expression, to the outcomes of drugs in clinical trials. these data are typically not connected together, which reduces the ease with which insights can be gained. linking open drug data ({lodd}) is a task force within the world wide web consortium\\'s ({w3c}) health care and life sciences interest group ({hcls} {ig}). {lodd} has surveyed publicly available data about drugs, created linked data representations of the data sets, and identified interesting scientific and business questions that can be answered once the data sets are connected. the task force provides recommendations for the best practices of exposing data in a linked data representation. in this paper, we present past and ongoing work of {lodd} and discuss the growing importance of linked data as a foundation for pharmaceutical {r\\\\&d} data sharing.\"office of medical education and faculty development, feinberg school of medicine, northwestern university, chicago, illinois, usa;  gordon center for research in medical education, miller school of medicine, university of miami, miami, florida, usa;  office for teaching and learning in medicine, vanderbilt university medical center, nashville, tennessee, usa\",a critical review of simulation-based medical education research: 2003–2009,\"objectives\\u2002 this article reviews and critically evaluates historical and contemporary research on simulation-based medical education ({sbme}). it also presents and discusses 12 features and best practices of {sbme} that teachers should know in order to use medical simulation technology to maximum educational benefit. methods\\u2002 this qualitative synthesis of {sbme} research and scholarship was carried out in two stages. firstly, we summarised the results of three {sbme} research reviews covering the years 1969–2003. secondly, we performed a selective, critical review of {sbme} research and scholarship published during 2003–2009. results\\u2002 the historical and contemporary research synthesis is reported to inform the medical education community about 12 features and best practices of {sbme}: (i) feedback; (ii) deliberate practice; (iii) curriculum integration; (iv) outcome measurement; (v) simulation fidelity; (vi) skill acquisition and maintenance; (vii) mastery learning; (viii) transfer to practice; (ix) team training; (x) high-stakes testing; (xi) instructor training, and (xii) educational and professional context. each of these is discussed in the light of available evidence. the scientific quality of contemporary {sbme} research is much improved compared with the historical record. conclusions\\u2002 development of and research into {sbme} have grown and matured over the past 40\\xa0years on substantive and methodological grounds. we believe the impact and educational utility of {sbme} are likely to increase in the future. more thematic programmes of research are needed. simulation-based medical education is a complex service intervention that needs to be planned and practised with attention to organisational contexts. medical education 2010: 44: 50–63\"university of rochester school of medicine and dentistry, rochester, ny, usa. ronald\\\\_epstein@urmc.rochester.edu\",assessment in medical education,\"stage as an attending physician working with a student for a week, you receive a form that asks you to evaluate the student\\'s fund of knowledge, procedural skills, professionalism, interest in learning, and ?systems-based practice.? you wonder which of these attributes you can reliably assess and how the data you provide will be used to further the student\\'s education. you also wonder whether other tests of knowledge and competence that students must undergo before they enter practice are equally problematic. in one way or another, most practicing physicians are involved in assessing the competence of trainees, peers, and other health .\\xa0.\\xa0.\"\"network science is already making an impact on the study of complex systems and offers a promising variety of tools to understand their formation and evolution in many disparate fields from technological networks to biological systems. even though new high-throughput technologies have rapidly been generating large amounts of genomic data, drug design has not followed the same development, and it is still complicated and expensive to develop new single-target drugs. nevertheless, recent approaches suggest that multi-target drug design combined with a network-dependent approach and large-scale systems-oriented strategies create a promising framework to combat complex multi-genetic disorders like cancer or diabetes. we here investigate the human network corresponding to the interactions between all {us} approved drugs and human therapies, defined by known relationships between drugs and their therapeutic applications. our results show that the average paths in this drug-therapy network are shorter than three steps, indicating that distant therapies are separated by a surprisingly low number of chemical compounds. we also identify a sub-network composed by drugs with high centrality measures in the drug-therapy network, which represent the structural backbone of this system and act as hubs routing information between distant parts of the network. these findings provide for the first time a global map of the large-scale organization of all known drugs and associated therapies, bringing new insights on possible strategies for future drug development. special attention should be given to drugs which combine the two properties of (a) having a high centrality value in the drug-therapy network and (b) acting on multiple molecular targets in the human system.\"\"health care is rich in evidence-based innovations, yet even when such innovations are implemented successfully in one location, they often disseminate slowly-if at all. diffusion of innovations is a major challenge in all industries including health care. this article examines the theory and research on the dissemination of innovations and suggests applications of that theory to health care. it explores in detail 3 clusters of influence on the rate of diffusion of innovations within an organization: the perceptions of the innovation, the characteristics of the individuals who may adopt the change, and contextual and managerial factors within the organization. this theory makes plausible at least 7 recommendations for health care executives who want to accelerate the rate of diffusion of innovations within their organizations: find sound innovations, find and support \"\"innovators,\"\" invest in \"\"early adopters,\"\" make early adopter activity observable, trust and enable reinvention, create slack for change, and lead by example.\"the bad, and the ugly.\",\"ecology is evolving rapidly and increasingly changing into a more open, accountable, interdisciplinary, collaborative and data-intensive science. discovering, integrating and analyzing massive amounts of heterogeneous data are central to ecology as researchers address complex questions at scales from the gene to the biosphere. ecoinformatics offers tools and approaches for managing ecological data and transforming the data into information and knowledge. here, we review the state-of-the-art and recent advances in ecoinformatics that can benefit ecologists and environmental scientists as they tackle increasingly challenging questions that require voluminous amounts of data across disciplines and scales of space and time. we also highlight the challenges and opportunities that remain.\"logic and information\",focusing on information flow will help us to understand better how cells and organisms work. biology stands at an interesting juncture. the past decades have seen remarkable advances in our understanding of how living organisms work.\"network motifs are statistically overrepresented sub-structures (sub-graphs) in a network, and have been recognized as \\'the simple building blocks of complex networks\\'. study of biological network motifs may reveal answers to many important biological questions. the main difficulty in detecting larger network motifs in biological networks lies in the facts that the number of possible sub-graphs increases exponentially with the network or motif size (node counts, in general), and that no known polynomial-time algorithm exists in deciding if two graphs are topologically equivalent. this article discusses the biological significance of network motifs, the motivation behind solving the motif-finding problem, and strategies to solve the various aspects of this problem. a simple classification scheme is designed to analyze the strengths and weaknesses of several existing algorithms. experimental results derived from a few comparative studies in the literature are discussed, with conclusions that lead to future research directions.\"\"objectives:\\u2002 to investigate the extent to which junior doctors in their first clinical positions retained information literacy skills taught as part of their undergraduate education. method:\\u2002 participants drawn from different training cohorts were interviewed about their recall of the instruction they had received, and their confidence in retrieving and evaluating information for clinical decision making. they completed a search based on a scenario related to their speciality. their self-assessment of their competency in conducting and evaluating a search was compared with an evaluation of their skills by an experienced observer. results:\\u2002 most participants recalled the training they received but had not retained high-level search skills, and lacked skills in identifying and applying best evidence. there was no apparent link between the type of training given and subsequent skill level. those whose postgraduate education required these skills were more successful in retrieving and appraising information. conclusion:\\u2002 commitment to evidence-based medicine from clinicians at all levels in the profession is needed to increase the information seeking skills of clinicians entering the work force.\"faculty of medicine, glasgow, uk. susan.jamieson@clinmed.gla.ac.uk\",likert scales: how to (ab)use them,ontario, canada.\",\"of studies, syntheses, synopses, summaries, and systems: the  ” {5s}” evolution of information services for evidence-based healthcare decisions\",\"the purpose of this review is to consolidate existing evidence from published systematic reviews on health information system ({his}) evaluation studies to inform {his} practice and research. fifty reviews published during 1994–2008 were selected for meta-level synthesis. these reviews covered five areas: medication management, preventive care, health conditions, data quality, and care process/outcome. after reconciliation for duplicates, 1276 {his} studies were arrived at as the non-overlapping corpus. on the basis of a subset of 287 controlled {his} studies, there is some evidence for improved quality of care, but in varying degrees across topic areas. for instance, 31/43 (72\\\\%) controlled {his} studies had positive results using preventive care reminders, mostly through guideline adherence such as immunization and health screening. key factors that influence {his} success included having in-house systems, developers as users, integrated decision support and benchmark practices, and addressing such contextual issues as provider knowledge and perception, incentives, and legislation/policy.\"usa\",learning to rank: from pairwise approach to listwise approach,\"the paper is concerned with learning to rank, which is to construct a model or a function for ranking objects. learning to rank is useful for document retrieval, collaborative filtering, and many other applications. several methods for learning to rank have been proposed, which take object pairs as \\'instances\\' in learning. we refer to them as the pairwise approach in this paper. although the pairwise approach offers advantages, it ignores the fact that ranking is a prediction task on list of objects. the paper postulates that learning to rank should adopt the listwise approach in which lists of objects are used as \\'instances\\' in learning. the paper proposes a new probabilistic method for the approach. specifically it introduces two probability models, respectively referred to as permutation probability and top k probability, to define a listwise loss function for learning. neural network and gradient descent are then employed as model and algorithm in the learning method. experimental results on information retrieval show that the proposed listwise approach performs better than the pairwise approach.\"\"this article presents a system for drug name recognition and classification in biomedical texts. the system combines information obtained by the unified medical language system ({umls}) {metamap} transfer ({mmtx}) program and nomenclature rules recommended by the world health organization ({who}) international nonproprietary names ({inns}) program to identify and classify pharmaceutical substances. moreover, the system is able to detect possible candidates for drug names that have not been detected by {mmtx} program by applying these rules, achieving, in this way, a broader coverage. this work is the first step in a method for automatic detection of drug interactions from biomedical texts, a specific type of adverse drug event of special interest in patient safety.\"mining biomedical data using {metamap} transfer ({mmtx}) and the unified medical language system ({umls}).,\"the use of network-based approaches to visualize and analyze different types of biologically relevant interaction data has become increasingly popular in recent years. topological studies of these interaction networks offer a means to assess the interconnectivity structure established among and between diseases, genes, proteins, and molecules from which influential conclusions and global trends in biology and drug discovery can be derived1, 2, 3.\"\"the traditional mediterranean diet is characterized by a high intake of olive oil, fruit, nuts, vegetables, and cereals; a moderate intake of fish and poultry; a low intake of dairy products, red meat, processed meats, and sweets; and wine in moderation, consumed with meals.1 in observational cohort studies2,3 and a secondary prevention trial (the lyon diet heart study),4 increasing adherence to the mediterranean diet has been consistently beneficial with respect to cardiovascular risk.2?4 a systematic review ranked the mediterranean diet as the most likely dietary model to provide protection against coronary heart disease.5 small clinical trials have uncovered .\\xa0.\\xa0.\"\"drug information databases used in {id} practice as {cdsts} can be valuable resources. {mm}, {mdr}, {lc}-{ahfs}, {ahfs}, and {cp} were shown to be superior in their scope and completeness of information, and {mm}, {ahfs}, and {mdr} provided no erroneous answers. there is room for improvement in all evaluated databases.\"\"summary: rapid advances in pharmaceutical sciences have brought ever-increasing interests in combined therapies for better clinical efficacy and safety, especially in cases of complicated and refractory diseases. innovative experimental technologies and theoretical frameworks are being actively developed for multicomponent drug research. in this work, we present the drug combination database, with aims to facilitate analyses of known drug combinations, to summarize patterns of beneficial drug interactions, and to provide a basis for theoretical modeling and simulation of such drug interactions. its current version (1.0) collected 499 approved or investigational drug combinations, including 40 unsuccessful drug combinations, involving 485 individual drugs, from >6000 references.\"\"{background}:supporting 21st century health care and the practice of evidence-based medicine ({ebm}) requires ubiquitous access to clinical information and to knowledge-based resources to answer clinical questions. many questions go unanswered, however, due to lack of skills in formulating questions, crafting effective search strategies, and accessing databases to identify best levels of {evidence.methods}:this randomized trial was designed as a pilot study to measure the relevancy of search results using three different interfaces for the {pubmed} search system. two of the search interfaces utilized a specific framework called {pico}, which was designed to focus clinical questions and to prompt for publication type or type of question asked. the third interface was the standard {pubmed} interface readily available on the web. study subjects were recruited from interns and residents on an inpatient general medicine rotation at an academic medical center in the {us}. thirty-one subjects were randomized to one of the three interfaces, given 3 clinical questions, and asked to search {pubmed} for a set of relevant articles that would provide an answer for each question. the success of the search results was determined by a precision score, which compared the number of relevant or gold standard articles retrieved in a result set to the total number of articles retrieved in that {set.results}:participants using the {pico} templates (protocol a or protocol b) had higher precision scores for each question than the participants who used protocol c, the standard {pubmed} web interface. (question 1: a = 35\\\\%, b = 28\\\\%, c = 20\\\\%; question 2: a = 5\\\\%, b = 6\\\\%, c = 4\\\\%; question 3: a = 1\\\\%, b = 0\\\\%, c = 0\\\\%) 95\\\\% confidence intervals were calculated for the precision for each question using a lower boundary of zero. however, the 95\\\\% confidence limits were overlapping, suggesting no statistical difference between the {groups.conclusion}:due to the small number of searches for each arm, this pilot study could not demonstrate a statistically significant difference between the search protocols. however there was a trend towards higher precision that needs to be investigated in a larger study to determine if {pico} can improve the relevancy of search results.\"\"in the first paper in a three-part series on health systems guidance, xavier {bosch-capblanch} and colleagues examine how guidance is currently formulated in low- and middle-income countries, and the challenges to developing such guidance.\"nhs r\\\\&d programme, summertown pavilion, middle way, oxford, oxfordshire, uk, ox2 7lg. shopewell@cochrane.co.uk\",handsearching versus electronic searching to identify reports of randomized trials,\"background systematic reviewers need to decide how best to reduce bias in identifying studies for their review. even when journals are indexed in electronic databases, it can still be difficult to identify all relevant studies reported in these journals. over 1700 journals have been or are being handsearched within the cochrane collaboration to identify reports of controlled trials in order to help address these problems. objectives objectives to review systematically empirical studies, which have compared the results of handsearching with the results of searching one or more electronic databases to identify reports of randomized trials. search methods search methods studies were sought from the cochrane methodology register (the cochrane library, issue 2, 2002), {medline} (1966 to week 1 july 2002), {embase} (1980 to week 25 2002), {amed} (1985 to june 2002), {biosis} (1985 to june 2002), {cinahl} (1982 to june 2002), {lisa} (1969 to july 2002) and {psycinfo} (1972 to may 2002). researchers who may have carried out relevant studies were contacted. selection criteria selection criteria a research study was considered eligible for this review if it compared handsearching with searching one or more electronic databases to identify reports of randomized trials. data collection and analysis data collection and analysis the main outcome measure was the number of reports of randomized trials identified by handsearching as compared to electronic searching. data were extracted on the electronic database searched, the complexity of electronic search strategy used, the characteristics of the journal reports identified, and the type of trial report identified. main results main results thirty-four studies were included. handsearching identified between 92\\\\% to 100\\\\% of the total number of reports of randomized trials found in the various comparisons in this review. searching {medline} retrieved 55\\\\%, {embase} 49\\\\% and {pyscinfo} 67\\\\%. the retrieval rate of the electronic database varied depending on the complexity of the search. the cochrane highly sensitive search strategy ({hsss}) identified 80\\\\% of the total number of reports of randomized trials found, searches categorised as \\'complex\\' (including the cochrane {hsss}) found 65\\\\% and \\'simple\\' found 42\\\\%. the retrieval rate for an electronic search was higher when the search was restricted to english language journals; 62\\\\% versus 39\\\\% for journals published in languages other than english. when the search was restricted to full reports of randomized trials, the retrieval rate for an electronic search improved: a complex search strategy (including the cochrane {hsss}) retrieved 82\\\\% of the total number of such reports of randomized trials. authors\\' conclusions authors\\' conclusions handsearching still has a valuable role to play in identifying reports of randomized trials for inclusion in systematic reviews of health care interventions, particularly in identifying trials reported as abstracts, letters and those published in languages other than english, together with all reports published in journals not indexed in electronic databases. however, where time and resources are limited, searching an electronic database using a complex search (or the cochrane {hsss}) will identify the majority of trials published as full reports in english language journals, provided, of course, that the relevant journals have been indexed in the database. 背景 人工和電腦搜尋隨機臨床試驗(randomized {trials)報告之比較系統性回顧者需要擬定搜尋相關研究文獻的方式﹐並減少過程中的偏差。即使電子資料庫已建立期刊的索引資料，要找出期刊中所有相關研究仍有困難。為了解決此一問題，cochrane} collaboration以人工方式搜尋1700種以上期刊中的對照臨床試驗(controlled trials)報告。 目標 目標 進行人工搜尋和電子搜尋多個資料庫中的隨機臨床試驗﹐並個別進行實證研究的系統性地回顧﹐比較二者結果差異。 搜尋策略 搜尋策略 {我們自cochrane} methodology register (the cochrane library, issue 2, 2002), {medline} (1966年至2002年7月第一周), {embase} (1980 年至2002年第25週), {amed} (1985年至2002年6月), {biosis} (1985年至2002年6月), {cinahl} ({1982年至2002年6月)﹐lisa} (1969年至2002年7月) {以及psycinfo} (1972年至2002年5月)﹐尋找相關研究，並聯繫可能進行過相關研究的人員。 選擇標準 選擇標準 凡同為凡是比較人工搜尋隨機臨床試驗報告和電子搜尋利用一個以上多個電子資料庫尋找隨機臨床試驗報告搜尋同樣資料的實證研究，均納入是本回顧探討的對象。 資料收集與分析 資料收集與分析 比較結果的依據指標主要是人工搜尋和電子搜尋找到的隨機臨床試驗報告的數目。另外收集的資料包括了所搜尋的電子資料庫，搜尋策略的複雜度，搜尋到的文獻報告特徵﹐以及這些臨床試驗報告的種類等。 主要結論 主要結論 {總共包含34個研究。在本回顧探討的各種不同比較當中﹐人工搜尋找到的隨機臨床試驗報告佔所有報告總數比例從92\\\\%到100\\\\%不等。從medline可找到報告總數的55}\\\\%，{embase} 可找到49\\\\% ﹐{pyscinfo有} 67\\\\%。電子資料庫的尋獲率(retrieval {rate)依搜尋策略的複雜度而定。cochrane} highly sensitive search strategy ({hsss}){可尋獲80％。使用進階搜尋策略(包含hsss}){可尋獲65％，一般搜尋可尋獲42％。如搜尋範圍限定英文期刊﹐電子資料庫尋獲率為62\\\\%﹐搜尋不是以英文出版的期刊尋獲率為39％。當搜尋範圍限定完整報告時，可提高電子資料庫尋獲率，以進階搜尋(包含hsss})可尋獲82％。 作者結論 作者結論 {當系統性回顧醫療處置類的研究文獻時，特別是當隨機臨床試驗是以摘要、投書、非英語等的形式發表﹐或發表在電子資料庫未收錄的雜誌時﹐以人工搜尋隨機臨床試驗報告仍具相當的價值。然而時間及資源有限之下，只要是電子資料庫有收錄的雜誌，以進階搜尋(或cochrane} {hsss})應能搜尋到在英文學術期刊完整發表之研究的絕大部份。 翻譯人 翻譯人 {本摘要由慈濟醫院葉日弌翻譯。此翻譯計畫由臺灣國家衛生研究院(national} health research institutes, taiwan)統籌。 總結 總結 無總結\"\"the overwhelming amount of available scholarly literature in the life sciences poses significant challenges to scientists wishing to keep up with important developments related to their research, but also provides a useful resource for the discovery of recent information concerning genes, diseases, compounds and the interactions between them. in this paper, we describe an algorithm called {bio-lda} that uses extracted biological terminology to automatically identify latent topics, and provides a variety of measures to uncover putative relations among topics and bio-terms. relationships identified using those approaches are combined with existing data in life science datasets to provide additional insight. three case studies demonstrate the utility of the {bio-lda} model, including association predication, association search and connectivity map generation. this combined approach offers new opportunities for knowledge discovery in many areas of biology including target identification, lead hopping and drug repurposing.\"'),\n",
       " ('6dd817ab0c2f730fc206faf5f5847bbb',\n",
       "  '\"new york, ny, usa\",how users assess web pages for information seeking,\"in this article, we investigate the criteria used by online searchers when assessing the relevance of web pages for information-seeking tasks. twenty-four participants were given three tasks each, and they indicated the features of web pages that they used when deciding about the usefulness of the pages in relation to the tasks. these tasks were presented within the context of a simulated work-task situation. we investigated the relative utility of features identified by participants (web page content, structure, and quality) and how the importance of these features is affected by the type of information-seeking task performed and the stage of the search. the results of this study provide a set of criteria used by searchers to decide about the utility of web pages for different types of tasks. such criteria can have implications for the design of systems that use or recommend web pages.\"usa\",contextual search and name disambiguation in email using graphs,\"similarity measures for text have historically been an important tool for solving information retrieval problems. in many interesting settings, however, documents are often closely connected to other documents, as well as other non-textual objects: for instance, email messages are connected to other messages via header information. in this paper we consider extended similarity metrics for documents and other objects embedded in graphs, facilitated via a lazy graph walk. we provide a detailed instantiation of this framework for email data, where content, social networks and a timeline are integrated in a structural graph. the suggested framework is evaluated for two email-related problems: disambiguating names in email documents, and threading. we show that reranking schemes based on the graph-walk similarity measures often outperform baseline methods, and that further improvements can be obtained by use of appropriate learning methods.\"usa\",information retrieval on the web,understanding context before using it,\"this paper presents an attempt to point out some problematic issues about the understanding of context. although frequently used in cognitive sciences or other disciplines, context stays a very ill-defined concept. our goal is to identify the main components of the context on the basis of the analysis of a corpus of 150 definitions coming mainly from the web in different domains of cognitive sciences and close disciplines. we analyzed this corpus of definitions through two methods, namely {lsa} [1], [2] and {stone} [3], [4], and we conclude that finally the content of all the definitions can be analyzed in terms of few parameters like constraint, influence, behavior, nature, structure and system.\"usa\",from x-rays to silly putty via uranus: serendipity and its role in web search,\"the act of encountering information unexpectedly has long been identified as valuable, both as a joy in itself and as part of task-focused problem solving. there has been a concern that highly accurate search engines and targeted personalization may reduce opportunities for serendipity on the web. we examine whether there is the potential for serendipitous encounters during web search, and whether improving search relevance through personalization reduces this potential. by studying web search query logs and the results people judge relevant and interesting, we find many of the queries people perform return interesting (potentially serendipitous) results that are not directly relevant. rather than harming serendipity, personalization appears to identify interesting results in addition to relevant ones.\"{nvs} is suitable for use as a quick screening test for limited literacy in primary health care settings.nih, dhhs, bethesda, md, usa.\",exploring user navigation during online health information seeking,understanding online user behavior is essential for designing user-friendly consumer health web sites. transaction log analysis ({tla}) provides a way to extract aggregate data about online behavior. this paper describes prevalent user navigation trends using {tla} methods at {clinicaltrials}.gov. preliminary results suggest that users typically access low-level pages directly from web-based search engines and consumer health sites/portals. a pilot user study is presented to illustrate a complementary research method that might be integrated with {tla} to provide a multidimensional view of online health information-seeking behavior. implications of the observed navigation behavior on the design of consumer health web sites from {tla} and users studies are discussed.\"{a timeless classic in how complex information should be presented graphically.  the strunk \\\\& white  of visual design. should occupy a place of honor--within arm\\'s reach--of everyone  attempting to understand or depict numerical data graphically.  the design of the book is an exemplar of the principles it espouses:  elegant typography and layout, and seamless integration of lucid text and perfectly chosen graphical examples. very highly recommended.}\"usa\",ranking in folksonomy systems: can context help?,\"folksonomy systems have shown to contribute to the quality of web search ranking strategies. in this paper, we analyze and compare different graph-based ranking algorithms, namely {folkrank}, {socialpagerank}, and {socialsimrank}. we enhance these algorithms by exploiting the context of tag assignmets, and evaluate the results on the {groupme}! dataset. in {groupme}!, users can organize and maintain arbitrary web resources in self-defined groups. when users annotate resources in {groupme}!, this can be interpreted in context of a certain group. the grouping activity delivers valuable semantic information about resources and their context. we show how to use this information to improve the detection of relevant search results, and compare different strategies for ranking result lists in folksonomy systems.\"usa\",exploring folksonomy for personalized search,\"as a social service in web 2.0, folksonomy provides the users the ability to save and organize their bookmarks online with \"\"social annotations\"\" or \"\"tags\"\". social annotations are high quality descriptors of the web pages\\' topics as well as good indicators of web users\\' interests. we propose a personalized search framework to utilize folksonomy for personalized search. specifically, three properties of folksonomy, namely the categorization, keyword, and structure property, are explored. in the framework, the rank of a web page is decided not only by the term matching between the query and the web page\\'s content but also by the topic matching between the user\\'s interests and the web page\\'s topics. in the evaluation, we propose an automatic evaluation framework based on folksonomy data, which is able to help lighten the common high cost in personalized search evaluations. a series of experiments are conducted using two heterogeneous data sets, one crawled from del.icio.us and the other from dogear. extensive experimental results show that our personalized search approach can significantly improve the search quality.\"usa\",personalized interactive faceted search,\"faceted search is becoming a popular method to allow users to interactively search and navigate complex information spaces. a faceted search system presents users with key-value metadata that is used for query refinement. while popular in e-commerce and digital libraries, not much research has been conducted on which metadata to present to a user in order to improve the search experience. nor are there repeatable benchmarks for evaluating a faceted search engine. this paper proposes the use of collaborative filtering and personalization to customize the search interface to each user\\'s behavior. this paper also proposes a utility based framework to evaluate the faceted interface. in order to demonstrate these ideas and better understand personalized faceted search, several faceted search algorithms are proposed and evaluated using the novel evaluation methodology.\"usa\",an aspectual interface for supporting complex search tasks,\"with the increasing importance of search systems on the web, there is a continuing push to design interfaces which are a better match with the kinds of real-world tasks in which users are engaged. in this paper, we consider how broad, complex search tasks may be supported via the search interface. in particular, we consider search tasks which may be composed of multiple aspects, or multiple related subtasks. for example, in decision making tasks the user may investigate multiple possible solutions before settling on a single, final solution, while other tasks, such as report writing, may involve searching on multiple interrelated topics.\"ny, usa\",adapting information retrieval systems to user queries,\"users enter queries that are short as well as long. the aim of this work is to evaluate techniques that can enable information retrieval ({ir}) systems to automatically adapt to perform better on such queries. by adaptation we refer to (1) modifications to the queries via user interaction, and (2) detecting that the original query is not a good candidate for modification. we show that the former has the potential to improve mean average precision ({map}) of long and short queries by 40\\\\% and 30\\\\% respectively, and that simple user interaction can help towards this goal. we observed that after inspecting the options presented to them, users frequently did not select any. we present techniques in this paper to determine beforehand the utility of user interaction to avoid this waste of time and effort. we show that our techniques can provide {ir} systems with the ability to detect and avoid interaction for unpromising queries without a significant drop in overall performance.\"portland 97201, usa. hersh@ohsu.edu\",how well do physicians use electronic information retrieval systems?: a framework for investigation and systematic review,\"objective.-- despite the proliferation of electronic information retrieval ({ir}) systems for physicians, their effectiveness has not been well assessed. the purpose of this review is to provide a conceptual framework and to apply the results of previous studies to this framework.  data sources.-- all sources of medical informatics and information science literature, including {medline}, along with bibliographies of textbooks in these areas, were searched from 1966 to january 1998.  study selection.-- all articles presenting either classifications of evaluation studies or their results, with an emphasis on those studying use by physicians.  data extraction.-- a framework for evaluation was developed, consisting of frequency of use, purpose of use, user satisfaction, searching utility, search failure, and outcomes. all studies were then assessed based on the framework.  data synthesis.-- due to the heterogeneity and simplistic study designs, no meta-analysis of studies could be done. general conclusions were drawn from data where appropriate. a total of 47 articles were found to include an evaluation component and were used to develop the framework. of these, 21 articles met the inclusion criteria for 1 or more of the categories in the framework. most use of {ir} systems by physicians still occurs with bibliographic rather than full-text databases. overall use of {ir} systems occurs just 0.3 to 9 times per physician per month, whereas physicians have 2 unanswered questions for every 3 patients.  conclusions.-- studies comparing {ir} systems with different searching features have not shown that advanced searching methods are significantly more effective than simple text word methods. most searches retrieve only one fourth to one half of the relevant articles on a given topic and, once retrieved, little is known about how these articles are interpreted or applied. these studies imply that further research and development are needed to improve system utility and performance. 10.1001/jama.280.15.1347\"\"department of information resource management, business school, nankai university, 94 weijin road, tianjin, china, 300071; school of communication and information, rutgers university, 4 huntington street, new brunswick, nj 08901\",an exploration of the relationships between work task and interactive information search behavior,\"abstract 10.1002/asi.21359.abs this study explores the relationships between work task and interactive information search behavior. work task was conceptualized based on a faceted classification of task. an experiment was conducted with six work-task types and simulated work-task situations assigned to 24 participants. the results indicate that users present different behavior patterns to approach useful information for different work tasks: they select information systems to search based on the work tasks at hand, different work tasks motivate different types of search tasks, and different facets controlled in the study play different roles in shaping users\\' interactive information search behavior. the results provide empirical evidence to support the view that work tasks and search tasks play different roles in a user\\'s interaction with information systems and that work task should be considered as a multifaceted variable. the findings provide a possibility to make predictions of a user\\'s information search behavior from his or her work task, and vice versa. thus, this study sheds light on task-based information seeking and search, and has implications in adaptive information retrieval ({ir}) and personalization of {ir}.\"usa\",some(what) grand challenges for information retrieval,\"although we see the positive results of information retrieval research embodied throughout the internet, on our computer desktops, and in many other aspects of daily life, at the same time we notice that people still have a wide variety of difficulties in finding information that is useful in resolving their problematic situations. this suggests that there still remain substantial challenges for research in {ir}. already in 1988, on the occasion of receiving the {acm} {sigir} gerard salton award, karen sp\\\\\"\"{a}rck jones suggested that substantial progress in information retrieval was likely only to come through addressing issues associated with users (actual or potential) of {ir} systems, rather than continuing {ir} research\\'s almost exclusive focus on document representation and matching and ranking techniques. in recent years it appears that her message has begun to be heard, yet we still have relatively few substantive results that respond to it. in this paper, i identify and discuss a few challenges for {ir} research which fall within the scope of association with users, and which i believe, if properly addressed, are likely to lead to substantial increases in the usefulness, usability and pleasurability of information retrieval.\"usa\",mining the search trails of surfing crowds: identifying relevant websites from user activity,\"the paper proposes identifying relevant information sources from the history of combined searching and browsing behavior of many web users. while it has been previously shown that user interactions with search engines can be employed to improve document ranking, browsing behavior that occurs beyond search result pages has been largely overlooked in prior work. the paper demonstrates that users\\' post-search browsing activity strongly reflects implicit endorsement of visited pages, which allows estimating topical relevance of web resources by mining large-scale datasets of search trails. we present heuristic and probabilistic algorithms that rely on such datasets for suggesting authoritative websites for search queries. experimental evaluation shows that exploiting complete post-search browsing trails outperforms alternatives in isolation (e.g., clickthrough logs), and yields accuracy improvements when employed as a feature in learning to rank for web search.\"\"the royal school of library and information science, 6 birketinget, dk-2300 copenhagen s, denmark\",the foundation of the concept of relevance,\"in 1975 tefko saracevic declared  ” the subject knowledge view” to be the most fundamental perspective of relevance. this paper examines the assumptions in different views of relevance, including  ” the system\\'s view” and  ” the user\\'s view” and offers a reinterpretation of these views. the paper finds that what was regarded as the most fundamental view by saracevic in 1975 has not since been considered (with very few exceptions). other views, which are based on less fruitful assumptions, have dominated the discourse on relevance in information retrieval and information science. many authors have reexamined the concept of relevance in information science, but have neglected the subject knowledge view, hence basic theoretical assumptions seem not to have been properly addressed. it is as urgent now as it was in 1975 seriously to consider  ” the subject knowledge view” of relevance (which may also be termed  ” the epistemological view”). the concept of relevance, like other basic concepts, is influenced by overall approaches to information science, such as the cognitive view and the domain-analytic view. there is today a trend toward a social paradigm for information science. this paper offers an understanding of relevance from such a social point of view.\"\"development of context-aware applications is inherently complex. these applications adapt to changing context information: physical context, computational context, and user context/tasks. context information is gathered from a variety of sources that differ in the quality of information they produce and that are often failure prone. the pervasive computing community increasingly understands that developing context-aware applications should be supported by adequate context information modelling and reasoning techniques. these techniques reduce the complexity of context-aware applications and improve their maintainability and evolvability. in this paper we discuss the requirements that context modelling and reasoning techniques should meet, including the modelling of a variety of context information types and their relationships, of high-level context abstractions describing real world situations using context information facts, of histories of context information, and of uncertainty of context information. this discussion is followed by a description and comparison of current context modelling and reasoning techniques and a lesson learned from this comparison.\"usa\",evaluation challenges and directions for {information-seeking} support systems,{issss} provide an exciting opportunity to extend previous information-seeking and interactive information retrieval evaluation models and create a research community that embraces diverse methods and broader participation.berkeley, ca, university of cambridge, uk, california digital library.\",{biotext} search engine: beyond abstract search.,\"the {biotext} search engine is a freely available web-based application that provides biologists with new ways to access the scientific literature. one novel feature is the ability to search and browse article figures and their captions. a grid view juxtaposes many different figures associated with the same keywords, providing new insight into the literature. an abstract/title search and list view shows at a glance many of the figures associated with each article. the interface is carefully designed according to usability principles and techniques. the search engine is a work in progress, and more functionality will be added over time. availability: http://biosearch.berkeley.edu.\"\"ontology–based information extraction is considered as an effective method to improve the performance of information extraction ({ie}) systems. in order to build a better {ie} system using ontology-based technique, the two challenges should be most taken into account are semantic elements extraction, and ontology enhancement. focusing on the two key points above, we propose an ontology-based information extraction system and its application information retrieval system in the health care domain. in ontology-based information extraction system, we propose a non part-of-speech based method for the semantic elements extraction and the ontology enhancement to overcome the above challenges. experiments and results show that all functions work well in the whole system. many kinds of semantic elements can be extracted by the proposed extraction functions regardless parts-of-speech of words. high ratio of new semantic elements is detected for the ontology and database enrichment that yield very good results in information retrieval. the system is implemented in vietnamese.\"\"web 2.0 functionality is changing the way consumers search for, evaluate, and use health information. what are some of the new \\x93consumer health 2.0\\x94 sites and their features? how will this trend toward participatory information processing affect traditional sites such as the national library of medicine\\'s {medlineplus}? how should librarians approach this new paradigm of health information-seeking? this article analyzes the current and potential environment for health information on the internet and the role of the librarian in that environment.\"usa\",how evaluator domain expertise affects search result relevance judgments,\"traditional search evaluation approaches have often relied on domain experts to evaluate results for each query. unfortunately, the range of topics present in any representative sample of web queries makes it impractical to have expert evaluators for every topic. in this paper, we investigate the effect of using \"\"generalist\"\" evaluators instead of experts in the domain of queries being evaluated. empirically, we ind that for queries drawn from domains requiring high expertise, (1) generalists tend to give shallow, inaccurate ratings as compared to experts. (2) further experiments show that generalists disagree on the underlying meaning of these queries significantly more often than experts, and often appear to \"\"give up\\'\\' and fall back on surface features such as keyword matching. (3) finally, by estimating the percentage of \"\"expertise requiring\\'\\' queries in a web query sample, we estimate the impact of using generalists, versus the ideal of having domain experts for every \"\"expertise requiring\\'\\' query.\"\"{objectives}: the eighteen-month evaluation of a clinical librarian project (october {2003-march} 2005) conducted in north wales, united kingdom ({uk}) assessed the benefits of clinical librarian support to clinical teams, the impact of mediated searching services, and the effectiveness of information skills training, including journal club support. {methods}: the evaluation assessed changes in teams\\' information-seeking behavior and their willingness to delegate searching to a clinical librarian. baseline (n = 69 responses, 73\\\\% response rate) and final questionnaire (n = 57, 77\\\\% response rate) surveys were complemented by telephone and face-to-face interviews (n = 33) among 3 sites served. those attending information skills training sessions (n = 130) completed evaluations at the session and were surveyed 1 month after training (n = 24 questionnaire responses, n = 12 interviews). {results}: health professionals in clinical teams reported that they were more willing to undertake their own searching, but also more willing to delegate some literature searching, than at the start of the project. the extent of change depended on the team and the type of information required. information skills training was particularly effective when organized around journal clubs. {conclusions}: collaboration with a clinical librarian increased clinician willingness to seek information. clinical librarian services should leverage structured training opportunities such as journal clubs.\"usa\",a comparison of query and term suggestion features for interactive searching,\"query formulation is one of the most difficult and important aspects of information seeking and retrieval. two techniques, term relevance feedback and query suggestion, provide methods to help users formulate queries, but each is limited in different ways. in this research we combine these two techniques by automatically creating query suggestions using term relevance feedback techniques. to evaluate our approach, we conducted an interactive information retrieval study with 55 subjects and 20 topics. each subject completed four topics, half with a term suggestion system and half with a query suggestion system. we also investigated the source of the suggestions: approximately half of all subjects were provided with system-generated suggestions, while half were provided with user-generated suggestions. results show that subjects used more query suggestions than term suggestions and saved more documents with these suggestions, even though there were no significant differences in performance. subjects preferred the query suggestion system and rated it higher along a number of dimensions including its ability to help them think of new approaches to searching. qualitative data provided insight into subjects\\' usage and ratings, and indicated that subjects often used the suggestions even when they did not click on them.\"usa\",strategy hubs: next-generation domain portals with search procedures,\"current search tools on the web, such as general-purpose search engines (e.g. google) and domain-specific portals (e.g. {medlineplus}), do not provide search procedures that guide users to form appropriately ordered sub-goals. the lack of such procedural knowledge often leads users searching in unfamiliar domains to retrieve incomplete information. in critical domains such as in healthcare, such ineffective searches can have dangerous consequences. to address this situation, we developed a new type of domain portal called a strategy hub. strategy hubs provide the critical search procedures and associated high-quality links that enable users to find comprehensive and accurate information. this paper describes how we collaborated with skin cancer physicians to systematically identify generalizeable search procedures to find comprehensive information about melanoma, and how these search procedures were made available through the strategy hub for healthcare. a pilot study suggests that this approach can improve the efficacy, efficiency, and satisfaction of even expert searchers. we conclude with insights on how to refine the design of the strategy hub, and how it can be used to provide search procedures across domains.\",adding context to preferences,\"to handle the overwhelming amount of information currently available, personalization systems allow users to specify the information that interests them through preferences. most often, users have different preferences depending on context. in this paper, we introduce a model for expressing such contextual preferences. context is modeled as a set of multidimensional attributes. we formulate the context resolution problem as the problem of (a) identifying those preferences that qualify to encompass the context state of a query and (b) selecting the most appropriate among them. we also propose an algorithm for context resolution that uses a data structure, called the profile tree, that indexes preferences based on their associated context. finally, we evaluate our approach from two perspectives: usability and performance.\"usa\",and what can context do for data?,an abstract is not available.usa\",extending average precision to graded relevance judgments,\"evaluation metrics play a critical role both in the context of comparative evaluation of the performance of retrieval systems and in the context of learning-to-rank ({ltr}) as objective functions to be optimized. many different evaluation metrics have been proposed in the {ir} literature, with average precision ({ap}) being the dominant one due a number of desirable properties it possesses. however, most of these measures, including average precision, do not incorporate graded relevance. in this work, we propose a new measure of retrieval effectiveness, the graded average precision ({gap}). {gap} generalizes average precision to the case of multi-graded relevance and inherits all the desirable characteristics of {ap}: it has a nice probabilistic interpretation, it approximates the area under a graded precision-recall curve and it can be justified in terms of a simple but moderately plausible user model. we then evaluate {gap} in terms of its informativeness and discriminative power. finally, we show that {gap} can reliably be used as an objective metric in learning to rank by illustrating that optimizing for {gap} using {softrank} and {lambdarank} leads to better performing ranking functions than the ones constructed by algorithms tuned to optimize for {ap} or {ndcg} even when using {ap} or {ndcg} as the test metrics.\"\"this report reviews some of the extensive literature in health literacy, much of it focused on the intersection of low literacy and the understanding of basic health care information. several articles describe methods for assessing health literacy as well as methods for assessing the readability of texts, although generally these latter have not been developed with health materials in mind. other studies have looked more closely at the mismatch between patients\\' literacy levels and the readability of materials intended for use by those patients. a number of studies have investigated the phenomenon of literacy from the perspective of patients\\' interactions in the health care setting, the disenfranchisement of some patients because of their low literacy skills, the difficulty some patients have in navigating the health care system, the quality of the communication between doctors and their patients including the cultural overlay of such exchanges, and ultimately the effect of low literacy on health outcomes. finally, the impact of new information technologies has been studied by a number of investigators. there remain many opportunities for conducting further research to gain a better understanding of the complex interactions between general literacy, health literacy, information technologies, and the existing health care infrastructure.\"usa\",analysis of long queries in a large scale search log,\"we propose to use the search log to study long queries, in order to understand the types of information needs that are behind them, and to design techniques to improve search effectiveness when they are used. long queries arise in many different applications, such as {cqa} (community-based question answering) and literature search, and they have been studied to some extent using {trec} data. they are also, however, quite common in web search, as can be seen by looking at the distribution of query lengths in a large scale search log. in this paper we analyze the long queries in the search log with the aim of identifying the characteristics of the most commonly occurring types of queries, and the issues involved with using them effectively in a search engine. in addition, we propose a simple yet effective method for evaluating the performance of the queries in the search log using a combination of the click data in the search log with the existing {trec} corpora.\"explaining user performance in information retrieval: challenges to {ir} evaluation,\"the paper makes three points of significance for {ir} research: (1) the cranfield paradigm of {ir} evaluation seems to lose power when one looks at human instead of system performance. (2) searchers using {ir} systems in real-life use rather short queries, which individually often have poor performance. however, when used in sessions, they may be surprisingly effective. the searcher\\'s strategies have not been sufficiently described and cannot therefore be properly understood, supported nor evaluated. (3) searchers in real-life seek to optimize the entire information access process, not just result quality. evaluation of output alone is insufficient to explain searcher behavior.\"usa\",contextual relevance feedback,usa\",efficient bayesian hierarchical user modeling for recommendation system,\"a content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual user\\'s interest. a system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a bayesian hierarchical model. learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive. the commonly used {em} algorithm converges very slowly due to the sparseness of the data in {ir} applications. this paper proposes a new fast learning technique to learn a large number of individual user profiles. the efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from netflix and {movielens}.\"uk\",given a context by any other name: methodological tools for taming the unruly beast,\"search engines are essential for finding information on the world wide web. we conducted a study to see how effective eight search engines are. expert searchers sought information on the web for users who had legitimate needs for information, and these users assessed the relevance of the information retrieved. we calculated traditional information retrieval measures of recall and precision at varying numbers of retrieved documents and used these as the bases for statistical comparisons of retrieval effectiveness among the eight search engines. we also calculated the likelihood that a document retrieved by one search engine was retrieved by other search engines as well.\"france. duc-tuan.tran@univ-rennes1.fr\",experiments in cross-language medical information retrieval using a mixing translation module.,\"given the ever-increasing scale and diversity of medical literature widely published in english on the internet, improving the performance of information retrieval by cross-language is an urgent research objective. cross-language medical information retrieval ({clmir}) consists of providing a query in one language and searching medical document collections in one or more different languages. our users of {clmir} are users who are able to read biomedical texts in english, but have difficulty formulating english queries. this paper proposes a {french/english} {clmir} system as a mixing model for supporting the retrieval of english medical documents. methods fall into the category of query translation approach in which we use a hybrid machine translation that combines a pattern-based module with a rule-based translator and includes three steps from pre- to- post-translation. in parallel to this hybrid machine translation, we use multilingual {umls} methasaurus as a complementary translator. the results show that using a mixing translation module outperforms machine translation-based method and thesaurus-based method used separately.\"factors determining the performance of indexing systems,brigham and women\\'s hospital, boston, ma 02115, usa. qzeng@dsg.harvard.edu\",term identification methods for consumer health vocabulary development.,\"{background}: the development of consumer health information applications such as health education websites has motivated the research on consumer health vocabulary ({chv}). term identification is a critical task in vocabulary development. because of the heterogeneity and ambiguity of consumer expressions, term identification for {chv} is more challenging than for professional health vocabularies. {objective}: for the development of a {chv}, we explored several term identification methods, including collaborative human review and automated term recognition methods. {methods}: a set of criteria was established to ensure consistency in the collaborative review, which analyzed 1893 strings. using the results from the human review, we tested two automated {methods-c}-value formula and a logistic regression model. {results}: the study identified 753 consumer terms and found the logistic regression model to be highly effective for {chv} term identification (area under the receiver operating characteristic curve = 95.5\\\\%). {conclusions}: the collaborative human review and logistic regression methods were effective for identifying terms for {chv} development.\"usa\",plurality: a context-aware personalized tagging system,\"we present the design of plurality, an interactive tagging system. plurality\\'s modular architecture allows users to automatically generate high-quality tags over web content, as well as over archival and personal content typically beyond the reach of existing web 2.0 social tagging systems. three of the salient features of plurality are: (i) its self-learning and feedback-sensitive capabilities based on a user\\'s personalized tagging style; (ii) its leveraging of the collective intelligence of existing social tagging services; and (iii) its context-awareness for optimizing tag suggestions, e.g., based on spatial or temporal features.\"usa\",incorporating contextual information in recommender systems using a multidimensional approach,\"the article presents a multidimensional ({md}) approach to recommender systems that can provide recommendations based on additional contextual information besides the typical information on users and items used in most of the current recommender systems. this approach supports multiple dimensions, profiling information, and hierarchical aggregation of recommendations. the article also presents a multidimensional rating estimation method capable of selecting two-dimensional segments of ratings pertinent to the recommendation context and applying standard collaborative filtering or other traditional two-dimensional rating estimation techniques to these segments. a comparison of the multidimensional and two-dimensional rating estimation approaches is made, and the tradeoffs between the two are studied. moreover, the article introduces a combined rating estimation method, which identifies the situations where the {md} approach outperforms the standard two-dimensional approach and uses the {md} approach in those situations and the standard two-dimensional approach elsewhere. finally, the article presents a pilot empirical study of the combined approach, using a multidimensional movie recommender system that was developed for implementing this approach and testing its performance.\"usa\",learning about the world through long-term query logs,\"in this article, we demonstrate the value of long-term query logs. most work on query logs to date considers only short-term (within-session) query information. in contrast, we show that long-term query logs can be used to learn about the world we live in. there are many applications of this that lead not only to improving the search engine for its users, but also potentially to advances in other disciplines such as medicine, sociology, economics, and more. in this article, we will show how long-term query logs can be used for these purposes, and that their potential is severely reduced if the logs are limited to short time horizons. we show that query effects are long-lasting, provide valuable information, and might be used to automatically make medical discoveries, build concept hierarchies, and generally learn about the sociological behavior of users. we believe these applications are only the beginning of what can be done with the information contained in long-term query logs, and see this work as a step toward unlocking their potential.\"ny, usa\",searching the web: the public and their queries,ny, usa\",the effects of domain knowledge on search tactic formulation,\"a search tactic is a set of search moves that are temporally and semantically related. the current study examined the tactics of medical students searching a factual database in microbiology. the students answered problems and searched the database on three occasions over a 9-month period. their search moves were analyzed in terms of the changes in search terms used from one cycle to the next, using two different analysis {methods.common} patterns were found in the students\\' search tactics; the most common approach was the specification of a concept, followed by the addition of one or more concepts, gradually narrowing the retrieved set before it was displayed. it was also found that the search tactics changed over time as the students\\' domain knowledge changed. these results have important implications for designers in developing systems that will support users\\' preferred ways of formulating searches. in addition, the research methods used (the coding scheme and the two data analysis methods--zero-order state transition matrices and maximal repeating patterns [{mrp}] analysis) are discussed in terms of their validity in future studies of search tactics.\"uk\",what we talk about when we talk about context,\"the emergence of ubiquitous computing as a new design paradigm poses significant challenges for human-computer interaction ({hci}) and interaction design. traditionally, {hci} has taken place within a constrained and well-understood domain of experience—single users sitting at desks and interacting with conventionally-designed computers employing screens, keyboards and mice for interaction. new opportunities have engendered considerable interest in  ” context-aware computing”—computational systems that can sense and respond to aspects of the settings in which they are used. however, considerable confusion surrounds the notion of  ” context”—what it means, what it includes and what role it plays in interactive systems. this paper suggests that the representational stance implied by conventional interpretations of  ” context” misinterprets the role of context in everyday human activity, and proposes an alternative model that suggests different directions for design.\"usa\",predicting query performance,\"we develop a method for predicting query performance by computing the relative entropy between a query language model and the corresponding collection language model. the resulting clarity score measures the coherence of the language usage in documents whose models are likely to generate the query. we suggest that clarity scores measure the ambiguity of a query with respect to a collection of documents and show that they correlate positively with average precision in a variety of {trec} test sets. thus, the clarity score may be used to identify ineffective queries, on average, without relevance information. we develop an algorithm for automatically setting the clarity score threshold between predicted poorly-performing queries and acceptable queries and validate it using {trec} data. in particular, we compare the automatic thresholds to optimum thresholds and also check how frequently results as good are achieved in sampling experiments that randomly assign queries to the two classes.\"usa\",information spreading in context,\"information spreading processes are central to human interactions. despite recent studies in online domains, little is known about factors that could affect the dissemination of a single piece of information. in this paper, we address this challenge by combining two related but distinct datasets, collected from a large scale privacy-preserving distributed social sensor system. we find that the social and organizational context significantly impacts to whom and how fast people forward information. yet the structures within spreading processes can be well captured by a simple stochastic branching model, indicating surprising independence of context. our results build the foundation of future predictive models of information flow and provide significant insights towards design of communication platforms.\"thorn 309, brigham and women\\'s hospital, harvard medical school, 75 francis street, boston, ma 02115, usa. qzeng@dsg.bwh.harvard.du\",positive attitudes and failed queries: an exploration of the conundrums of consumer health information retrieval,\"several studies have found that consumers report a high level of satisfaction with the internet as a health information resource. belied by this positive attitude, however, are other studies reporting that consumers were often unsuccessful in searching for health information. in this paper, we present an interview and observation study in which we asked health consumers to search for health information on the internet after first stating their search goals. upon the conclusion of the session they were asked to evaluate their searches. we found that many consumers were unable to find satisfactory information when performing a specific query, while in general the group viewed health information retrieval ({hir}) on the internet in a positive light. we analyzed the observed search sessions to determine what factors accounted for the failure of specific searches and positive attitudes, and also discussed potential informatics solutions.\"\"we present a general purpose solution to web content and services perusal by means of mobile devices, named social {context-aware} browser. this is a novel approach for information access based on users\\' context, that exploits social and collaborative models to overtake the limits of the existing solutions. instead of relying on a pool of experts and on a rigid categorization, as it is usually done in the context-aware field, our solution allows the crowd of users to model, control, and manage the contextual knowledge through collaboration and participation. to have a dynamic and user-tailored context representation, and to enhance the process of retrieval based on users\\' actual situation, the community of users is encouraged to define the contexts of interest, to share, use, and discuss them, and to associate context to content and resources (web pages, services, applications, etc.). this paper provides an overall presentation of our solution, describing the idea, the implementation, and the evaluation through a benchmark based methodology.\"usa\",modeling actions of {pubmed} users with n-gram language models,\"transaction logs from online search engines are valuable for two reasons: first, they provide insight into human information-seeking behavior. second, log data can be used to train user models, which can then be applied to improve retrieval systems. this article presents a study of logs from {pubmed} \\\\&\\\\#174; , the public gateway to the {medline} \\\\&\\\\#174;  database of bibliographic records from the medical and biomedical primary literature. unlike most previous studies on general web search, our work examines user activities with a highly-specialized search engine. we encode user actions as string sequences and model these sequences using  n -gram language models. the models are evaluated in terms of perplexity and in a sequence prediction task. they help us better understand how {pubmed} users search for information and provide an enabler for improving users\\' search experience.\"usa\",effects of position and number of relevant documents retrieved on users\\' evaluations of system performance,\"information retrieval research has demonstrated that system performance does not always correlate positively with user performance, and that users often assign positive evaluation scores to search systems even when they are unable to complete tasks successfully. this research investigated the relationship between objective measures of system performance and users\\' perceptions of that performance. in this study, subjects evaluated the performance of four search systems whose search results were manipulated systematically to produce different orderings and numbers of relevant documents. three laboratory studies were conducted with a total of eighty-one subjects. the first two studies investigated the effect of the order of five relevant and five nonrelevant documents in a search results list containing ten results on subjects\\' evaluations. the third study investigated the effect of varying the number of relevant documents in a search results list containing ten results on subjects\\' evaluations. results demonstrate linear relationships between subjects\\' evaluations and the position of relevant documents in a search results list and the total number of relevant documents retrieved. of the two, number of relevant documents retrieved was a stronger predictor of subjects\\' evaluation ratings and resulted in subjects using a greater range of evaluation scores.\"usa\",\"the what, who, where, when, why and how of context-awareness\",\"when humans talk with humans, they are able to use implicit situational information, or  context,  to increase the conversational bandwidth. this ability to use contextual information does not transfer well to human-computer interaction. part of the problem is the impoverished mechanisms for providing input to computers. another aspect of the problem is that often we don\\'t know what contextual information is relevant, useful, or even how to use it. however, by improving the computer\\'s access to its context, we can increase the richness of communication in human-computer interaction and make it possible to produce more useful computational services.\"usa\",segment-level display time as implicit feedback: a comparison to eye tracking,\"we examine two basic sources for implicit relevance feedback on the segment level for search personalization: eye tracking and display time. a controlled study has been conducted where 32 participants had to view documents in front of an eye tracker, query a search engine, and give explicit relevance ratings for the results. we examined the performance of the basic implicit feedback methods with respect to improved ranking and compared their performance to a pseudo relevance feedback baseline on the segment level and the original ranking of a web search engine.\"measures of semantic similarity and relatedness in the biomedical domain,\"measures of semantic similarity between concepts are widely used in natural language processing. in this article, we show how six existing domain-independent measures can be adapted to the biomedical domain. these measures were originally based on {wordnet}, an english lexical database of concepts and relations. in this research, we adapt these measures to the {snomed}-{ct}^(r) ontology of medical concepts. the measures include two path-based measures, and three measures that augment path-based measures with information content statistics from corpora. we also derive a context vector measure based on medical corpora that can be used as a measure of semantic relatedness. these six measures are evaluated against a newly created test bed of 30 medical concept pairs scored by three physicians and nine medical coders. we find that the medical coders and physicians differ in their ratings, and that the context vector measure correlates most closely with the physicians, while the path-based measures and one of the information content measures correlates most closely with the medical coders. we conclude that there is a role both for more flexible measures of relatedness based on information derived from corpora, as well as for measures that rely on existing ontological structures.\"ny, usa\",variations in relevance judgments and the measurement of retrieval effectiveness,\"test collections have traditionally been used by information retrieval researchers to improve their retrieval strategies. to be viable as a laboratory tool, a collection must reliably rank different retrieval variants according to their true effectiveness. in particular, the relative effectiveness of two retrieval strategies should be insensitive to modest changes in the relevant document set since individual relevance assessments are known to vary widely. the test collections developed in the {trec} workshops have become the collections of choice in the retrieval research community. to verify their reliability, {nist} investigated the effect changes in the relevance assessments have on the evaluation of retrieval results. very high correlations were found among the rankings of systems produced using different relevance judgment sets. the high correlations indicate that the comparative evaluation of retrieval performance is stable despite substantial differences in relevance judgments, and thus reaffirm the use of the {trec} collections as laboratory tools.\"usa\",exploiting query reformulations for web search result diversification,\"when a web user\\'s underlying information need is not clearly specified from the initial query, an effective approach is to diversify the results retrieved for this query. in this paper, we introduce a novel probabilistic framework for web search result diversification, which explicitly accounts for the various aspects associated to an underspecified query. in particular, we diversify a document ranking by estimating how well a given document satisfies each uncovered aspect and the extent to which different aspects are satisfied by the ranking as a whole. we thoroughly evaluate our framework in the context of the diversity task of the {trec} 2009 web track. moreover, we exploit query reformulations provided by three major web search engines ({wses}) as a means to uncover different query aspects. the results attest the effectiveness of our framework when compared to state-of-the-art diversification approaches in the literature. additionally, by simulating an upper-bound query reformulation mechanism from official {trec} data, we draw useful insights regarding the effectiveness of the query reformulations generated by the different {wses} in promoting diversity.\"usa\",the role of knowledge in conceptual retrieval: a study in the domain of clinical medicine,\"despite its intuitive appeal, the hypothesis that retrieval at the level of \"\"concepts\"\" should outperform purely term-based approaches remains unverified empirically. in addition, the use of \"\"knowledge\"\" has not consistently resulted in performance gains. after identifying possible reasons for previous negative results, we present a novel framework for \"\"conceptual retrieval\"\" that articulates the types of knowledge that are important for information seeking. we instantiate this general framework in the domain of clinical medicine based on the principles of evidence-based medicine ({ebm}). experiments show that an {ebm}-based scoring algorithm dramatically outperforms a state-of-the-art baseline that employs only term statistics. ablation studies further yield a better understanding of the performance contributions of different components. finally, we discuss how other domains can benefit from knowledge-based approaches.\"usa\",how does clickthrough data reflect retrieval quality?,\"automatically judging the quality of retrieval functions based on observable user behavior holds promise for making retrieval evaluation faster, cheaper, and more user centered. however, the relationship between observable user behavior and retrieval quality is not yet fully understood. we present a sequence of studies investigating this relationship for an operational search engine on the {arxiv}.org e-print archive. we find that none of the eight absolute usage metrics we explore (e.g., number of clicks, frequency of query reformulations, abandonment) reliably reflect retrieval quality for the sample sizes we consider. however, we find that paired experiment designs adapted from sensory analysis produce accurate and reliable statements about the relative quality of two retrieval functions. in particular, we investigate two paired comparison tests that analyze clickthrough data from an interleaved presentation of ranking pairs, and we find that both give accurate and consistent results. we conclude that both paired comparison tests give substantially more accurate and sensitive evaluation results than absolute usage metrics in our domain.\"usa\",contextualising tags in collaborative tagging systems,\"collaborative tagging systems are now popular tools for organising and sharing information on the web. while collaborative tagging offers many advantages over the use of controlled vocabularies, they also suffer from problems such as the existence of polysemous tags. we investigate how the different contexts in which individual tags are used can be revealed automatically without consulting any external resources. we consider several different network representations of tags and documents, and apply a graph clustering algorithm on these networks to obtain groups of tags or documents corresponding to the different meanings of an ambiguous tag. our experiments show that networks which explicitly take the social context into account are more likely to give a better picture of the semantics of a tag.\"usa\",\"redundancy, diversity and interdependent document relevance\",an abstract is not available.usa\",recommended reading for {ir} research students,\"the strategic workshop on information retrieval at lorne ({swirl} 2004) was held in lorne, australia, from 8-10 december 2004, sec http://www.cs.mu.oz/\\\\~{}alistair/swirl2004/ for further information. a total of 38 international and australian researchers and australian graduate students took part.\"usa\",beyond the session timeout: automatic hierarchical segmentation of search topics in query logs,\"most analysis of web search relevance and performance takes a single query as the unit of search engine interaction. when studies attempt to group queries together by task or session, a timeout is typically used to identify the boundary. however, users query search engines in order to accomplish tasks at a variety of granularities, issuing multiple queries as they attempt to accomplish tasks. in this work we study real sessions manually labeled into hierarchical tasks, and show that timeouts, whatever their length, are of limited utility in identifying task boundaries, achieving a maximum precision of only 70\\\\%. we report on properties of this search task hierarchy, as seen in a random sample of user interactions from a major web search engine\\'s log, annotated by human editors, learning that 17\\\\% of tasks are interleaved, and 20\\\\% are hierarchically organized. no previous work has analyzed or addressed automatic identification of interleaved and hierarchically organized search tasks. we propose and evaluate a method for the automated segmentation of users\\' query streams into hierarchical units. our classifiers can improve on timeout segmentation, as well as other previously published approaches, bringing the accuracy up to 92\\\\% for identifying fine-grained task boundaries, and 89-97\\\\% for identifying pairs of queries from the same task when tasks are interleaved hierarchically. this is the first work to identify, measure and automatically segment sequences of user queries into their hierarchical structure. the ability to perform this kind of segmentation paves the way for evaluating search engines in terms of user task completion.\"usa\",effective and efficient user interaction for long queries,\"handling long queries can involve either pruning the query to retain only the important terms (reduction), or expanding the query to include related concepts (expansion). while automatic techniques to do so exist, roughly 25\\\\% performance improvements in terms of {map} have been realized in past work through interactive variants. we show that selectively reducing or expanding a query leads to an average improvement of 51\\\\% in {map} over the baseline for standard {trec} test collections. we demonstrate how user interaction can be used to achieve this improvement. most interaction techniques present users with a fixed number of options for all queries. we achieve improvements by interacting less with the user, i.e., we present techniques to identify the optimal number of options to present to users, resulting in an interface with an average of 70\\\\% fewer options to consider. previous algorithms supporting interactive reduction and expansion are exponential in nature. to extend their utility to operational environments, we present techniques to make the complexity of the algorithms polynomial. we finally present an analysis of long queries that continue to exhibit poor performance in spite of our new techniques.\"usa\",understanding web browsing behaviors through weibull analysis of dwell time,\"dwell time on web pages has been extensively used for various information retrieval tasks. however, some basic yet important questions have not been sufficiently addressed, eg, what distribution is appropriate to model the distribution of dwell times on a web page, and furthermore, what the distribution tells us about the underlying browsing behaviors. in this paper, we draw an analogy between abandoning a page during web browsing and a system failure in reliability analysis, and propose to model the dwell time using the weibull distribution. using this distribution provides better goodness-of-fit to real world data, and it uncovers some interesting patterns of user browsing behaviors not previously reported. for example, our analysis reveals that web browsing in general exhibits a significant \"\"negative aging\"\" phenomenon, which means that some initial screening has to be passed before a page is examined in detail, giving rise to the browsing behavior that we call \"\"screen-and-glean.\"\" in addition, we demonstrate that dwell time distributions can be reasonably predicted purely based on low-level page features, which broadens the possible applications of this study to situations where log data may be unavailable.\"usa\",query dependent pseudo-relevance feedback based on wikipedia,\"pseudo-relevance feedback ({prf}) via query-expansion has been proven to be e®ective in many information retrieval ({ir}) tasks. in most existing work, the top-ranked documents from an initial search are assumed to be relevant and used for {prf}. one problem with this approach is that one or more of the top retrieved documents may be non-relevant, which can introduce noise into the feedback process. besides, existing methods generally do not take into account the significantly different types of queries that are often entered into an {ir} system. intuitively, wikipedia can be seen as a large, manually edited document collection which could be exploited to improve document retrieval effectiveness within {prf}. it is not obvious how we might best utilize information from wikipedia in {prf}, and to date, the potential of wikipedia for this task has been largely unexplored. in our work, we present a systematic exploration of the utilization of wikipedia in {prf} for query dependent expansion. specifically, we classify {trec} topics into three categories based on wikipedia: 1) entity queries, 2) ambiguous queries, and 3) broader queries. we propose and study the effectiveness of three methods for expansion term selection, each modeling the wikipedia based pseudo-relevance information from a different perspective. we incorporate the expansion terms into the original query and use language modeling {ir} to evaluate these methods. experiments on four {trec} test collections, including the large web collection {gov2}, show that retrieval performance of each type of query can be improved. in addition, we demonstrate that the proposed method out-performs the baseline relevance model in terms of precision and robustness.\"halifax, nova scotia, b3h 1w5 canada\",a field study characterizing web-based information-seeking tasks,\"previous studies have examined various aspects of user behavior on the web, including general information-seeking patterns, search engine use, and revisitation habits. little research has been conducted to study how users navigate and interact with their web browser across different information-seeking tasks. we have conducted a field study of 21 participants, in which we logged detailed web usage and asked participants to provide task categorizations of their web usage based on the following categories: fact finding, information gathering, browsing, and transactions. we used implicit measures logged during each task session to provide usage measures such as dwell time, number of pages viewed, and the use of specific browser navigation mechanisms. we also report on differences in how participants interacted with their web browser across the range of information-seeking tasks. within each type of task, we found several distinguishing characteristics. in particular, information gathering tasks were the most complex; participants spent more time completing this task, viewed more pages, and used the web browser functions most heavily during this task. the results of this analysis have been used to provide implications for future support of information seeking on the web as well as direction for future research in this area.\"usa\",characterizing and predicting search engine switching behavior,\"search engine switching describes the voluntarily transition from one web search engine to another. in this paper we present a study of search engine switching behavior that combines large-scale log-based analysis and survey data. we characterize aspects of switching behavior, and develop and evaluate predictive models of switching behavior using features of the active query, the current session, and user search history. our findings provide insight into the decision-making processes of search engine users and demonstrate the relationship between switching and factors such as dissatisfaction with the quality of the results, the desire for broader topic coverage or verification of encountered information, and user preferences. the findings also reveal sufficient consistency in users\\' search behavior prior to engine switching to afford accurate prediction of switching events. predictive models may be useful for search engines who may want to modify the search experience if they can accurately anticipate a switch.\"usa\",\"search, interrupted: understanding and predicting search task continuation\",\"many important search tasks require multiple search sessions to complete. tasks such as travel planning, large purchases, or job searches can span hours, days, or even weeks. inevitably, life interferes, requiring the searcher either to recover the \"\"state\"\" of the search manually (most common), or plan for interruption in advance (unlikely). the goal of this work is to better understand, characterize, and automatically detect search tasks that will be continued in the near future. to this end, we analyze a query log from the bing web search engine to identify the types of intents, topics, and search behavior patterns associated with long-running tasks that are likely to be continued. using our insights, we develop an effective prediction algorithm that significantly outperforms both the previous state-of-the-art method, and even the ability of human judges, to predict future task continuation. potential applications of our techniques would allow a search engine to pre-emptively \"\"save state\"\" for a searcher (e.g., by caching search results), perform more targeted personalization, and otherwise better support the searcher experience for interrupted search tasks.\"190 elizabeth st, toronto, ontario, canada m5g 2c4. ey@yi.com\",empirical studies assessing the quality of health information for consumers on the world wide web: a systematic review.,ny, usa\",modeling individual cognitive structure in contextual information retrieval,\"in contextual information retrieval ({cir}), the retrieval of information depends on the time and place of the submitting query, history of interaction, task in hand, and many other factors that are not given explicitly, but lie implicitly in the interaction and surroundings of searching, namely the context [p. ingwersen, n. belkin, information retrieval in context, {acm} {sigir} forum 2 (2004)]. a user\\'s individual cognition is one of important contextual factors to help understand his or her personal needs. in this paper, we give a formal definition for a user\\'s individual cognitive structure ({ics}) in {cir}, and propose an approach called {dosam} to model it. {dosam} is inspired by the spreading activation model of psychology, and built on the domain ontology, while its goal is to get a user\\'s cognitive structure. cost analysis of construction algorithm shows that it is feasible to get {ics} by {dosam}, and personalized search experimental results on a digital library indicate that {ics} based search can improve the search effectiveness and a user\\'s satisfaction.\"\"research on relevance has established a conceptual consensus that stresses the importance of studying relevance judgments from a perspective that takes the users of retrieval systems into account. yet little research has investigated how actual system users make relevance judgments. theoretical claims pertaining to the nature of the relevance judgment process, thus, remain untested. this study is a step in the empirical exploration of the evolutionary nature of relevance judgments. the study intensively focuses on a single person with a real information problem. she was observed during both her online searching and document retrieval. the subject made her relevance evaluations first using bibliographic records and then using full-text documents as is typical of search processes that rely on bibliographic retrieval tools. the data consist of printouts of records and full-texts containing the subject\\'s evaluation markings as well as transcripts of think-aloud protocols and her responses to questions during post-search interview sessions. the mental model concept is employed for analysis purposes and is operationalized as the research subject\\'s changing perception of the information that she needs for her purposes as expressed in her relevance judgments. specifically, the think-aloud protocols and markings of texts provide indications of the state of the subject\\'s mental model and its change as she interacted with the materials that she retrieved and selected as relevant or possibly relevant. frequencies of terms marked at the stage of record evaluation and the topical categories highlighted at the stage of document review were computed to provide a more concrete indication of the topical change in the subject\\'s mental model of the needed information. the study also identified the set of judgment criteria that the subject applied during her evaluation of online records. special attention during the analysis was paid to anomalous judgment behaviors demonstrated by the subject such as her deselection process in record evaluation and topic reformulation in document evaluation. overall, the findings help untangle the relevance judgment process for one individual and one situation. in doing so, the findings provide a preliminary anchor for understanding the nature of the relevance judgment process of people engaged in an information search process. the subject\\'s deselection of items and associated application of judgment criteria provide specific insights into how relevance judgment occurs.\"usa\",the relationship between accessibility and usability of websites,\"accessibility and usability are well established concepts for user interfaces and websites. usability is precisely defined, but there are different approaches to accessibility. in addition, different possible relationships could exist between problems encountered by disabled and non-disabled users, yet little empirical data have been gathered on this question. guidelines for accessibility and usability of websites provide ratings of the importance of problems for users, yet little empirical data have been gathered to validate these ratings. a study investigated the accessibility of two websites with 6 disabled (blind) and 6 non-disabled (sighted) people. problems encountered by the two groups comprised two intersecting sets, with approximately 15\\\\% overlap. for one of the two websites, blind people rated problems significantly more severely than sighted people. there was high agreement between participants as to the severity of problems, and agreement between participants and researchers. however, there was no significant agreement between either participants or researchers and the importance/priority ratings provided by accessibility and usability guidelines. practical and theoretical implications of these results are discussed.\"this paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. the procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. these procedures are illustrated with a clinical diagnosis example from the epidemiological literature.\"in this paper, we present five user experiments on incorporating behavioral information into the relevance feedback process. in particular, we concentrate on ranking terms for query expansion and selecting new terms to add to the user\\'s query. our experiments are an attempt to widen the evidence used for relevance feedback from simply the relevant documents to include information on how users are searching. we show that this information can lead to more successful relevance feedback techniques. we also show that the presentation of relevance feedback to the user is important in the success of relevance feedback.\"dc, usa\",improving web search using contextual retrieval,\"contextual retrieval is a critical technique for today\\\\&\\\\#8217;s search engines in terms of facilitating queries and returning relevant information. this paper reports on the development and evaluation of a system designed to tackle some of the challenges associated with contextual information retrieval from the world wide web ({www}). the developed system has been designed with a view to capturing both implicit and explicit user data which is used to develop a personal contextual profile. such profiles can be shared across multiple users to create a shared contextual knowledge base. these are used to refine search queries and improve both the search results for a user as well as their search experience. an empirical study has been undertaken to evaluate the system against a number of hypotheses. in this paper, results related to one are presented that support the claim that users can find information more readily using the contextual search system.\"\"predicting the preferences of users and providing the personalized services or products based on their preferences are the important issues. however, the research considering users\\' preferences on context-aware computing is a relatively insufficient research field. hence, this paper aims to propose an agent-based framework for providing the personalized services using context history on context-aware computing. based on the proposed framework, we implement a prototype system to show the feasibility of the framework. previous researches require that the users input their preference manually, but this research provides the personalized services extracting the relationship between users\\' profile and services under the same context automatically.\"shreveport.\",rapid estimate of adult literacy in medicine: a shortened screening instrument.,\"{background}: this study was conducted to validate a shortened version of the rapid estimate of adult literacy in medicine ({realm}). this screening instrument is designed to be used in public health and primary care settings to identify patients with low reading levels. it provides reading grade estimates for patients who read below a ninth-grade level. the {realm} can be administered in one to two minutes by personnel with minimal training. {methods}: two hundred and three patients in four university hospital clinics (internal medicine, family practice, ambulatory care, and obstetrics/gynecology) were given the {realm} and three other standardized reading tests: the reading recognition section of the peabody individual achievement {test-revised} ({piat}-r), the wide range achievement {test-revised} ({wrat}-r), and the slosson oral reading {test-revised} ({sort}-r). one hundred inmates at a state prison were also given the {realm} twice, one week apart, to determine test-retest reliability. {results}: the {realm} correlated well with the three other tests. (correlation coefficients were 0.97 [{piat}-r], 0.96 [{sort}-r], and 0.88 [{wrat}-r].) all correlations were significant at p < .0001. test-retest reliability was 0.99 (p < .001). {conclusions}: the {realm} provides an estimate of patient reading ability, displays excellent concurrent validity with standardized reading tests, and is a practical instrument for busy primary care settings.\"\"new york, ny, usa\",content-based multimedia information retrieval,\"extending beyond the boundaries of science, art, and culture, content-based multimedia information retrieval provides new paradigms and methods for searching through the myriad variety of media all over the world. this survey reviews 100\\\\&plus; recent articles on content-based multimedia information retrieval and discusses their role in current research directions which include browsing and search paradigms, user studies, affective computing, learning, semantic queries, new features and media types, high performance indexing, and evaluation techniques. based on the current state of the art, we discuss the major challenges for the future.\"the success of the search engine may be our newtonian paradigm for the web. it enables us to do so much information discovery that it is difficult to imagine what we cannot do with it.usa\",summarizing local context to personalize global web search,\"the {pc} desktop is a very rich repository of personal information, efficiently capturing user\\'s interests. in this paper we propose a new approach towards an automatic personalization of web search in which the user specific information is extracted from such local desktops, thus allowing for an increased quality of user profiling, while sharing less private information with the search engine. more specifically, we investigate the opportunities to select personalized query expansion terms for web search using three different desktop oriented approaches: summarizing the entire desktop data, summarizing only the desktop documents relevant to each user query, and applying natural language processing techniques to extract dispersive lexical compounds from relevant desktop resources. our experiments with the google {api} showed at least the latter two techniques to produce a very strong improvement over current web search.\"usa\",evaluation of an inference network-based retrieval model,an abstract is not available.usa\",a study of remembered context for information access from personal digital archives,\"retrieval from personal archives (or human digital memories ({hdms})) is set to become a significant challenge in information retrieval ({ir}) research. these archives are unique in that the items in them are personal to the owner and as such the owner may have personal memories associated with the items. it is recognized that the harnessing of an individual\\'s memories about {hdm} items can be used as context data (such as user location at the time of item access) to aid retrieval. we present a pilot study, using one subject\\'s {hdm}, of remembered context data and its utility in retrieval. our results explore the types of context data best remembered for different item types and categories over time and show that context appears to become a more important factor in effective {hdm} {ir} over time as the subject\\'s recall of contents declines.\"\"syndromic surveillance uses health-related data that precede diagnosis and signal a sufficient probability of a case or an outbreak to warrant further public health response. while most syndromic surveillance systems rely on data from clinical encounters with health professionals, i started to explore in 2004 whether analysis of trends in internet searches can be useful to predict outbreaks such as influenza epidemics and prospectively gathered data on internet search trends for this purpose. there is an excellent correlation between the number of clicks on a keyword-triggered link in google with epidemiological data from the flu season 2004/2005 in canada (pearson correlation coefficient of current week clicks with the following week influenza cases r=.91). the \"\"google ad sentinel method\"\" proved to be more timely, more accurate and - with a total cost of can\\\\$365.64 for the entire flu-season - considerably cheaper than the traditional method of reports on influenza-like illnesses observed in clinics by sentinel physicians. systematically collecting and analyzing health information demand data from the internet has considerable potential to be used for syndromic surveillance. tracking web searches on the internet has the potential to predict population-based events relevant for public health purposes, such as real outbreaks, but may also be confounded by \"\"epidemics of fear\"\". data from such \"\"infodemiology studies\"\" should also include longitudinal data on health information supply.\"usa\",{pagerank}: standing on the shoulders of giants,\"the roots of google\\'s {pagerank} can be traced back to several early, and equally remarkable, ranking techniques.\"usa\",discovering and using groups to improve personalized search,\"personalized web search takes advantage of information about an individual to identify the most relevant results for that person. a challenge for personalization lies in collecting user profiles that are rich enough to do this successfully. one way an individual\\'s profile can be augmented is by using data from other people. to better understand whether groups of people can be used to benefit personalized search, we explore the similarity of query selection, desktop information, and explicit relevance judgments across people grouped in different ways. the groupings we explore fall along two dimensions: the longevity of the group members\\' relationship, and how explicitly the group is formed. we find that some groupings provide valuable insight into what members consider relevant to queries related to the group focus, but that it can be difficult to identify valuable groups implicitly. building on these findings, we explore an algorithm to \"\"groupize\"\" (versus \"\"personalize\"\") web search results that leads to a significant improvement in result ranking on group-relevant queries.\"usa\",beyond hyperlinks: organizing information footprints in search logs to support effective browsing,\"while current search engines serve known-item search such as homepage finding very well, they generally cannot support exploratory search effectively. in exploratory search, users do not know their information needs precisely and also often lack the needed knowledge to formulate effective queries, thus querying alone, as supported by the current search engines, is insufficient, and browsing into related information would be very useful. currently, browsing is mostly done by following hyperlinks embedded on web pages. in this paper, we propose to leverage search logs to allow a user to browse beyond hyperlinks with a multi-resolution topic map constructed based on search logs. specifically, we treat search logs as \"\"footprints\"\" left by previous users in the information space and build a multi-resolution topic map to semantically capture and organize them in multiple granularities. such a topic map can support a user to zoom in, zoom out, and navigate horizontally over the information space, and thus provide flexible and effective browsing capabilities for end users. to test the effectiveness of the proposed methods of supporting browsing, we rely on real search logs and a commercial search engine to implement our proposed methods. our experimental results show that the proposed topic map is effective to support browsing beyond hyperlinks.\"usa\",personalized ranking: a contextual ranking approach,new york, new york, usa.\",the practical impact of ontologies on biomedical informatics.,portland, oregon 97201, usa. hersh@ohsu.edu\",factors associated with success in searching {medline} and applying evidence to answer clinical questions.,\"this study sought to assess the ability of medical and nurse practitioner students to use {medline} to obtain evidence for answering clinical questions and to identify factors associated with the successful answering of questions. a convenience sample of medical and nurse practitioner students was recruited. after completing instruments measuring demographic variables, computer and searching attitudes and experience, and cognitive traits, the subjects were given a brief orientation to {medline} searching and the techniques of evidence-based medicine. the subjects were then given 5 questions (from a pool of 20) to answer in two sessions using the ovid {medline} system and the oregon health \\\\& science university library collection. each question was answered using three possible responses that reflected the quality of the evidence. all actions capable of being logged by the ovid system were captured. statistical analysis was performed using a model based on generalized estimating equations. the relevance-based measures of recall and precision were measured by defining end queries and having relevance judgments made by physicians who were not associated with the study. forty-five medical and 21 nurse practitioner students provided usable answers to 324 questions. the rate of correctness increased from 32.3 to 51.6 percent for medical students and from 31.7 to 34.7 percent for nurse practitioner students. ability to answer questions correctly was most strongly associated with correctness of the answer before searching, user experience with {medline} features, the evidence-based medicine question type, and the spatial visualization score. the spatial visualization score showed multi-colinearity with student type (medical vs. nurse practitioner). medical and nurse practitioner students obtained comparable recall and precision, neither of which was associated with correctness of the answer. medical and nurse practitioner students in this study were at best moderately successful at answering clinical questions correctly with the assistance of literature searching. the results confirm the importance of evaluating both search ability and the ability to use the resulting information to accomplish a clinical task.\"usa\",{ir} evaluation methods for retrieving highly relevant documents,\"this paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in {ir} experiments. it is argued that evaluation methods should credit {ir} methods for their ability to retrieve highly relevant documents. this is desirable from the user point of view in modern large {ir} environments. the proposed methods are (1) a novel application of {p-r} curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. we then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. the test was run with a best match retrieval system ({in-query1}) in a text database consisting of newspaper articles. the results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. the differences between the query types are practically essential and statistically significant. more generally, the novel evaluation methods and the case demonstrate that non-dichotomous relevance assessments are applicable in {ir} experiments, may reveal interesting phenomena, and allow harder testing of {ir} methods.\"usa\",forming test collections with no system pooling,\"forming test collection relevance judgments from the pooled output of multiple retrieval systems has become the standard process for creating resources such as the {trec}, {clef}, and {ntcir} test collections. this paper presents a series of experiments examining three different ways of building test collections where no system pooling is used. first, a collection formation technique combining manual feedback and multiple systems is adapted to work with a single retrieval system. second, an existing method based on pooling the output of multiple manual searches is re-examined: testing a wider range of searchers and retrieval systems than has been examined before. third, a new approach is explored where the ranked output of a single automatic search on a single retrieval system is assessed for relevance: no pooling whatsoever. using established techniques for evaluating the quality of relevance judgments, in all three cases, test collections are formed that are as good as {trec}.\"usa\",context-sensitive information retrieval using implicit feedback,\"a major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored. in this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting. we propose several context-sensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents. we use the {trec} {ap} data to create a test collection with search context information, and quantitatively evaluate our models using this test set. experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.\"ny, usa\",evaluation of a mobile information system in context,\"the motivation for this work has been to provide relevant information to the right situation for mobile phone users. we have iteratively developed applications for travellers and tourists to provide a general travel guide with local tourist information. this used a combination of wireless tag technology and information from content service providers. the paper presents our evaluation methodology and how we used it to develop the applications. the methodology is user centred, iterative, and progressive. it combines information retrieval ({ir}) evaluation methods with human-computer interaction ({hci}) development techniques. two groups of three user studies and a large survey were conducted in context with the studies scaling up to 238 participants and 438 user survey responses. the participants tested the applications in context, including the {ir} functions. they also filled in a pre and post questionnaire immediately before and after the system use. some participants were additionally interviewed.\"usa\",user search behaviors within a library gateway,this poster reports on user searching behavior within two information gateways developed at the university of illinois at {urbana-champaign} library. these gateways are built around a locally developed metasearch engine and are designed to assist users with search query formulation and modification. search behavior data is being collected in custom transaction logs that gather user search arguments along with any system actions and contextual search assistance suggestions.usa\",digital libraries\\' support for the user\\'s \\'information journey\\',usa\",towards context-aware search by learning a very large variable length hidden markov model from search logs,\"capturing the context of a user\\'s query from the previous queries and clicks in the same session may help understand the user\\'s information need. a context-aware approach to document re-ranking, query suggestion, and {url} recommendation may improve users\\' search experience substantially. in this paper, we propose a general approach to context-aware search. to capture contexts of queries, we learn a variable length hidden markov model ({vlhmm}) from search sessions extracted from log data. although the mathematical model is intuitive, how to learn a large {vlhmm} with millions of states from hundreds of millions of search sessions poses a grand challenge. we develop a strategy for parameter initialization in {vlhmm} learning which can greatly reduce the number of parameters to be estimated in practice. we also devise a method for distributed {vlhmm} learning under the map-reduce model. we test our approach on a real data set consisting of 1.8 billion queries, 2.6 billion clicks, and 840 million search sessions, and evaluate the effectiveness of the {vlhmm} learned from the real data on three search applications: document re-ranking, query suggestion, and {url} recommendation. the experimental results show that our approach is both effective and efficient.\"regent court, 211 portobello street, sheffield s1 4dp, uk\",a review of factors influencing user satisfaction in information retrieval,\"the authors investigate factors influencing user satisfaction in information retrieval. it is evident from this study that user satisfaction is a subjective variable, which can be influenced by several factors such as system effectiveness, user effectiveness, user effort, and user characteristics and expectations. therefore, information retrieval evaluators should consider all these factors in obtaining user satisfaction and in using it as a criterion of system effectiveness. previous studies have conflicting conclusions on the relationship between user satisfaction and system effectiveness; this study has substantiated these findings and supports using user satisfaction as a criterion of system effectiveness.\"cognitive effects in information seeking and retrieval,usa\",combining audio content and social context for semantic music discovery,\"when attempting to annotate music, it is important to consider both acoustic content and social context. this paper explores techniques for collecting and combining multiple sources of such information for the purpose of building a query-by-text music retrieval system. we consider two representations of the acoustic content (related to timbre and harmony) and two social sources (social tags and web documents). we then compare three algorithms that combine these information sources: calibrated score averaging ({csa}), {rankboost}, and kernel combination support vector machines ({kc}-{svm}). we demonstrate empirically that each of these algorithms is superior to algorithms that use individual information sources.\"\"in this paper the term  ” implicit human-computer interaction” is defined. it is discussed how the availability of processing power and advanced sensing technology can enable a shift in {hci} from explicit interaction, such as direct manipulation {guis}, towards a more implicit interaction based on situational context. in the paper, an algorithm is given based on a number of questions to identify applications that can facilitate implicit interaction. an {xml}-based language to describe implicit {hci} is proposed. the language uses contextual variables that can be grouped using different types of semantics as well as actions that are called by triggers. the term of perception is discussed and four basic approaches are identified that are useful when building context-aware applications. two examples, a wearable context awareness component and a sensor-board, show how sensor-based perception can be implemented. it is also discussed how situational context can be exploited to improve input and output of mobile devices.\"miami, fl 33199; applications user experience, oracle corporation, 500 oracle parkway, redwood shores, ca 94065\",best practices and future visions for search user interfaces,\"the authors describe a set of best practices that were developed to assist in the design of search user interfaces. search user interfaces represent a challenging design domain because novices who have no desire to learn the mechanics of search engine architecture or algorithms often use them. these can lead to frustration and task failure when it is not addressed by the user interface. the best practices are organized into five domains: the corpus, search algorithms, user and task context, the search interface, and mobility. in each section the authors present an introduction to the design challenges related to the domain and a set of best practices for creating a user interface that facilitates effective use by a broad population of users and tasks.\"usa\",understanding the relationship between searchers\\' queries and information goals,\"we describe results from web search log studies aimed at elucidating user behaviors associated with queries and destination {urls} that appear with different frequencies. we note the diversity of information goals that searchers have and the differing ways that goals are specified. we examine rare and common information goals that are specified using rare or common queries. we identify several significant differences in user behavior depending on the rarity of the query and the destination {url}. we find that searchers are more likely to be successful when the frequencies of the query and destination {url} are similar. we also establish that the behavioral differences observed for queries and goals of varying rarity persist even after accounting for potential confounding variables, including query length, search engine ranking, session duration, and task difficulty. finally, using an information-theoretic measure of search difficulty, we show that the benefits obtained by search and navigation actions depend on the frequency of the information goal.\"730 east beach blvd, long beach, ms 39560\",determining the context of text using augmented latent semantic indexing,\"latent semantic analysis has been used for several years to improve the performance of document library searches. we show that latent semantic analysis, augmented with a {part-of-speech} tagger, may be an effective algorithm for classifying a textual document as well. using brille\\'s {part-of-speech} tagger, we truncate the singular value decomposition used in latent semantic analysis to reduce the size of the word-frequency matrix. this method is then tested on a toy problem, and has shown to increase search accuracy. we then relate these results to natural language processing and show that latent semantic analysis can be combined with context free grammars to infer semantic meaning from natural language. english is the natural language currently being used.\"usa\",a model to estimate intrinsic document relevance from the clickthrough logs of a web search engine,\"we propose a new model to interpret the clickthrough logs of a web search engine. this model is based on explicit assumptions on the user behavior. in particular, we draw conclusions on a document relevance by observing the user behavior after he examined the document and not based on whether a user clicks or not a document url. this results in a model based on intrinsic relevance, as opposed to perceived relevance. we use the model to predict document relevance and then use this as feature for a \"\"learning to rank\"\" machine learning algorithm. comparing the ranking functions obtained by training the algorithm with and without the new feature we observe surprisingly good results. this is particularly notable given that the baseline we use is the heavily optimized ranking function of a leading commercial search engine. a deeper analysis shows that the new feature is particularly helpful for non navigational queries and queries with a large abandonment rate or a large average number of queries per session. this is important because these types of query is considered to be the most difficult to solve.\"usa\",assessing the scenic route: measuring the value of search trails in web logs,\"search trails mined from browser or toolbar logs comprise queries and the post-query pages that users visit. implicit endorsements from many trails can be useful for search result ranking, where the presence of a page on a trail increases its query relevance. follow-ing a search trail requires user effort, yet little is known about the benefit that users obtain from this activity versus, say, sticking with the clicked search result or jumping directly to the destination page at the end of the trail. in this paper, we present a log-based study estimating the user value of trail following. we compare the relevance, topic coverage, topic diversity, novelty, and utility of full trails over that provided by sub-trails, trail origins (landing pages), and trail destinations (pages where trails end). our findings demonstrate significant value to users in following trails, especially for certain query types. the findings have implications for the design of search systems, including trail recommendation systems that display trails on search result pages.\"usa\",word sense disambiguation: a survey,\"word sense disambiguation ({wsd}) is the ability to identify the meaning of words in context in a computational manner. {wsd} is considered an {ai}-complete problem, that is, a task whose solution is at least as hard as the most difficult problems in artificial intelligence. we introduce the reader to the motivations for solving the ambiguity of words and provide a description of the task. we overview supervised, unsupervised, and knowledge-based approaches. the assessment of {wsd} systems is discussed in the context of the {senseval/semeval} campaigns, aiming at the objective evaluation of systems participating in several different disambiguation tasks. finally, applications, open problems, and future directions are discussed.\"usa\",user language model for collaborative personalized search,\"traditional personalized search approaches rely solely on individual profiles to construct a user model. they are often confronted by two major problems: data sparseness and cold-start for new individuals. data sparseness refers to the fact that most users only visit a small portion of web pages and hence a very sparse user-term relationship matrix is generated, while cold-start for new individuals means that the system cannot conduct any personalization without previous browsing history. recently, community-based approaches were proposed to use the group\\'s social behaviors as a supplement to personalization. however, these approaches only consider the commonality of a group of users and still cannot satisfy the diverse information needs of different users. in this article, we present a new approach, called collaborative personalized search. it considers not only the commonality factor among users for defining group user profiles and global user profiles, but also the specialties of individuals. then, a statistical user language model is proposed to integrate the individual model, group user model and global user model together. in this way, the probability that a user will like a web page is calculated through a two-step smoothing mechanism. first, a global user model is used to smooth the probability of unseen terms in the individual profiles and provide aggregated behavior of global users. then, in order to precisely describe individual interests by looking at the behaviors of similar users, users are clustered into groups and group-user models are constructed. the group-user models are integrated into an overall model through a cluster-based language model. the behaviors of the group users can be utilized to enhance the performance of personalized search. this model can alleviate the two aforementioned problems and provide a more effective personalized search than previous approaches. large-scale experimental evaluations are conducted to show that the proposed approach substantially improves the relevance of a search over several competitive methods.\"usa\",the effects of topic familiarity on information search behavior,\"we describe results from a preliminary investigation of the relationship between topic familiarity and information search behavior. two types of information search behaviors are considered: reading time and efficacy. our results indicate that as one\\'s familiarity with a topic increases, one\\'s searching efficacy increases and one\\'s reading time decreases. these results suggest that it may be possible to infer topic familiarity from information search behavior.\"usa\",interactive effects of age and interface differences on search strategies and performance,\"we present results from an experiment that studied the information search behavior of younger and older adults in a medical decision-making task. to study how different combination of tasks and interfaces influenced search strategies and decision-making outcomes, we varied information structures of two interfaces and presented different task descriptions to participants. we found that younger adults tended to use different search strategies in different combination of tasks and interfaces, and older adults tended to use the same top-down strategies across conditions. we concluded that older adults were able to perform mental transformation of medical terms more effectively than younger adults. thus older adults did not require changing strategies to maintain the same level of performance.\"usa\",predicting searcher frustration,\"when search engine users have trouble finding information, they may become frustrated, possibly resulting in a bad experience (even if they are ultimately successful). in a user study in which participants were given difficult information seeking tasks, half of all queries submitted resulted in some degree of self-reported frustration. a third of all successful tasks involved at least one instance of frustration. by modeling searcher frustration, search engines can predict the current state of user frustration and decide when to intervene with alternative search strategies to prevent the user from becoming more frustrated, giving up, or switching to another search engine. we present several models to predict frustration using features extracted from query logs and physical sensors. we are able to predict frustration with a mean average precision of 65\\\\% from the physical sensors, and 87\\\\% from the query log features.\"maryland.\",lexical methods for managing variation in biomedical terminologies.,\"access to biomedical terminologies is hampered by the high degree of variability inherent in natural language terms and in the terminologies themselves. the lexicon, lexical programs, databases, and indexes included with the 1994 release of the {umls} knowledge sources are designed to help users manage this variability. we describe these resources and illustrate their flexibility and usefulness in providing enhanced access to data in the {umls} metathesaurus.\"\"this paper presents an investigation about how to automatically formulate effective queries using full or partial relevance information (i.e., the terms that are in relevant documents) in the context of relevance feedback ({rf}). the effects of adding relevance information in the {rf} environment are studied via controlled experiments. the conditions of these controlled experiments are formalized into a set of assumptions that form the framework of our study. this framework is called idealized relevance feedback ({irf}) framework. in our {irf} settings, we confirm the previous findings of relevance feedback studies. in addition, our experiments show that better retrieval effectiveness can be obtained when (i) we normalize the term weights by their ranks, (ii) we select weighted terms in the top k retrieved documents, (iii) we include terms in the initial title queries, and (iv) we use the best query sizes for each topic instead of the average best query size where they produce at most five percentage points improvement in the mean average precision ({map}) value. we have also achieved a new level of retrieval effectiveness which is about 55-60\\\\% {map} instead of 40+\\\\% in the previous findings. this new level of retrieval effectiveness was found to be similar to a level using a {trec} ad hoc test collection that is about double the number of documents in the {trec}-3 test collection used in previous works.\"usa\",\"an interactive, smart notepad for context-sensitive information seeking\",\"we are building an interactive, smart notepad system where users enter brief notes to drive a dynamic information-seeking process. in this paper, we focus on describing our work from two aspects: 1) dynamic interpretation of user notes in context to infer a user\\'s information needs, and 2) automatic generation of data queries to satisfy the inferred user needs. compared to existing information systems, our work offers three unique contributions. first, our system allows users to focus on what to retrieve instead of how, since users can use brief notes to express their information needs without worrying about specific retrieval details. second, users can use notes to efficiently request multiple pieces of information at once instead of issuing one query at a time. third, users can easily update any part of their notes to obtain new or updated information. whenever a user\\'s notes are modified, our system automatically detects and evaluates all affected note sections to retrieve new or updated information. our preliminary evaluation shows the promise of this work.\"\"{abstract\\\\&nbsp;\\\\&nbsp;this} paper investigates the number of expansion terms to use in automatic query expansion by examining the behavior of eight retrieval systems participating in the {nrrc} reliable information access workshop. the results demonstrate that current systems are able to obtain nearly all of the benefit of using a fixed number of expansion terms per topic, but significant additional improvement is possible if systems were able to accurately select the best number of expansion terms on a per topic basis. when optimizing average effectiveness as measured by mean average precision, using a fixed number of terms increases the score a large amount for a small number of topics but has little effect for most topics. the analysis further suggests that when a topic is helped by automatic feedback, the increase is from a set of terms that reinforce each other rather than from the system finding a single excellent term.\"ny, usa\",search characteristics in different types of web-based {ir} environments: are they the same?,\"transaction logs from four different web-based information retrieval environments (bibliographic databank, {opac}, search engine, specialized search system) were analyzed for empirical regularities in search characteristics to determine whether users engage in different behaviors in different web-based search environments. descriptive statistics and relative frequency distributions related to term usage, query formulation, and session duration were tabulated. the analysis revealed that there are differences in these characteristics. users were more likely to engage in extensive searching using the {opac} and specialized search system. surprisingly, the bibliographic databank search environment resulted in the most parsimonious searching, more similar to a general search engine. although on the surface web-based search facilities may appear similar, users do engage in different search behaviors.\"context and information retrieval,summary 10.1002/9780470033647.ch7.abs this chapter contains sections titled: * introduction * what is context? * context in information retrieval * context modelling and representation * context and content * related topics * evaluating context-aware {ir} systems * summary * exercises * referencesuniversity of strathclyde,interactive information retrieval,usa\",multiple approaches to analysing query diversity,\"in this paper we examine user queries with respect to diversity: providing a mix of results across different interpretations. using two query log analysis techniques (click entropy and reformulated queries), 14.9 million queries from the microsoft live search log were analysed. we found that a broad range of query types may benefit from diversification. additionally, although there is a correlation between word ambiguity and the need for diversity, the range of results users may wish to see for an ambiguous query stretches well beyond traditional notions of word sense.\"usa\",personalizing search via automated analysis of interests and activities,\"we formulate and study search algorithms that consider a user\\'s prior interactions with a wide variety of content to personalize that user\\'s current web search. rather than relying on the unrealistic assumption that people will precisely specify their intent when searching, we pursue techniques that leverage implicit information about the user\\'s interests. this information is used to re-rank web search results within a relevance feedback framework. we explore rich models of user interests, built from both search-related information, such as previously issued queries and previously visited web pages, and other information about the user such as documents and email the user has read and created. our research suggests that rich representations of the user and the corpus are important for personalization, but that it is possible to approximate these representations and provide efficient client-side algorithms for personalizing search. we show that such personalization algorithms can significantly improve on current web search.\"usa\",using query contexts in information retrieval,\"user query is an element that specifies an information need, but it is not the only one. studies in literature have found many contextual factors that strongly influence the interpretation of a query. recent studies have tried to consider the user\\'s interests by creating a user profile. however, a single profile for a user may not be sufficient for a variety of queries of the user. in this study, we propose to use  query-specific contexts  instead of user-centric ones, including  context around query  and  context within query . the former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations. in this paper, both types of context are integrated in an {ir} model based on language modeling. our experiments on several {trec} collections show that each of the context factors brings significant improvements in retrieval effectiveness.\"\"personalized web browsing and search hope to provide web information that matches a user\\'s personal interests. a key feature in developing successful personalized web applications is to build user model that accurately represents a user\\'s interests. this paper deals with the problem of modeling web users by means of personal ontology. a web log preparation system discovering user\\'s semantic navigation sessions is presented first. such semantic sessions could be used as the input of constructing ontology-based user model. our construction of user model is based on a semantic representation of the user activity. we build the user model without user interaction, automatically monitoring the user\\'s browsing habits, constructing the user ontology from semantic sessions. each semantic session updates the user model in such a way that the conceptual behavior history of the user is recorded in user ontology. after building the initial model from visited web pages, techniques are investigated to estimate model convergence. in particular, the overall performance of our ontology-based user model is also presented and favorably compared to other model using a flat, unstructured list of topics in the experimental systems.\"ny, usa\",adapting information retrieval to query contexts,\"in current {ir} approaches documents are retrieved only according to the terms specified in the query. the same answers are returned for the same query whatever the user and the search goal are. in reality, many other contextual factors strongly influence document\\'s relevance and they should be taken into account in {ir} operations. this paper proposes a method, based on language modeling, to integrate several contextual factors so that document ranking will be adapted to the specific query contexts. we will consider three contextual factors in this paper: the topic domain of the query, the characteristics of the document collection, as well as context words within the query. each contextual factor is used to generate a new query language model to specify some aspect of the information need. all these query models are then combined together to produce a more complete model for the underlying information need. our experiments on {trec} collections show that each contextual factor can positively influence the {ir} effectiveness and the combined model results in the highest effectiveness. this study shows that it is both beneficial and feasible to integrate more contextual factors in the current {ir} practice.\"usa\",studying trailfinding algorithms for enhanced web search,\"search engines return ranked lists of web pages in response to queries. these pages are starting points for post-query navigation, but may be insufficient for search tasks involving multiple steps. search trails mined from toolbar logs start with a query and contain pages visited by one user during post-query navigation. implicit endorsements from many trails can enhance result ranking. rather than using trails solely to improve ranking, it may also be worth providing trail information directly to users. in this paper, we quantify the benefit that users currently obtain from trail-following and compare different methods for finding the best trail for a given query and each top-ranked result. we compare the relevance, topic coverage, topic diversity, and utility of trails selected using different methods, and break out findings by factors such as query type and origin relevance. our findings demonstrate value in trails, highlight interesting differences in the performance of trailfinding algorithms, and show we can find best-trails for a query that outperform the trails most users follow. findings have implications for enhancing web information seeking using trails.\"usa\",predicting escalations of medical queries based on web page structure and content,\"logs of users\\' searches on web health topics can exhibit signs of escalation of medical concerns, where initial queries about common symptoms are followed by queries about serious, rare illnesses. we present an effort to predict such escalations based on the structure and content of pages encountered during medical search sessions. we construct and then characterize the performance of classifiers that predict whether an escalation will occur after the access of a page. our findings have implications for ranking algorithms and the design of search interfaces.\"usa\",evaluating the accuracy of implicit feedback from clicks and query reformulations in web search,\"this article examines the reliability of implicit feedback generated from clickthrough data and query reformulations in world wide web ({www}) search. analyzing the users\\' decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. while this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences derived from clicks are reasonably accurate on average. we find that such relative preferences are accurate not only between results from an individual query, but across multiple sets of results within chains of query reformulations.\"ny, usa\",cognitive and task influences on web searching behavior,\"users\\' individual differences and tasks are important factors that influence the use of information systems. two independent investigations were conducted to study the impact of differences in users\\' cognition and search tasks on web search activities and outcomes. strong task effects were found on search activities and outcomes, whereas interactions between cognitive and task variables were found on search activities only. these results imply that the flexibility of the web and web search engines allows different users to complete different search tasks successfully. however, the search techniques used and the efficiency of the searches appear to depend on how well the individual searcher fits with the specific task.\"usa\",spatial variation in search engine queries,\"local aspects of web search - associating web content and queries with geography - is a topic of growing interest. however, the underlying question of how spatial variation is manifested in search queries is still not well understood. here we develop a probabilistic framework for quantifying such spatial variation; on complete yahoo! query logs, we find that our model is able to localize large classes of queries to within a few miles of their natural centers based only on the distribution of activity for the query. our model provides not only an estimate of a query\\'s geographic center, but also a measure of its spatial dispersion, indicating whether it has highly local interest or broader regional or national appeal. we also show how variations on our model can track geographically shifting topics over time, annotate a map with each location\\'s \"\"distinctive queries\"\", and delineate the \"\"spheres of influence\"\" for competing queries in the same general domain.\"usa\",placing search in context: the concept revisited,\"keyword-based search engines are in widespread use today as a popular means for web-based information retrieval. although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. this paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. this paradigm is implemented in practice in the {intellizap} system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (\"\"the context\"\"). the context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. the latter are submitted to a host of general and domain-specific search engines. search results are then semantically reranked, using context. experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the web.\"los angeles, usa.\",{indexfinder}: a method of extracting key concepts from clinical texts for indexing.,\"extracting key concepts from clinical texts for indexing is an important task in implementing a medical digital library. several methods are proposed for mapping free text into standard terms defined by the unified medical language system ({umls}). for example, natural language processing techniques are used to map identified noun phrases into concepts. they are, however, not appropriate for real time applications. therefore, in this paper, we present a new algorithm for generating all valid {umls} concepts by permuting the set of words in the input text and then filtering out the irrelevant concepts via syntactic and semantic filtering. we have implemented the algorithm as a web-based service that provides a search interface for researchers and computer programs. our preliminary experiment shows that the algorithm is effective at discovering relevant {umls} concepts while achieving a throughput of {43k} bytes of text per second. the tool can extract key concepts from clinical texts for indexing.\"explicit search result diversification through sub-queries,\"queries submitted to a retrieval system are often ambiguous. in such a situation, a sensible strategy is to diversify the ranking of results to be retrieved, in the hope that users will find at least one of these results to be relevant to their information need. in this paper, we introduce {xquad}, a novel framework for search result diversification that builds such a diversified ranking by explicitly accounting for the relationship between documents retrieved for the original query and the possible aspects underlying this query, in the form of sub-queries. we evaluate the effectiveness of {xquad} using a standard {trec} collection. the results show that our framework markedly outperforms state-of-the-art diversification approaches under a simulated best-case scenario. moreover, we show that its effectiveness can be further improved by estimating the relative importance of each identified sub-query. finally, we show that our framework can still outperform the simulated best-case scenario of the state-of-the-art diversification approaches using sub-queries automatically derived from the baseline document ranking itself.\"usa\",selecting good expansion terms for pseudo-relevance feedback,\"pseudo-relevance feedback assumes that most frequent terms in the pseudo-feedback documents are useful for the retrieval. in this study, we re-examine this assumption and show that it does not hold in reality - many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful to the retrieval. we also show that good expansion terms cannot be distinguished from bad ones merely on their distributions in the feedback documents and in the whole collection. we then propose to integrate a term classification process to predict the usefulness of expansion terms. multiple additional features can be integrated in this process. our experiments on three {trec} collections show that retrieval effectiveness can be much improved when term classification is used. in addition, we also demonstrate that good terms should be identified directly according to their possible impact on the retrieval effectiveness, i.e. using supervised learning, instead of unsupervised learning.\"usa\",learning to rank relational objects and its application to web search,usa\",modeling the impact of short- and long-term behavior on search personalization,\"user behavior provides many cues to improve the relevance of search results through personalization. one aspect of user behavior that provides especially strong signals for delivering better relevance is an individual\\'s history of queries and clicked documents. previous studies have explored how short-term behavior or long-term behavior can be predictive of relevance. ours is the first study to assess how short-term (session) behavior and long-term (historic) behavior interact, and how each may be used in isolation or in combination to optimally contribute to gains in relevance through search personalization. our key findings include: historic behavior provides substantial benefits at the start of a search session; short-term session behavior contributes the majority of gains in an extended search session; and the combination of session and historic behavior out-performs using either alone. we also characterize how the relative contribution of each model changes throughout the duration of a session. our findings have implications for the design of search systems that leverage user behavior to personalize the search experience.\"usa\",actively predicting diverse search intent from user browsing behaviors,\"this paper is concerned with actively predicting search intent from user browsing behavior data. in recent years, great attention has been paid to predicting user search intent. however, the prediction was mostly passive because it was performed only after users submitted their queries to search engines. it is not considered why users issued these queries, and what triggered their information needs. according to our study, many information needs of users were actually triggered by what they have browsed. that is, after reading a page, if a user found something interesting or unclear, he/she might have the intent to obtain further information and accordingly formulate a search query. actively predicting such search intent can benefit both search engines and their users. in this paper, we propose a series of technologies to fulfill this task. first, we extract all the queries that users issued after reading a given page from user browsing behavior data. second, we learn a model to effectively rank these queries according to their likelihoods of being triggered by the page. third, since search intents can be quite diverse even if triggered by the same page, we propose an optimization algorithm to diversify the ranked list of queries obtained in the second step, and then suggest the list to users. we have tested our approach on large-scale user browsing behavior data obtained from a commercial search engine. the experimental results have shown that our approach can predict meaningful queries for a given page, and the search performance for these queries can be significantly improved by using the triggering page as contextual information.\"usa\",activity put in context: identifying implicit task context within the user\\'s document interaction,\"modern desktop search is ill-fitted to our personal document workspace. on one hand, many of the methods which render web search effective cannot be applied on the desktop. on the other, desktop search does not take full advantage of attributes that are unique to our personal documents. in this work, we present confluence, a desktop search system that addresses this problem by capturing the task context within which a user interacts with their documents. this context is then integrated with traditional desktop search techniques to enable task-based document retrieval.\"forskningsveien 2b, 4. etg., 0027 oslo, norway. laura.slaughter@rikshospitalet.no\",semantic representation of consumer questions and physician answers.,\"the aim of this study was to identify the underlying semantics of health consumers\\' questions and physicians\\' answers in order to analyze the semantic patterns within these texts. we manually identified semantic relationships within question-answer pairs from {ask-the-doctor} web sites. identification of the semantic relationship instances within the texts was based on the relationship classes and structure of the unified medical language system ({umls}) semantic network. we calculated the frequency of occurrence of each semantic relationship class, and conceptual graphs were generated, joining concepts together through the semantic relationships identified. we then analyzed whether representations of physician\\'s answers exactly matched the form of the question representations. lastly, we examined characteristics of the answer conceptual graphs. we identified 97 semantic relationship instances in the questions and 334 instances in the answers. the most frequently identified semantic relationship in both questions and answers was brings\\\\_about (causal). we found that the semantic relationship propositions identified in answers that most frequently contain a concept also expressed in the question were: brings\\\\_about, isa, co\\\\_occurs\\\\_with, diagnoses, and treats. using extracted semantic relationships from real-life questions and answers can produce a valuable analysis of the characteristics of these texts. this can lead to clues for creating semantic-based retrieval techniques that guide users to further information. for example, we determined that both consumers and physicians often express causative relationships and these play a key role in leading to further related concepts.\"usa\",{concept-based} information retrieval using explicit semantic analysis,\"information retrieval systems traditionally rely on textual keywords to index and retrieve documents. keyword-based retrieval may return inaccurate and incomplete results when different keywords are used to describe the same concept in the documents and in the queries. furthermore, the relationship between these related keywords may be semantic rather than syntactic, and capturing it thus requires access to comprehensive human world knowledge. concept-based retrieval methods have attempted to tackle these difficulties by using manually built thesauri, by relying on term cooccurrence data, or by extracting latent word relationships and concepts from a corpus. in this article we introduce a new concept-based retrieval approach based on explicit semantic analysis ({esa}), a recently proposed method that augments keyword-based text representation with concept-based features, automatically extracted from massive human knowledge repositories such as wikipedia. our approach generates new text features automatically, and we have found that high-quality feature selection becomes crucial in this setting to make the retrieval more focused. however, due to the lack of labeled data, traditional feature selection methods cannot be used, hence we propose new methods that use self-generated labeled training data. the resulting system is evaluated on several {trec} datasets, showing superior performance over previous state-of-the-art results.\"\"this article reports on a detailed investigation of {pubmed} users\\' needs and behavior as a step toward improving biomedical information retrieval. {pubmed} is providing free service to researchers with access to more than 19 million citations for biomedical articles from {medline} and life science journals. it is accessed by millions of users each day. efficient search tools are crucial for biomedical researchers to keep abreast of the biomedical literature relating to their own research. this study provides insight into {pubmed} users\\' needs and their behavior. this investigation was conducted through the analysis of one month of log data, consisting of more than 23 million user sessions and more than 58 million user queries. multiple aspects of users\\' interactions with {pubmed} are characterized in detail with evidence from these logs. despite having many features in common with general web searches, biomedical information searches have unique characteristics that are made evident in this study. {pubmed} users are more persistent in seeking information and they reformulate queries often. the three most frequent types of search are search by author name, search by gene/protein, and search by disease. use of abbreviation in queries is very frequent. factors such as result set size influence users\\' decisions. analysis of characteristics such as these plays a critical role in identifying users\\' information needs and their search habits. in turn, such an analysis also provides useful insight for improving biomedical information retrieval.\"usa\",tagging for use: an analysis of use-centred resource description,\"this paper discusses use-centred resource description, a practice whereby an information object is assigned metadata that describes what it can be used for, as opposed to what it is. we look at precedents for this practice in the literature and present three cases of use-centred description in diverse information environments. through analysis and comparison of the cases, we develop a preliminary framework for use-centred resource description.\"\"we generated networks of journal relationships from citation and download data, and determined journal impact rankings from these networks using a set of social network centrality metrics. the resulting journal impact rankings were compared to the {isi} {if}. results indicate that, although social network metrics and {isi} {if} rankings deviate moderately for citation-based journal networks, they differ considerably for journal networks derived from download data. we believe the results represent a unique aspect of general journal impact that is not captured by the {isi} {if}. these results furthermore raise questions regarding the validity of the {isi} {if} as the sole assessment of journal impact, and suggest the possibility of devising impact metrics based on usage information in general.\"usa\",effects of performance feedback on users\\' evaluations of an interactive {ir} system,\"in this study, we seek to understand how providing feedback to users about their performances with an interactive information retrieval ({iir}) system impacts their evaluations of that system. sixty subjects completed three recall-based searching tasks with an experimental {iir} system and were asked to evaluate the system after each task and after finishing all three tasks. before completing the final evaluation, three-fourths of the subjects were provided with feedback about their performances. subjects were assigned randomly to one of four feedback conditions: a baseline condition where no feedback was provided; an actual feedback condition where subjects were provided with their real performances; and two conditions where subjects were deceived and told that they performed very well or very poorly. results show that the type of feedback provided significantly affected subjects\\' system evaluations; most importantly there was a significant difference in subjects\\' satisfaction ratings before and after feedback was provided in the actual feedback condition.\"speech, and communication)\",\"{with a preface by george miller  <p>wordnet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. its design is inspired by current psycholinguistic and computational theories of human lexical memory. english nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. different relations link the synonym sets.  <p>the purpose of this volume is twofold. first, it discusses the design of wordnet and the theoretical motivations behind it. second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains.  <p>contributors: reem al-halimi, robert c. berwick, j. f. m. burg, martin chodorow, christiane fellbaum, joachim grabowski, sanda harabagiu, marti a. hearst, graeme hirst, douglas a. jones, rick kazman, karen t. kohl, shari landes, claudia leacock, george a. miller, katherine j. miller, dan moldovan, naoyuki nomura, uta priss, philip resnik, david st-onge, randee tengi, reind p. van de riet, ellen voorhees.}\"{hci} and information search: capturing task and searcher characteristics through \\'user ability to specify information need\\',\"{human-computer} interaction ({hci}) is all about the way in which people interact with computer systems. this paper focuses on the cognitive aspects of {hci} when a user is searching for information, so as to facilitate effective user interactions with vast amounts of available information. search engines provide a \\'one-size-fits-all\\' model, which do not adequately cater to the differing needs of searchers at different points in time (continuously changing situations in time/space, as per sense-making theory). we posit that from a system designer\\'s point of view, capturing the \\'user ability to specify his information need\\' will help operationalize task/searcher characteristics (hence the user need) and help the designer provide better interfaces for search that fit the needs of the user and lead to search efficacy and searcher satisfaction. the study should advance {hci} for search through greater understanding of user needs, enhance search interfaces and lead to theory development.\"usa\",reusable test collections through experimental design,\"portable, reusable test collections are a vital part of research and development in information retrieval. reusability is difficult to assess, however. the standard approach--simulating judgment collection when groups of systems are held out, then evaluating those held-out systems--only works when there is a large set of relevance judgments to draw on during the simulation. as test collections adapt to larger and larger corpora, it becomes less and less likely that there will be sufficient judgments for such simulation experiments. thus we propose a methodology for information retrieval experimentation that collects evidence for or against the reusability of a test collection while judgments are being made. using this methodology along with the appropriate statistical analyses, researchers will be able to estimate the reusability of their test collections while building them and implement \"\"course corrections\"\" if the collection does not seem to be achieving desired levels of reusability. we show the robustness of our design to inherent sources of variance, and provide a description of an actual implementation of the framework for creating a large test collection.\",embedding emotional context in recommender systems,\"emotions are crucial for user\\'s decision making in recommendation processes. we first introduce ambient recommender systems, which arise from the analysis of new trends on the exploitation of the emotional context in the next generation of recommender systems. we then explain some results of these new trends in real-world applications through the smart prediction assistant ({spa}) platform in an intelligent learning guide with more than three million users. while most approaches to recommending have focused on algorithm performance. {spa} makes recommendations to users on the basis of emotional information acquired in an incremental way. this article provides a cross-disciplinary perspective to achieve this goal in such recommender systems through a {spa} platform. the methodology applied in {spa} is the result of a bunch of technology transfer projects for large real-world rccommender systems.\"the turn: integration of information seeking and retrieval in context (the information retrieval series),\"{\\\\_the} turn\\\\_ analyzes the research of information seeking and retrieval ({is}\\\\&r)and proposes a new direction of integrating research in these two areas: thefields should turn off their separate and narrow paths and construct a newavenue of research. an essential direction for this avenue is context as givenin the subtitle integration of information seeking and retrieval in {context.other} essential themes in the book {include:is}\\\\&r research models, frameworks and theories; search and works tasks andsituations in context; interaction between humans and machines; informationacquisition, relevance and information use; research design and methodologybased on a structured set of explicit variables - all set into the holisticcognitive approach. the present monograph invites the reader into aconstruction project - there is much research to do for a contextualunderstanding of {is}\\\\&{r.\\\\_the} turn\\\\_ represents a wide-ranging perspective of {is}\\\\&r by providing a novelunique research framework, covering both individual and social aspects ofinformation behavior, including the generation, searching, retrieval and useof information. regarding traditional laboratory information retrievalresearch, the monograph proposes the extension of research toward actors,search and work tasks, {ir} interaction and utility of information. regardingtraditional information seeking research, it proposes the extension towardinformation access technology and work task {contexts.\\\\_the} turn\\\\_ is the first synthesis of research in the broad area of {is}\\\\&rranging from systems oriented laboratory {ir} research to social scienceoriented information seeking studies.\"usa\",collaborative annotation for context-aware retrieval,\"we discuss how collaborative annotations can be exploited to simplify and improve the management of context and resources in the context-aware retrieval field. we apply this approach to our context aware browser, a general purpose solution to web content perusal by means of mobile devices, based on the user\\'s context. instead of relying on a pool of experts and on a rigid categorization, as it is usually done in the context-aware field, our solution allows the crowd of users to model, control and manage the contextual knowledge through collaboration and participation. we propose two models and we outline an example of application.\"usa\",{sigir} 2003 workshop report: implicit measures of user interests and preferences,\"it is widely believed that many queries submitted to search engines are inherently ambiguous (e.g., java and apple). however, few studies have tried to classify queries based on ambiguity and to answer  ” what the proportion of ambiguous queries is”. this paper deals with these issues. first, we clarify the definition of ambiguous queries by constructing the taxonomy of queries from being ambiguous to specific. second, we ask human annotators to manually classify queries. from manually labeled results, we observe that query ambiguity is to some extent predictable. third, we propose a supervised learning approach to automatically identify ambiguous queries. experimental results show that we can correctly identify 87\\\\% of labeled queries with the approach. finally, by using our approach, we estimate that about 16\\\\% of queries in a real search log are ambiguous.\"usa\",amplifying community content creation with mixed initiative information extraction,\"although existing work has explored both information extraction and community content creation, most research has focused on them in isolation. in contrast, we see the greatest leverage in the synergistic pairing of these methods as two interlocking feedback cycles. this paper explores the potential synergy promised if these cycles can be made to accelerate each other by exploiting the same edits to advance both community content creation and learning-based information extraction. we examine our proposed synergy in the context of wikipedia infoboxes and the kylin information extraction system. after developing and refining a set of interfaces to present the verification of kylin extractions as a non primary task in the context of wikipedia articles, we develop an innovative use of web search advertising services to study people engaged in some other primary task. we demonstrate our proposed synergy by analyzing our deployment from two complementary perspectives: (1) we show we accelerate community content creation by using kylin\\'s information extraction to significantly increase the likelihood that a person visiting a wikipedia article as a part of some other primary task will spontaneously choose to help improve the article\\'s infobox, and (2) we show we accelerate information extraction by using contributions collected from people interacting with our designs to significantly improve kylin\\'s extraction performance.\"usa\",the {u}nified {m}edical {l}anguage {s}ystem: moving beyond the vocabulary of bibliographic retrieval,2400 e. hartford avenue, po box 413, milwaukee, wi 53210, usa.\",a cognitive evaluation of four online search engines for answering definitional questions posed by physicians.,\"the internet is having a profound impact on physicians\\' medical decision making. one recent survey of 277 physicians showed that 72\\\\% of physicians regularly used the internet to research medical information and 51\\\\% admitted that information from web sites influenced their clinical decisions. this paper describes the first cognitive evaluation of four state-of-the-art internet search engines: google (i.e., google and {scholar.google}), {medqa}, onelook, and {pubmed} for answering definitional questions (i.e., questions with the format of \"\"what is x?\"\") posed by physicians. onelook is a portal for online definitions, and {medqa} is a question answering system that automatically generates short texts to answer specific biomedical questions. our evaluation criteria include quality of answer, ease of use, time spent, and number of actions taken. our results show that {medqa} outperforms onelook and {pubmed} in most of the criteria, and that {medqa} surpasses google in time spent and number of actions, two important efficiency criteria. our results show that google is the best system for quality of answer and ease of use. we conclude that google is an effective search engine for medical definitions, and that {medqa} exceeds the other search engines in that it provides users direct answers to their questions; while the users of the other search engines have to visit several sites before finding all of the pertinent information.\"usa\",personalized query expansion for the web,\"the inherent ambiguity of short keyword queries demands for enhanced methods for web retrieval. in this paper we propose to improve such web queries by expanding them with terms collected from each user\\'s personal information repository, thus implicitly personalizing the search output. we introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri. our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings. subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query. a separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.\"\"{background}: {uptodate} and {pubmed} are popular sources for medical information. data regarding the efficiency of {pubmed} and {uptodate} in daily medical care are lacking. {objective}: the purpose of this observational study was to describe the percentage of answers retrieved by these information sources, comparing search results with regard to different medical topics and the time spent searching for an answer. {methods}: a total of 40 residents and 30 internists in internal medicine working in an academic medical center searched {pubmed} and {uptodate} using an observation portal during daily medical care. the information source used for searching and the time needed to find an answer to the question were recorded by the portal. information was provided by searchers regarding the topic of the question, the situation that triggered the question, and whether an answer was found. {results}: we analyzed 1305 patient-related questions sent to {pubmed} and/or {uptodate} between october 1, 2005 and march 31, 2007 using our portal. a complete answer was found in 594/1125 (53\\\\%) questions sent to {pubmed} or {uptodate}. a partial or full answer was obtained in 729/883 (83\\\\%) {uptodate} searches and 152/242 (63\\\\%) {pubmed} searches (p < .001). {uptodate} answered more questions than {pubmed} on all major medical topics, but a significant difference was detected only when the question was related to etiology (p < .001) or therapy (p = .002). time to answer was 241 seconds ({sd} 24) for {uptodate} and 291 seconds ({sd} 7) for {pubmed}. {conclusions}: specialists and residents in internal medicine generally use less than 5 minutes to answer patient-related questions in daily care. more questions are answered using {uptodate} than {pubmed} on all major medical topics.\"usa\",mining rich session context to improve web search,\"user browsing information, particularly their non-search related activity, reveals important contextual information on the preferences and the intent of web users. in this paper, we expand the use of browsing information for web search ranking and other applications, with an emphasis on analyzing individual user sessions for creating aggregate models. in this context, we introduce {clickrank}, an efficient, scalable algorithm for estimating web page and web site importance from browsing information. we lay out the theoretical foundation of {clickrank} based on an intentional surfer model and analyze its properties. we evaluate its effectiveness for the problem of web search ranking, showing that it contributes significantly to retrieval performance as a novel web search feature. we demonstrate that the results produced by {clickrank} for web search ranking are highly competitive with those produced by other approaches, yet achieved at better scalability and substantially lower computational costs. finally, we discuss novel applications of {clickrank} in providing enriched user web search experience, highlighting the usefulness of our approach for non-ranking tasks.\"usa\",beyond {dcg}: user behavior as a predictor of a successful search,\"web search engines are traditionally evaluated in terms of the relevance of web pages to individual queries. however, relevance of web pages does not tell the complete picture, since an individual query may represent only a piece of the user\\'s information need and users may have different information needs underlying the same queries. in this work, we address the problem of predicting user search goal success by modeling user behavior. we show empirically that user behavior alone can give an accurate picture of the success of the user\\'s web search goals, without considering the relevance of the documents displayed. in fact, our experiments show that models using user behavior are more predictive of goal success than those using document relevance. we build novel sequence models incorporating time distributions for this task and our experiments show that the sequence and time distribution models are more accurate than static models based on user behavior, or predictions based on document relevance.\"usa\",understanding information retrieval interactions: theoretical and practical implications,\"a fundamental objective of human–computer interaction research is to make systems more usable, more useful, and to provide users with experiences fitting their specific background knowledge and objectives. the challenge in an information-rich world is not only to make information available to people at any time, at any place, and in any form, but specifically to say the  ” right” thing at the  ” right” time in the  ” right” way. designers of collaborative human–computer systems face the formidable task of writing software for millions of users (at design time) while making it work as if it were designed for each individual user (only known at use time). user modeling research has attempted to address these issues. in this article, i will first review the objectives, progress, and unfulfilled hopes that have occurred over the last ten years, and illustrate them with some interesting computational environments and their underlying conceptual frameworks. a special emphasis is given to high-functionality applications and the impact of user modeling to make them more usable, useful, and learnable. finally, an assessment of the current state of the art followed by some future challenges is given.\"\"increasingly, consumers engage in health information seeking via the internet. taking a communication perspective, this review argues why public health professionals should be concerned about the topic, considers potential benefits, synthesizes quality concerns, identifies criteria for evaluating online health information and critiques the literature. more than 70 000 websites disseminate health information; in excess of 50 million people seek health information online, with likely consequences for the health care system. the internet offers widespread access to health information, and the advantages of interactivity, information tailoring and anonymity. however, access is inequitable and use is hindered further by navigational challenges due to numerous design features (e.g. disorganization, technical language and lack of permanence). increasingly, critics question the quality of online health information; limited research indicates that much is inaccurate. meager information-evaluation skills add to consumers\\' vulnerability, and reinforce the need for quality standards and widespread criteria for evaluating health information. extant literature can be characterized as speculative, comprised of basic \\'how to\\' presentations, with little empirical research. future research needs to address the internet as part of the larger health communication system and take advantage of incorporating extant communication concepts. not only should research focus on the \\'net-gap\\' and information quality, it also should address the inherently communicative and transactional quality of internet use. both interpersonal and mass communication concepts open avenues for investigation and understanding the influence of the internet on health beliefs and behaviors, health care, medical outcomes, and the health care system.\"usa\",analyzing and evaluating query reformulation strategies in web search logs,\"users frequently modify a previous search query in hope of retrieving better results. these modifications are called query reformulations or query refinements. existing research has studied how web search engines can propose reformulations, but has given less attention to how people perform query reformulations. in this paper, we aim to better understand how web searchers refine queries and form a theoretical foundation for query reformulation. we study users\\' reformulation strategies in the context of the {aol} query logs. we create a taxonomy of query refinement strategies and build a high precision rule-based classifier to detect each type of reformulation. effectiveness of reformulations is measured using user click behavior. most reformulation strategies result in some benefit to the user. certain strategies like add/remove words, word substitution, acronym expansion, and spelling correction are more likely to cause clicks, especially on higher ranked results. in contrast, users often click the same result as their previous query or select no results when forming acronyms and reordering words. perhaps the most surprising finding is that some reformulations are better suited to helping users when the current results are already fruitful, while other reformulations are more effective when the results are lacking. our findings inform the design of applications that can assist searchers; examples are described in this paper.\",personalized search: integrating collaboration and social networks,\"abstract despite improvements in their capabilities, search engines still fail to provide users with only relevant results. one reason is that most search engines implement a  ” one size fits all” approach that ignores personal preferences when retrieving the results of a user\\'s query. recent studies (smyth, 2010) have elaborated the importance of personalizing search results and have proposed integrating recommender system methods for enhancing results using contextual and extrinsic information that might indicate the user\\'s actual needs. in this article, we review recommender system methods used for personalizing and improving search results and examine the effect of two such methods that are merged for this purpose. one method is based on collaborative users\\' knowledge; the second integrates information from the user\\'s social network. we propose new methods for collaborative-and social-based search and demonstrate that each of these methods, when separately applied, produce more accurate search results than does a purely keyword-based search engine (referred to as  ” standard search engine”), where the social search engine is more accurate than is the collaborative one. however, separately applied, these methods do not produce a sufficient number of results (low coverage). nevertheless, merging these methods with those implemented by standard search engines overcomes the low-coverage problem and produces personalized results for users that display significantly more accurate results while also providing sufficient coverage than do standard search engines. the improvement, however, is significant only for topics for which the diversity of terms used for queries among users is low.\"usa\",learning latent semantic relations from clickthrough data for query suggestion,\"for a given query raised by a specific user, the  query suggestion  technique aims to recommend relevant queries which potentially suit the information needs of that user. due to the complexity of the web structure and the ambiguity of users\\' inputs, most of the suggestion algorithms suffer from the problem of poor recommendation accuracy. in this paper, aiming at providing semantically relevant queries for users, we develop a novel, effective and efficient two-level query suggestion model by mining clickthrough data, in the form of two bipartite graphs (user-query and {query-url} bipartite graphs) extracted from the clickthrough data. based on this, we first propose a joint matrix factorization method which utilizes two bipartite graphs to learn the low-rank query latent feature space, and then build a query similarity graph based on the features. after that, we design an online ranking algorithm to propagate similarities on the query similarity graph, and finally recommend latent semantically relevant queries to users. experimental analysis on the clickthrough data of a commercial search engine shows the effectiveness and the efficiency of our method.\"\"{abstract\\\\&nbsp;\\\\&nbsp;most} web search engines use the content of the web documents and their link structures to assess the relevance of the document to the user\\'s query. with the growth of the information available on the web, it becomes difficult for such web search engines to satisfy the user information need expressed by few keywords. first, personalized information retrieval is a promising way to resolve this problem by modeling the user profile by his general interests and then integrating it in a personalized document ranking model. in this paper, we present a personalized search approach that involves a graph-based representation of the user profile. the user profile refers to the user interest in a specific search session defined as a sequence of related queries. it is built by means of score propagation that allows activating a set of semantically related concepts of reference ontology, namely the {odp}. the user profile is maintained across related search activities using a graph-based merging strategy. for the purpose of detecting related search activities, we define a session boundary recognition mechanism based on the kendall rank correlation measure that tracks changes in the dominant concepts held by the user profile relatively to a new submitted query. personalization is performed by re-ranking the search results of related queries using the user profile. our experimental evaluation is carried out using the {hard} 2003 {trec} collection and showed that our session boundary recognition mechanism based on the kendall measure provides a significant precision comparatively to other non-ranking based measures like the cosine and the {webjaccard} similarity measures. moreover, results proved that the graph-based search personalization is effective for improving the search accuracy.\"usa\",automatic document indexing in large medical collections,,\"collaborative search: who, what, where, when, why, and how\",exploiting session context for information retrieval - a comparative study,\"hard queries are known to benefit from relevance feedback provided by users. it is, however, also known that users are generally reluctant to provide feedback when searching for information. a natural resort not demanding any active user participation is to exploit implicit feedback from the previous user search behavior, i.e., from the context of the current search session. in this work, we present a comparative study on the performance of the three most prominent retrieval models, the vector-space, probabilistic, and language-model based retrieval frameworks, when additional session context is incorporated.\"ny, usa\",contextual factors affecting the utility of surrogates within exploratory search,usa\",do you want to take notes?: identifying research missions in yahoo! search pad,\"addressing user\\'s information needs has been one of the main goals of web search engines since their early days. in some cases, users cannot see their needs immediately answered by search results, simply because these needs are too complex and involve multiple aspects that are not covered by a single web or search results page. this typically happens when users investigate a certain topic in domains such as education, travel or health, which often require collecting facts and information from many pages. we refer to this type of activities as \"\"research missions\"\". these research missions account for 10\\\\% of users\\' sessions and more than 25\\\\% of all query volume, as verified by a manual analysis that was conducted by yahoo! editors. we demonstrate in this paper that such missions can be automatically identified on-the-fly, as the user interacts with the search engine, through careful runtime analysis of query flows and query sessions. the on-the-fly automatic identification of research missions has been implemented in search pad, a novel yahoo! application that was launched in 2009, and that we present in this paper. search pad helps users keeping trace of results they have consulted. its novelty however is that unlike previous notes taking products, it is automatically triggered only when the system decides, with a fair level of confidence, that the user is undertaking a research mission and thus is in the right context for gathering notes. beyond the search pad specific application, we believe that changing the level of granularity of query modeling, from an isolated query to a list of queries pertaining to the same research missions, so as to better reflect a certain type of information needs, can be beneficial in a number of other web search applications. session-awareness is growing and it is likely to play, in the near future, a fundamental role in many on-line tasks: this paper presents a first step on this path.\"\"{abstract\\\\&nbsp;\\\\&nbsp;this} paper investigates the effectiveness of using {mesh}® in {pubmed} through its automatic query expansion process: automatic term mapping ({atm}). we run boolean searches based on a collection of 55 topics and about 160,000 {medline}® citations used in the 2006 and 2007 {trec} genomics tracks. for each topic, we first automatically construct a query by selecting keywords from the question. next, each query is expanded by {atm}, which assigns different search tags to terms in the query. three search tags: [{mesh} terms], [text words], and [all fields] are chosen to be studied after expansion because they all make use of the {mesh} field of indexed {medline} citations. furthermore, we characterize the two different mechanisms by which the {mesh} field is used. retrieval results using {mesh} after expansion are compared to those solely based on the words in {medline} title and abstracts. the aggregate retrieval performance is assessed using both f-measure and mean rank precision. experimental results suggest that query expansion using {mesh} in {pubmed} can generally improve retrieval performance, but the improvement may not affect end {pubmed} users in realistic situations.\"usa\",interest-based personalized search,\"web search engines typically provide search results without considering user interests or context. we propose a personalized search approach that can easily extend a conventional search engine on the client side. our mapping framework automatically maps a set of known user interests onto a group of categories in the open directory project ({odp}) and takes advantage of manually edited data available in {odp} for training text classifiers that correspond to, and therefore categorize and personalize search results according to user interests. in two sets of controlled experiments, we compare our personalized categorization system ({pcat}) with a list interface system ({list}) that mimics a typical search engine and with a nonpersonalized categorization system ({cat}). in both experiments, we analyze system performances on the basis of the type of task and query length. we find that {pcat} is preferable to {list} for information gathering types of tasks and for searches with short queries, and {pcat} outperforms {cat} in both information gathering and finding types of tasks, and for searches associated with free-form queries. from the subjects\\' answers to a questionnaire, we find that {pcat} is perceived as a system that can find relevant web pages quicker and easier than {list} and {cat}.\"usa\",the indexable web is more than 11.5 billion pages,\"in this short paper we estimate the size of the public indexable web at 11.5 billion pages. we also estimate the overlap and the index size of google, {msn}, {ask/teoma} and yahoo!\"college park, md 20742, usa.\",evaluation of {pico} as a knowledge representation for clinical questions.,\"the paradigm of evidence-based medicine ({ebm}) recommends that physicians formulate clinical questions in terms of the problem/population, intervention, comparison, and outcome. together, these elements comprise a {pico} frame. although this framework was developed to facilitate the formulation of clinical queries, the ability of {pico} structures to represent physicians\\' information needs has not been empirically investigated. this paper evaluates the adequacy and suitability of {pico} frames as a knowledge representation by analyzing 59 real-world primary-care clinical questions. we discovered that only two questions in our corpus contain all four {pico} elements, and that 37\\\\% of questions contain both intervention and outcome. our study reveals prevalent structural patterns for the four types of clinical questions: therapy, diagnosis, prognosis, and etiology. we found that the {pico} framework is primarily centered on therapy questions, and is less suitable for representing other types of clinical information needs. challenges in mapping natural language questions into {pico} structures are also discussed. although we point out limitations of the {pico} framework, our work as a whole reaffirms its value as a tool to assist physicians practicing {ebm}.\"modelling both the context and the user,\"research into context-aware computing risks losing sight of the user. this paper discusses how different types of information about a user, ranging from information about the current context to information about the user\\\\&rsquo;s long-term properties, can simultaneously be relevant to a given adaptation decision. pointers are given to two areas of research that can help with the integration of a broader range of information into context-aware systems: research on user-adaptive systems and on decision-theoretic methods.\"usa\",investigating the roles of knowledge and cognitive abilities in older adult information seeking on the web,\"this study investigated the influences of knowledge, particularly internet, web browser, and search engine knowledge, as well as cognitive abilities on older adult information seeking on the internet. the emphasis on aspects of cognition was informed by a modeling framework of search engine information-seeking behavior. participants from two older age groups were recruited: twenty people in a younger-old group (ages 60-70) and twenty people in an older-old group (ages 71-85). ten younger adults (ages 18-39) served as a comparison group. all participants had at least some internet search experience. the experimental task consisted of six realistic search problems, all involving information related to health and well-being and which varied in degree of complexity. the results indicated that though necessary, internet-related knowledge was not sufficient in explaining information-seeking performance, and suggested that a combination of both knowledge and key cognitive abilities is important for successful information seeking. in addition, the cognitive abilities that were found to be critical for task performance depended on the search problem\\'s complexity. also, significant differences in task performance between the younger and the two older age groups were found on complex, but not on simple problems. overall, the results from this study have implications for instructing older adults on internet information seeking and for the design of web sites.\"usa\",relevance judgements within the context of work tasks,this paper describes the empirical testing of part of a model of relevance manifestations. the research described here is part of a larger study: this paper specifically looks at the types of relevance judgements (manifestations of relevance) that are made by users executing works tasks in different contexts. the relevance judgements of users engaged in three different types of work task (different contexts) were captured through the use of questionnaires at the end of the work task. the different work tasks were chosen to represent different contexts of information use in order to establish whether the context of the information need and work task has an influence on the way that information sources are evaluated and used. it was found that the context of work task performed has a statistically significant influence on the type of relevance judgement that is made.classification models for the prediction of clinicians\\' information needs.,the results suggest that classification models based on infobutton usage data are a promising method for the prediction of content topics that a clinician would choose to answer patient care questions while using an {emr} system.usa\",inferring query intent from reformulations and clicks,many researchers have noted that web search queries are often ambiguous or unclear. we present an approach for identifying the popular meanings of queries using web search logs and user click behavior. we show our approach to produce more complete and user-centric intents than expert judges by evaluating on {trec} queries. this approach was also used by the {trec} 2009 web track judges to obtain more representative topic descriptions from real queries.ny, usa\",a comprehensive and systematic model of user evaluation of web search engines: i. theory and background,\"the project proposes and tests a comprehensive and systematic model of user evaluation of web search engines. the project contains two parts. part i describes the background and the model including a set of criteria and measures, and a method for implementation. it includes a literature review for two periods. the early period (1995-1996) portrays the settings for developing the model and the later period (1997-2000) places two applications of the model among contemporary evaluation work. part {ii} presents one of the applications that investigated the evaluation of four major search engines by 36 undergraduates from three academic disciplines. it reports results from statistical analyses of quantitative data for the entire sample and among disciplines, and content analysis of verbal data containing users\\' reasons for satisfaction. the proposed model aims to provide systematic feedback to engine developers or service providers for system improvement and to generate useful insight for system design and tool choice. the model can be applied to evaluating other compatible information retrieval systems or information retrieval ({ir}) techniques. it intends to contribute to developing a theory of relevance that goes beyond topicality to include value and usefulness for designing user-oriented information retrieval systems.\"usa\",context-aware information retrieval on a ubiquitous medical learning environment,sydney 2052, australia. f.magrabi@unsw.edu.au\",what factors are associated with the integration of evidence retrieval technology into routine general practice settings?,\"{background}: information retrieval systems have the potential to improve patient care but little is known about the variables which influence clinicians\\' uptake and use of systems in routine work. {aim}: to determine which factors influenced use of an online evidence retrieval system. {design} {of} {study}: computer logs and pre- and post-system survey analysis of a 4-week clinical trial of the quick clinical online evidence system involving 227 general practitioners across australia. {results}: online evidence use was not linked to general practice training or clinical experience but female clinicians conducted more searches than their male counterparts (mean use=14.38 searches, {s.d}.=11.68 versus mean use=8.50 searches, {s.d}.=9.99; t=2.67, d.f.=157, p=0.008). practice characteristics such as hours worked, type and geographic location of clinic were not associated with search activity. information seeking was also not related to participants\\' perceived information needs, computer skills, training nor internet connection speed. clinicians who reported direct improvements in patient care as a result of system use had significantly higher rates of system use than other users (mean use=12.55 searches, {s.d}.=13.18 versus mean use=8.15 searches, {s.d}.=9.18; t=2.322, d.f.=154 p=0.022). comparison of participants\\' views pre- and post- the trial, showed that post-trial clinicians expressed more positive views about searching for information during a consultation (chi(2)=27.40, d.f.=4, p< or =0.001) and a significantly greater number reported seeking information between consultations as a result of having access to an online evidence system in their consulting rooms (chi(2)=9.818, d.f.=2, p=0.010). {conclusion}: clinicians\\' use of an online evidence system was directly related to their reported experiences of improvements in patient care. post-trial clinicians positively changed their views about having time to search for information and pursued more questions during clinic hours.\"ny, usa\",polyrepresentation of information needs and semantic entities: elements of a cognitive theory for information retrieval interaction,note: {ocr} errors may be found in this reference list extracted from the full text article.  {acm} has opted to expose the complete list rather than only correct and linked references.\"the subject of context has received a great deal of attention in the information retrieval ({ir}) literature over the past decade, primarily in studies of information seeking and {ir} interactions. recently, attention to context in {ir} has expanded to address new problems in new environments. in this paper we outline five overlapping dimensions of context which we believe to be important constituent elements and we discuss how they are related to different issues in {ir} research. the papers in this special issue are summarized with respect to how they represent work that is being conducted within these dimensions of context. we conclude with future areas of research which are needed in order to fully understand the multidimensional nature of context in {ir}.\"usa\",personalizing information retrieval for multi-session tasks: the roles of task stage and task type,\"dwell time as a user behavior has been found in previous studies to be an unreliable predictor of document usefulness, with contextual factors such as the user\\'s task needing to be considered in its interpretation. task stage has been shown to influence search behaviors including usefulness judgments, as has task type. this paper reports on an investigation of how task stage and task type may help predict usefulness from the time that users spend on retrieved documents, over the course of several information seeking episodes. a 3-stage controlled experiment was conducted with 24 participants, each coming 3 times to work on 3 sub-tasks of a general task, couched either as \"\"parallel\"\" or \"\"dependent\"\" task type. the full task was to write a report on the general topic, with interim documents produced for each sub-task. results show that task stage can help in inferring document usefulness from decision time, especially in the parallel task. the findings can be used to increase accuracy in predicting document usefulness and accordingly in personalizing search for multi-session tasks.\"ny, usa\",user-defined relevance criteria: an exploratory study,118 college drive \\\\#5146, hattiesburg, ms 39406.\",exploring the relationships between work task and search task in information search,\"to provide a basis for making predictions of the characteristics of search task ({st}), based on work task ({wt}), and to explore the nature of {wt} and {st}, this study examines the relationships between {wt} and {st} (inter-relationships) and the relationships between the different facets of both {wt} and {st} (intra-relationships), respectively. a faceted classification of task was used to conceptualize work task and search task. twenty-four pairs of work tasks and their associated search tasks were collected, by semistructured interviews, and classified based on the  classification. the results indicate that work task shapes different facets or sub-facets of its associated search tasks to different degrees. several sub-facets of search task, such as time (length), objective task  complexity, and subjective task complexity, are most strongly affected by work task. the results demonstrate that it is necessary to consider difficulty and complexity as different constructs when investigating their influence on information search behavior. the exploration of intra-relationships illustrates the difference of work task and search task in their nature. the findings provide empirical evidence to support the view that work task and search task are multi-faceted variables and their different effects on users\\' information search behavior should be examined.\"ny, usa\",web search behavior of internet experts and newbies,\"searching for relevant information on the world wide web is often a laborious and frustrating task for casual and experienced users. to help improve searching on the web based on a better understanding of user characteristics, we investigate what types of knowledge are relevant for web-based information seeking, and which knowledge structures and strategies are involved. two experimental studies are presented, which address these questions from different angles and with different methodologies. in the first experiment, 12 established internet experts are first interviewed about search strategies and then perform a series of realistic search tasks on the world wide web. from this study a model of information seeking on the world wide web is derived and then tested in a second study. in the second experiment two types of potentially relevant types of knowledge are compared directly. effects of web experience and domain-specific background knowledge are investigated with a series of search tasks in an economics-related domain (introduction of the euro currency). we find differential and combined effects of both web experience and domain knowledge: while successful search performance requires the combination of the two types of expertise, specific strategies directly related to web experience or domain knowledge can be identified.\"doctors, and the internet.\",{10.1056/nejmp0911938}usa\",how does search behavior change as search becomes more difficult?,\"search engines make it easy to check facts online, but finding some specific kinds of information sometimes proves to be difficult. we studied the behavioral signals that suggest that a user is having trouble in a search task. first, we ran a lab study with 23 users to gain a preliminary understanding on how users\\' behavior changes when they struggle finding the information they\\'re looking for. the observations were then tested with 179 participants who all completed an average of 22.3 tasks from a pool of 100 tasks. the large-scale study provided quantitative support for our qualitative observations from the lab study. when having difficulty in finding information, users start to formulate more diverse queries, they use advanced operators more, and they spend a longer time on the search result page as compared to the successful tasks. the results complement the existing body of research focusing on successful search strategies.\"usa\",concept-based biomedical text retrieval,\"one challenging problem for biomedical text retrieval is to find accurate synonyms or name variants for biomedical entities. in this paper, we propose a new concept-based approach to tackle this problem. in this approach, a set of concepts instead of keywords will be extracted from a query first. then these concepts will be used for retrieval purpose. the experiment results show that the proposed approach can boost the retrieval performance and it generates very good results on 2005 {trec} genomics data sets.\"usa\",ranking refinement and its application to information retrieval,usa\",optimizing web search using social annotations,\"this paper explores the use of social annotations to improve websearch. nowadays, many services, e.g. del.icio.us, have been developed for web users to organize and share their favorite webpages on line by using social annotations. we observe that the social annotations can benefit web search in two aspects: 1) the annotations are usually good summaries of corresponding webpages; 2) the count of annotations indicates the popularity of webpages. two novel algorithms are proposed to incorporate the above information into page ranking: 1) {socialsimrank} ({ssr})calculates the similarity between social annotations and webqueries; 2) {socialpagerank} ({spr}) captures the popularity of webpages. preliminary experimental results show that {ssr} can find the latent semantic association between queries and annotations, while {spr} successfully measures the quality (popularity) of a webpage from the web users\\' perspective. we further evaluate the proposed methods empirically with 50 manually constructed queries and 3000 auto-generated queries on a dataset crawledfrom delicious. experiments show that both {ssr} and {sprbenefit} web search significantly.\"usa\",a cluster-based resampling method for pseudo-relevance feedback,\"typical pseudo-relevance feedback methods assume the top-retrieved documents are relevant and use these pseudo-relevant documents to expand terms. the initial retrieval set can, however, contain a great deal of noise. in this paper, we present a cluster-based resampling method to select better pseudo-relevant documents based on the relevance model. the main idea is to use document clusters to find dominant documents for the initial retrieval set, and to repeatedly feed the documents to emphasize the core topics of a query. experimental results on large-scale web {trec} collections show significant improvements over the relevance model. for justification of the resampling approach, we examine relevance density of feedback documents. a higher relevance density will result in greater retrieval accuracy, ultimately approaching true relevance feedback. the resampling approach shows higher relevance density than the baseline relevance model on all collections, resulting in better retrieval accuracy in pseudo-relevance feedback. this result indicates that the proposed method is effective for pseudo-relevance feedback.\"2008-09-16 15:24:33,,a context modeling survey,\"context-awareness is one of the drivers of the ubiquitous computing paradigm, whereas a well designed model is a key accessor to the context in any context-aware system. this paper provides a survey of the the most relevant current approaches to modeling context for ubiquitous computing. numerous approaches are reviewed, classified relative to their core elements and evaluated with respect to their appropriateness for ubiquitous computing. 1.\"usa\",improving the effectiveness of information retrieval with local context analysis,\"techniques for automatic query expansion have been extensively studied in information research as a means of addressing the word mismatch between queries and documents. these techniques can be categorized as either global or local. while global techniques rely on analysis of a whole collection to discover word relationships, local techniques emphasize analysis of the top-ranked documents retrieved for a query. while local techniques have shown to be more effective that global techniques in general, existing local techniques are not robust and can seriously hurt retrieved when few of the retrieval documents are relevant. we propose a new technique, called local context analysis, which selects expansion terms based on cooccurrence with the query terms within the  top-ranked documents. experiments on a number of collections, both english and {non-english}, show that local context analysis offers more effective and consistent retrieval results.\"h\\\\\"\"{a}rtelstrasse 16-18, 04107 leipzig, germany.\",the role of foundational relations in the alignment of biomedical ontologies.,\"as medical information becomes increasingly available and individuals take a more active role in managing their personal health, it is essential for scholars to better understand the general public\\'s information-seeking behavior. the study reported here explores the use of the world wide web to seek health information in a contemporary information-media environment. drawing from uses and gratifications theory and the comprehensive model of health information seeking, perceptions of traditional information sources (e.g., mass media, one\\'s health care provider, etc.) are posited to predict use of the web to seek health information and perceptions of information acquired from searches. data from the health information national trends survey ({hints}; <{i>n}</i>\\\\&nbsp;=\\\\&nbsp;3982) were analyzed to test study hypotheses. trust in information-oriented media, entertainment-oriented media, and one\\'s health care provider all predicted web use behavior and perceptions. the implications of the findings for research on information seeking and the role of the web in patient empowerment are discussed.\"objective to determine the significance of the english wikipedia as a source of online health information.usa\",evaluating implicit measures to improve web search,\"of growing interest in the area of improving the search experience is the collection of implicit user behavior measures (implicit measures) as indications of user interest and user satisfaction. rather than having to submit explicit user feedback, which can be costly in time and resources and alter the pattern of use within the search experience, some research has explored the collection of implicit measures as an efficient and useful alternative to collecting explicit measure of interest from {users.this} research article describes a recent study with two main objectives. the first was to test whether there is an association between explicit ratings of user satisfaction and implicit measures of user interest. the second was to understand what implicit measures were most strongly associated with user satisfaction. the domain of interest was web search. we developed an instrumented browser to collect a variety of measures of user activity and also to ask for explicit judgments of the relevance of individual pages visited and entire search sessions. the data was collected in a workplace setting to improve the generalizability of the {results.results} were analyzed using traditional methods (e.g., bayesian modeling and decision trees) as well as a new usage behavior pattern analysis ( ” gene analysis”). we found that there was an association between implicit measures of user activity and the user\\'s explicit satisfaction ratings. the best models for individual pages combined clickthrough, time spent on the search result page, and how a user exited a result or ended a search session (exit type/end action). behavioral patterns (through the gene analysis) can also be used to predict user satisfaction for search sessions.\"\"\"\"class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. all the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. slides and additional exercises (with solutions for lecturers) are also available through the book\\'s supporting website to help course instructors prepare their lectures.\"\" -- publisher\\'s description.\"usa\",machine learning in automated text categorization,\"the automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. in the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. the advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. this survey discusses the main approaches to text categorization that fall within the machine learning paradigm. we will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.\"usa\",\"time, location and interest: an empirical and user-centred study\",\"the importance of context in meeting user information needs has gained increasing interest. when developing interactive information retrieval systems, we do need to consider how contextual information might be used to improve information retrieval. in this paper, we present a user-centred experiment that focuses on three potential context attributes. these are time, location, and user\\'s interest. the experiment involved tasks using a scenario that would be suitable for mobile situations - one very promising area for the application of context information that can help to deliver personalised services. the scenario involves situations with local events such as jazz concerts and includes the use of a simplified map to help visualise locations. the effect of the three attributes and the interactions between them are analysed and discussed. the effects in most cases were considerable and data analysis showed statistically significant effects. the study shows that time, location, and interest matter to users in mobile situations. there appears to be a priority emerging in the relative importance of these attributes for the mobile user. also, the results show high order interaction effects between the attributes\"university of new south wales, kensington, 2052, australia. j.westbrook@unsw.edu.au\",do online information retrieval systems help experienced clinicians answer clinical questions?,md, usa. blh@nlm.nih.gov\",the unified medical language system: an informatics research collaboration.,usa\",exploratory information search by domain experts and novices,\"the arising popularity of social tagging system has the potential to transform traditional web search into a new era of social search. based on the finding that domain expertise could influence search behavior in traditional search engines, we hypothesized and tested the idea that domain expertise would have similar influence on search behavior in a social tagging system. we conducted an experiment comparing search behavior of experts and novices when they searched using a tradition search engine and a social tagging system. results from our experiment showed that experts relied more on their own domain knowledge to generate search queries, while novices were influenced more by social cues in the social tagging system. experts were also found to conform to each other more than novices in their choice of bookmarks and tags. implications on the design of future social information systems are discussed.\"bethesda, md 20894, usa.\",semi-automatic indexing of full text biomedical articles.,weston area health trust, weston-super-mare, bs23 4tq, uk\",internet-based information-seeking behaviour amongst doctors and nurses: a short review of the literature,\"background:\\u2002 reviews of how doctors and nurses search for online information are relatively rare, particularly where research examines how they decide whether to use internet-based resources. original research into their online searching behaviour is also rare, particularly in real world clinical settings. as is original research into their online searching behaviour. this review collates some of the existing evidence, from 1995 to 2009. objectives:\\u2002 to establish whether there are any significant differences in the ways and reasons why doctors and nurses seek out online information; to establish how nurses and doctors locate information online; to establish whether any conclusions can be drawn from the existing evidence that might assist health and medical libraries in supporting users. methods:\\u2002 an initial scoping literature search was carried out on {pubmed} and {cinahl} to identify existing reviews of the subject area and relevant original research between 1995 and 2009. following refinement, further searches were carried out on embase (ovid), {lisa} and {lista}. following the initial scoping search, two journals were identified as particularly relevant for further table of contents searching. articles were exclused where the main focus was on patients searching for information or where the focus was the evaluation of online-based educational software or tutorials. articles were included if they were review or meta-analysis articles, where they reported original research, and where the primary focus of the online search was for participants\\' ongoing continuing professional development ({cpd}). the relevant articles are outlined, with details of numbers of participants, response rates, and the user groups. results:\\u2002 there appear to be no significant differences between the reasons why doctors and nurses seek online internet-based evidence, or the ways in which they locate that evidence. reasons for searching for information online are broadly the same: primarily patient care and {cpd} (continuing professional development). the perceived barriers to accessing online information are the same in both groups. there is a lack of awareness of the library as a potential online information enabler. conclusions:\\u2002 libraries need to examine their policy and practice to ensure that they facilitate access to online evidence-based information, particularly where users are geographically remote or based in the community rather than in a hospital setting. librarians also need to take into account the fact that medical professionals on duty may not be able to take advantage of the academic model of online information research. further research is recommended into the difference between the idealised academic model of searching and real world practicalities; and how other user groups search, for example patients.\"pittsburgh, pa, usa.\",internet usage by low-literacy adults seeking health information: an observational analysis.,usa\",context-sensitive ranking,\"contextual preferences take the form that item i1 is preferred to item  i 2  in the context of  x . for example, a preference might state the choice for nicole kidman over penelope cruz in drama movies, whereas another preference might choose penelope cruz over nicole kidman in the context of spanish dramas. various sources provide preferences independently and thus preferences may contain cycles and contradictions. we reconcile democratically the preferences accumulated from various sources and use them to create a priori orderings of tuples in an off-line preprocessing step. only a few representative orders are saved, each corre-sponding to a set of contexts. these orders and associated contexts are used at query time to expeditiously provide ranked answers. we formally define contextual preferences, provide algorithms for creating orders and processing queries, and present experimental results that show their efficacy and practical utility.\"\"the classical probability ranking principle ({prp}) forms the theoretical basis for probabilistic information retrieval ({ir}) models, which are dominating {ir} theory since about 20\\xa0years. however, the assumptions underlying the {prp} often do not hold, and its view is too narrow for interactive information retrieval ({iir}). in this article, a new theoretical framework for interactive retrieval is proposed: the basic idea is that during {iir}, a user moves between situations. in each situation, the system presents to the user a list of choices, about which s/he has to decide, and the first positive decision moves the user to a new situation. each choice is associated with a number of cost and probability parameters. based on these parameters, an optimum ordering of the choices can the derived—the {prp} for {iir}. the relationship of this rule to the classical {prp} is described, and issues of further research are pointed out.\"ny, usa\",a review of web searching studies and a framework for future research,usa\",the demographics of web search,\"how does the web search behavior of \"\"rich\"\" and \"\"poor\"\" people differ? do men and women tend to click on difffferent results for the same query? what are some queries almost exclusively issued by african americans? these are some of the questions we address in this study. our research combines three data sources: the query log of a major {us}-based web search engine, profile information provided by 28 million of its users (birth year, gender and {zip} code), and {us}-census information including detailed demographic information aggregated at the level of {zip} code. through this combination we can annotate each query with, e.g. the average per-capita income in the {zip} code it originated from. though conceptually simple, this combination immediately creates a powerful user modeling tool. the main contributions of this work are the following. first, we provide a demographic description of a large sample of search engine users in the {us} and show that it agrees well with the distribution of the {us} population. second, we describe how different segments of the population differ in their search behavior, e.g. with respect to the queries they formulate or the {urls} they click. third, we explore applications of our methodology to improve web search relevance and to provide better query suggestions. these results enable a wide range of applications including improving web search and advertising where, for instance, targeted advertisements for \"\"family vacations\"\" could be adapted to the (expected) income.\"\"this paper provides a narrative review of the available literature from the past 10\\xa0years (1996–2006) that focus on the information seeking behaviour of doctors. the review considers the literature in three sub-themes: theme\\xa01, the information needs of doctors includes information need, frequency of doctors\\' questions and types of information needs; theme\\xa02, information seeking by doctors embraces pattern of information resource use, time spent searching, barriers to information searching and information searching skills; theme\\xa03, information sources utilized by doctors comprises the number of sources utilized, comparison of information sources consulted, computer usage, ranking of information resources, printed resource use, personal digital assistant ({pda}) use, electronic database use and the internet. the review is wide ranging. it would seem that the traditional methods of face-to-face communication and use of hard-copy evidence still prevail amongst qualified medical staff in the clinical setting. the use of new technologies embracing the new digital age in information provision may influence this in the future. however, for now, it would seem that there is still research to be undertaken to uncover the most effective methods of encouraging clinicians to use the best evidence in everyday practice.\"universidad nacional del sur,bah\\\\\\'{\\\\i}a blanca, argentina; laboratorio de investigaci\\\\\\'{o}n y desarrollo en inteligencia artificial (lidia),departamento de ciencias e ingenier\\\\\\'{\\\\i}a de la computaci\\\\\\'{o}n, universidad nacional del sur,bah\\\\\\'{\\\\i}a blanca, argentina; laboratorio de investigaci\\\\\\'{o}n y desarrollo en computaci\\\\\\'{o}n cient\\\\\\'{\\\\i}fica (lidecc),planta piloto de ingenier\\\\\\'{\\\\i}a qu\\\\\\'{\\\\i}mica, cct-conicet-bah\\\\\\'{\\\\i}a blanca, argentina\",multiobjective evolutionary algorithms for context-based search,\"formulating high-quality queries is a key aspect of context-based search. however, determining the effectiveness of a query is challenging because multiple objectives, such as high precision and high recall, are usually involved. in this work, we study techniques that can be applied to evolve contextualized queries when the criteria for determining query quality are based on multiple objectives. we report on the results of three different strategies for evolving queries: (a) single-objective, (b) multiobjective with pareto-based ranking, and (c) multiobjective with aggregative ranking. after a comprehensive evaluation with a large set of topics, we discuss the limitations of the single-objective approach and observe that both the pareto-based and aggregative strategies are highly effective for evolving topical queries. in particular, our experiments lead us to conclude that the multiobjective techniques are superior to a baseline as well as to well-known and ad hoc query reformulation techniques.\"in 46202, usa. cmcdonald@regenstrief.org\",\"{loinc}, a universal standard for identifying laboratory observations: a 5-year update.\",\"the logical observation identifier names and codes ({loinc}) database provides a universal code system for reporting laboratory and other clinical observations. its purpose is to identify observations in electronic messages such as health level seven ({hl7}) observation messages, so that when hospitals, health maintenance organizations, pharmaceutical manufacturers, researchers, and public health departments receive such messages from multiple sources, they can automatically file the results in the right slots of their medical records, research, and/or public health systems. for each observation, the database includes a code (of which 25 000 are laboratory test observations), a long formal name, a \"\"short\"\" 30-character name, and synonyms. the database comes with a mapping program called regenstrief {loinc} mapping assistant ({relma}({tm})) to assist the mapping of local test codes to {loinc} codes and to facilitate browsing of the {loinc} results. both {loinc} and {relma} are available at no cost from http://www.regenstrief.org/loinc/. the {loinc} medical database carries records for >30 000 different observations. {loinc} codes are being used by large reference laboratories and federal agencies, e.g., the {cdc} and the department of veterans affairs, and are part of the health insurance portability and accountability act ({hipaa}) attachment proposal. internationally, they have been adopted in switzerland, hong kong, australia, and canada, and by the german national standards organization, the deutsches instituts f\\\\\"\"{u}r normung. laboratories should include {loinc} codes in their outbound {hl7} messages so that clinical and research clients can easily integrate these results into their clinical and research repositories. laboratories should also encourage instrument vendors to deliver {loinc} codes in their instrument outputs and demand {loinc} codes in {hl7} messages they get from reference laboratories to avoid the need to lump so many referral tests under the \"\"send out lab\"\" code.\"harvard medical school, boston, ma, usa. qzeng@dsg.bwh.harvard.edu\",characteristics of consumer terminology for health information retrieval.,\"{objectives}: as millions of consumers perform health information retrieval online, the mismatch between their terminology and the terminologies of the information sources could become a major barrier to successful retrievals. to address this problem, we studied the characteristics of consumer terminology for health information retrieval. {methods}: our study focused on consumer queries that were used on a consumer health service web site and a consumer health information web site. we analyzed data from the site-usage logs and conducted interviews with patients. {results}: our findings show that consumers\\' information retrieval performance is very poor. there are significant mismatches at all levels (lexical, semantic and mental models) between the consumer terminology and both the information source terminology and standard medical vocabularies. {conclusions}: comprehensive terminology support on all levels is needed for consumer health information retrieval.\"ca, usa\",evaluating the effectiveness of personalized web search,\"although personalized search has been under way for many years and many personalization algorithms have been investigated, it is still unclear whether personalization is consistently effective on different queries for different users and under different search contexts. in this paper, we study this problem and provide some findings. we present a large-scale evaluation framework for personalized search based on query logs and then evaluate five personalized search algorithms (including two click-based ones and three topical-interest-based ones) using 12-day query logs of windows live search. by analyzing the results, we reveal that personalized web search does not work equally well under various situations. it represents a significant improvement over generic web search for some queries, while it has little effect and even harms query performance under some situations. we propose click entropy as a simple measurement on whether a query should be personalized. we further propose several features to automatically predict when a query will benefit from a specific personalization algorithm. experimental results show that using a personalization algorithm for queries selected by our prediction model is better than using it simply for all queries.\"usa\",cumulated gain-based evaluation of {ir} techniques,\"modern large retrieval environments tend to overwhelm their users by their large output. since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation. in order to develop {ir} techniques in this direction, it is necessary to develop evaluation approaches and methods that credit {ir} methods for their ability to retrieve highly relevant documents. this can be done by extending traditional evaluation methods, that is, recall and precision based on binary relevance judgments, to graded relevance judgments. alternatively, novel measures based on graded relevance judgments may be developed. this article proposes several novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. the first one accumulates the relevance scores of retrieved documents along the ranked result list. the second one is similar but applies a discount factor to the relevance scores in order to devaluate late-retrieved documents. the third one computes the relative-to-the-ideal performance of {ir} techniques, based on the cumulative gain they are able to yield. these novel measures are defined and discussed and their use is demonstrated in a case study using {trec} data: sample system run results for 20 queries in {trec}-7. as a relevance base we used novel graded relevance judgments on a four-point scale. the test results indicate that the proposed measures credit {ir} methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. the graphs based on the measures also provide insight into the performance {ir} techniques and allow interpretation, for example, from the user point of view.\"usa\",context-aware ranking in web search,\"the context of a search query often provides a search engine meaningful hints for answering the current query better. previous studies on context-aware search were either focused on the development of context models or limited to a relatively small scale investigation under a controlled laboratory setting. particularly, about context-aware ranking for web search, the following two critical problems are largely remained unsolved. first, how can we take advantage of different types of contexts in ranking? second, how can we integrate context information into a ranking model? in this paper, we tackle the above two essential problems analytically and empirically. we develop different ranking principles for different types of contexts. moreover, we adopt a learning-to-rank approach and integrate the ranking principles into a state-of-the-art ranking model by encoding the context information as features of the model. we empirically test our approach using a large search log data set obtained from a major commercial search engine. our evaluation uses both human judgments and implicit user click data. the experimental results clearly show that our context-aware ranking approach improves the ranking of a commercial search engine which ignores context information. furthermore, our method outperforms a baseline method which considers context information in ranking.\"\"past research has identified many different types of relevance in information retrieval ({ir}). so far, however, most evaluation of {ir} systems has been through batch experiments conducted with test collections containing only expert, topical relevance judgements. recently, there has been some movement away from this traditional approach towards interactive, more user-centred methods of evaluation. however, these are expensive for evaluators in terms both of time and of resources. this paper describes a new evaluation methodology, using a task-oriented test collection, which combines the advantages of traditional non-interactive testing with a more user-centred emphasis. the main features of a task-oriented test collection are the adoption of the task, rather than the query, as the primary unit of evaluation and the naturalistic character of the relevance judgements.\"usa\",\"how people recall, recognize, and reuse search results\",\"when a person issues a query, that person has expectations about the search results that will be returned. these expectations can be based on the current information need, but are also influenced by how the searcher believes the search engine works, where relevant results are expected to be ranked, and any previous searches the individual has run on the topic. this paper looks in depth at how the expectations people develop about search result lists during an initial query affect their perceptions of and interactions with future repeat search result lists. three studies are presented that give insight into how people recall, recognize, and reuse results. the first study (a study of  recall ) explores what people recall about previously viewed search result lists. the second study (a study of  recognition ) builds on the first to reveal that people often recognize a result list as one they have seen before even when it is quite different. as long as those aspects that the searcher remembers about the initial list remain the same, other aspects can change significantly. this is advantageous because, as the third study (a study of  reuse ) shows, when a result list appears to have changed, people have trouble re-using the previously viewed content in the list. they are less likely to find what they are looking for, less happy with the result quality, more likely to find the task hard, and more likely to take a long time searching. although apparent consistency is important for reuse, people\\'s inability to recognize change makes consistency without stagnation possible. new relevant results can be presented where old results have been forgotten, making both old and new content easy to find.\"irvine, usa. pratt@ics.uci.edu\",{querycat}: automatic categorization of {medline} queries.,\"a searcher\\'s inability to formulate an appropriate query can result in an overwhelming number of retrieved documents. our approach to this problem is to use information about common types or categories of queries to (1) reformulate the user\\'s initial query and (2) create an informative organization of the retrieved documents from the reformulated query. to achieve these goals, we first must identify which common categories or types of queries are the best abstraction of the user\\'s specific query. in this paper, we describe a system that performs this first step of categorizing the user\\'s query. our system uses a two-phased approach: a lexical analysis phase, and a semantic analysis phase. an evaluation of our system demonstrates that its query categorization corresponds reasonably well to the query categorizations by medical librarians and physicians.\"france.\",a method of cross-lingual consumer health information retrieval.,\"{objectives}: this paper presents a method of cross-language information retrieval aiming to make medical information available to patients in french and english, regardless of the query language they wish to use. {methods}: we describe the two {mesh}-related terminologies used in this work. we show that the french patient synonyms included in {cismef} can be automatically mapped to the english consumer-oriented health topics used in {medlineplus}, via the {mesh} thesaurus. the links between french and english patient terms thus inferred can subsequently be exploited to automatically translate patient queries. {results}: 129 {medlineplus} topics have been mapped to 142 {cismef} patient synonyms. contextual links for cross-language retrieval have been added to the patient dedicated french information gateway {cismef}. conclusion: we have presented an efficient method for cross-lingual patient information retrieval in french and english, which may also be applied to other language pairs, subject to the availability of patient terminologies and of the {mesh} thesaurus in these languages.\"\"this paper describes ongoing study that examines problems with existing patient health information sources and investigates an approach for linking (i.e.   integrating) data from a patient\\'s medical record(s) with relevant health information on the web. the aim is to provide patients with simplified,  customized and controlled access to web information. data from patient medical records are extracted and linked with relevant health information on the web through a web search service. these are made available to patients through a web   portal that we refer to as the patient knowledge base ({patientkb}). our integration approach utilizes term semantics (i.e. meaning) to enrich the web search and simplify medical terms for patients. in the current implementation,  patients have guided, secure and relatively customized access to basic and relevant web information on their diagnoses. future implementation will attempt to achieve further customization, extensibility and safety features. this paper   investigates how ideas presented in an earlier study can be implemented. 10.1177/1460458206061202\"usa\",evaluation by comparing result sets in context,\"familiar evaluation methodologies for information retrieval ({ir}) are not well suited to the task of comparing systems in many real settings. these systems and evaluation methods must support contextual, interactive retrieval over changing, heterogeneous data collections, including private and confidential {information.we} have implemented a comparison tool which can be inserted into the natural {ir} process. it provides a familiar search interface, presents a small number of result sets in side-by-side panels, elicits searcher judgments, and logs interaction events. the tool permits study of real information needs as they occur, uses the documents actually available at the time of the search, and records judgments taking into account the instantaneous needs of the {searcher.we} have validated our proposed evaluation approach and explored potential biases by comparing different {whole-of-web} search facilities using a web-based version of the tool. in four experiments, one with supplied queries in the laboratory and three with real queries in the workplace, subjects showed no discernable left-right bias and were able to reliably distinguish between high- and low-quality result sets. we found that judgments were strongly predicted by simple implicit {measures.following} validation we undertook a case study comparing two leading {whole-of-web} search engines. the approach is now being used in several ongoing investigations.\"usa\",adaptive relevance feedback in information retrieval,\"relevance feedback has proven very effective for improving retrieval accuracy. a difficult yet important problem in all relevance feedback methods is how to optimally balance the original query and feedback information. in the current feedback methods, the balance parameter is usually set to a fixed value across all the queries and collections. however, due to the difference in queries and feedback documents, this balance parameter should be optimized for each query and each set of feedback documents. in this paper, we present a learning approach to adaptively predict the optimal balance coefficient for each query and each collection. we propose three heuristics to characterize the balance between query and feedback information. taking these three heuristics as a road map, we explore a number of features and combine them using a regression approach to predict the balance coefficient. our experiments show that the proposed adaptive relevance feedback is more robust and effective than the regular fixed-coefficient feedback.\"usa\",studying the use of popular destinations to enhance web search interaction,\"we present a novel web search interaction feature which, for a given query, provides links to websites frequently visited by other users with similar information needs. these popular destinations complement traditional search results, allowing direct navigation to authoritative resources for the query topic. destinations are identified using the history of search and browsing behavior of many users over an extended time period, whose collective behavior provides a basis for computing source authority. we describe a user study which compared the suggestion of destinations with the previously proposed suggestion of related queries, as well as with traditional, unaided web search. results show that search enhanced by destination suggestions outperforms other systems for exploratory tasks, with best performance obtained from mining past user behavior at query-level granularity.\"using clicks as implicit judgments: expectations versus observations advances in information retrieval,\"clickthrough data has been the subject of increasing popularity as an implicit indicator of user feedback. previous analysis has suggested that user click behaviour is subject to a quality bias—that is, users click at different rank positions when viewing effective search results than when viewing less effective search results. based on this observation, it should be possible to use click data to infer the quality of the underlying search system. in this paper we carry out a user study to systematically investigate how click behaviour changes for different levels of search system effectiveness as measured by information retrieval performance metrics. our results show that click behaviour does not vary systematically with the quality of search results. however, click behaviour does vary significantly between individual users, and between search topics. this suggests that using direct click behaviour—click rank and click frequency—to infer the quality of the underlying search system is problematic. further analysis of our user click data indicates that the correspondence between clicks in a search result list and subsequent confirmation that the clicked resource is actually relevant is low. using clicks as an implicit indication of relevance should therefore be done with caution.\"usa\",a taxonomy of web search,\"classic {ir} (information retrieval) is inherently predicated on users searching for information, the so-called \"\"information need\"\". but the need behind a web search is often not informational -- it might be navigational (give me the url of the site i want to reach) or transactional (show me sites where i can perform a certain transaction, e.g. shop, download a file, or find a map). we explore this taxonomy of web searches and discuss how global search engines evolved to deal with web-specific needs.\"1200 commercial street, emporia, ks 66801\",describing and predicting information-seeking behavior on the web,\"this study focuses on the task as a fundamental factor in the context of information seeking. the purpose of the study is to characterize kinds of tasks and to examine how different kinds of task give rise to different kinds of information-seeking behavior on the web. for this, a model for information-seeking behavior was used employing dimensions of information-seeking strategies ({iss}), which are based on several behavioral dimensions. the analysis of strategies was based on data collected through an experiment designed to observe users\\' behaviors. three tasks were assigned to 30 graduate students and data were collected using questionnaires, search logs, and interviews. the qualitative and quantitative analysis of the data identified 14 distinct information-seeking strategies. the analysis showed significant differences in the frequencies and patterns of {iss} employed between three tasks. the results of the study are intended to facilitate the development of task-based information-seeking models and to further suggest web information system designs that support the user\\'s diverse tasks.\"usa\",ranking using multiple document types in desktop search,\"a typical desktop environment contains many document types (email, presentations, web pages, pdfs, etc.) each with different metadata. predicting which types of documents a user is looking for in the context of a given query is a crucial part of providing effective desktop search. the problem is similar to selecting resources in distributed {ir}, but there are some important differences. in this paper, we quantify the impact of type prediction in producing a merged ranking for desktop search and introduce a new prediction method that exploits type-specific metadata. in addition, we show that type prediction performance and search effectiveness can be further enhanced by combining existing methods of type prediction using discriminative learning models. our experiments employ pseudo-desktop collections and a human computation game for acquiring realistic and reusable queries.\"usa\",are click-through data adequate for learning web search rankings?,\"learning-to-rank algorithms, which can automatically adapt ranking functions in web search, require a large volume of training data. a traditional way of generating training examples is to employ human experts to judge the relevance of documents. unfortunately, it is difficult, time-consuming and costly. in this paper, we study the problem of exploiting click-through data for learning web search rankings that can be collected at much lower cost. we extract pairwise relevance preferences from a large-scale aggregated click-through dataset, compare these preferences with explicit human judgments, and use them as training examples to learn ranking functions. we find click-through data are useful and effective in learning ranking functions. a straightforward use of aggregated click-through data can outperform human judgments. we demonstrate that the strategies are only slightly affected by fraudulent clicks. we also reveal that the pairs which are very reliable, e.g., the pairs consisting of documents with large click frequency differences, are not sufficient for learning.\"usa\",watch what i watch: using community activity to understand content,\"this paper presents a high-level overview of yahoo research berkeley\\'s approach to multimedia research and the ideas motivating it. this approach is characterized primarily by a shift away from building subsystems that attempt to discover or understand the \"\"meaning\"\" of media content toward systems and algorithms that can usefully utilize information about how media content is being used in specific contexts; a shift from semantics to pragmatics. we believe that, at least for the domain of consumer and web videos, the latter provides a more promising basis for indexing media content in ways that satisfy user needs. to illustrate our approach, we present ongoing work on several applications which generate and utilize contextual usage meta-data to provide novel and useful media experiences.\"dec,2007-04-09 14:30:16,,context-aware computing applications,usa\",context-aware query classification,\"understanding users\\'search intent expressed through their search queries is crucial to web search and online advertisement. web query classification ({qc}) has been widely studied for this purpose. most previous {qc} algorithms classify individual queries without considering their context information. however, as exemplified by the well-known example on query \"\"jaguar\"\", many web queries are short and ambiguous, whose real meanings are uncertain without the context information. in this paper, we incorporate context information into the problem of query classification by using conditional random field ({crf}) models. in our approach, we use neighboring queries and their corresponding clicked {urls} (web pages) in search sessions as the context information. we perform extensive experiments on real world search logs and validate the effectiveness and effciency of our approach. we show that we can improve the f1 score by 52\\\\% as compared to other state-of-the-art baselines.\"\"context has long been considered very useful to help the user assess the actual relevance of a document. in web searching, context can help assess the relevance of a web page by showing how the page is related to other pages in the same web site, for example. such information is very difficult to convey and visualize in a user friendly way. in this paper we present the design, implementation and evaluation of a graphical visualization tool aimed at helping users to determine the relevance of a web page by displaying the structure of the web site the page belongs to. the results of an initial evaluation suggest that this visualization technique helps the user navigate large web sites and find useful information in an effective way, without increasing the cognitive load of the user. 10.1177/0165551506067123\"usa\",clustering web queries,\"despite the wide applicability of clustering methods, their evaluation remains a problem. in this paper, we present a metric for the evaluation of clustering methods. the data set to be clustered is viewed as a sample from a larger population, with clustering quality measured in terms of our predicted ability to discriminate between members of this population. we measure this property by training a classifier to recognize each cluster and measuring the accuracy of this classifier, normalized by a notion of expected accuracy. to demonstrate the applicability of this metric we apply it to web queries. we investigated a commercially oriented data set of 1700 queries and a general data set of 4000 queries. both sets are taken from the logs of a commercial web search engine. clustering is based on the contents of search engine result pages generated by executing the queries on the search engine from which they were taken. multiple clustering algorithms are crossed with various weighting schemes to produce multiple clusterings of each query set. our metric is used evaluate these clusterings. the results on the commercially oriented data set are compared to two pre-existing manual labelings, and are also used in an ad clickthrough experiment.\"usa\",enriching user profiling with affective features for the improvement of a multimodal recommender system,\"recommender systems have been systematically applied in industry and academia to help users cope with information uncertainty. however, given the multiplicity of the preferences and needs it has been shown that no approach is suitable for all users in all situations. thus, it is believed that an effective recommender system should incorporate a variety of techniques and features to offer valuable recommendations and enhance the search experience. in this paper we propose a novel video search interface that employs a multimodal recommender system, which can predict topical relevance. the multimodal recommender accounts for interaction data, contextual information, as well as users\\' affective responses, and exploits these information channels to provide meaningful recommendations of unseen videos. our experiment shows that the multimodal interaction feature is a promising way to improve the performance of recommendation.\"university of heidelberg, bergheimer str 58, 69115 heidelberg, germany. ey@yi.com\",\"how do consumers search for and appraise health information on the world wide web? qualitative study using focus groups, usability tests, and in-depth interviews\",\"objectives: to describe techniques for retrieval and appraisal used by consumers when they search for health information on the {internet.design}: qualitative study using focus groups, naturalistic observation of consumers searching the world wide web in a usability laboratory, and in-depth {interviews.participants}: a total of 21 users of the internet participated in three focus group sessions. 17 participants were given a series of health questions and observed in a usability laboratory setting while retrieving health information from the web; this was followed by in-depth {interviews.setting}: heidelberg, {germany.results}: although their search technique was often suboptimal, internet users successfully found health information to answer questions in an average of 5 minutes 42 seconds (median 4 minutes 18 seconds) per question. participants in focus groups said that when assessing the credibility of a website they primarily looked for the source, a professional design, a scientific or official touch, language, and ease of use. however, in the observational study, no participants checked any  ” about us” sections of websites, disclaimers, or disclosure statements. in the post-search interviews, it emerged that very few participants had noticed and remembered which websites they had retrieved information {from.conclusions}: further observational studies are needed to design and evaluate educational and technological innovations for guiding consumers to high quality health information on the web. what is already known on this topic what is already known on this topic little is known about how consumers retrieve and assess the quality of health information on the internet qualitative data are needed to design educational and technological innovations to guide consumers to high quality health information what this study adds what this study adds users of the internet explore only the first few links on general search engines when seeking health information consumers say that when assessing the credibility of a site they primarily look for the source, a professional design, and a variety of other criteria in practice, internet users do not check the  ” about us” sections of websites, try to find out who authors or owners of the site are, or read disclaimers or disclosure statements very few internet users later remember from which websites they retrieved information or who stood behind the sites what is already known on this topic little is known about how consumers retrieve and assess the quality of health information on the {internetqualitative} data are needed to design educational and technological innovations to guide consumers to high quality health {informationwhat} this study adds users of the internet explore only the first few links on general search engines when seeking health {informationconsumers} say that when assessing the credibility of a site they primarily look for the source, a professional design, and a variety of other {criteriain} practice, internet users do not check the  ” about us” sections of websites, try to find out who authors or owners of the site are, or read disclaimers or disclosure {statementsvery} few internet users later remember from which websites they retrieved information or who stood behind the sites\"contextual recommendation,\"the role of context in our daily interaction with our environment has been studied in psychology, linguistics, artificial intelligence, information retrieval, and more recently, in pervasive/ubiquitous computing. however, context has been largely ignored in research into recommender systems specifically and personalization in general. in this paper we describe how context can be brought to bear on recommender systems. as a means for achieving this, we propose a fundamental shift in terms of how we model a user within a recommendation system: inspired by models of human memory developed in psychology, we distinguish between a user\\'s short term and long term memories, define a recommendation process that uses these two memories, using context-based retrieval cues to retrieve relevant preference information from long term memory and use it in conjunction with the information stored in short term memory for generating recommendations. we also describe implementations of recommender systems and personalization solutions based on this framework and show how this results in an increase in recommendation quality.\"usa\",measuring search engine quality,\"the effectiveness of twenty public search engines is evaluated using {trec}-inspired methods and a set of 54 queries taken from real web search logs. the world wide web is taken as the test collection and a combination of crawler and text retrieval system is evaluated. the engines are compared on a range of measures derivable from binary relevance judgments of the first seven live results returned. statistical testing reveals a significant difference between engines and high intercorrelations between measures. surprisingly, given the dynamic nature of the web and the time elapsed, there is also a high correlation between results of this study and a previous study by gordon and pathak. for nearly all engines, there is a gradual decline in precision at increasing cutoff after some initial fluctuation. performance of the engines as a group is found to be inferior to the group of participants in the {trec}-8 large web task, although the best engines approach the median of those systems. shortcomings of current web search evaluation methodology are identified and recommendations are made for future improvements. in particular, the present study and its predecessors deal with queries which are assumed to derive from a need to find a selection of documents relevant to a topic. by contrast, real web search reflects a range of other information need types which require different judging and different measures.\"towards a better understanding of context and {context-awareness},an abstract is not available.usa\",\"the good, the bad, and the random: an eye-tracking study of ad quality in web search\",\"we investigate how people interact with web search engine result pages using eye-tracking. while previous research has focused on the visual attention devoted to the 10 organic search results, this paper examines other components of contemporary search engines, such as ads and related searches. we systematically varied the type of task (informational or navigational), the quality of the ads (relevant or irrelevant to the query), and the sequence in which ads of different quality were presented. we measured the effects of these variables on the distribution of visual attention and on task performance. our results show significant effects of each variable. the amount of visual attention that people devote to organic results depends on both task type and ad quality. the amount of visual attention that people devote to ads depends on their quality, but not the type of task. interestingly, the sequence and predictability of ad quality is also an important factor in determining how much people attend to ads. when the quality of ads varied randomly from task to task, people paid little attention to the ads, even when they were good. these results further our understanding of how attention devoted to search results is influenced by other page elements, and how previous search experiences influence how people attend to the current page.\"usa\",how do users find things with {pubmed}?: towards automatic utility evaluation with user simulations,\"in the context of document retrieval in the biomedical domain, this paper explores the complex relationship between the quality of initial query results and the overall utility of an interactive retrieval system. we demonstrate that a content-similarity browsing tool can compensate for poor retrieval results, and that the relationship between retrieval performance and overall utility is non-linear. arguments are advanced with user simulations, which characterize the relevance of documents that a user might encounter with different browsing strategies. with broader implications to {ir}, this work provides a case study of how user simulations can be exploited as a formative tool for automatic utility evaluation. simulation-based studies provide researchers with an additional evaluation tool to complement interactive and cranfield-style experiments.\"halifax, nova scotia, canada; faculty of computer science, dalhousie university, halifax, nova scotia, canada; faculty of computer science, dalhousie university, halifax, nova scotia, canada\",a goal-based classification of web information tasks,\"while researchers have been studying user activity on the web since its inception, there remains a lack of understanding of the high level tasks in which users engage on the web. we have recently conducted a field study in which participants were asked to annotate all web usage with a task description and categorization. based on our analysis of participants\\' recorded tasks during the field study, as well as previous research, we have developed a goal-based classification of information tasks which describes user activities on the web.\"usa\",evaluation of methods for relative comparison of retrieval systems based on clickthroughs,\"the cranfield evaluation method has some disadvantages, including its high cost in labor and inadequacy for evaluating interactive retrieval techniques. as a very promising alternative, automatic comparison of retrieval systems based on observed clicking behavior of users has recently been studied. several methods have been proposed, but there has so far been no systematic way to assess which strategy is better, making it difficult to choose a good method for real applications. in this paper, we propose a general way to evaluate these relative comparison methods with two measures: utility to {users(utu}) and effectiveness of {differentiation(eod}). we evaluate two state of the art methods by systematically simulating different retrieval scenarios. inspired by the weakness of these methods revealed through our evaluation, we further propose a novel method by considering the positions of clicked documents. experiment results show that our new method performs better than the existing methods.\"usa\",the state of the art in automating usability evaluation of user interfaces,\"usability evaluation is an increasingly important part of the user interface design process. however, usability evaluation can be expensive in terms of time and human resources, and automation is therefore a promising way to augment existing approaches. this article presents an extensive survey of usability evaluation methods, organized according to a new taxonomy that emphasizes the role of automation. the survey analyzes existing techniques, identifies which aspects of usability evaluation automation are likely to be of use in future research, and suggests new ways to expand existing approaches to better support usability evaluation.\"harvard medical school, boston, ma, usa.\",identifying consumer-friendly display ({cfd}) names for health concepts.,\"we have developed a systematic methodology using corpus-based text analysis followed by human review to assign \"\"consumer-friendly display ({cfd}) names\"\" to medical concepts from the national library of medicine ({nlm}) unified medical language system ({umls}) metathesaurus. using {nlm} {medlineplus} queries as a corpus of consumer expressions and a collaborative web-based tool to facilitate review, we analyzed 425 frequently occurring concepts. as a preliminary test of our method, we evaluated 34 ana-lyzed concepts and their {cfd} names, using a questionnaire modeled on standard reading assessments. the initial results that consumers (n=10) are more likely to understand and recognize {cfd} names than alternate labels suggest that the approach is useful in the development of consumer health vocabularies for displaying understandable health information.\"ny, usa\",information seeking behavior of academic scientists,ny, usa\",where should the person stop and the information search interface start?,\"many users of online and other automated information systems want to take advantage of the speed and power of automated retrieval, while still controlling and directing the steps of the search themselves. they do not want the system to take over and carry out the search entirely for them. yet the objective of much of current theory and experimentation in information retrieval systems and interfaces is to design systems in which the user has either no or only reactive involvement with the search process. it is argued here that the advanced information retrieval research community is missing an opportunity to design systems that are in better harmony with the actual preferences of many users—sophisticated systems that provide an optimal combination of searcher control and system retrieval power. the user may be provided effective means of directing the search if capabilities specific to the information retrieval process, that is, strategic behaviors normally associated with information searching, are incorporated into the interface. there are many questions concerning (1) the degree of user vs. system involvement in the search, and (2) the size, or chunking, of activities; that is, how much and what type of activity the user should be able to direct the system to do at once. these two dimensions are analyzed and a number of configurations of system capability that combine user and system control are presented and discussed. in the process, the concept of the information search stratagem is introduced, and particular attention is paid to the provision of strategic, as opposed to purely procedural capabilities for the searcher. finally, certain types of user-system relationship are selected as deserving particular attention in future information retrieval system design, and arguments are made to support the recommendations.\",2009-04-08 15:57:45,,{towards a better understanding of context and context-awareness},\"the use of context is important in interactive applications. it is par- ticularly important for applications where the user\\'s context is changing rap- idly, such as in both handheld and ubiquitous computing. in order to better un- derstand how we can use context and facilitate the building of context-aware applications, we need to more fully understand what constitutes a context- aware application and what context is. towards this goal, we have surveyed existing work in context-aware computing. in this paper, we provide an over- view of the results of this survey and, in particular, definitions and categories of context and context-aware. we conclude with recommendations for how this better understanding of context inform a framework for the development of context-aware applications.\"usa\",global ranking by exploiting user clicks,\"it is now widely recognized that user interactions with search results can provide substantial relevance information on the documents displayed in the search results. in this paper, we focus on extracting relevance information from one source of user interactions, i.e., user click data, which records the sequence of documents being clicked and not clicked in the result set during a user search session. we formulate the problem as a global ranking problem, emphasizing the importance of the sequential nature of user clicks, with the goal to predict the relevance labels of all the documents in a search session. this is distinct from conventional learning to rank methods that usually design a ranking model defined on a single document; in contrast, in our model the relational information among the documents as manifested by an aggregation of user clicks is exploited to rank all the documents jointly. in particular, we adapt several sequential supervised learning algorithms, including the conditional random field ({crf}), the sliding window method and the recurrent sliding window method, to the global ranking problem. experiments on the click data collected from a commercial search engine demonstrate that our methods can outperform the baseline models for search results re-ranking.\"\"health care systems will integrate new computing paradigms in the coming years. context-awareness computing is a research field which often refers to health care as an interesting and rich area of application. through a survey of the research literature, we intended to derive an objective view of the actual dynamism of context awareness in health care, and to identify strengths and weaknesses in this field. after discussing definitions of context, we proposed a simple framework to analyse and characterize the use of context through three main axes. we then focused on context-awareness computing and reported on the main teams working in this area. we described some of the context-awareness projects in health care. a deeper analysis of the hospital-based projects demonstrated the gap between recommendations expressed for modelling context awareness and the actual use in a prototype. finally, we identified pitfalls encountered in this area of research. a number of opportunities remain for this evolving field of research. we found relatively few groups with such a specific focus. as yet there is no consensus as to the most appropriate models or attributes to include in context awareness. we conclude that a greater understanding of which aspects of context are important in a health care setting is required; the inherent sociotechnical nature of context-aware applications in health care; and the need to draw on a number of disciplines to conduct this research.\"usa\",comparative analysis of clicks and judgments for {ir} evaluation,\"queries and click-through data taken from search engine transaction logs is an attractive alternative to traditional test collections, due to its volume and the direct relation to end-user querying. the overall aim of this paper is to answer the question: how does click-through data differ from explicit human relevance judgments in information retrieval evaluation? we compare a traditional test collection with manual judgments to transaction log based test collections---by using queries as topics and subsequent clicks as pseudo-relevance judgments for the clicked results.\"usa\",a study of web usability for older adults seeking online health resources,\"the web offers older adult users immediate access to health resources that might not otherwise be available. older adult users, however, may encounter web barriers associated with normal aging and lower education. the national institute on aging web guidelines were used to assess the usability of 125 web sites offering health resources. performance, translation, and reading complexity were also assessed. results showed that many of the sampled sites were not senior-friendly. only 12\\\\&percnt; of the sites offered a spanish version, many containing nontranslated text. approximately a third of sampled sites required a college education to comprehend extracted health information.\"contents, and usage data ({data-centric} systems and applications)\",\"{<p>web mining aims to discover useful information and knowledge from the web hyperlink structure, page contents, and usage data. although web mining uses many conventional data mining techniques, it is not purely an application of traditional data mining due to the semistructured and unstructured nature of the web data and its heterogeneity. it has also developed many of its own algorithms and techniques.</p> <p>liu has written a comprehensive text on web data mining. key topics of structure mining, content mining, and usage mining are covered both in breadth and in depth. his book brings together all the essential concepts and algorithms from related areas such as data mining, machine learning, and text processing to form an authoritative and coherent text. </p> <p>the book offers a rich blend of theory and practice, addressing seminal research ideas, as well as examining the technology from a practical point of view. it is suitable for students, researchers and practitioners interested in web mining both as a learning text and a reference book. lecturers can readily use it for classes on data mining, web mining, and web search. additional teaching materials such as lecture slides, datasets, and implemented algorithms are available online.</p>}\"usa\",socially filtered web search: an approach using social bookmarking tags to personalize web search,\"today\\'s knowledge workers are confronted with an ever increasing information overload while searching for needed information in the web. common search engines do not take into account the current work context of the user. but we consider context information as an effective means to implicitly narrow the information space of the web. in this paper we present a novel approach that increases the relevance of search results by considering the current work context. we track the user\\'s web browsing behavior, store visited pages and build up a user model based on this information. as the user browses, the stored {urls} of the visited pages are enhanced with tags from social bookmarking sites. based on the user model and the retrieved bookmarks we developed an easy-to-use and easy-to-configure clientside web search engine that refines the original search query with these tags. our approach follows the design principle of non-intrusiveness. that means we present the context-sensitive personalized adapted search results together with the original non-adaptive search results. we developed an open architecture that allows the user to reconfigure the system to use different metadata providers and search engines. in order to prove our architecture we implemented a firefox add-on.\"\"examines the concepts involved in user studies, information needs, and information seeking behavior to propose a basis and direction for future research in information science. twenty-eight references are listed. ({raa})\"\"the web provides an unprecedented opportunity to evaluate ideas quickly using controlled experiments, also called randomized experiments, {a/b} tests (and their generalizations), split tests, {control/treatment} tests, {multivariable} tests ({mvt}) and parallel flights. controlled experiments embody the best scientific design for establishing a causal relationship between changes and their influence on user-observable behavior. we provide a practical guide to conducting online experiments, where end-users can help guide the development of features. our experience indicates that significant learning and return-on-investment ({roi}) are seen when development teams listen to their customers, not to the highest paid person\\'s opinion ({hippo}). we provide several examples of controlled experiments with surprising results. we review the important ingredients of running controlled experiments, and discuss their limitations (both technical and organizational). we focus on several areas that are critical to experimentation, including statistical power, sample size, and techniques for variance reduction. we describe common architectures for experimentation systems and analyze their advantages and disadvantages. we evaluate randomization and hashing techniques, which we show are not as simple in practice as is often assumed. controlled experiments typically generate large amounts of data, which can be analyzed using data mining techniques to gain deeper understanding of the factors influencing the outcome of interest, leading to new hypotheses and creating a virtuous cycle of improvements. organizations that embrace controlled experiments with clear evaluation criteria can evolve their systems with automated optimizations and real-time analyses. based on our extensive practical experience with multiple systems and organizations, we share key lessons that will help practitioners in running trustworthy controlled experiments.\"texas, usa\",improving automatic query classification via {semi-supervised} learning,\"accurate topical classification of user queries allows for increased effectiveness and efficiency in general-purpose web search systems. such classification becomes critical if the system is to return results not just from a general web collection but from topic-specific back-end databases as well. maintaining sufficient classification recall is very difficult as web queries are typically short, yielding few features per query. this feature sparseness coupled with the high query volumes typical for a large-scale search service makes manual and supervised learning approaches alone insufficient. we use an application of computational linguistics to develop an approach for mining the vast amount of unlabeled data in web query logs to improve automatic topical web query classification. we show that our approach in combination with manual matching and supervised learning allows us to classify a substantially larger proportion of queries than any single technique. we examine the performance of each approach on a real web query stream and show that our combined method accurately classifies 46\\\\% of queries, outperforming the recall of best single approach by nearly 20\\\\%, with a 7\\\\% improvement in overall effectiveness.\"revisiting {ir} techniques for collaborative search strategies,this paper revisits some of the established information retrieval ({ir}) techniques to investigate effective collaborative search strategies. we devised eight search strategies that divided labour and shared knowledge in teams using relevance feedback and clustering. we evaluated the performance of strategies with a user simulation enhanced by a query-pooling method. our results show that relevance feedback is successful at formulating effective collaborative strategies while further effort is needed for clustering. we also measured the extent to which additional members improved the performance and an effect of search progress on the improvement.usa\",the impact of history length on personalized search,\"personalized search is a promising way to better serve different users\\' information needs. search history is one of the major information sources for search personalization. we investigated the impact of history length on the effectiveness of personalized ranking. we carried out task-based user study for web search, and obtained ranked relevance judgments for all queries. query contexts derived from previous queries in the same task are used to re-rank results for the current query. experimental results show that the performance of personalization generally improves as more queries are accumulated, but most of the benefits come from a few immediately preceding queries.\"ny, usa\",on contexts of information seeking,\"while surprisingly little has been written about context at a meaningful level, context is central to most theoretical approaches to information seeking. in this essay i explore in more detail three senses of context. first, i look at context as equivalent to the situation in which a process is immersed. second, i discuss contingency approaches that detail active ingredients of the situation that have specific, predictable effects. third, i examine major frameworks for meaning systems. then, i discuss how a deeper appreciation of context can enhance our understanding of the process of information seeking by examining two vastly different contexts in which it occurs: organizational and cancer-related, an exemplar of everyday life information seeking. this essay concludes with a discussion of the value that can be added to information seeking research and theory as a result of a deeper appreciation of context, particularly in terms of our current multi-contextual environment and individuals taking an active role in contextualizing.\"spain. agonzalez.gapm10@salud.madrid.org\",information needs and {information-seeking} behavior of primary care physicians,{purpose} the aim of this study was to determine the information needs of primary care physicians in spain and to describe their information-seeking patterns.milwaukee, p.o. box 413, milwaukee, wi 53201; school of information sciences, college of communication and information, university of tennessee at knoxville, knoxville, tn 37996-0341\",identifying web search session patterns using cluster analysis: a comparison of three search environments,\"session characteristics taken from large transaction logs of three web search environments (academic web site, public search engine, consumer health information portal) were modeled using cluster analysis to determine if coherent session groups emerged for each environment and whether the types of session groups are similar across the three environments. the analysis revealed three distinct clusters of session behaviors common to each environment: ldquohit and runrdquo sessions on focused topics, relatively brief sessions on popular topics, and sustained sessions using obscure terms with greater query modification. the findings also revealed shifts in session characteristics over time for one of the datasets, away from ldquohit and runrdquo sessions toward more popular search topics. a better understanding of session characteristics can help system designers to develop more responsive systems to support search features that cater to identifiable groups of searchers based on their search behaviors. for example, the system may identify struggling searchers based on session behaviors that match those identified in the current study to provide context sensitive help.\"\"one of the cornerstones of any intelligent entity is the ability to understand how occurrences in the surrounding world influence its own behaviour. different states, or situations, in its environment should be taken into account when reasoning or acting. when dealing with different situations, context is the key element used to infer possible actions and information needs. the activities of the perceiving agent and other entities are arguably one of the most important features of a situation; this is equally true whether the agent is artificial or not.\"usa\",active relevance feedback for difficult queries,\"relevance feedback has been demonstrated to be an effective strategy for improving retrieval accuracy. the existing relevance feedback algorithms based on language models and vector space models are not effective in learning from negative feedback documents, which are abundant if the initial query is difficult. the probabilistic retrieval model has the advantage of being able to naturally improve the estimation of both the relevant and non-relevant models. the dirichlet compound multinomial ({dcm}) distribution, which relies on hierarchical bayesian modeling techniques, is a more appropriate generative model for the probabilistic retrieval model than the traditional multinomial distribution. we propose a new relevance feedback algorithm, based on a mixture model of the {dcm} distribution, to effectively model the overlaps between the positive and negative feedback documents. consequently, the new algorithm improves the retrieval performance substantially for difficult queries. to further reduce human relevance evaluation, we propose a new active learning algorithm in conjunction with the new relevance feedback model. the new active learning algorithm implicitly models the diversity, density and relevance of unlabeled data in a transductive experimental design framework. experimental results on several {trec} datasets show that both the relevance feedback and active learning algorithm significantly improve retrieval accuracy.\"usa\",workshop on web information seeking and interaction,\"the world wide web has provided access to a diverse range of information sources and systems. people engaging with this rich network of information may need to interact with different technologies, interfaces, and information providers in the course of a single search task. these systems may offer different interaction affordances and require users to adapt their information-seeking strategies. not only is this challenging for users, but it also presents challenges for the designers of interactive systems, who need to make their own system useful and usable to broad user groups. the popularity of web browsing and web search engines has given rise to distinct forms of information-seeking behaviour, and new interaction styles, but we do not yet fully understand these or their implications for the development of new systems.\"usa\",searching with context,\"contextual search refers to proactively capturing the information need of a user by automatically augmenting the user query with information extracted from the search context; for example, by using terms from the web page the user is currently browsing or a file the user is currently {editing.we} present three different algorithms to implement contextual search for the web. the first,  it query rewriting ({qr}) , augments each query with appropriate terms from the search context and uses an off-the-shelf web search engine to answer this augmented query. the second,   rank-biasing ({rb}) , generates a representation of the context and answers queries using a custom-built search engine that exploits this representation. the third,  iterative filtering meta-search ({ifm}) , generates multiple subqueries based on the user query and appropriate terms from the search context, uses an off-the-shelf search engine to answer these subqueries, and re-ranks the results of the subqueries using rank aggregation {methods.we} extensively evaluate the three methods using 200 contexts and over 24,000 human relevance judgments of search results. we show that while {qr} works surprisingly well, the relevance and recall can be improved using {rb} and substantially more using {ifm}. thus, {qr}, {rb}, and {ifm} represent a cost-effective design spectrum for contextual search.\"usa\",mining dependency relations for query expansion in passage retrieval,usa\",the role of ontologies in {context-aware} recommender systems,usa\",personalizing queries based on networks of composite preferences,\"people\\'s preferences are expressed at varying levels of granularity and detail as a result of partial or imperfect knowledge. one may have some preference for a general class of entities, for example, liking comedies, and another one for a fine-grained, specific class, such as disliking recent thrillers with al pacino. in this article, we are interested in capturing such complex, multi-granular preferences for personalizing database queries and in studying their impact on query results. we organize the collection of one\\'s preferences in a  preference network  (a directed acyclic graph), where each node refers to a subclass of the entities that its parent refers to, and whenever they both apply, more specific preferences override more generic ones. we study query personalization based on networks of preferences and provide efficient algorithms for identifying relevant preferences, modifying queries accordingly, and processing personalized queries. finally, we present results of both synthetic and real-user experiments, which: (a) demonstrate the efficiency of our algorithms, (b) provide insight as to the appropriateness of the proposed preference model, and (c) show the benefits of query personalization based on composite preferences compared to simpler preference representations.\"\"searchers seldom make use of the advanced searching features that could improve the quality of the search process because they do not know these features exist, do not understand how to use them, or do not believe they are effective or efficient. information retrieval systems offering automated assistance could greatly improve search effectiveness by suggesting or implementing assistance automatically. a critical issue in designing such systems is determining when the system should intervene in the search process. in this paper, we report the results of an empirical study analyzing when during the search process users seek automated searching assistance from the system and when they implement the assistance. we designed a fully functional, automated assistance application and conducted a study with 30 subjects interacting with the system. the study used a {2g} {trec} document collection and {trec} topics. approximately 50\\\\% of the subjects sought assistance, and over 80\\\\% of those implemented that assistance. results from the evaluation indicate that users are willing to accept automated assistance during the search process, especially after viewing results and locating relevant documents. we discuss implications for interactive information retrieval system design and directions for future research.\"usa\",methods and techniques for the evaluation of user-adaptive systems,\"this article presents a comprehensive overview of methods and techniques used for the evaluation of user-adaptive systems. it describes the methodologies derived both from the evaluation of human–computer interaction systems and from information retrieval and information filtering systems by giving examples of the application of these methodologies in the user-adaptive systems. the state of the art and the main results in the evaluation of these systems are reported. in particular, empirical evaluation and layered approaches are discussed in detail. finally, focus on less explored methodologies, such as qualitative approaches (e.g. grounded theory), is proposed.\"representing context in web search with ontological user profiles,\"one of the key factors for effective personalization of information access is the user context. we propose a framework which integrates several critical elements that make up the user context, namely, the user\\'s short-term behavior, semantic knowledge from ontologies that provide explicit representations of the domain of interest, and long-term user profiles revealing interests and trends. our proposed approach involves implicitly building ontological user profiles by assigning interest scores to existing concepts in a domain ontology. these profiles are, therefore, maintained and updated as annotated instances of a reference domain ontology. we propose a spreading activation algorithm for maintaining the interest scores in the user profile based on the user\\'s ongoing behavior. our experimental results show that the user context can be effectively utilized for web search personalization. specifically, re-ranking the search results based on interest scores derived from the semantic evidence in an ontological user profile provides better search results by proficiently bringing results closer to the top when they are most relevant to the user.\"\"{abstract\\\\&nbsp;\\\\&nbsp;traditional} information retrieval systems aim at satisfying most users for most of their searches, leaving aside the context in which the search takes place. we propose to model two main aspects of context: the themes of the user\\'s information need and the specific data the user is looking for to achieve the task that has motivated his search. both aspects are modeled by means of ontologies. documents are semantically indexed according to the context representation and the user accesses information by browsing the ontologies. the model has been applied to a case study that has shown the added value of such a semantic representation of context.\"usa\",toward a model of children\\'s information seeking behavior in using digital libraries,\"this paper presents an empirical model of arabic-speaking children\\'s interaction with the international children\\'s digital library ({icdl}). the model is based on data collected from ten children ages 6--10 who interacted with the {icdl} to find information for assigned and self-generated tasks. two contexts influenced children\\'s information seeking behavior: 1. the non-naturalistic laboratory environment where they used the {icdl} as volunteers rather than as part of their everyday life or as a requirement for an assignment, and 2. the international and multicultural nature of the {icdl} that provided access to an arabic book collection, but did not support analytical searching in arabic. the model presents 7 modes that characterized children\\'s information seeking behavior and the range of moves associated with them. underlying the behavior is the children\\'s information need and their affective states that consisted of uncertainty and anxiety in the beginning and certainty and satisfaction upon completing the tasks.\"usa\",display time as implicit feedback: understanding task effects,\"recent research has had some success using the length of time a user displays a document in their web browser as implicit feedback for document preference. however, most studies have been confined to specific search domains, such as news, and have not considered the effects of task on display time, and the potential impact of this relationship on the effectiveness of display time as implicit feedback. we describe the results of an intensive naturalistic study of the online information-seeking behaviors of seven subjects during a fourteen-week period. throughout the study, subjects\\' online information-seeking activities were monitored with various pieces of logging and evaluation software. subjects were asked to identify the tasks with which they were working, classify the documents that they viewed according to these tasks, and evaluate the usefulness of the documents. results of a user-centered analysis demonstrate no general, direct relationship between display time and usefulness, and that display times differ significantly according to specific task, and according to specific user.\"usa\",understanding user goals in web search,\"previous work on understanding user web search behavior has focused on how people search and what they are searching for, but not why they are searching. in this paper, we describe a framework for understanding the underlying goals of user searches, and our experience in using the framework to manually classify queries from a web search engine. our analysis suggests that so-called navigational\"\" searches are less prevalent than generally believed while a previously unexplored \"\"resource-seeking\"\" goal may account for a large fraction of web searches. we also illustrate how this knowledge of user search goals might be used to improve future web search engines.\"beautiful evidence,usa\",the use of dynamic contexts to improve casual internet searching,\"research has shown that most users\\' online information searches are suboptimal. query optimization based on a relevance feedback or genetic algorithm using dynamic query contexts can help casual users search the internet. these algorithms can draw on implicit user feedback based on the surrounding links and text in a search engine result set to expand user queries with a variable number of keywords in two manners. positive expansion adds terms to a user\\'s keywords with a boolean \"\"and,\"\" negative expansion adds terms to the user\\'s keywords with a boolean \"\"not.\"\" each algorithm was examined for three user groups, high, middle, and low achievers, who were classified according to their overall performance. the interactions of users with different levels of expertise with different expansion types or algorithms were evaluated. the genetic algorithm with negative expansion tripled recall and doubled precision for low achievers, but high achievers displayed an opposed trend and seemed to be hindered in this condition. the effect of other conditions was less substantial.\"usa\",a new interpretation of average precision,\"we consider the question of whether average precision, as a measure of retrieval effectiveness, can be regarded as deriving from a model of user searching behaviour. it turns out that indeed it can be so regarded, under a very simple stochastic model of user behaviour.\"ny, usa\",collaborative information seeking: a field study of a multidisciplinary patient care team,usa\",searching the wikipedia with contextual information,\"we propose a framework for searching the wikipedia with contextual information. our framework extends the typical keyword search, by considering queries of the type (q,p), where q is a set of terms (as in classical web search), and p is a source wikipedia document. the query terms q represent the information that the user is interested in finding, and the document p provides the context of the query. the task is to rank other documents in wikipedia with respect to their relevance to the query terms q given the context document p. by associating a context to the query terms, the search results of a search initiated in a particular page can be made more relevant.\"\"the concept of an  ” information space” provides a powerful metaphor for guiding the design of interactive retrieval systems. we present a case study of related article search, a browsing tool designed to help users navigate the information space defined by results of the {pubmed} ®  search engine. this feature leverages content-similarity links that tie {medline} ®  citations together in a vast document network. we examine the effectiveness of related article search from two perspectives: a topological analysis of networks generated from information needs represented in the {trec} 2005 genomics track and a query log analysis of real {pubmed} users. together, data suggest that related article search is a useful feature and that browsing related articles has become an integral part of how users interact with {pubmed}.\"usa\",exploring social annotations for information retrieval,\"social annotation has gained increasing popularity in many web-based applications, leading to an emerging research area in text analysis and information retrieval. this paper is concerned with developing probabilistic models and computational algorithms for social annotations. we propose a unified framework to combine the modeling of social annotations with the language modeling-based methods for information retrieval. the proposed approach consists of two steps: (1) discovering topics in the contents and annotations of documents while categorizing the users by domains; and (2) enhancing document and query language models by incorporating user domain interests as well as topical background models. in particular, we propose a new general generative model for social annotations, which is then simplified to a computationally tractable hierarchical bayesian network. then we apply smoothing techniques in a risk minimization framework to incorporate the topical information to language models. experiments are carried out on a real-world annotation data set sampled from del.icio.us. our results demonstrate significant improvements over the traditional approaches.\"\"{in the computer age, it is essential for individuals to develop skills and strategies for manipulating, storing, and retrieving electronic information. this book considers how electronic technologies have changed these skills and strategies and augmented the fundamental human activity of information seeking.  writing from the point of view of the user rather than the computer, the author makes a case for creating new interface designs that allow information seekers to choose what strategy to apply according to their immediate needs. such systems may be designed by providing information seekers with alternative interface mechanisms for displaying and manipulating multiple levels of representation for information objects. this book is multidisciplinary in approach and aims to bridge the perspectives of information science, computer science and education.  it will be essential reading for researchers and graduate students in these fields.}\"national center for biotechnology information, national library of medicine, national institutes of health, bldg. 38a, 8600 rockville pike, bethesda, md 20894; computational biology branch, national center for biotechnology information, national library of medicine, national institutes of health, bldg. 38a, 8600 rockville pike, bethesda, md 20894; principal investigator, computational biology branch, national center for biotechnology information, national library of medicine, national institutes of health, bldg. 38a, 8600 rockville pike, bethesda, md 20894\",how to interpret {pubmed} queries and why it matters,\"a significant fraction of queries in {pubmedtm} are multiterm queries without parsing instructions. generally, search engines interpret such queries as collections of terms, and handle them as a boolean conjunction of these terms. however, analysis of queries in {pubmedtm} indicates that many such queries are meaningful phrases, rather than simple collections of terms. in this study, we examine whether or not it makes a difference, in terms of retrieval quality, if such queries are interpreted as a phrase or as a conjunction of query terms. and, if it does, what is the optimal way of searching with such queries. to address the question, we developed an automated retrieval evaluation method, based on machine learning techniques, that enables us to evaluate and compare various retrieval outcomes. we show that the class of records that contain all the search terms, but not the phrase, qualitatively differs from the class of records containing the phrase. we also show that the difference is systematic, depending on the proximity of query terms to each other within the record. based on these results, one can establish the best retrieval order for the records. our findings are consistent with studies in proximity searching.\"620 michigan ave. n.e., washington, d.c. 20064\",effects of contextual factors on image searching on the web,\"this research examined college students\\' image searching processes on the web. the study\\'s objective was to collect empirical data on students\\' search needs and identify what contextual factors had a significant influence on their image searching tactics. while confirming common search behaviors such as google-dominant use, short queries, rare use of advanced search options, and checking few search result pages, the findings also revealed a significantly different effect of contextual factors on the tactics of querying and navigating, performance, and relevance judgment. in particular, interaction activities were differentiated by task goals, level of searching expertise, and work task stages. the results suggested that context-sensitive services and interface features would better suit web users\\' actual needs and enhance their searching experience.\"101 louis shores building, tallahassee, fl\",a model for online consumer health information quality,\"this article describes a model for online consumer health information consisting of five quality criteria constructs. these constructs are grounded in empirical data from the perspectives of the three main sources in the communication process: health information providers, consumers, and intermediaries, such as web directory creators and librarians, who assist consumers in finding healthcare information. the article also defines five constructs of web page structural markers that could be used in information quality evaluation and maps these markers to the quality criteria. findings from correlation analysis and multinomial logistic tests indicate that use of the structural markers depended significantly on the type of web page and type of information provider. the findings suggest the need to define genre-specific templates for quality evaluation and the need to develop models for an automatic genre-based classification of health information web pages. in addition, the study showed that consumers may lack the motivation or literacy skills to evaluate the information quality of health web pages, which suggests the need to develop accessible automatic information quality evaluation tools and ontologies.\"\"the profusion of online resources calls for tools and methods to help internet users find precisely what they are looking for. quality controlled gateway {cismef} provides such services for health resources. however, the human cost of maintaining and updating the catalogue are increasingly high. this paper presents the automatic indexing system currently developed in the {cismef} team to be used as such for preliminary indexing, or after human reviewing for the final indexing. the system architecture, using the {intex} platform for {mesh} term extraction is detailed. the results of a first evaluation tend to indicate that the automatic indexing strategy is relevant, as it achieves a precision comparable to that of other existing operational systems. moreover, the system presented in this paper retrieves keyword/qualifier pairs as opposed to single terms, therefore providing a significantly more precise indexing. further development and tests will be carried out in order to improve the coverage of the dictionaries, and validate the efficiency of the system in the indexers\\' everyday work.\"usa\",a comparison of general vs personalised affective models for the prediction of topical relevance,\"information retrieval systems face a number of challenges, originating mainly from the semantic gap problem. implicit feedback techniques have been employed in the past to address many of these issues. although this was a step towards the right direction, a need to personalise and tailor the search experience to the user-specific needs has become evident. in this study we examine ways of personalising affective models trained on facial expression data. using personalised data we adapt these models to individual users and compare their performance to a general model. the main goal is to determine whether the behavioural differences of users have an impact on the models\\' ability to determine topical relevance and if, by personalising them, we can improve their accuracy. for modelling relevance we extract a set of features from the facial expression data and classify them using support vector machines. our initial evaluation indicates that accounting for individual differences and applying personalisation introduces, in most cases, a noticeable improvement in the models\\' performance.\"usa\",query reformulation using anchor text,\"query reformulation techniques based on query logs have been studied as a method of capturing user intent and improving retrieval effectiveness. the evaluation of these techniques has primarily, however, focused on proprietary query logs and selected samples of queries. in this paper, we suggest that anchor text, which is readily available, can be an effective substitute for a query log and study the effectiveness of a range of query reformulation techniques (including log-based stemming, substitution, and expansion) using standard {trec} collections. our results show that log-based query reformulation techniques are indeed effective with standard collections, but expansion is a much safer form of query modification than word substitution. we also show that using anchor text as a simulated query log is as least as effective as a real log for these techniques.\"\"this paper provides overview and instruction regarding the evaluation of interactive information retrieval systems with users. the primary goal of this article is to catalog and compile material related to this topic into a single source. this article (1) provides historical background on the development of user-centered approaches to the evaluation of interactive information retrieval systems; (2) describes the major components of interactive information retrieval system evaluation; (3) describes different experimental designs and sampling strategies; (4) presents core instruments and data collection techniques and measures; (5) explains basic data analysis techniques; and (4) reviews and discusses previous studies. this article also discusses validity and reliability issues with respect to both measures and methods, presents background information on research ethics and discusses some ethical issues which are specific to studies of interactive information retrieval ({iir}). finally, this article concludes with a discussion of outstanding challenges and future research directions.\"usa\",designing mediation for context-aware applications,\"many context-aware services make the assumption that the context they use is completely accurate. however, in reality, both sensed and interpreted context is often ambiguous. a challenge facing the development of realistic and deployable context-aware services, therefore, is the ability to handle ambiguous context. although some of this ambiguity may be resolved using automatic techniques, we argue that correct handling of ambiguous context will often need to involve the user. we use the term mediation to refer to the dialogue that ensues between the user and the system. in this article, we describe an architecture that supports the building of context-aware services that assume context is ambiguous and allows for mediation of ambiguity by mobile users in aware environments. we present design guidelines that arise from supporting mediation over space and time, issues not present in the graphical user interface domain where mediation has typically been used in the past. we illustrate the use of our architecture and evaluate it through an example context-aware application, a word predictor system.\"usa\",indexicality: understanding mobile human-computer interaction in context,\"a lot of research has been done within the area of mobile computing and context-awareness over the last 15 years, and the idea of systems adapting to their context has produced promising results for overcoming some of the challenges of user interaction with mobile devices within various specialized domains. however, today it is still the case that only a limited body of theoretically grounded knowledge exists that can explain the relationship between users, mobile system user interfaces, and their context. lack of such knowledge limits our ability to elevate learning from the mobile systems we develop and study from a concrete to an abstract level. consequently, the research field is impeded in its ability to leap forward and is limited to incremental steps from one design to the next. addressing the problem of this void, this article contributes to the body of knowledge about mobile interaction design by promoting a theoretical approach for describing and understanding the relationship between user interface representations and user context. specifically, we promote the concept of indexicality derived from semiotics as an analytical concept that can be used to describe and understand a design. we illustrate the value of the indexicality concept through an analysis of empirical data from evaluations of three prototype systems in use. based on our analytical and empirical work we promote the view that users interpret information in a mobile computer user interface through creation of meaningful indexical signs based on the ensemble of context and system.\"\"school of information and library science, university of north carolina at chapel hill, 100 manning hall, cb \\\\#3360, chapel hill, nc 27599-3360\",\"measuring online information seeking context, part 2: findings and discussion\",\"context is one of the most important concepts in information seeking and retrieval research. however, the challenges of studying context are great; thus, it is more common for researchers to use context as a post hoc explanatory factor, rather than as a concept that drives inquiry. the purpose of this study was to develop a method for collecting data about information seeking context in natural online environments, and identify which aspects of context should be considered when studying online information seeking. the study is reported in two parts. in this, the second part, results and implications of this research are presented. part 1 (kelly, 2006) discussed previous literature on information seeking context and behavior, situated the current study within this literature, and described the naturalistic, longitudinal research design that was used to examine and measure the online information seeking context of seven users during a 14-week period. results provide support for the value of the method in studying online information seeking context, the relative importance of various measures of context, how these measures change over time, and, finally, the relationship between these measures. in particular, results demonstrate significant differences in distributions of usefulness ratings according to task and topic.\"usa\",real life information retrieval: a study of user queries on the web,\"we analyzed transaction logs of a set of 51,473 queries posed by 18,113 users of excite, a major internet search service. we provide data on: (i) queries --- the number of search terms, and the use of logic and modifiers, (ii) sessions --- changes in queries during a session, number of pages viewed, and use of relevance feedback, and (iii) terms --- their rank/frequency distribution and the most highly used search terms. common mistakes are also observed. implications are discussed.\"usa\",{rcv1}: a new benchmark collection for text categorization research,\"reuters corpus volume i ({rcv1}) is an archive of over 800,000 manually categorized newswire stories recently made available by reuters, ltd. for research purposes. use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. drawing on interviews with reuters personnel and access to reuters documentation, we describe the coding policy and quality control procedures used in producing the {rcv1} data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. we refer to the original data as {rcv1}-v1, and the corrected data as {rcv1}-v2. we benchmark several widely used supervised learning methods on {rcv1}-v2, illustrating the collection\\'s properties, suggesting new directions for research, and providing baseline results for future studies. we make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.\"\"adverse drug events cause substantial morbidity and mortality and are often discovered after a drug comes to market. we hypothesized that internet users may provide early clues about adverse drug events via their online information-seeking. we conducted a large-scale study of web search log data gathered during 2010. we pay particular attention to the specific drug pairing of paroxetine and pravastatin, whose interaction was reported to cause hyperglycemia after the time period of the online logs used in the analysis. we also examine sets of drug pairs known to be associated with hyperglycemia and those not associated with hyperglycemia. we find that anonymized signals on drug interactions can be mined from search logs. compared to analyses of other sources such as electronic health records ({ehr}), logs are inexpensive to collect and mine. the results demonstrate that logs of the search activities of populations of computer users can contribute to drug safety surveillance.\"usa\",\"think locally, search globally; context based information retrieval\",\"this paper proposes the use of local context as a way to semantic information retrieval. in our model, rather than trying to formalize the contents of the documents among which the search is done (e.g. by formally annotating them), we try to automatically build a representation of the context in which the search is done. we consider that search is always done as part of an activity, and that the search context is determined by the activity that is carried out at a particular moment. many activities that a person is engaged in are carried out with the help of a computer, and leave a digital trace in the files that are created in connection to it. we take these files, and the structural relations between the directories in which they are stored, as a starting point for the representation of context.\"usa\",improving web search ranking by incorporating user behavior information,\"we show that incorporating user behavior data can significantly improve ordering of top results in real web search setting. we examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features. we report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine. we show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31\\\\% relative to the original performance.\"nj, usa\",the {smart} retrieval system---experiments in automatic document processing,an abstract is not available.usa\",do user preferences and evaluation measures line up?,\"this paper presents results comparing user preference for search engine rankings with measures of effectiveness computed from a test collection. it establishes that preferences and evaluation measures correlate: systems measured as better on a test collection are preferred by users. this correlation is established for both \"\"conventional web retrieval\"\" and for retrieval that emphasizes diverse results. the {ndcg} measure is found to correlate best with user preferences compared to a selection of other well known measures. unlike previous studies in this area, this examination involved a large population of users, gathered through crowd sourcing, exposed to a wide range of retrieval systems, test collections and search tasks. reasons for user preferences were also gathered and analyzed. the work revealed a number of new results, but also showed that there is much scope for future work refining effectiveness measures to better capture user preferences.\"usa\",unsupervised query segmentation using generative language models and wikipedia,\"in this paper, we propose a novel unsupervised approach to query segmentation, an important task in web search. we use a generative query model to recover a query\\'s underlying concepts that compose its original segmented form. the model\\'s parameters are estimated using an expectation-maximization ({em}) algorithm, optimizing the minimum description length objective function on a partial corpus that is specific to the query. to augment this unsupervised learning, we incorporate evidence from wikipedia.\"usa\",improved query difficulty prediction for the web,\"query performance prediction aims to predict whether a query will have a high average precision given retrieval from a particular collection, or low average precision. an accurate estimator of the quality of search engine results can allow the search engine to decide to which queries to apply query expansion, for which queries to suggest alternative search terms, to adjust the sponsored results, or to return results from specialized collections. in this paper we present an evaluation of state of the art query prediction algorithms, both post-retrieval and pre-retrieval and we analyze their sensitivity towards the retrieval algorithm. we evaluate query difficulty predictors over three widely different collections and query sets and present an analysis of why prediction algorithms perform significantly worse on web data. finally we introduce improved clarity, and demonstrate that it outperforms state-of-the-art predictors on three standard collections, including two large web collections.\"usa\",analysis of a very large web search engine query log,\"in this paper we present an analysis of an {altavista} search engine query log consisting of approximately 1 billion entries for search requests over a period of six weeks. this represents almost 285 million user sessions, each an attempt to fill a single information need. we present an analysis of individual queries, query duplication, and query sessions. we also present results of a correlation analysis of the log entries, studying the interaction of terms within queries. our data supports the conjecture that web users differ significantly from the user assumed in the standard information retrieval literature. specifically, we show that web users type in short queries, mostly look at the first 10 results only, and seldom modify the query. this suggests that traditional information retrieval techniques may not work well for answering web search requests. the correlation analysis showed that the most highly correlated items are constituents of phrases. this result indicates it may be useful for search engines to consider search terms as parts of phrases even if the user did not explicitly specify them as such.\"usa\",utilizing physical and social context to improve recommender systems,\"context has rarely been incorporated into recommender systems so far, but physical (e.g. a user\\'s location) or social (e.g. the social network of a user)context can be useful sources for improving recommender systems. in this paper, we first discuss some principles for context-awareness in recommender systems. then we present our hybrid recommender system for recommending applications to users of mobile devices. finally, we describe our approach to utilize social networks to enhance collaborative filtering. our evaluation shows that the social recommender outperforms traditional collaborative filtering algorithms in our scenario.\"rouen, france.\",strategies for health information retrieval.,the implemented heuristics contribute significantly with good results in maximising as much as possible the recall of the {doc\\'cismef} search tool.usa\",characterizing the influence of domain expertise on web search behavior,\"domain experts search differently than people with little or no domain knowledge. previous research suggests that domain experts employ different search strategies and are more successful in finding what they are looking for than non-experts. in this paper we present a large-scale, longitudinal, log-based analysis of the effect of domain expertise on web search behavior in four different domains (medicine, finance, law, and computer science). we characterize the nature of the queries, search sessions, web sites visited, and search success for users identified as experts and non-experts within these domains. large-scale analysis of real-world interactions allows us to understand how expertise relates to vocabulary, resource use, and search task under more realistic search conditions than has been possible in previous small-scale studies. building upon our analysis we develop a model to predict expertise based on search behavior, and describe how knowledge about domain expertise can be used to present better results and query suggestions to users and to help non-experts gain expertise.\"we research whether the inclusion of information about an information user\\'s social environment and his position in the social network of his peers leads to an improval in search effectiveness. traditional information retrieval methods fail to address the fact that information production and consumption are social activities. we ameliorate this problem by extending the domain model of information retrieval to include social networks. we describe a technique for information retrieval in such an enviroment and evaluate it in comparison to vector space retrieval.lexington 40536-0298, usa.\",biobibliometrics: information retrieval and visualization from co-occurrences of gene names in medline abstracts.,,information retrieval in context using various health terminologies,\"information retrieval is a branch of computer science concerned with the acquisition, storage, search and selection of information. from the user point of view, the access to information can be carried out in a deliberate way through an information retrieval system, or in a passive way through an information filtration system. {cismef} (catalogue and index of the french-speaking medical sites) is a health portal aiming to catalogue and index the most important french-speaking institutional health information sources in order to make them available to health professionals, medical students and the general public. the internet resources were manually indexed and have remained mono-terminological from 1995 to 2007 originally based exclusively on the {mesh} thesaurus (medical subject headings). categorization allows a contextual information retrieval parameterized according to the user\\'s needs. in 2007, the {cismef} team directed its objectives towards a multi-terminological universe by the integration of the medical data heterogeneous sources into its back-office. to date, the practical application is the creation of a bilingual ({french/english}) drug information portal in order to facilitate the user information retrieval about drugs.\",contextual ranking of keywords using click data,\"the problem of automatically extracting the most interesting and relevant keyword phrases in a document has been studied extensively as it is crucial for a number of applications. these applications include contextual advertising, automatic text summarization, and user-centric entity detection systems. all these applications can potentially benefit from a successful solution as it enables computational efficiency (by decreasing the input size), noise reduction, or overall improved user {satisfaction.in} this paper, we study this problem and focus on improving the overall quality of user-centric entity detection systems. first, we review our concept extraction technique, which relies on search engine query logs. we then define a new feature space to represent the interesting ness of concepts, and describe a new approach to estimate their relevancy for a given context. we utilize click through data obtained from a large scale user-centric entity detection system - contextual shortcuts - to train a model to rank the extracted concepts, and evaluate the resulting model extensively again based on their click through data. our results show that the learned model outperforms the baseline model, which employs similar features but whose weights are tuned carefully based on empirical observations, and reduces the error rate from 30.22\\\\% to 18.66\\\\%.\"management of users\\' privacy preferences in context,\"there has been intensive research on user-controlled privacy from the perspective of agent automation of privacy related user tasks. the {w3c}\\'s platform for privacy preferences ({p3p}) specifies standards that can be used by {p3p} agents to automatically retrieve a web-site\\'s privacy policy on how the users\\' data is collected, stored, and managed, and then to determine whether they are compatible with the user\\'s privacy preferences. current approaches to managing user\\'s privacy do not capture context, are not user-friendly, and do cater well to the dynamic nature of privacy preferences very well. clearly, the user\\'s privacy preferences depend on the context of the user\\'s online activity and the user\\'s preferences evolve with user\\'s experience and changing levels of trust in various organizations and domains. we propose a model for user\\'s privacy preferences that incorporates the context for user activity and we apply it using a case-based reasoning ({cbr}) approach that relates the current activity to previous activities stored in the case-base and thus forms an intuitive and understandable process. we describe the context model for privacy preferences, how {cbr} is used to create a new contextual case from the web-site\\'s privacy policy and the user\\'s current activity, and how the {cbr} retrieves matching cases to be applied to the retrieved privacy policy.\"interacting with context,\"context is dodgy - just as the human computer user: hard to predict, erroneous, and probabilistic in nature. linking the two together i.e. creating context-aware user interfaces ({uis}) remains a great challenge in computer science since ubiquitous computing calls for lean, situated, and focused {uis} that can be operated on the move or intertwined with primary tasks grabbing the user\\'s attention. the paper reviews major categories of context that matter at the seam of humans and computers, emphasizing quality issues. approaches to the marriage of context-awareness and user modeling are highlighted, including our own approach. both sides of the coin are inspected: the improvement of {uis} by means of quality attributed context information and, to a lesser extent, the challenge to convey context quality to the user as part of the interaction.\"usa\",a longitudinal study of how highlighting web content change affects people\\'s web interactions,\"the web is constantly changing, but most tools used to access web content deal only with what can be captured at a single instance in time. as a result, web users may not have a good understanding of the changes that occur. in this paper we show that making web content change explicitly visible allows people to interact with the web in new ways. we present a longitudinal study in which 30 people used a web browser plug-in that caches visited pages and highlights text changes to those pages when revisited. we used a survey to capture their understanding of web page change and their own revisitation patterns at the beginning of use and after one month. for a majority of the participants, we also logged their web page visits and associated content change. exposing change is more valuable to our participants than initially expected, making them aware of how dynamic content they visit is and changing their interactions with it.\"\"this paper provides an introduction to interactive information retrieval--the study of human interaction with information retrieval systems. interactive information retrieval may be contrasted with the \\\\&amp;quot;system-centered \\\\&amp;quot; view of information retrieval in which changes to information retrieval system variables are manipulated in isolation from users in laboratory situations. the paper elucidates current models of interactive information retrieval, namely, the episodic model, the stratified model, the interactive feedback and search process model, and the global model of polyrepresentation. future directions for research in the field are discussed.\"usa\",knowledge-based query expansion to support scenario-specific retrieval of medical free text,\"in retrieving medical free text, users are often interested in answers relevant to certain scenarios, scenarios that correspond to common tasks in medical practice, e.g.,  ” treatment” or  ” diagnosis” of a disease. consequently, the queries they pose are often scenario-specific, e.g.,  ” lung cancer, treatment.” a fundamental challenge in handling such queries is that scenario terms in the query (e.g.  ” treatment”) are too general to match specialized terms in relevant documents (e.g.  ” lung excision”). in this paper we propose a knowledge-based query expansion method that exploits the {umls} knowledge source to append the original query with additional terms that are specifically relevant to the query\\'s scenario(s). we compare the proposed method with statistical expansion that only explores statistical term correlation and expands terms that are not necessarily scenario specific. our study on the {ohsumed} testbed shows that the knowledge-based method which results in scenario-specific expansion is able to improve more than 5\\\\% over the statistical method on average, and about 10\\\\% for queries that mention certain scenarios, such as  ” treatment of a disease” and  ” differential diagnosis of a symptom/disease.”\",big macs and eigenfactor scores: don\\'t let correlation coefficients fool you,\"abstract 10.1002/asi.21374.abs the eigenfactor™ metrics provide an alternative way of evaluating scholarly journals based on an iterative ranking procedure analogous to google\\'s {pagerank} algorithm. these metrics have recently been adopted by thomson reuters and are listed alongside the impact factor in the journal citation reports. but do these metrics differ sufficiently so as to be a useful addition to the bibliometric toolbox? davis (2008) has argued that they do not, based on his finding of a 0.95 correlation coefficient between eigenfactor score and total citations for a sample of journals in the field of medicine. this conclusion is mistaken; in this article, we illustrate the basic statistical fallacy to which davis succumbed. we provide a complete analysis of the 2006 journal citation reports and demonstrate that there are statistically and economically significant differences between the information provided by the eigenfactor metrics and that provided by impact factor and total citations.\"ny, usa\",new measurements for search engine evaluation proposed and tested,\"a set of measurements is proposed for evaluating web search engine performance. some measurement are adapted from the concepts of recall and precision, which are commonly used in evaluating traditional information retrieval systems. others are newly developed to evaluate search engine stability, an issue unique to web information retrieval systems. an experiment was conducted to test these new measurements by applying them to a performance comparison of three commercial search engines: google, {altavista}, and teoma. twenty-four subjects ranked four sets of web pages and their rankings were used as benchmarks against which to compare search engine performance. results show that the proposed measurements are able to distinguish search engine performance very well.\"usa\",using context to improve predictive modeling of customers in personalization applications,\"the idea that context is important when predicting customer behavior has been maintained by scholars in marketing and data mining. however, no systematic study measuring how much the contextual information really matters in building customer models in personalization applications has been done before. in this paper, we study how important the contextual information is when predicting customer behavior and how to use it when building customer models. it is done by conducting an empirical study across a wide range of experimental conditions. the experimental results show that context does matter when modeling the behavior of individual customers and that it is possible to infer the context from the existing data with reasonable accuracy in certain cases. it is also shown that significant performance improvements can be achieved if the context is \"\"cleverly\"\" modeled, as described in this paper. these findings have significant implications for data miners and marketers. they show that contextual information does matter in personalization and companies have different opportunities to both make context valuable for improving predictive performance of customers\\' behavior and decreasing the costs of gathering contextual information.\"usa\",comparing collaborative and independent search in a recall-oriented task,\"search interfaces are mainly designed to support a single searcher at a time. we therefore have a limited understanding of how an interface can support search where more than one searcher concurrently pursues a shared information need. this paper investigated the performance and user behaviour of concurrent search. based on a recall-oriented search task, a user study was carried out to compare an independent search condition to collaborative search conditions. the results show that the collaborative conditions helped searchers diversify search vocabulary while reducing redundant documents to be bookmarked within teams. however, these effects were found to be insufficient to improve the retrieval effectiveness. we discussed the implications for concurrent search support based on our findings.\"usa\",advances and challenges in log analysis,logs contain a wealth of information to help manage systems.ny, usa\",\"a re-examination of relevance: toward a dynamic, situational definition\",an abstract is not available.,persona: a contextualized and personalized web search,\"recent advances in graph-based search techniques derived from kleinberg\\'s (1997) work have been impressive. this paper further improves the graph-based search algorithm in two dimensions. firstly, variants of kleinberg\\'s techniques do not take into account the semantics of the query string nor of the nodes being searched. as a result, polysemy of query words cannot be resolved. this paper presents an interactive query scheme utilizing the simple web taxonomy provided by the open directory project to resolve meanings of a user query. secondly, we extend a recently proposed personalized version of the kleinberg algorithm (chang et al., 2000). simulation results are presented to illustrate the sensitivity of our technique. we outline the implementation of our algorithm in the persona personalized web search system.\"ny, usa\",an evaluation of adaptive filtering in the context of realistic task-based information exploration,iowa city, usa.\",using the world wide web to answer clinical questions: how efficient are different methods of information retrieval?,\"{background}: the world wide web (web) has the potential to revolutionize information retrieval in medicine. however, the best method of information retrieval from the web is not known. the purpose of our study was to compare medical search engines, general-purpose search engines, medical meta-lists, and commercial sites on the web with regard to their efficiency in retrieving medical information. {methods}: ten questions were identified from a database of questions posed by primary care clinicians. authoritative answers were identified. searches were performed using 1 commercial site, 4 general search engines, 9 medicine-specific search engines, and 2 medical meta-lists. the main outcome measures were the number of questions answered by each web site, the correctness of the answers, the number of links followed to get an answer, and how well documented the answer was using the health on the net criteria. {results}: {md} consult, a commercial site, answered 6 of 10 questions. hardin {md} (a meta-list) and excite and {hotbot} (general search engines) each answered 5 questions. the medicine-specific search engines performed poorly, answering an average of only 1 question. {md} consult and {hotbot} required the least number of links to find an answer. {md} consult and hardin {md} had the best documented answers. {conclusions}: medicine-specific search engines on the web fare poorly in answering clinical questions when compared with general search engines. {md} consult, excite, {hotbot}, and hardin {md} found the greatest number of answers.\"\"{abstract\\\\&nbsp;\\\\&nbsp;thousands} of users issue keyword queries to the web search engines to find information on a number of topics. since the users may have diverse backgrounds and may have different expectations for a given query, some search engines try to personalize their results to better match the overall interests of an individual user. this task involves two great challenges. first the search engines need to be able to effectively identify the user interests and build a profile for every individual user. second, once such a profile is available, the search engines need to rank the results in a way that matches the interests of a given user. in this article, we present our work towards a personalized web search engine and we discuss how we addressed each of these challenges. since users are typically not willing to provide information on their personal preferences, for the first challenge, we attempt to determine such preferences by examining the click history of each user. in particular, we leverage a topical ontology for estimating a user\\'s topic preferences based on her past searches, i.e. previously issued queries and pages visited for those queries. we then explore the semantic similarity between the user\\'s current query and the query-matching pages, in order to identify the user\\'s current topic preference. for the second challenge, we have developed a ranking function that uses the learned past and current topic preferences in order to rank the search results to better match the preferences of a given user. our experimental evaluation on the google query-stream of human subjects over a period of 1\\\\&nbsp;month shows that user preferences can be learned accurately through the use of our topical ontology and that our ranking function which takes into account the learned user preferences yields significant improvements in the quality of the search results.\"usa\",relevance criteria for e-commerce: a crowdsourcing-based experimental analysis,\"we discuss the concept of relevance criteria in the context of {e-commerce} search. a vast body of research literature describes the beyond-topical criteria used to determine the relevance of the document to the need. we argue that in an {e-commerce} scenario there are some differences, and novel and different criteria can be used to determine relevance. we experimentally validate this hypothesis by means of amazon mechanical turk using a crowdsourcing approach.\"usa\",a basis for information retrieval in context,\"information retrieval ({ir}) models based on vector spaces have been investigated for a long time. nevertheless, they have recently attracted much research interest. in parallel, context has been rediscovered as a crucial issue in information retrieval. this article presents a principled approach to modeling context and its role in ranking information objects using vector spaces. first, the article outlines how a basis of a vector space naturally represents context, both its properties and factors. second, a ranking function computes the probability of context in the objects represented in a vector space, namely, the probability that a contextual factor has affected the preparation of an object.\"ny, usa\",characteristics of question format web queries: an exploratory study,\"web queries in question format are becoming a common element of a user\\'s interaction with web search engines. web search services such as ask jeeves - a publicly accessible question and answer ({q\\\\&a}) search engine-request users to enter question format queries. this paper provides results from a study examining queries in question format submitted to two different web search engines - ask jeeves that explicitly encourages queries in question format and the excite search service that does not explicitly encourage queries in question format. we identify the characteristics of queries in question format in two different data sets: (1) 30,000 ask jeeves queries and 15,575 excite queries, including the nature, length, and structure of queries in question format. findings include: (1) 50\\\\% of ask jeeves queries and less than 1\\\\% of excite were in question format, (2) most users entered only one query in question format with little query reformulation, (3) limited range of formats for queries in question format - mainly \"\"where\"\", \"\"what\"\", or \"\"how\"\" questions, (4) most common question query format was \"\"where can i find ......... \"\" for general information on a topic, and (5) non-question queries may be in request format. overall, four types of user web queries were identified: keyword, boolean, question, and request. these findings provide an initial mapping of the structure and content of queries in question and request format. implications for web search services are discussed.\"usa\",implicit user modeling for personalized search,\"information retrieval systems (e.g., web search engines) are critical for overcoming information overload. a major deficiency of existing retrieval systems is that they generally lack user modeling and are not adaptive to individual users, resulting in inherently non-optimal retrieval performance. for example, a tourist and a programmer may use the same word \"\"java\"\" to search for different information, but the current search systems would return the same results. in this paper, we study how to infer a user\\'s interest from the user\\'s search context and use the inferred implicit user model for personalized search. we present a decision theoretic framework and develop techniques for implicit user modeling in information retrieval. we develop an intelligent client-side web search agent ({ucair}) that can perform eager implicit feedback, e.g., query expansion based on previous queries and immediate result reranking based on clickthrough information. experiments on web search show that our search agent can improve search accuracy over the popular google search engine.\"usa\",evaluation of contextual information retrieval effectiveness: overview of issues and research,\"the increasing prominence of information arising from a wide range of sources delivered over electronic media has made traditional information retrieval systems less effective. indeed, users are overwhelmed by the information delivered by such systems in response to their queries, particularly when the latter are ambiguous. in order to tackle this problem, the state-of-the-art reveals that there is a growing interest towards contextual information retrieval which relies on various sources of evidence issued from the user\\'s search background and environment like interests, preferences, time and location, in order to improve the retrieval accuracy. contextual information retrieval systems are based on different definitions of the core concept of user\\'s context, various user\\'s context modeling approaches and several techniques of document relevance measurement, but all share the goal of providing the most useful information to the users in accordance with their context. however, the evaluation methodologies conceived in the past several years for traditional information retrieval and widely used in the evaluation campaigns have been challenged by the consideration of user\\'s context in the information retrieval process. thus, we recognize that a critical review of existing evaluation methodologies in contextual information retrieval area is needed in order to design and develop standard evaluation frameworks. we present in this paper a comprehensive survey of contextual information retrieval evaluation methodologies and provide insights into how and why they are appropriate to measure the retrieval effectiveness. we also highlight some of the research challenges ahead that would constitute substantive research area for future research.\"usa\",towards task-based personal information management evaluations,\"personal information management ({pim}) is a rapidly growing area of research concerned with how people store, manage and refind information. a feature of {pim} research is that many systems have been designed to assist users manage and refind information, but very few have been evaluated. this has been noted by several scholars and explained by the difficulties involved in performing {pim} evaluations. the difficulties include that people re-find information from within unique personal collections; researchers know little about the tasks that cause people to re-find information; and numerous privacy issues concerning personal information. in this paper we aim to facilitate {pim} evaluations by addressing each of these difficulties. in the first part, we present a diary study of information re-finding tasks. the study examines the kind of tasks that require users to refind information and produces a taxonomy of refinding tasks for email messages and web pages. in the second part, we propose a task-based evaluation methodology based on our findings and examine the feasibility of the approach using two different methods of task creation.\"usa\",meeting of the {minds}: an information retrieval research agenda,\"since its inception in the late 1950s, the field of information retrieval ({ir}) has developed tools that help people find, organize, and analyze information. the key early influences on the field are well-known. among them are h. p. luhn\\'s pioneering work, the development of the vector space retrieval model by salton and his students, cleverdon\\'s development of the cranfield experimental methodology, sp\\\\\"\"{a}rck jones\\' development of idf, and a series of probabilistic retrieval models by robertson and croft. until the development of the {worldwideweb} (web), {ir} was of greatest interest to professional information analysts such as librarians, intelligence analysts, the legal community, and the pharmaceutical industry.\",2009-02-04 18:26:02,,contexts of relevance for information retrieval system design,\"users judge relevance in various dimensions, but systems traditionally only support matching of queries to documents or document representations on an algorithmic or topical level. we argue that systems should support users in order for them to make relevance judgements on the level of cognitive relevance, situational relevance, and socio-cognitive relevance as well. current studies in the field of information retrieval and seeking are discussed from a relevance point of view, in order to show how systems might be adapted to assist users in making multi-dimensional relevance judgements.\"usa\",personalized information retrieval based on context and ontological knowledge,\"context modeling has long been acknowledged as a key aspect in a wide variety of problem domains. in this paper we focus on the combination of contextualization and personalization methods to improve the performance of personalized information retrieval. the key aspects in our proposed approach are (1) the explicit distinction between historic user context and live user context, (2) the use of ontology-driven representations of the domain of discourse, as a common, enriched representational ground for content meaning, user interests, and contextual conditions, enabling the definition of effective means to relate the three of them, and (3) the introduction of fuzzy representations as an instrument to properly handle the uncertainty and imprecision involved in the automatic interpretation of meanings, user attention, and user wishes. based on a formal grounding at the representational level, we propose methods for the automatic extraction of persistent semantic user preferences, and live, ad-hoc user interests, which are combined in order to improve the accuracy and reliability of personalization for retrieval.\"usa\",a historical view of context,\"this paper examines a number of the approaches, origins and ideals of context-aware systems design, looking particularly at the way that history influences what we do in our ongoing activity. as a number of sociologists and philosophers have pointed out, past social interaction, as well as past use of the heterogeneous mix of media, tools and artifacts that we use in our everyday activity, influence our ongoing interaction with the people and media at hand. we suggest that one\\'s experience and history is thus part of one\\'s current context, with patterns of use temporally and subjectively combining and interconnecting different media as well as different modes of use of those media. one such mode of use is transparent use, put forward by weiser as ubicomp\\'s design ideal. one theoretical finding is that this design ideal is unachievable or incomplete because transparent and more focused analytical use are interdependent, affecting and feeding into each other through one\\'s experience and history. using these theoretical points, we discuss a number of context-aware system designs that make good use of history in supporting ongoing user activity.\"query expansion using external evidence,\"automatic query expansion may be used in document retrieval to improve search effectiveness. traditional query expansion methods are based on the document collection itself. for example, pseudo-relevance feedback ({prf}) assumes that the top retrieved documents are relevant, and uses the terms extracted from those documents for query expansion. however, there are other sources of evidence that can be used for expansion, some of which may give better search results with greater efficiency at query time. in this paper, we use the external evidence, especially the hints obtained from external web search engines to expand the original query. we explore 6 different methods using search engine query log, snippets and search result documents. we conduct extensive experiments, with state of the art {prf} baselines and careful parameter tuning, on three {trec} collections: {ap}, {wt10g}, {gov2}. log-based methods do not show consistent significant gains, despite being very efficient at query-time. snippet-based expansion, using the summaries provided by an external search engine, provides significant effectiveness gains with good efficiency at query-time.\"usa\",personalized news recommendation based on click behavior,\"online news reading has become very popular as the web provides access to news articles from millions of sources around the world. a key challenge of news websites is to help users find the articles that are interesting to read. in this paper, we present our research on developing personalized news recommendation system in google news. for users who are logged in and have explicitly enabled web history, the recommendation system builds profiles of users\\' news interests based on their past click behavior. to understand how users\\' news interests change over time, we first conducted a large-scale analysis of anonymized google news users click logs. based on the log analysis, we developed a bayesian framework for predicting users\\' current news interests from the activities of that particular user and the news trends demonstrated in the activity of all users. we combine the content-based recommendation mechanism which uses learned user profiles with an existing collaborative filtering mechanism to generate personalized news recommendations. the hybrid recommender system was deployed in google news. experiments on the live traffic of google news website demonstrated that the hybrid method improves the quality of news recommendation and increases traffic to the site.\"official medical association of barcelona, spain.\",\"{medcircle}: collaboration for internet rating, certification, labelling and evaluation of health information on the {world-wide}-web.\",\"we describe {medcircle}, an {eu}-funded semantic web project to implement the first steps towards a global, collaborative rating and guidance system for health information proposed in the {medcertain} project. in {medcircle}, three european gateway sites for consumer health information will implement the metadata vocabulary {hiddel} (health information disclosure, description and evaluation language). {hiddel} allows portals and gateways to make the results of their evaluations accessible as {xml}/{rdf}. the three participating national portals are: {aqumed} (agency for quality in medicine) patienten-information, de, {comb} (official medical college of barcelona) and {cismef}, a quality-controlled health gateway developed at rouen university hospital. other health subject gateways, accreditation, or rating services are invited to join the collaboration simply by implementing {hiddel} on their gateways. widespread implementation {hiddel} will allow intelligent agents or client-side software to harvest statements and opinions about the trustworthiness of other websites, assisting users in selecting trustworthy websites. the {medcircle} project builds on, expands and continues work on rating health information on the internet piloted within the {medcertain} project. while {medcertain} provided the core technologies and software for rating and \"\"trustmarking\"\" health information, {medcircle} is built around these technologies and involves a wider medical community to assess health information, demonstrating the power of collaborative and interoperable evaluations in a semantic web environment. {medcircle} is a project with the overall objective to develop and promote technologies able to guide consumers to trustworthy health information on the internet, to establish a global web of trust for networked health information, and to empower consumers to positively select high quality health information on the web. other aims include refinement and expansion of {hiddel}, to become a standard vocabulary and interchange format for self- and third-party ratings of health information.\"usa\",toward a multidisciplinary model of context to support context-aware computing,\"capturing, defining, and modeling the essence of context are challenging, compelling, and prominent issues for interdisciplinary research and discussion. the roots of its emergence lie in the inconsistencies and ambivalent definitions across and within different research specializations (e.g., philosophy, psychology, pragmatics, linguistics, computer science, and artificial intelligence). within the area of computer science, the advent of mobile context-aware computing has stimulated broad and contrasting interpretations due to the shift from traditional static desktop computing to heterogeneous mobile environments. this transition poses many challenging, complex, and largely unanswered research issues relating to contextual interactions and usability. to address those issues, many researchers strongly encourage a multidisciplinary approach. the primary aim of this article is to review and unify theories of context within linguistics, computer science, and psychology. summary models within each discipline are used to propose an outline and detailed multidisciplinary model of context involving (a) the differentiation of focal and contextual aspects of the user and application\\'s world, (b) the separation of meaningful and incidental dimensions, and (c) important user and application processes. the models provide an important foundation in which complex mobile scenarios can be conceptualized and key human and social issues can be identified. the models were then applied to different applications of context-aware computing involving user communities and mobile tourist guides. the authors\\' future work involves developing a user-centered multidisciplinary design framework (based on their proposed models). this will be used to design a large-scale user study investigating the usability issues of a context-aware mobile computing navigation aid for visually impaired people.\"usa\",personalized web exploration with task models,\"personalized web search has emerged as one of the hottest topics for both the web industry and academic researchers. however, the majority of studies on personalized search focused on a rather simple type of search, which leaves an important research topic - the personalization in exploratory searches - as an under-studied area. in this paper, we present a study of personalization in task-based information exploration using a system called {tasksieve}. {tasksieve} is a web search system that utilizes a relevance feedback based profile, called a \"\"task model\"\", for personalization. its innovations include flexible and user controlled integration of queries and task models, task-infused text snippet generation, and on-screen visualization of task models. through an empirical study using human subjects conducting task-based exploration searches, we demonstrate that {tasksieve} pushes significantly more relevant documents to the top of search result lists as compared to a traditional search system. {tasksieve} helps users select significantly more accurate information for their tasks, allows the users to do so with higher productivity, and is viewed more favorably by subjects under several usability related characteristics.\"usa\",predicting short-term interests using activity-based search context,\"a query considered in isolation offers limited information about a searcher\\'s intent. query context that considers pre-query activity (e.g., previous queries and page visits), can provide richer information about search intentions. in this paper, we describe a study in which we developed and evaluated user interest models for the current query, its context (from pre-query session activity), and their combination, which we refer to as intent. using large-scale logs, we evaluate how accurately each model predicts the user\\'s short-term interests under various experimental conditions. in our study we: (i) determine the extent of opportunity for using context to model intent; (ii) compare the utility of different sources of behavioral evidence (queries, search result clicks, and web page visits) for building predictive interest models, and; (iii) investigate optimally combining the query and its context by learning a model that predicts the context weight for each query. our findings demonstrate significant opportunity in leveraging contextual information, show that context and source influence predictive accuracy, and show that we can learn a near-optimal combination of the query and context for each query. the findings can inform the design of search systems that leverage contextual information to better understand, model, and serve searchers\\' information needs.\"health {information—seeking} behavior,\"seeking information about one\\'s health is increasingly documented as a key coping strategy in health-promotive activities and psychosocial adjustment to illness. in this article, the authors critically examine the scientific literature from 1982 to 2006 on the concept of health information—seeking behavior ({hisb}) to determine its level of maturity and clarify the concept\\'s essential characteristics. a principle-based method of concept analysis provides the framework for exploring the nature of {hisb}. the authors reviewed approximately 100 published articles and five books reporting on {hisb}. although {hisb} is a popular concept used in various contexts, most {hisb} definitions provide little insight into the concept\\'s specific meanings. the authors describe the concept\\'s characteristics, contributing to a clearer understanding of {hisb}, and discuss operationalizations, antecedents, and outcomes of {hisb}. such an analysis of {hisb} might guide further theorizing on this highly relevant concept and assist health care providers in designing optimal informational interventions.\"usa\",what are you looking for?: an eye-tracking study of information usage in web search,\"web search services are among the most heavily used applications on the world wide web. perhaps because search is used in such a huge variety of tasks and contexts, the user interface must strike a careful balance to meet all user needs. we describe a study that used eye tracking methodologies to explore the effects of changes in the presentation of search results. we found that adding information to the contextual snippet significantly improved performance for informational tasks but degraded performance for navigational tasks. we discuss possible reasons for this difference and the design implications for better presentation of search results.\"usa\",an introduction to {roc} analysis,\"receiver operating characteristics ({roc}) graphs are useful for organizing classifiers and visualizing their performance. {roc} graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. although {roc} graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. the purpose of this article is to serve as an introduction to {roc} graphs and as a guide for using them in research.\"dresden, germany.\",{gopubmed}: exploring {pubmed} with the gene ontology,\"the biomedical literature grows at a tremendous rate and {pubmed} comprises already over 15\\u2009000\\u2009000 abstracts. finding relevant literature is an important and difficult problem. we introduce {gopubmed}, a web server which allows users to explore {pubmed} search results with the gene ontology ({go}), a hierarchically structured vocabulary for molecular biology. {gopubmed} provides the following benefits: first, it gives an overview of the literature abstracts by categorizing abstracts according to the {go} and thus allowing users to quickly navigate through the abstracts by category. second, it automatically shows general ontology terms related to the original query, which often do not even appear directly in the abstract. third, it enables users to verify its classification because {go} terms are highlighted in the abstracts and as each term is labelled with an accuracy percentage. fourth, exploring {pubmed} abstracts with {gopubmed} is useful as it shows definitions of {go} terms without the need for further look up. {gopubmed} is online at www.gopubmed.org. querying is currently limited to 100 papers per query.\"\"information overload is a problem for users of {medline}, the database of biomedical literature that indexes over 17 million articles. various techniques have been developed to retrieve high quality or important articles. some techniques rely on using the number of citations as a measurement of an article\\'s importance. unfortunately, citation information is proprietary, expensive, and suffers from  ” citation lag.” {medline} users have a variety of information needs. although some users require high recall, many users are looking for a  ” few good articles” on a topic. for these users, precision is more important than recall. we present and evaluate a method for identifying articles likely to be highly cited by using information available at the time of listing in {medline}. the method uses a score based on medical subject headings ({mesh}) terms, journal impact factor ({jif}), and number of authors. this method can filter large {medline} result sets (>1000 articles) returned by actual user queries to produce small, highly cited result sets.\"\"although we see the positive results of information retrieval research embodied throughout the internet, on our computer desktops, and in many other aspects of daily life, at the same time we notice that people still have a wide variety of difficulties in finding information that is useful in resolving their problematic situations. this suggests that there still remain substantial challenges for research in {ir}. already in 1988, on the occasion of receiving the {acm} {sigir} gerard salton award, karen sp\\\\\"\"{a}rck jones suggested that substantial progress in information retrieval was likely only to come through addressing issues associated with users (actual or potential) of {ir} systems, rather than continuing {ir} research\\'s almost exclusive focus on document representation and matching and ranking techniques. in recent years it appears that her message has begun to be heard, yet we still have relatively few substantive results that respond to it. in this talk, i identify a few challenges for {ir} research which fall within the scope of association with users, and which i believe, if properly addressed, are likely to lead to substantial increases in the usefulness, usability and pleasurability of information retrieval.\"ny, usa\",a model for understanding collaborative information behavior in context: a study of two healthcare teams,brisbane, q4102, australia. hangwitang@yahoo.com\",googling for a diagnosis—use of google as a diagnostic aid: internet based study,usa\",eye-tracking analysis of user behavior in {www} search,\"we investigate how users interact with the results page of a {www} search engine using eye-tracking. the goal is to gain insight into how users browse the presented abstracts and how they select links for further exploration. such understanding is valuable for improved interface design, as well as for more accurate interpretations of implicit feedback (e.g. clickthrough) for machine learning. the following presents initial results, focusing on the amount of time spent viewing the presented abstracts, the total number of abstract viewed, as well as measures of how thoroughly searchers evaluate their results set.\"nj, usa\",patterns of search: analyzing and modeling web query refinement,an abstract is not available.\"{background}:information overload, increasing time constraints, and inappropriate search strategies complicate the detection of clinical practice guidelines ({cpgs}). the aim of this study was to provide clinicians with recommendations for search strategies to efficiently identify relevant {cpgs} in {sumsearch} and google {scholar.methods}:we compared the retrieval efficiency (retrieval performance) of search strategies to identify {cpgs} in {sumsearch} and google scholar. for this purpose, a two-term {glad} ({guideline} and disease) strategy was developed, combining a defined {cpg} term with a specific disease term ({mesh} term). we used three different {cpg} terms and nine {mesh} terms for nine selected diseases to identify the most efficient {glad} strategy for each search engine. the retrievals for the nine diseases were pooled. to compare {glad} strategies, we used a manual review of all retrievals as a reference standard. the {cpgs} detected had to fulfil predefined criteria, e.g., the inclusion of therapeutic recommendations. retrieval performance was evaluated by calculating so-called diagnostic parameters (sensitivity, specificity, and \"\"number needed to read\"\" [{nnr}]) for search {strategies.results}:the search yielded a total of 2830 retrievals; 987 (34.9\\\\%) in google scholar and 1843 (65.1\\\\%) in {sumsearch}. altogether, we found 119 unique and relevant guidelines for nine diseases (reference standard). overall, the {glad} strategies showed a better retrieval performance in {sumsearch} than in google scholar. the performance pattern between search engines was similar: search strategies including the term \"\"guideline\"\" yielded the highest sensitivity ({sumsearch}: 81.5\\\\%; google scholar: 31.9\\\\%), and search strategies including the term \"\"practice guideline\"\" yielded the highest specificity ({sumsearch}: 89.5\\\\%; google scholar: 95.7\\\\%), and the lowest {nnr} ({sumsearch}: 7.0; google scholar: {9.3).conclusion}:{sumsearch} is a useful tool to swiftly gain an overview of available {cpgs}. its retrieval performance is superior to that of google scholar, where a search is more time consuming, as substantially more retrievals have to be reviewed to detect one relevant {cpg}. in both search engines, the {cpg} term \"\"guideline\"\" should be used to obtain a comprehensive overview of {cpgs}, and the term \"\"practice guideline\"\" should be used if a less time consuming approach for the detection of {cpgs} is desired.\"\"the context in which a search takes place affects the information retrieval ({ir}) process. it affects the searcher\\'s interaction with the {ir} system, his expectations and his decisions about the documents he retrieves. therefore, knowing more about what features are important in a searcher\\'s context and what they are used for, can help design more useful and successful {ir} systems. this paper has three main contributions. it starts with a literature review on the definition of context and on context taxonomies (1). a systematic representation of context features and uses, based on related work, is then proposed (2) and used in a survey on the use of context features in {ir} (3). this analysis has concluded that interaction context is the most used category of features and indexing and searching are the tasks where context features are most employed. this work, an initial phase of a {phd} research, provides a systematic review of what is being done in the area and proposes a taxonomy for {ir}.\"\"search has arguably become the dominant paradigm for finding information on the world wide web. in order to build a successful search engine, there are a number of challenges that arise where techniques from artificial intelligence can be used to have a significant impact. in this paper, we explore a number of problems related to finding information on the web and discuss approaches that have been employed in various research programs, including some of those at google. specifically, we examine issues of such as web graph analysis, statistical methods for inferring meaning in text, and the retrieval and analysis of newsgroup postings, images, and sounds. we show that leveraging the vast amounts of data on web, it is possible to successfully address problems in innovative ways that vastly improve on standard, but often data impoverished, methods. we also present a number of open research problems to help spur further research in these areas.\"usa\",affective feedback: an investigation into the role of emotions in the information seeking process,\"user feedback is considered to be a critical element in the information seeking process, especially in relation to relevance assessment. current feedback techniques determine content relevance with respect to the cognitive and situational levels of interaction that occurs between the user and the retrieval system. however, apart from real-life problems and information objects, users interact with intentions, motivations and feelings, which can be seen as critical aspects of cognition and decision-making. the study presented in this paper serves as a starting point to the exploration of the role of emotions in the information seeking process. results show that the latter not only interweave with different physiological, psychological and cognitive processes, but also form distinctive patterns, according to specific task, and according to specific user.\"usa.\",using concept relations to improve ranking in information retrieval.,\"despite improved search engine technology, most searches return numerous documents not directly related to the query. this problem is mitigated if relevant documents appear high on a ranked list of search results. we propose that some queries and the underlying information needs can be modeled as relationships between concepts (relations), and we match relations in queries to relations in documents to try to improve ranking of search results. we investigate four techniques to identify two relationships important in medicine, causes and treats, to improve the ranking of medical text documents relevant to clinical questions about causation and treatment. preliminary results suggest that identifying relation instances can improve the ranking of search results.\"usa\",exploiting contextual information in recommender systems,\"recommender systems help an on-line user to tame information overload and are being used now in complex domains where it could be beneficial to exploit context-awareness, e.g., in travel recommendation. technically, in recommender systems we can interpret context as a set of constraints or preferences over the usage of items determined by the contextual conditions (e.g., today it is raining or the user is in a particular location). in fact, there is a lack of approaches to deal effectively with contextual data. this thesis investigates some approaches to exploit context in recommender systems. it provides a general architecture of context-aware recommender systems and analyzes separate components of this model. the main focus is to investigate new approaches that can bring a real added value to users. in this paper i also describe my initial results on item selection and item weighting for context-dependent collaborative filtering ({cf}). moreover, i shall present my ongoing research on {cf} hybridization using context.\"usa\",contextual relevance feedback in web information retrieval,\"in this paper, we present an alternative approach to the problem of contextual relevance feedback in web-based information retrieval. our approach utilises a rich contextual model that exploits a user\\'s implicit and explicit data. each user\\'s implicit data are gathered from their internet search histories on their local machine. the user\\'s explicit data are captured from a lexical database, a shared contextual knowledge base and domain-specific concepts using data mining techniques and a relevance feedback approach. this data is later used by our approach to modify queries to more accurately reflect the user\\'s interests as well as to continually build the user\\'s contextual profile and a shared contextual knowledge base. finally, the approach retrieves personalised or contextual search results from the search engine using the modified/expanded query. preliminary experiments indicate that our approach has the potential to not only aid in the contextual relevance feedback but also contribute towards the long term goal of intelligent relevance feedback in web-based information retrieval.\"\"we describe the development of an abbreviated version of the test of functional health literacy in adults ({tofhla}) to measure patients\\' ability to read and understand health-related materials. the {tofhla} was reduced from 17 numeracy items and 3 prose passages to 4 numeracy items and 2 prose passages ({s-tofhla}). the maximum time for administration was reduced from 22 minutes to 12. in a group of 211 patients given the {s-tofhla}, cronbach\\'s alpha was 0.68 for the 4 numeracy items and 0.97 for the 36 items in the 2 prose passages. the correlation (spearman) between the {s-tofhla} and the rapid estimate of adult literacy in medicine ({realm}) was 0.80, although there were important disagreements between the two tests. the {s-tofhla} is a practical measure of functional health literacy with good reliability and validity that can be used by health educators to identify individuals who require special assistance to achieve learning goals.\"ny, usa\",\"real life, real users, and real needs: a study and analysis of user queries on the web\",an abstract is not available.\"our results suggest that {pubmed} searches with the clinical queries filter are more precise than with the advanced scholar search in google scholar for respiratory care topics. {pubmed} appears to be more practical to conduct efficient, valid searches for informing evidence-based patient-care protocols, for guiding the care of individual patients, and for educational purposes.\"\"in the field of information retrieval ({ir}), researchers and practitioners are often faced with a demand for valid approaches to evaluate the performance of retrieval systems. the cranfield experiment paradigm has been dominant for the in-vitro evaluation of {ir} systems. alternative to this paradigm, laboratory-based user studies have been widely used to evaluate interactive information retrieval ({iir}) systems, and at the same time investigate users\\' information searching behaviours. major drawbacks of laboratory-based user studies for evaluating {iir} systems include the high monetary and temporal costs involved in setting up and running those experiments, the lack of heterogeneity amongst the user population and the limited scale of the experiments, which usually involve a relatively restricted set of users. in this paper, we propose an alternative experimental methodology to laboratory-based user studies. our novel experimental methodology uses a crowdsourcing platform as a means of engaging study participants. through crowdsourcing, our experimental methodology can capture user interactions and searching behaviours at a lower cost, with more data, and within a shorter period than traditional laboratory-based user studies, and therefore can be used to assess the performances of {iir} systems. in this article, we show the characteristic differences of our approach with respect to traditional {iir} experimental and evaluation procedures. we also perform a use case study comparing crowdsourcing-based evaluation with laboratory-based evaluation of {iir} systems, which can serve as a tutorial for setting up crowdsourcing-based {iir} evaluations.\"usa\",resonance on the web: web dynamics and revisitation patterns,\"the web is a dynamic, ever-changing collection of information accessed in a dynamic way. this paper explores the relationship between web page content change (obtained from an hourly crawl of over {40k} pages) and people\\'s revisitation to those pages (collected via a large scale log analysis of {2.3m} users). we identify the relationship, or resonance, between revisitation behavior and the amount and type of changes on those pages. by coupling our large scale log analysis with a complementary user study we explore the intent behind the revisitation behavior we observed. using the notion of resonance to identify the likely content of interest, we describe a number of ways interaction with changing and revisited information can be better supported. we illustrate how understanding the association between change and revisitation might improve browser, crawler, and search engine design, and present a specific example of how knowledge of both can enable relevant content to be highlighted.\"\"the nature of the task that leads a person to engage in information interaction, as well as of information seeking and searching tasks, have been shown to influence individuals\\' information behavior. classifying tasks in a domain has been viewed as a departure point of studies on the relationship between tasks and human information behavior. however, previous task classification schemes either classify tasks with respect to the requirements of specific studies or merely classify a certain category of task. such approaches do not lead to a holistic picture of task since a task involves different aspects. therefore, the present study aims to develop a faceted classification of task, which can incorporate work tasks and information search tasks into the same classification scheme and characterize tasks in such a way as to help people make predictions of information behavior. for this purpose, previous task classification schemes and their underlying facets are reviewed and discussed. analysis identifies essential facets and categorizes them into generic facets of task and common attributes of task. generic facets of task include source of task, task doer, time, action, product, and goal. common attributes of task includes task characteristics and user\\'s perception of task. corresponding sub-facets and values are identified as well. in this fashion, a faceted classification of task is established which could be used to describe users\\' work tasks and information search tasks. this faceted classification provides a framework to further explore the relationships among work tasks, search tasks, and interactive information retrieval and advance adaptive {ir} systems design.\"usa\",predicting user interests from contextual information,\"search and recommendation systems must include contextual information to effectively model users\\' interests. in this paper, we present a systematic study of the effectiveness of five variant sources of contextual information for user interest modeling. post-query navigation and general browsing behaviors far outweigh direct search engine interaction as an information-gathering activity. therefore we conducted this study with a focus on website recommendations rather than search results. the five contextual information sources used are: social, historic, task, collection, and user interaction. we evaluate the utility of these sources, and overlaps between them, based on how effectively they predict users\\' future interests. our findings demonstrate that the sources perform differently depending on the duration of the time window used for future prediction, and that context overlap outperforms any isolated source. designers of website suggestion systems can use our findings to provide improved support for post-query navigation and general browsing behaviors.\"usa\",mining term association patterns from search logs for effective query reformulation,\"search engine logs are an emerging new type of data that offers interesting opportunities for data mining. existing work on mining such data has mostly attempted to discover knowledge at the level of queries (e.g., query clusters). in this paper, we propose to mine search engine logs for patterns at the level of terms through analyzing the relations of terms inside a query. we define two novel term association patterns (i.e., context-sensitive term substitutions and term additions) and propose new methods for mining such patterns from search engine logs. these two patterns can be used to address the mis-specification and under-specification problems of ineffective queries. experiment results on real search engine logs show that the mined context-sensitive term substitutions can be used to effectively reword queries and improve their accuracy, while the mined context-sensitive term addition patterns can be used to support query refinement in a more effective way.\"\"a key to the web\\'s success is the power of search. the elegant way in which search results are returned is usually remarkably effective. however, for exploratory search in which users need to learn, discover, and understand novel or complex topics, there is substantial room for improvement. human computer interaction researchers and web browser designers have developed novel strategies to improve web search by enabling users to conveniently visualize, manipulate, and organize their web search results. this monograph offers fresh ways to think about search-related cognitive processes and describes innovative design approaches to browsers and related tools. for instance, while key word search presents users with results for specific information (e.g., what is the capitol of peru), other methods may let users see and explore the contexts of their requests for information (related or previous work, conflicting information), or the properties that associate groups of information assets (group legal decisions by lead attorney). we also consider the both traditional and novel ways in which these strategies have been evaluated. from our review of cognitive processes, browser design, and evaluations, we reflect on the future opportunities and new paradigms for exploring and interacting with web search results.\"{user-oriented} and cognitive models of information retrieval,\"the domain of user-oriented and cognitive information retrieval ({ir}) is first discussed, followed by a discussion on the dimensions and types of models one may build for the domain. the focus of the present entry is on the models of user-oriented and cognitive {ir}, not on their empirical applications. several models with different emphases on user-oriented and cognitive {ir} are presented\\x97ranging from overall approaches and relevance models to procedural models, cognitive models, and task-based models. the present entry does not discuss empirical findings based on the models.\"usa\",to personalize or not to personalize: modeling queries with variation in user intent,\"in most previous work on personalized search algorithms, the results for all queries are personalized in the same manner. however, as we show in this paper, there is a lot of variation across queries in the benefits that can be achieved through personalization. for some queries, everyone who issues the query is looking for the same thing. for other queries, different people want very different results even though they express their need in the same way. we examine variability in user intent using both explicit relevance judgments and large-scale log analysis of user behavior patterns. while variation in user behavior is correlated with variation in explicit relevance judgments the same query, there are many other factors, such as result entropy, result quality, and task that can also affect the variation in behavior. we characterize queries using a variety of features of the query, the results returned for the query, and people\\'s interaction history with the query. using these features we build predictive models to identify queries that can benefit from personalization.\"usa\",tagging for health information organisation and retrieval,note: {ocr} errors may be found in this reference list extracted from the full text article.  {acm} has opted to expose the complete list rather than only correct and linked references.usa\",generating semantically enriched user profiles for web personalization,ann arbor, mi, usa. shakmatt@umich.edu\",adolescents searching for health information on the internet: an observational study.,\"adolescents\\' access to health information on the internet is partly a function of their ability to search for and find answers to their health-related questions. adolescents may have unique health and computer literacy needs. although many surveys, interviews, and focus groups have been utilized to understand the information-seeking and information-retrieval behavior of adolescents looking for health information online, we were unable to locate observations of individual adolescents that have been conducted in this context. this study was designed to understand how adolescents search for health information using the internet and what implications this may have on access to health information. a convenience sample of 12 students (age 12-17 years) from 1 middle school and 2 high schools in southeast michigan were provided with 6 health-related questions and asked to look for answers using the internet. researchers recorded 68 specific searches using software that captured screen images as well as synchronized audio recordings. recordings were reviewed later and specific search techniques and strategies were coded. a qualitative review of the verbal communication was also performed. out of 68 observed searches, 47 (69\\\\%) were successful in that the adolescent found a correct and useful answer to the health question. the majority of sites that students attempted to access were retrieved directly from search engine results (77\\\\%) or a search engine\\'s recommended links (10\\\\%); only a small percentage were directly accessed (5\\\\%) or linked from another site (7\\\\%). the majority (83\\\\%) of followed links from search engine results came from the first 9 results. incorrect spelling (30 of 132 search terms), number of pages visited within a site (ranging from 1-15), and overall search strategy (eg, using a search engine versus directly accessing a site), were each important determinants of success. qualitative analysis revealed that participants used a trial-and-error approach to formulate search strings, scanned pages randomly instead of systematically, and did not consider the source of the content when searching for health information. this study provides a useful snapshot of current adolescent searching patterns. the results have implications for constructing realistic simulations of adolescent search behavior, improving distribution and usefulness of web sites with health information relevant to adolescents, and enhancing educators\\' knowledge of what specific pitfalls students are likely to encounter.\"\"<{p>medical} informatics and biomedical computing have grown in quantum measure over the past decade. an abundance of advances have come to the foreground in this field with the vast amounts of biomedical and genomic data, the internet, and the wide application of computer use in all aspects of medical, biological, and health care research and practice. <{strong}>{medical} {informatics}: knowledge management and data mining in {biomedicine</strong}> covers the basic foundations of the area while extending the foundational material to include the recent leading-edge research in the field. the newer concepts, techniques, and practices of biomedical knowledge management and data mining are introduced and examined in detail. it is the research and applications in these areas that are raising the technical horizons and expanding the utility of informatics to an increasing number of biomedical professionals and {researchers.</p}> <{p></p}> <{p>the} book is divided into three major topical sections. </p> <{p>section} i presents the foundational information and knowledge management material and includes topics such as: bioinformatics challenges and standards, security and privacy, ethical and social issues, and biomedical knowledge mapping. </p> <{p>section} {ii} discusses the topics which are relevant to knowledge representations \\\\& access and includes topics such as: representations of medical concepts and relationships, genomic information retrieval, {3d} medical informatics, public access to anatomic images, and creating and maintaining biomedical ontologies. </p> <{p>section} {iii} examines the emerging application research in data mining, biomedical textual mining, and knowledge discovery research and includes topics such as: semantic parsing and analysis for patient records, biological relationships, gene pathways, and metabolic networks, exploratory genomic data analysis, joint learning using data and text mining, and disease informatics and outbreak {detection.</p}> <{p>the} book is a comprehensive presentation of the foundations and leading application research in medical informatics/biomedicine. these concepts and techniques are illustrated with detailed case studies. </p> <{p></p}> <{p>the} authors are widely recognized professors and researchers in schools of medicine and information systems from the university of arizona, university of washington, columbia university, and oregon health \\\\& science university. in addition, individual expert contributing authors have been commissioned to write chapters for the book on their respective topical {expertise.</p}>\"a {context-aware} movie preference model using a bayesian network for recommendation and promotion,\"this paper proposes a novel approach for constructing users\\' movie preference models using bayesian networks. the advantages of the constructed preference models are 1) consideration of users\\' context in addition to users\\' personality, 2) multiple applications, such as recommendation and promotion. data acquisition process through a {www} questionnaire survey and a bayesian network model construction process using the data are described. the effectiveness of the constructed model in terms of recommendation and promotion is also demonstrated through experiments.\"harvard medical school, boston, ma 02115, usa. qzeng@dsg.harvard.edu\",patient and clinician vocabulary: how different are they?,usa\",web search personalization with ontological user profiles,every user has a distinct background and a specific goal when searching for information on the web. the goal of web search personalization is to tailor search results to a particular user based on that user\\'s interests and preferences. effective personalization of information access involves two important challenges: accurately identifying the user context and organizing the information in such a way that matches the particular context. we present an approach to personalized search that involves building models of user context as ontological profiles by assigning implicitly derived interest scores to existing concepts in a domain ontology. a spreading activation algorithm is used to maintain the interest scores based on the user\\'s ongoing behavior. our experiments show that re-ranking the search results based on the interest scores and the semantic evidence in an ontological user profile is effective in presenting the most relevant results to the user.a contextual user model for web personalization,\"over the past years, information personalization has provided several valuable achievements on the improvement and optimization of web searching and recommendation taking into account user\\'s interests, preferences and contextual information. the main objective of a personalization system is to perform an information retrieval process taking into account the perception and the interest of the end-users. this paper focuses on how to model the user and his context in an extensible way that can be interpreted and used for personalization. we describe the architecture that provides personalization facilities based on the contextual user model for tourism usage.\"usa\",ready to buy or just browsing?: detecting web searcher goals from interaction data,\"an improved understanding of the relationship between search intent, result quality, and searcher behavior is crucial for improving the effectiveness of web search. while recent progress in user behavior mining has been largely focused on aggregate server-side click logs, we present a new class of search behavior models that also exploit fine-grained user interactions with the search results. we show that mining these interactions, such as mouse movements and scrolling, can enable more effective detection of the user\\'s search goals. potential applications include automatic search evaluation, improving search ranking, result presentation, and search advertising. we describe extensive experimental evaluation over both controlled user studies, and logs of interaction data collected from hundreds of real users. the results show that our method is more effective than the current state-of-the-art techniques, both for detection of searcher goals, and for an important practical application of predicting ad clicks for a given search session.\"multiple evidence combination in web site search based on users\\' access histories,\"despite the success of global search engines, web site search is still problematic in its retrieval accuracy. in this study, we propose to extract terms based on users\\' access histories to build web page representations, and then use multiple evidence combination to combine these log-based terms with text-based and anchor-based terms. we test different combination approaches and baseline retrieval models. our experimental results show that the server log, when used in multiple evidence combination, can improve the effectiveness of the web site search, whereas the impact on different models is different.\"usa\",a conceptual framework and a toolkit for supporting the rapid prototyping of context-aware applications,\"computing devices and applications are now used beyond the desktop, in diverse environments, and this trend toward ubiquitous computing is accelerating. one challenge that remains in this emerging research field is the ability to enhance the behavior of any application by informing it of the context of its use. by context, we refer to any information that characterizes a situation related to the interaction between humans, applications, and the surrounding environment. context-aware applications promise richer and easier interaction, but the current state of research in this field is still far removed from that vision. this is due to 3 main problems: (a) the notion of context is still ill defined, (b) there is a lack of conceptual models and methods to help drive the design of context-aware applications, and (c) no tools are available to jump-start the development of context-aware applications. in this anchor article, we address these 3 problems in turn. we first define context, identify categories of contextual information, and characterize context-aware application behavior. though the full impact of context-aware computing requires understanding very subtle and high-level notions of context, we are focusing our efforts on the pieces of context that can be inferred automatically from sensors in a physical environment. we then present a conceptual framework that separates the acquisition and representation of context from the delivery and reaction to context by a context-aware application. we have built a toolkit, the context toolkit, that instantiates this conceptual framework and supports the rapid development of a rich space of context-aware applications. we illustrate the usefulness of the conceptual framework by describing a number of context-aware applications that have been prototyped using the context toolkit. we also demonstrate how such a framework can support the investigation of important research challenges in the area of context-aware computing.\"ny, usa\",relevance: a review of the literature and a framework for thinking on the notion in information science. part {ii}: nature and manifestations of relevance,\"all is flux. —plato on knowledge in the theaetetus (about 369 {bc}) relevance is a, if not even the, key notion in information science in general and information retrieval in particular. this two-part critical review traces and synthesizes the scholarship on relevance over the past 30 years or so and provides an updated framework within which the still widely dissonant ideas and works about relevance might be interpreted and related. it is a continuation and update of a similar review that appeared in 1975 under the same title, considered here as being part i. the present review is organized in two parts: part {ii} addresses the questions related to nature and manifestations of relevance, and part {iii} addresses questions related to relevance behavior and effects. in part {ii}, the nature of relevance is discussed in terms of meaning ascribed to relevance, theories used or proposed, and models that have been developed. the manifestations of relevance are classified as to several kinds of relevance that form an interdependent system of relevancies. in part {iii}, relevance behavior and effects are synthesized using experimental and observational works that incorporated data. in both parts, each section concludes with a summary that in effect provides an interpretation and synthesis of contemporary thinking on the topic treated or suggests hypotheses for future research. analyses of some of the major trends that shape relevance work are offered in conclusions. {\\\\copyright} 2007 wiley periodicals, inc.\"usa\",implicit interest indicators,\"recommender systems provide personalized suggestions about items that users will find interesting.  typically, recommender systems require a user interface that can ``intelligently\\'\\' determine the interest of a user and use this information to make suggestions.  the common solution, ``explicit ratings\\'\\', where users tell the system what they think about a piece of information, is well-understood and fairly precise.  however, having to stop to enter explicit ratings can alter normal patterns of browsing and reading. a more ``intelligent\\'\\' method is to useimplicit ratings, where a rating is obtained by a method other than obtaining it directly from the user.  these implicit interest indicators have obvious advantages, including removing the cost of the user rating, and that every user interaction with the system can contribute to an implicit {rating.current} recommender systems mostly do not use implicit ratings, nor is the ability of implicit ratings to predict actual user interest well-understood.  this research studies the correlation between various implicit ratings and the explicit rating for a single web page.  a web browser was developed to record the user\\'s actions (implicit ratings) and the explicit rating of a page.  actions included mouse clicks, mouse movement, scrolling and elapsed time. this browser was used by over 80 people that browsed more than 2500 web {pages.using} the data collected by the browser, the individual implicit ratings and some combinations of implicit ratings were analyzed and compared with the explicit rating.  we found that the time spent on a page, the amount of scrolling on a page and the combination of time and scrolling had a strong correlation with explicit interest, while individual scrolling methods and mouse-clicks were ineffective in predicting explicit interest.\"usa\",a user browsing model to predict search engine click data from past observations.,\"search engine click logs provide an invaluable source of relevance information but this information is biased because we ignore which documents from the result list the users have actually seen before and after they clicked. otherwise, we could estimate document relevance by simple counting. in this paper, we propose a set of assumptions on user browsing behavior that allows the estimation of the probability that a document is seen, thereby providing an unbiased estimate of document relevance. to train, test and compare our model to the best alternatives described in the literature, we gather a large set of real data and proceed to an extensive cross-validation experiment. our solution outperforms very significantly all previous models. as a side effect, we gain insight into the browsing behavior of users and we can compare it to the conclusions of an eye-tracking experiments by joachims et al. [12]. in particular, our findings confirm that a user almost always see the document directly after a clicked document. they also explain why documents situated just after a very relevant document are clicked more often.\"usa\",implicit feedback for inferring user preference: a bibliography,an abstract is not available.\"new york, ny, usa\",the concept of relevance in {ir},\"this article introduces the concept of relevance as viewed and applied in the context of {ir} evaluation, by presenting an overview of the multidimensional and dynamic nature of the concept. the literature on relevance reveals how the relevance concept, especially in regard to the multidimensionality of relevance, is many faceted, and does not just refer to the various relevance criteria users may apply in the process of judging relevance of retrieved information objects. from our point of view, the multidimensionality of relevance explains why some will argue that no consensus has been reached on the relevance concept. thus, the objective of this article is to present an overview of the many different views and ways by which the concept of relevance is used—leading to a consistent and compatible understanding of the concept. in addition, special attention is paid to the type of situational relevance. many researchers perceive situational relevance as the most realistic type of user relevance, and therefore situational relevance is discussed with reference to its potential dynamic nature, and as a requirement for interactive information retrieval ({iir}) evaluation.\"\"the principle of polyrepresentation offers a theoretical framework for handling multiple contexts in information retrieval ({ir}). this paper presents an empirical laboratory study of polyrepresentation in restricted mode of the information space with focus on inter and intra-document features. the cystic fibrosis test collection indexed in the best match system {inquery} constitutes the experimental setting. overlaps between five functionally and/or cognitively different document representations are identified. supporting the principle of polyrepresentation, results show that in general overlaps generated by three or four representations of different nature have higher precision than those generated from two representations or the single fields. this result pertains to both structured and unstructured query mode in best match retrieval, however, with the latter query mode demonstrating higher performance. the retrieval overlaps containing search keys from the bibliographic references provide the best retrieval performance and minor {mesh} terms the worst. it is concluded that a highly structured query language is necessary when implementing the principle of polyrepresentation in a best match {ir} system because the principle is inherently boolean. finally a re-ranking test shows promising results when search results are re-ranked according to precision obtained in the overlaps whilst re-ranking by citations seems less useful when integrated into polyrepresentative applications.\"usa\",agglomerative clustering of a search engine query log,an abstract is not available.usa\",mining long-term search history to improve search accuracy,\"long-term search history contains rich information about a user\\'s search preferences, which can be used as search context to improve retrieval performance. in this paper, we study statistical language modeling based methods to mine contextual information from long-term search history and exploit it for a more accurate estimate of the query language model. experiments on real web search data show that the algorithms are effective in improving search accuracy for both fresh and recurring queries. the best performance is achieved when using clickthrough data of past searches that are related to the current query.\"\"10.1177/0165551507086989  this paper is a personal take on the history of evaluation experiments in information retrieval. it describes some of the early experiments that were formative in our understanding, and goes on to discuss the current dominance of {trec} (the text {retrieval} conference) and to assess its impact.\"usa\",learning implicit user interest hierarchy for context in personalization,\"abstract\\\\&nbsp;\\\\&nbsp; to provide a more robust context for personalization, we desire to extract a continuum of general to specific interests of a user, called a user interest hierarchy ({uih}). the higher-level interests are more general, while the lower-level interests are more specific. a {uih} can represent a user\\'s interests at different abstraction levels and can be learned from the contents (words/phrases) in a set of web pages bookmarked by a user. we propose a divisive hierarchical clustering ({dhc}) algorithm to group terms (topics) into a hierarchy where more general interests are represented by a larger set of terms. our approach does not need user involvement and learns the {uih}  ” implicitly”. to enrich features used in the {uih}, we used phrases in addition to words. our experiment indicates that {dhc} with the augmented expected mutual information ({aemi}) correlation function and {maxchildren} threshold-finding method built more meaningful {uihs} than the other combinations on average; using words and phrases as features improved the quality of {uihs}.\"usa\",matching task profiles and user needs in personalized web search,\"personalization has been deemed one of the major challenges in information retrieval with a significant potential for providing better search experience to individual users. especially, the need for enhanced user models better capturing elements such as users\\' goals, tasks, and contexts has been identified. in this paper, we introduce a statistical language model for user tasks representing different granularity levels of a user profile, ranging from very specific search goals to broad topics. we propose a personalization framework that selectively matches the actual user information need with relevant past user tasks, and allows to dynamically switch the course of personalization from re-finding very precise information to biasing results to general user interests. in the extreme, our model is able to detect when the user\\'s search and browse history is not appropriate for aiding the user in satisfying her current information quest. instead of blindly applying personalization to all user queries, our approach refrains from undue actions in these cases, accounting for the user\\'s desire of discovering new topics, and changing interests over time. the effectiveness of our method is demonstrated by an empirical user study.\"usa\",the loquacious user: a document-independent source of terms for query expansion,\"in this paper we investigate the effectiveness of a document-independent technique for eliciting feedback from users about their information problems. we propose that such a technique can be used to elicit terms from users for use in query expansion and as a follow-up when ambiguous queries are initially posed by users. we design a feedback form to obtain additional information from users, administer the form to users after initial querying, and create a series of experimental runs based on the information that we obtained from the form. results demonstrate that the form was successful at eliciting more information from users and that this additional information significantly improved retrieval performance. our results further demonstrate a strong relationship between query length and performance.\"100 manning hall, cb \\\\#3360, chapel hill, nc 27599-3360\",\"measuring online information seeking context, part 1: background and method\",\"context is one of the most important concepts in information seeking and retrieval research. however, the challenges of studying context are great; thus, it is more common for researchers to use context as a post hoc explanatory factor, rather than as a concept that drives inquiry. the purposes of this study were to develop a method for collecting data about information seeking context in natural online environments, and identify which aspects of context should be considered when studying online information seeking. the study is reported in two parts. in this, the first part, the background and method are presented. results and implications of this research are presented in part 2 (kelly, in press). part 1 discusses previous literature on information seeking context and behavior and situates the current work within this literature. this part further describes the naturalistic, longitudinal research design that was used to examine and measure the online information seeking contexts of users during a 14-week period. in this design, information seeking context was characterized by a user\\'s self-identified tasks and topics, and several attributes of these, such as the length of time the user expected to work on a task and the user\\'s familiarity with a topic. at weekly intervals, users evaluated the usefulness of the documents that they viewed, and classified these documents according to their tasks and topics. at the end of the study, users provided feedback about the study method.\"queensland university of technology; mcgill university; maastricht university,multitasking behavior,usa\",search engines that learn from implicit feedback,\"search-engine logs provide a wealth of information that machine-learning techniques can harness to improve search quality. with proper interpretations that avoid inherent biases, a search engine can use training data extracted from the logs to automatically tailor ranking functions to a particular user group or collection.\",medical visual information retrieval: state of the art and challenges ahead,\"today\\'s medical institutions produce enormous amounts of data on patients, including multimedia data, which is increasingly produced in digital form. these data in their clinical context contain much information and experience that is currently not being used up to its full potential. through the digital form the data has become accessible for automatic analysis and treatment for a variety of applications. at the same time, the variety of images produced can be confusing even for trained specialists causing an information overload exists for many medical doctors. this suggests that content-based image retrieval can be a valuable tool for helping manage these data and access the right information at the right time. this article gives a short state of the art of content-based medical image retrieval followed by a description of the {medgift} project on image retrieval with its main components. then, several challenges are used to illustrate areas where much more work is currently needed to advance biomedical image retrieval. this shows that we have now progresses beyond the phase, where medical doctors transfer a database to computer scientists to only evaluate their algorithms. we conclude that visual information retrieval can have a real impact in the medical field if the techniques can adapt to this rapidly changing field and get integrated into the workflow in radiology and other medical fields.\"ny, usa\",search user interfaces: best practices and future visions,\"the author describes some of the challenges, decisions, and processes that affected the design and development of the search user interface for version 2 of the digital library for earth system education ({dlese}; www.dlese.org), released july 29, 2003. the {dlese} is a community-led effort funded by the national science foundation and is part of the national science digital library ({nsdl}).\"usa\",comparing citation contexts for information retrieval,\"in previous work, we have shown that using terms from around citations in citing papers to index the cited paper, in addition to the cited paper\\'s own terms, can improve retrieval effectiveness. now, we investigate how to select text from around the citations in order to extract good index terms. we compare the retrieval effectiveness that results from a range of contexts around the citations, including no context, the entire citing paper, some fixed windows and several variations with linguistic motivations. we conclude with an analysis of the benefits of more complex, linguistically motivated methods for extracting citation index terms, over using a fixed window of terms. we speculate that there might be some advantage to using computational linguistic techniques for this task.\"\"the questionnaire is an important technique for gathering data from subjects during interactive information retrieval ({ir}) experiments. research in survey methodology, public opinion polling and psychology has demonstrated a number of response biases and behaviors that subjects exhibit when responding to questionnaires. furthermore, research in human-computer interaction has demonstrated that subjects tend to inflate their ratings of systems when completing usability questionnaires. in this study we investigate the relationship between questionnaire mode and subjects\\' responses to a usability questionnaire comprised of closed and open questions administered during an interactive {ir} experiment. three questionnaire modes (pen-and-paper, electronic and interview) were explored with 51 subjects who used one of two information retrieval systems. results showed that subjects\\' quantitative evaluations of systems were significantly lower in the interview mode than in the electronic mode. with respect to open questions, subjects in the interview mode used significantly more words than subjects in the pen-and-paper or electronic modes to communicate their responses, and communicated a significantly higher number of response units, even though the total number of unique response units was roughly the same across condition. finally, results showed that subjects in the pen-and-paper mode were the most efficient in communicating their responses to open questions. these results suggest that researchers should use the interview mode to elicit responses to closed questions from subjects and either pen-and-paper or electronic modes to elicit responses to open questions.\"usa\",personalizing web search results by reading level,\"traditionally, search engines have ignored the reading difficulty of documents and the reading proficiency of users in computing a document ranking. this is one reason why web search engines do a poor job of serving an important segment of the population: children. while there are many important problems in interface design, content filtering, and results presentation related to addressing children\\'s search needs, perhaps the most fundamental challenge is simply that of providing relevant results at the right level of reading difficulty. at the opposite end of the proficiency spectrum, it may also be valuable for technical users to find more advanced material or to filter out material at lower levels of difficulty, such as tutorials and introductory texts. we show how reading level can provide a valuable new relevance signal for both general and personalized web search. we describe models and algorithms to address the three key problems in improving relevance for search using reading difficulty: estimating user proficiency, estimating result difficulty, and re-ranking based on the difference between user and result reading level profiles. we evaluate our methods on a large volume of web query traffic and provide a large-scale log analysis that highlights the importance of finding results at an appropriate reading level for the user.\"\"department of computer science and information engineering, national taiwan university, taipei, taiwan, republic of china; institute of information science, academia sinica, taipei, taiwan, republic of china; department of computer science and information engineering, national taiwan university, taipei, taiwan, republic of china\",relevant term suggestion in interactive web search based on contextual information in query session logs,\"abstract 10.1002/asi.10256.abs this paper proposes an effective term suggestion approach to interactive web search. conventional approaches to making term suggestions involve extracting co-occurring keyterms from highly ranked retrieved documents. such approaches must deal with term extraction difficulties and interference from irrelevant documents, and, more importantly, have difficulty extracting terms that are conceptually related but do not frequently co-occur in documents. in this paper, we present a new, effective log-based approach to relevant term extraction and term suggestion. using this approach, the relevant terms suggested for a user query are those that co-occur in similar query sessions from search engine logs, rather than in the retrieved documents. in addition, the suggested terms in each interactive search step can be organized according to its relevance to the entire query session, rather than to the most recent single query as in conventional approaches. the proposed approach was tested using a proxy server log containing about two million query transactions submitted to search engines in taiwan. the obtained experimental results show that the proposed approach can provide organized and highly relevant terms, and can exploit the contextual information in a user\\'s query session to make more effective suggestions.\"usa\",large scale query log analysis of re-finding,\"although web search engines are targeted towards helping people find new information, people regularly use them to re-find web pages they have seen before. researchers have noted the existence of this phenomenon, but relatively little is understood about how re-finding behavior differs from the finding of new information. this paper dives deeply into the differences via analysis of three large-scale data sources: 1) query logs (queries, clicks, result impressions), 2) web browsing logs ({url} visits), and 3) a daily web crawl (page content). it appears that people learn valuable information about the pages they find that helps them re-find what they are looking for later; compared to the initial finding query, re-finding queries are typically shorter, and rank the re-found {url} higher. while many instances of re-finding probably serve as a type of bookmark for a known {url}, others seem to represent the resumption of a previous task; results clicked at the end of a session are more likely than those at the beginning to be re-found during a later session, while re-finding is more likely to happen at the beginning of a session than at the end. additionally, we observe differences in cross-session and intra-session re-finding that may indicate different types of re-finding tasks. our findings suggest there is a rich opportunity for search engines to take advantage of re-finding behavior as a means to improve the search experience.\"modeling context information in pervasive computing systems,\"as computing becomes more pervasive, the nature of applications must change accordingly. in particular, applications must become more flexible in order to respond to highly dynamic computing environments, and more autonomous, to reflect the growing ratio of applications to users and the corresponding decline in the attention a user can devote to each. that is, applications must become more context-aware. to facilitate the programming of such applications, infrastructure is required to gather, manage, and disseminate context information to applications. this paper is concerned with the development of appropriate context modeling concepts for pervasive computing, which can form the basis for such a context management infrastructure. this model overcomes problems associated with previous context models, including their lack of formality and generality, and also tackles issues such as wide variations in information quality, the existence of complex relationships amongst context information and temporal aspects of context.\"ny, usa\",context-aware systems: a literature review and classification,\"nowadays, numerous journals and conferences have published articles related to context-aware systems, indicating many researchers\\' interest. therefore, the goal of this paper is to review the works that were published in journals, suggest a new classification framework of context-aware systems, and explore each feature of classification framework. this paper is based on a literature review of context-aware systems from 2000 to 2007 using a keyword index and article title search. the classification framework is developed based on the architecture of context-aware systems, which consists of the following five layers: concept and research layer, network layer, middleware layer, application layer and user infrastructure layer. the articles are categorized based on the classification framework. this paper allows researchers to extract several lessons learned that are important for the implementation of context-aware systems.\"usa\",investigating behavioral variability in web search,\"understanding the extent to which people\\'s search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help world wide web users search more effectively. in this paper we describe a longitudinal log-based study that investigated variability in people.s interaction behavior when engaged in search-related activities on the {web.allwe} analyze the search interactions of more than two thousand volunteer users over a five-month period, with the aim of characterizing differences in their interaction {styles.allthe} findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users, and within and between the search queries they {submit.allour} findings also suggest two classes of extreme user. navigators and explorers. whose search interaction is highly consistent or highly variable. lessons learned from these users can inform the design of tools to support effective web-search interactions for everyone.\"personalized search results with user interest hierarchies learnt from bookmarks,\"personalized web search incorporates an individual user\\'s interests when deciding relevant results to return. while, most web search engines are usually designed to serve all users, without considering the interests of individual users. we propose a method to (re)rank the results from a search engine using a learned user profile, called a user interest hierarchy ({uih}), from web pages that are of interest to the user. the user\\'s interest in web pages will be determined implicitly, without directly asking the user. experimental results indicate that our personalized ranking methods, when used with a popular search engine, can yield more potentially interesting web pages for individual users.\"inter-patient distance metrics using {snomed} {ct} defining relationships,usa\",comparing the sensitivity of information retrieval metrics,\"information retrieval effectiveness is usually evaluated using measures such as normalized discounted cumulative gain ({ndcg}), mean average precision ({map}) and precision at some cutoff (precision@k) on a set of judged queries. recent research has suggested an alternative, evaluating information retrieval systems based on user behavior. particularly promising are experiments that interleave two rankings and track user clicks. according to a recent study, interleaving experiments can identify large differences in retrieval effectiveness with much better reliability than other click-based methods. we study interleaving in more detail, comparing it with traditional measures in terms of reliability, sensitivity and agreement. to detect very small differences in retrieval effectiveness, a reliable outcome with standard metrics requires about 5,000 judged queries, and this is about as reliable as interleaving with 50,000 user impressions. amongst the traditional measures, {ndcg} has the strongest correlation with interleaving. finally, we present some new forms of analysis, including an approach to enhance interleaving sensitivity.\"usa\",query-sets: using implicit feedback and query patterns to organize web documents,\"in this paper we present a new document representation model based on implicit user feedback obtained from search engine queries. the main objective of this model is to achieve better results in non-supervised tasks, such as clustering and labeling, through the incorporation of usage data obtained from search engine queries. this type of model allows us to discover the motivations of users when visiting a certain document. the terms used in queries can provide a better choice of features, from the user\\'s point of view, for summarizing the web pages that were clicked from these queries. in this work we extend and formalize as \"\"query model\"\" an existing but not very well known idea of \"\"query view\"\" for document representation. furthermore, we create a novel model based on \"\"frequent query patterns\"\" called the \"\"query-set model\"\". our evaluation shows that both \"\"query-based\"\" models outperform the vector-space model when used for clustering and labeling documents in a website. in our experiments, the query-set model reduces by more than 90\\\\% the number of features needed to represent a set of documents and improves by over 90\\\\% the quality of the results. we believe that this can be explained because our model chooses better features and provides more accurate labels according to the user\\'s expectations.\"usa\",enabling context-sensitive information seeking,\"information seeking is an important but often difficult task, especially when it involves large and complex data sets. we hypothesize that a context-sensitive interaction paradigm would greatly assist users in their information seeking. such a paradigm would allow users to both express their requests and receive requested information in context. driven by this hypothesis, we have taken rigorous steps to design, develop, and evaluate a full-fledged, context-sensitive information system. we started with a {wizard-of-oz} ({woz}) study to verify the effectiveness of our envi-sioned system. we then built a fully automated system based on the findings from our {woz} study. we targeted the development and integration of two sets of technologies: context-sensitive mul-timodal input interpretation and multimedia output generation. finally, we formally evaluated the usability of our system in real world conditions. the results show that our system greatly improves the users\\' ability to perform practical information-seek-ing tasks. these results not only confirm our initial hypothesis, but they also indicate the practicality of our approaches.\"usa\",exploring searcher interactions for distinguishing types of commercial intent,\"an improved understanding of the relationship between search intent, result quality, and searcher behavior is crucial for improving the effectiveness of web search. while recent progress in user behavior mining has been largely focused on aggregate server-side click logs, we present a new search behavior model that incorporates fine-grained user interactions with the search results. we show that mining these interactions, such as mouse movements and scrolling, can enable more effective detection of the user\\'s search intent. potential applications include automatic search evaluation, improving search ranking, result presentation, and search advertising. as a case study, we report results on distinguishing between \"\"research\"\" and \"\"purchase\"\" variants of commercial intent, that show our method to be more effective than the current state-of-the-art.\"usa\",using the wisdom of the crowds for keyword generation,\"in the sponsored search model, search engines are paid by businesses that are interested in displaying ads for their site alongside the search results. businesses bid for keywords, and their ad is displayed when the keyword is queried to the search engine. an important problem in this process is \\'keyword generation\\': given a business that is interested in launching a campaign, suggest keywords that are related to that campaign. we address this problem by making use of the query logs of the search engine. we identify queries related to a campaign by exploiting the associations between queries and {urls} as they are captured by the user\\'s clicks. these queries form good keyword suggestions since they capture the \"\"wisdom of the crowd\"\" as to what is related to a site. we formulate the problem as a semi-supervised learning problem, and propose algorithms within the markov random field model. we perform experiments with real query logs, and we demonstrate that our algorithms scale to large query logs and produce meaningful results.\"usa\",context in problem solving: a survey,\"dealing with context seems to be a major challenge which the field of artificial intelligence ({ai}) will have to face in the next few years. the importance of the challenge is borne out by the various scientific events focusing on context which have been held since 1995. this does not mean that context has not been studied before—it has in such domains as natural language processing—but it has yet to be studied in depth since previous research has only covered few aspects of the problem. we present in this paper a survey of the literature dealing directly and explicitly with context whatever the domain. this makes it possible for us to have a clear view of context in {ai}. one of the conclusions of this survey is to point out the existence of different types of context in areas such as the representation of knowledge in a computer system, the reasoning that the system carries out using the knowledge, and the interaction the system has with people.\"usa\",evaluating verbose query processing techniques,\"verbose or long queries are a small but significant part of the query stream in web search, and are common in other applications such as collaborative question answering ({cqa}). current search engines perform well with keyword queries but are not, in general, effective for verbose queries. in this paper, we examine query processing techniques which can be applied to verbose queries prior to submission to a search engine in order to improve the search engine\\'s results. we focus on verbose queries that have sentence-like structure, but are not simple \"\"wh-\"\" questions, and assume the search engine is a \"\"black box.\"\" we evaluated the output of two search engines using queries from a {cqa} service and our results show that, among a broad range of techniques, the most effective approach is to simply reduce the length of the query. this can be achieved effectively by removing \"\"stop structure\"\" instead of only stop words. we show that the process of learning and removing stop structure from a query can be effectively automated.\"science park 107, amsterdam, the netherlands; ilk, tilburg university, p.o. box 90153, tilburg, the netherlands\",contextual factors for finding similar experts,\"expertise-seeking research studies how people search for expertise and choose whom to contact in the context of a specific task. an important outcome are models that identify factors that influence expert finding. expertise retrieval addresses the same problem, expert finding, but from a system-centered perspective. the main focus has been on developing content-based algorithms similar to document search. these algorithms identify matching experts primarily on the basis of the textual content of documents with which experts are associated. other factors, such as the ones identified by expertise-seeking models, are rarely taken into account. in this article, we extend content-based expert-finding approaches with contextual factors that have been found to influence human expert finding. we focus on a task of science communicators in a knowledge-intensive environment, the task of finding similar experts, given an example expert. our approach combines expertise-seeking and retrieval research. first, we conduct a user study to identify contextual factors that may play a role in the studied task and environment. then, we design expert retrieval models to capture these factors. we combine these with content-based retrieval models and evaluate them in a retrieval experiment. our main finding is that while content-based features are the most important, human participants also take contextual factors into account, such as media experience and organizational structure. we develop two principled ways of modeling the identified factors and integrate them with content-based retrieval models. our experiments show that models combining content-based and contextual factors can significantly outperform existing content-based models.\",context aware information retrieval for enhanced situation awareness,\"in the coalition forces, users are increasingly challenged with the issues of information overload and correlation of information from heterogeneous sources. users might need different pieces of information, ranging from information about a single building, to the resolution strategy of a global conflict. sometimes, the time, location and past history of information access can also shape the information needs of users. information systems need to help users pull together data from disparate sources according to their expressed needs (as represented by system queries), as well as less specific criteria. information consumers have varying roles, tasks/missions, goals and agendas, knowledge and background, and personal preferences. these factors can be used to shape both the execution of user queries and the form in which retrieved information is packaged. however, full automation of this daunting information aggregation and customization task is not possible with existing approaches. in this paper we present an infrastructure for context-aware information retrieval to enhance situation awareness. the infrastructure provides each user with a customized, mission-oriented system that gives access to the right information from heterogeneous sources in the context of a particular task, plan and/or mission. the approach lays on five intertwined fundamental concepts, namely workflow, context, ontology, profile and information aggregation. the exploitation of this knowledge, using appropriate domain ontologies, will make it feasible to provide contextual assistance in various ways to the work performed according to a user\\'s task-relevant information requirements. this paper formalizes these concepts and their interrelationships.\"ny, usa\",on test collections for adaptive information retrieval,\"traditional cranfield test collections represent an abstraction of a retrieval task that sparck jones calls the \\'\\'core competency\\'\\' of retrieval: a task that is necessary, but not sufficient, for user retrieval tasks. the abstraction facilitates research by controlling for (some) sources of variability, thus increasing the power of experiments that compare system effectiveness while reducing their cost. however, even within the highly-abstracted case of the cranfield paradigm, meta-analysis demonstrates that the user/topic effect is greater than the system effect, so experiments must include a relatively large number of topics to distinguish systems\\' effectiveness. the evidence further suggests that changing the abstraction slightly to include just a bit more characterization of the user will result in a dramatic loss of power or increase in cost of retrieval experiments. defining a new, feasible abstraction for supporting adaptive {ir} research will require winnowing the list of all possible factors that can affect retrieval behavior to a minimum number of essential factors.\"usa\",supporting intelligent web search,\"search engines continue to struggle to provide everyday users with a service capable of delivering focussed results that are relevant to their information needs. moreover, traditional search engines really only provide users with a starting point for their information search. that is, upon selecting a page from a search result list, the interaction between user and search engine is effectively over and the user must continue their search alone. in this article, we argue that a comprehensive search service needs to provide the user with more help, both at the result list level and beyond, and we outline some recommendations for intelligent web search support. we introduce the {searchguide} web search support system and we describe how it fulfils the requirements for a search support system, providing evaluation results where applicable.\"usa\",to each his own: personalized content selection based on text comprehensibility,\"imagine a physician and a patient doing a search on antibiotic resistance. or a chess amateur and a grandmaster conducting a search on alekhine\\'s defence. although the topic is the same, arguably the two users in each case will satisfy their information needs with very different texts. yet today search engines mostly adopt the one-size-fits-all solution, where personalization is restricted to topical preference. we found that users do not uniformly prefer simple texts, and that the text comprehensibility level should match the user\\'s level of preparedness. consequently, we propose to model the comprehensibility of texts as well as the users\\' reading proficiency in order to better explain how different users choose content for further exploration. we also model topic-specific reading proficiency, which allows us to better explain why a physician might choose to read sophisticated medical articles yet simple descriptions of {slr} cameras. we explore different ways to build user profiles, and use collaborative filtering techniques to overcome data sparsity. we conducted experiments on large-scale datasets from a major web search engine and a community question answering forum. our findings confirm that explicitly modeling text comprehensibility can significantly improve content ranking (search results or answers, respectively).\"'),\n",
       " ('37769fdc48d9a70ccf3a3245f7375230',\n",
       "  'usa\",crowdsourcing user studies with mechanical turk,\"user studies are important for many aspects of the design process and involve techniques ranging from informal surveys to rigorous laboratory studies. however, the costs involved in engaging users often requires practitioners to trade off between sample size, time requirements, and monetary costs. micro-task markets, such as amazon\\'s mechanical turk, offer a potential paradigm for engaging a large number of users for low time and monetary costs. here we investigate the utility of a micro-task market for collecting user measurements, and discuss design considerations for developing remote micro user evaluation tasks. although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach.\"usa\",from x-rays to silly putty via uranus: serendipity and its role in web search,\"the act of encountering information unexpectedly has long been identified as valuable, both as a joy in itself and as part of task-focused problem solving. there has been a concern that highly accurate search engines and targeted personalization may reduce opportunities for serendipity on the web. we examine whether there is the potential for serendipitous encounters during web search, and whether improving search relevance through personalization reduces this potential. by studying web search query logs and the results people judge relevant and interesting, we find many of the queries people perform return interesting (potentially serendipitous) results that are not directly relevant. rather than harming serendipity, personalization appears to identify interesting results in addition to relevant ones.\"usa\",trustworthy online controlled experiments: five puzzling outcomes explained,\"online controlled experiments are often utilized to make data-driven decisions at amazon, microsoft, {ebay}, facebook, google, yahoo, zynga, and at many other companies. while the theory of a controlled experiment is simple, and dates back to sir ronald a. fisher\\'s experiments at the rothamsted agricultural experimental station in england in the 1920s, the deployment and mining of online controlled experiments at scale--thousands of experiments now--has taught us many lessons. these exemplify the proverb that the difference between theory and practice is greater in practice than in theory. we present our learnings as they happened: puzzling outcomes of controlled experiments that we analyzed deeply to understand and explain. each of these took multiple-person weeks to months to properly analyze and get to the often surprising root cause. the root causes behind these puzzling results are not isolated incidents; these issues generalized to multiple experiments. the heightened awareness should help readers increase the trustworthiness of the results coming out of controlled experiments. at microsoft\\'s bing, it is not uncommon to see experiments that impact annual revenue by millions of dollars, thus getting trustworthy results is critical and investing in understanding anomalies has tremendous payoff: reversing a single incorrect decision based on the results of an experiment can fund a whole team of analysts. the topics we cover include: the {oec} (overall evaluation criterion), click tracking, effect trends, experiment length and power, and carryover effects.\"usa\",the anatomy of a large-scale social search engine,\"we present aardvark, a social search engine. with aardvark, users ask a question, either by instant message, email, web input, text message, or voice. aardvark then routes the question to the person in the user\\'s extended social network most likely to be able to answer that question. as compared to a traditional web search engine, where the challenge lies in finding the right document to satisfy a user\\'s information need, the challenge in a social search engine like aardvark lies in finding the right person to satisfy a user\\'s information need. further, while trust in a traditional search engine is based on authority, in a social search engine like aardvark, trust is based on intimacy. we describe how these considerations inform the architecture, algorithms, and user interface of aardvark, and how they are reflected in the behavior of aardvark users.\"\"{<i>inviting disaster</i>, by technology and history writer james r. chiles, is an unusual book: it appeals to the macabre desires that keep us riveted to highway accidents, while knowledgeably discoursing on the often preventable mistakes that caused them. at its heart are colorful stories behind more than 50 of the most infamous catastrophes that periodically chilled the advance of the industrial age. there are both those well remembered (the 1986 challenger explosion, for example) and those now largely forgotten (a 1937 gas explosion at a texas school that killed 298). but along with lively depictions of these deadly devastations and white-knuckle calamities--the u.s. battleship maine, apollo 13, and three mile island among them--chiles offers an informed analysis of the unfortunate chain of events that brought them about. and by grouping like incidents to show how fatal \"\"system fractures\"\" eventually developed through a combination of human error and mechanical malfunction, he also suggests how we might sidestep such tragedies in the future. in so, doing he fashions these spectacular accounts of failed planes, trains, ships, bridges, dams, factories, and other conveyances and facilities into a cautionary tale about technological progress. --<i>howard rothman</i>} {<h4 align=center>combining captivating storytelling with eye-opening findings, <i>inviting disaster</i> delves inside some of history\\'s worst catastrophes in order to show how increasingly \"\"smart\"\" systems leave us wide open to human tragedy.</h4><p>weaving a dramatic narrative that explains how breakdowns in these systems result in such disasters as the chain reaction crash of the air france concorde to the meltdown at the chernobyl nuclear power station, chiles vividly demonstrates how the battle between man and machine may be escalating beyond manageable limits -- and why we all have a stake in its outcome. <p>included in this edition is a special introduction providing a behind-the-scenes look at the world trade center catastrophe. combining firsthand accounts of employees\\' escapes with an in-depth look at the structural reasons behind the towers\\' collapse, chiles addresses the question, were the towers \"\"two tall heroes\"\" or structures with a fatal flaw?</p>}\"\"early detection of potential experts in question answering communities user modeling, adaption and personalization\",\"question answering communities ({qa}) are sustained by a handful of experts who provide a large number of high quality answers. identifying these experts during the first few weeks of their joining the community can be beneficial as it would allow community managers to take steps to develop and retain these potential experts. in this paper, we explore approaches to identify potential experts as early as within the first two weeks of their association with the {qa}. we look at users\\' behavior and estimate their motivation and ability to help others. these qualities enable us to build classification and ranking models to identify users who are likely to become experts in the future. our results indicate that the current experts can be effectively identified from their early behavior. we asked community managers to evaluate the potential experts identified by our algorithm and their analysis revealed that quite a few of these users were already experts or on the path of becoming experts. our retrospective analysis shows that some of these potential experts had already left the community, highlighting the value of early identification and engagement.\"usa\",topical query decomposition,\"we introduce the problem of query decomposition, where we are given a query and a document retrieval system, and we want to produce a small set of queries whose union of resulting documents corresponds approximately to that of the original query. ideally, these queries should represent coherent, conceptually well-separated topics. we provide an abstract formulation of the query decomposition problem, and we tackle it from two different perspectives. we first show how the problem can be instantiated as a specific variant of a set cover problem, for which we provide an efficient greedy algorithm. next, we show how the same problem can be seen as a constrained clustering problem, with a very particular kind of constraint, i.e., clustering with predefined clusters. we develop a two-phase algorithm based on hierarchical agglomerative clustering followed by dynamic programming. our experiments, conducted on a set of actual queries in a web scale search engine, confirm the effectiveness of the proposed solutions.\"i tweet passionately: twitter users, context collapse, and the imagined audience\",\"social media technologies collapse multiple audiences into single contexts, making it difficult for people to use the same techniques online that they do to handle multiplicity in face-to-face conversation. this article investigates how content producers navigate \\'imagined audiences\\' on twitter. we talked with participants who have different types of followings to understand their techniques, including targeting different audiences, concealing subjects, and maintaining authenticity. some techniques of audience management resemble the practices of \\'micro-celebrity\\' and personal branding, both strategic self-commodification. our model of the networked audience assumes a many-to-many communication through which individuals conceptualize an imagined audience evoked through their tweets.\"usa\",crowdsourcing and knowledge sharing: strategic user behavior on taskcn,\"witkeys are a thriving type of web-based knowledge sharing market in china, supporting a form of crowdsourcing. in a witkey site, users offer a small award for a solution to a task, and other users compete to have their solution selected. in this paper, we examine the behavior of users on one of the biggest witkey websites in china, taskcn.com. on taskcn, we observed several characteristics in users\\' activity over time. most users become inactive after only a few submissions. others keep attempting tasks. over time, users tend to select tasks where they are competing against fewer opponents to increase their chances of winning. they will also, perhaps counterproductively, select tasks with higher expected rewards. yet, on average, they do not increase their chances of winning, and in some categories of tasks, their chances actually decrease. this does not paint the full picture, however, because there is a very small core of successful users who manage not only to win multiple tasks, but to increase their win-to-submission ratio over time. this core group proposes nearly 20\\\\% of the winning solutions on the site. the patterns we observe on taskcn, we believe, hold clues to the future of crowdsourcing and freelance marketplaces, and raise interesting design implications for such sites.\",twitter use by the {u.s}. congress,\"twitter is a microblogging and social networking service with millions of members and growing at a tremendous rate. with the buzz surrounding the service have come claims of its ability to transform the way people interact and share information and calls for public figures to start using the service. in this study, we are interested in the type of content that legislators are posting to the service, particularly by members of the united states congress. we read and analyzed the content of over 6,000 posts from all members of congress using the site. our analysis shows that congresspeople are primarily using twitter to disperse information, particularly links to news articles about themselves and to their blog posts, and to report on their daily activities. these tend not to provide new insights into government or the legislative process or to improve transparency; rather, they are vehicles for self-promotion. however, twitter is also facilitating direct communication between congresspeople and citizens, though this is a less popular activity. we report on our findings and analysis and discuss other uses of twitter for legislators.\"usa\",predictors of answer quality in online {q\\\\&amp;a} sites,\"question and answer ({q\\\\&a}) sites such as yahoo! answers are places where users ask questions and others answer them. in this paper, we investigate predictors of answer quality through a comparative, controlled field study of responses provided across several online {q\\\\&a} sites. along with several quantitative results concerning the effects of factors such as question topic and rhetorical strategy, we present two high-level messages. first, you get what you pay for in {q\\\\&a} sites. answer quality was typically higher in google answers (a fee-based site) than in the free sites we studied, and paying more money for an answer led to better outcomes. second, we find that a {q\\\\&a} site\\'s community of users contributes to its success. yahoo! answers, a {q\\\\&a} site where anybody can answer questions, outperformed sites that depend on specific individuals to answer questions, such as library reference services.\"usa\",learning to recognize reliable users and content in social media with coupled mutual reinforcement,\"community question answering ({cqa}) has emerged as a popular forum for users to pose questions for other users to answer. over the last few years, {cqa} portals such as naver and yahoo! answers have exploded in popularity, and now provide a viable alternative to general purpose web search. at the same time, the answers to past questions submitted in {cqa} sites comprise a valuable knowledge repository which could be a gold mine for information retrieval and automatic question answering. unfortunately, the quality of the submitted questions and answers varies widely - increasingly so that a large fraction of the content is not usable for answering queries. previous approaches for retrieving relevant and high quality content have been proposed, but they require large amounts of manually labeled data -- which limits the applicability of the supervised approaches to new sites and domains. in this paper we address this problem by developing a semi-supervised coupled mutual reinforcement framework for simultaneously calculating content quality and user reputation, that requires relatively few labeled examples to initialize the training process. results of a large scale evaluation demonstrate that our methods are more effective than previous approaches for finding high-quality answers, questions, and users. more importantly, our quality estimation significantly improves the accuracy of search over {cqa} archives over the state-of-the-art methods.\"critical mass: how one thing leads to another,usa\",the impact of caching on search engines,\"in this paper we study the trade-offs in designing efficient caching systems for web search engines. we explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs.caching posting lists. using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. we propose a new algorithm for static caching of posting lists, which outperforms previous methods. we also study the problem of finding the optimal way to split the static cache between answers and posting lists. finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.\"usa\",motivating participation by displaying the value of contribution,\"one of the important challenges faced by designers of online communities is eliciting sufficent contributions from community members. users in online communities may have difficulty either in finding opportunities to add value, or in understanding the value of their contributions to the community. various social science theories suggest that showing users different perspectives on the value they add to the community will lead to differing amounts of contribution. the present study investigates a design augmentation for an existing community web site that could benefit from additional contribution. the augmented interface includes individualized opportunities for contribution and an estimate of the value of each contribution to the community. the value is computed in one of four different ways: (1) value to self; (2) value to a small group the user has affinity with; (3) value to a small group the user does not have affinity with; and (4) value to the entire user community. the study compares the effectiveness of the different notions of value to 160 community members.\"usa\",socializing or knowledge sharing?: characterizing social intent in community question answering,\"knowledge sharing communities, such as wikipedia or yahoo! answers, add greatly to the wealth of information available on the web. they represent complex social ecosystems that rely on user paricipation and the quality of users\\' contributions to prosper. however, quality is harder to achieve when knowledge sharing is facilitated through a high degree of personal interactions. the individuals\\' objectives may change from knowledge sharing to socializing, with a profound impact on the community and the value it delivers to the broader population of web users. in this paper we provide new insights into the types of content that is shared through community question answering ({cqa}) services. we demonstrate an approach that combines in-depth content analysis with social network analysis techniques. we adapted the undirected inductive coding method to analyze samples of user questions and arrive at a comprehensive typology of the user intent. in our analysis we focused on two types of intent, social vs. non-social, and define measures of social engagement to characterize the users\\' participation and content contributions. our approach is applicable to a broad class of online communities and can be used to monitor the dynamics of community ecosystems.\"usa\",preferential behavior in online groups,\"online communities in the form of message boards, listservs, and newsgroups continue to represent a considerable amount of the social activity on the internet. every year thousands of groups ourish while others decline into relative obscurity; likewise, millions of members join a new community every year, some of whom will come to manage or moderate the conversation while others simply sit by the sidelines and observe. these processes of group formation, growth, and dissolution are central in social science, and in an online venue they have ramifications for the design and development of community software in this paper we explore a large corpus of thriving online communities. these groups vary widely in size, moderation and privacy, and cover an equally diverse set of subject matter. we present a broad range of descriptive statistics of these groups. using metadata from groups, members, and individual messages, we identify users who post and are replied-to frequently by multiple group members; we classify these high-engagement users based on the longevity of their engagements. we show that users who will go on to become long-lived, highly-engaged users experience significantly better treatment than other users from the moment they join the group, well before there is an opportunity for them to develop a long-standing relationship with members of the group we present a simple model explaining long-term heavy engagement as a combination of user-dependent and group-dependent factors. using this model as an analytical tool, we show that properties of the user alone are sufficient to explain 95\\\\% of all memberships, but introducing a small amount of per-group information dramatically improves our ability to model users belonging to multiple groups.\"usa\",the post that {wasn\\'t}: exploring self-censorship on facebook,\"social networking site users must decide what content to share and with whom. many social networks, including facebook, provide tools that allow users to selectively share content or block people from viewing content. however, sometimes instead of targeting a particular audience, users will self-censor, or choose not to share. we report the results from an 18-participant user study designed to explore self-censorship behavior as well as the subset of unshared content participants would have potentially shared if they could have specifically targeted desired audiences. we asked participants to report all content they thought about sharing but decided not to share on facebook and interviewed participants about why they made sharing decisions and with whom they would have liked to have shared or not shared. participants reported that they would have shared approximately half the unshared content if they had been able to exactly target their desired audiences.\"usa\",\"finding high quality content in social media, with an application to community-based question answering\",\"the quality of user-generated content varies drastically from excellent to abuse and spam. as the availability of such content increases, the task of identifying high-quality content sites based on user contributions --social media sites -- becomes increasingly important. social media in general exhibit a rich variety of information sources: in addition to the content itself, there is a wide array of non-content information available, such as links between items and explicit quality ratings from members of the community. in this paper we investigate methods for exploiting such community feedback to automatically identify high quality content. as a test case, we focus on yahoo! answers, a large community question/answering portal that is particularly rich in the amount and types of content and social interactions available in it. we introduce a general classification framework for combining the evidence from different sources of information, that can be tuned automatically for a given social media type and quality definition. in particular, for the community question/answering domain, we show that our system is able to separate high-quality items from the rest with an accuracy close to that of humans\"usa\",a taxonomy of web search,\"classic {ir} (information retrieval) is inherently predicated on users searching for information, the so-called \"\"information need\"\". but the need behind a web search is often not informational -- it might be navigational (give me the url of the site i want to reach) or transactional (show me sites where i can perform a certain transaction, e.g. shop, download a file, or find a map). we explore this taxonomy of web searches and discuss how global search engines evolved to deal with web-specific needs.\"usa\",is this urgent?: exploring time-sensitive information needs in collaborative question answering,\"as online collaborative question answering ({cqa}) servicessuch as yahoo! answers and baidu knows are attracting users, questions, and answers at an explosive rate, the truly urgent and important questions are increasingly getting lost in the crowd. that is, questions that require immediate responses are pushed out of the way by the trivial but more recently arriving questions. unlike other questions in collaborative question answering ({cqa}) for which users might be willing to wait until good answers appear, urgent questions are likely to be of interest to the asker only if answered in the next few minutes or hours. for such questions, late responses are either not useful or are simply not applicable. unfortunately, current collaborative question-answering systems do not distinguish urgent questions from the rest, and could thus be ineffective for urgent information needs. we explore text- and data- mining methods for automatically identifying urgent questions in the {cqa} setting. our results indicate that modeling the question context (i.e., the particular forum/category where the question was posted) can increase classification accuracy compared to the text of the question alone.\"usa\",over-exposed?: privacy patterns and considerations in online and mobile photo sharing,\"as sharing personal media online becomes easier and widely spread, new privacy concerns emerge - especially when the persistent nature of the media and associated context reveals details about the physical and social context in which the media items were created. in a first-of-its-kind study, we use context-aware camerephone devices to examine privacy decisions in mobile and online photo sharing. through data analysis on a corpus of privacy decisions and associated context data from a real-world system, we identify relationships between location of photo capture and photo privacy settings. our data analysis leads to further questions which we investigate through a set of interviews with 15 users. the interviews reveal common themes in privacy considerations: security, social disclosure, identity and convenience. finally, we highlight several implications and opportunities for design of media sharing applications, including using past privacy patterns to prevent oversights and errors.\"syracuse, ny 13244-2340, usa.\",methodological challenges in research on sexual risk behavior: {ii}. accuracy of self-reports.,\"assessing sexual behavior with self-report is essential to research on a variety of health topics, including pregnancy and infertility, sexually transmitted infections, and sexual health and functioning. recent methodological research has provided new insights regarding the accuracy of self-reports of sexual behavior. we review these studies, paying particular attention to a promising new development: the use of computer-assisted assessments. the collection of sexual risk behavior data with computers has increased dramatically in recent years, but little is known about the accuracy of such assessments. we summarize the evidence, discuss methodological issues that arise in studies evaluating the accuracy of self-reports, and offer recommendations for future research.\"usa\",increasing engagement through early recommender intervention,\"social network sites rely on the contributions of their members to create a lively and enjoyable space. recent research has focused on using personalization and recommender technologies to encourage participation of existing members. in this work we present an early-intervention approach to encouraging participation and engagement, which makes recommendations to new users during their sign-up process. our recommender system exploits external social media to produce people and profile entry recommendations for new users. we present results of a live user study, showing that users who received recommendations at sign-up created more social connections, contributed more content, and were on the whole more engaged with the system, contributing more without prompt and returning more often. we further show that recommendations for multiple content types yield significantly better results, in terms of user contribution and consumption; and that recommendations of more active users yield a higher return rate.\"usa\",feed me: motivating newcomer contribution in social network sites,\"social networking sites ({sns}) are only as good as the content their users share. therefore, designers of {sns} seek to improve the overall user experience by encouraging members to contribute more content. however, user motivations for contribution in {sns} are not well understood. this is particularly true for newcomers, who may not recognize the value of contribution. using server log data from approximately 140,000 newcomers in facebook, we predict long-term sharing based on the experiences the newcomers have in their first two weeks. we test four mechanisms: social learning, singling out, feedback, and distribution. in particular, we find support for social learning: newcomers who see their friends contributing go on to share more content themselves. for newcomers who are initially inclined to contribute, receiving feedback and having a wide audience are also predictors of increased sharing. on the other hand, singling out appears to affect only those newcomers who are not initially inclined to share. the paper concludes with design implications for motivating newcomer sharing in online communities.\"usa\",information revelation and privacy in online social networks,\"participation in social networking sites has dramatically increased in recent years. services such as friendster, tribe, or the facebook allow millions of individuals to create online profiles and share personal information with vast networks of friends - and, often, unknown numbers of strangers. in this paper we study patterns of information revelation in online social networks and their privacy implications. we analyze the online behavior of more than 4,000 carnegie mellon university students who have joined a popular social networking site catered to colleges. we evaluate the amount of information they disclose and study their usage of the site\\'s privacy settings. we highlight potential attacks on various aspects of their privacy, and we show that only a minimal percentage of users changes the highly permeable privacy preferences.\"usa\",from spaces to places: emerging contexts in mobile privacy,\"mobile privacy concerns are central to ubicomp and yet remain poorly understood. we advocate a diversified approach, enabling the cross-interpretation of data from complementary methods. however, mobility imposes a number of limitations on the methods that can be effectively employed. we discuss how we addressed this problem in an empirical study of mobile social networking. we report on how, by combining a variation of experience sampling and contextual interviews, we have started focusing on a notion of context in relation to privacy, which is subjectively defined by emerging socio-cultural knowledge, functions, relations and rules. with reference to gieryn\\'s sociological work, we call this place, as opposed to a notion of context that is objectively defined by physical and factual elements, which we call space. we propose that the former better describes the context for mobile privacy.\"usa\",earthquake shakes {twitter} users: real-time event detection by social sensors,\"twitter, a popular microblogging service, has received much attention recently. an important characteristic of twitter is its real-time nature. for example, when an earthquake occurs, people make many twitter posts (tweets) related to the earthquake, which enables detection of earthquake occurrence promptly, simply by observing the tweets. as described in this paper, we investigate the real-time interaction of events such as earthquakes in twitter and propose an algorithm to monitor tweets and to detect a target event. to detect a target event, we devise a classifier of tweets based on features such as the keywords in a tweet, the number of words, and their context. subsequently, we produce a probabilistic spatiotemporal model for the target event that can find the center and the trajectory of the event location. we consider each twitter user as a sensor and apply kalman filtering and particle filtering, which are widely used for location estimation in ubiquitous/pervasive computing. the particle filter works better than other comparable methods for estimating the centers of earthquakes and the trajectories of typhoons. as an application, we construct an earthquake reporting system in japan. because of the numerous earthquakes and the large number of twitter users throughout the country, we can detect an earthquake with high probability (96\\\\% of earthquakes of japan meteorological agency ({jma}) seismic intensity scale 3 or more are detected) merely by monitoring tweets. our system detects earthquakes promptly and sends e-mails to registered users. notification is delivered much faster than the announcements that are broadcast by the {jma}.\"usa\",knowledge sharing and yahoo answers: everyone knows something,\"yahoo answers ({ya}) is a large and diverse question-answer forum, acting not only as a medium for sharing technical knowledge, but as a place where one can seek advice, gather opinions, and satisfy one\\'s curiosity about a countless number of things. in this paper, we seek to understand {ya}\\'s knowledge sharing and activity. we analyze the forum categories and cluster them according to content characteristics and patterns of interaction among the users. while interactions in some categories resemble expertise sharing forums, others incorporate discussion, everyday advice, and support. with such a diversity of categories in which one can participate, we find that some users focus narrowly on specific topics, while others participate across categories. this not only allows us to map related categories, but to characterize the entropy of the users\\' interests. we find that lower entropy correlates with receiving higher answer ratings, but only for categories where factual expertise is primarily sought after. we combine both user attributes and answer characteristics to predict, within a given category, whether a particular answer will be chosen as the best answer by the asker.\"usa\",loose tweets: an analysis of privacy leaks on twitter,\"twitter has become one of the most popular microblogging sites for people to broadcast (or \"\"tweet\"\") their thoughts to the world in 140 characters or less. since these messages are available for public consumption, one may expect these tweets not to contain private or incriminating information. nevertheless we observe a large number of users who unwittingly post sensitive information about themselves and other people for whom there may be negative consequences. while some awareness exists of such privacy issues on social networks such as twitter and facebook, there has been no quantitative, scientific study addressing this problem. in this paper we make three major contributions. first, we characterize the nature of privacy leaks on twitter to gain an understanding of what types of private information people are revealing on it. we specifically analyze three types of leaks: divulging vacation plans, tweeting under the influence of alcohol, and revealing medical conditions. second, using this characterization we build automatic classifiers to detect incriminating tweets for these three topics in real time in order to demonstrate the real threat posed to users by, e.g., burglars and law enforcement. third, we characterize who leaks information and how. we study both self- incriminating primary leaks and secondary leaks that reveal sensitive information about others, as well as the prevalence of leaks in status updates and conversation tweets. we also conduct a cross-cultural study to investigate the prevalence of leaks in tweets originating from the united states, united kingdom and singapore. finally, we discuss how our classification system can be used as a defense mechanism to alert users of potential privacy leaks.\"usa\",finding high-quality content in social media,\"the quality of user-generated content varies drastically from excellent to abuse and spam. as the availability of such content increases, the task of identifying high-quality content sites based on user contributions --social media sites -- becomes increasingly important. social media in general exhibit a rich variety of information sources: in addition to the content itself, there is a wide array of non-content information available, such as links between items and explicit quality ratings from members of the community. in this paper we investigate methods for exploiting such community feedback to automatically identify high quality content. as a test case, we focus on yahoo! answers, a large community question/answering portal that is particularly rich in the amount and types of content and social interactions available in it. we introduce a general classification framework for combining the evidence from different sources of information, that can be tuned automatically for a given social media type and quality definition. in particular, for the community question/answering domain, we show that our system is able to separate high-quality items from the rest with an accuracy close to that of humans\"positive organizational behavior, and the impact of emotions on decision making.;  a ph.d. student of organizational behavior and theory at the tepper school of business, carnegie mellon university. his research focuses on work group effectiveness.;  a ph.d. student at the university of minnesota, department of computer science. her research currently focuses on technology for collaboration via mobile computing devices equipped with location-based information services. she is completing the design and implementation for a mobile technology called placemail, which will allow people to leave (and access) notes for themselves and others at places they frequent. more information is available at http://www.cs.umn.edu/\\\\~{}ludford.;  a ph.d. student in management information systems at the joseph m. katz graduate school of business, university of pittsburgh. her research interests include online community maintenance, community tool design, user participation, and technology acceptance.;  a ph.d. student of organizational behavior and theory at the tepper school of business, carnegie mellon university. her research interests include psychological contracts, social exchange and trust, ideological currency in distributed teams, knowledge management for remote workers, and social networks in virtual communities.;  a graduate student at the department of economics university of michigan, ann arbor. her research interest lies in the areas of behavioral public economics, experimental economics, and economics of internet.;  a ph.d. student in the computer science department, university of minnesota. his research interests include motivation in online communities, recommender systems, and the effects of technologies on the people that use them.;  a staff scientist in the computer science department, university of minnesota. his research interests are collaborative systems, recommender systems, and data mining.;  an associate professor of computer science and engineering at the university of minnesota. his research interests are human-computer interaction and computer-mediated communication. he is especially interested in the use of technology to help people create and develop strong social ties.;  a ph.d. student in the department of computer science \\\\& engineering, university of minnesota. he is interested in applying machine learning and data mining techniques to novel applications and problems, particularly in human computer interactions.;  a professor at the university of michigan school of information. his research focuses on recommender and reputation systems and other forms of sociotechnical capital.;  herbert a. simon professor of human-computer interaction at carnegie mellon university. dr. kraut has broad interests in the design and social impact of computing and conducts research on everyday use of the internet, technology and conversation, collaboration in small work groups, computing in organizations, and contributions to online communities. his most recent work examines factors influencing the success of online communities and ways to apply psychology theory to their design. more information is available at http://www.cs.cmu.edu/\\\\~{}kraut\",using social psychology to motivate contributions to online communities,\"under-contribution is a problem for many online communities. social psychology theories of social loafing and goal-setting can lead to mid-level design goals to address this problem. we tested design principles derived from these theories in four field experiments involving members of an online movie recommender community. in each of the experiments participated were given different explanations for the value of their contributions. as predicted by theory, individuals contributed when they were reminded of their uniqueness and when they were given specific and challenging goals. however, other predictions were disconfirmed. for example, in one experiment, participants given group goals contributed more than those given individual goals. the article ends with suggestions and challenges for mining design implications from social science theories.\"usa\",understanding user goals in web search,\"previous work on understanding user web search behavior has focused on how people search and what they are searching for, but not why they are searching. in this paper, we describe a framework for understanding the underlying goals of user searches, and our experience in using the framework to manually classify queries from a web search engine. our analysis suggests that so-called navigational\"\" searches are less prevalent than generally believed while a previously unexplored \"\"resource-seeking\"\" goal may account for a large fraction of web searches. we also illustrate how this knowledge of user search goals might be used to improve future web search engines.\",social {q\\\\&a},\"this article presents a review and analysis of the research literature in social {q\\\\&a} ({sqa}), a term describing systems where people ask, answer, and rate content while interacting around it. the growth of {sqa} is contextualized within the broader trend of user-generated content from usenet to web 2.0, and alternative definitions of {sqa} are reviewed. {sqa} sites have been conceptualized in the literature as simultaneous examples of tools, collections, communities, and complex sociotechnical systems. major threads of {sqa} research include user-generated and algorithmic question categorization, answer classification and quality assessment, studies of user satisfaction, reward structures, and motivation for participation, and how trust and expertise are both operationalized by and emerge from {sqa} sites. directions for future research are discussed, including more refined conceptions of {sqa} site participants and their roles, unpacking the processes by which social capital is achieved, managed, and wielded in {sqa} sites, refining question categorization, conducting research within and across a wider range of {sqa} sites, the application of economic and game-theoretic models, and the problematization of {sqa} itself.\",best-answer selection criteria in a social {q\\\\&a} site from the user-oriented relevance perspective,\"abstract as an attempt to better understand how people seek, share, and evaluate information in a social {q\\\\&a} environment, this study identifies the selection criteria people employ when they select best answers in yahoo! answers in the context of relevance research. using content analysis, we analyzed the comments people left upon the selection of best answers to their own questions. from 1,200 samples of comments, only 465 mentioned the specific reasons for their selection, thus becoming eligible for analysis. through an iterative process of evaluating the types of comments, the best-answer selection criteria were inductively derived and grouped into seven value categories: content value, cognitive value, socio-emotional value, information source value, extrinsic value, utility, and general statement. while many of the identified criteria overlap with those found in previous relevance studies, the socio-emotional value was particularly prominent in this study, especially when people ask for opinions and suggestions. these findings reflect the characteristics of a social {q\\\\&a} site and extend our understanding of the relevance of an electronic environment where people bring their every day problem-solving and decision-making tasks.\"\"social media technologies let people connect by creating and sharing content. we examine the use of twitter by famous people to conceptualize celebrity as a practice. on twitter, celebrity is practiced through the appearance and performance of \\'backstage\\' access. celebrity practitioners reveal what appears to be personal information to create a sense of intimacy between participant and follower, publicly acknowledge fans, and use language and cultural references to create affiliations with followers. interactions with other celebrity practitioners and personalities give the impression of candid, uncensored looks at the people behind the personas. but the indeterminate \\'authenticity\\' of these performances appeals to some audiences, who enjoy the game playing intrinsic to gossip consumption. while celebrity practice is theoretically open to all, it is not an equalizer or democratizing discourse. indeed, in order to successfully practice celebrity, fans must recognize the power differentials intrinsic to the relationship.\"usa\",\"questions in, knowledge in?: a study of naver\\'s question answering community\",\"large general-purposed community question-answering sites are becoming popular as a new venue for generating knowledge and helping users in their information needs. in this paper we analyze the characteristics of knowledge generation and user participation behavior in the largest question-answering online community in south korea, naver {knowledge-in}. we collected and analyzed over 2.6 million question/answer pairs from fifteen categories between 2002 and 2007, and have interviewed twenty six users to gain insights into their motivations,roles, usage and expertise. we find altruism, learning, and competency are frequent motivations for top answerers to participate, but that participation is often highly intermittent. using a simple measure of user performance, we find that higher levels of participation correlate with better performance. we also observe that users are motivated in part through a point system to build a comprehensive knowledge database. these and other insights have significant implications for future knowledge generating online communities.\"usa\",understanding the intent behind mobile information needs,\"mobile phones are becoming increasingly popular as a means of information access while on-the-go. mobile users are likely to be interested in locating different types of content. however, the mobile space presents a number of key challenges, many of which go beyond issues with device characteristics such as screen-size and input capabilities. in particular, changing contexts such as location, time, activity and social interactions are likely to impact on the types of information needs that arise. in order to offer personalized, effective mobile services we need to understand mobile users in more detail. thus we carried out a four-week diary study of mobile information needs, looking in particular at the goal/intent behind mobile information needs, the topics users are interested in and the impact of mobile contexts such as location and time on user needs.\"usa\",\"\"\"alone together?\"\": exploring the social dynamics of massively multiplayer online games\",\"massively multiplayer online games ({mmogs}) routinely attract millions of players but little empirical data is available to assess their players\\' social experiences. in this paper, we use longitudinal data collected directly from the game to examine play and grouping patterns in one of the largest {mmogs}: world of warcraft. our observations show that the prevalence and extent of social activities in {mmogs} might have been previously over-estimated, and that gaming communities face important challenges affecting their cohesion and eventual longevity. we discuss the implications of our findings for the design of future games and other online social spaces.\"usa\",deciphering mobile search patterns: a study of yahoo! mobile search queries,\"in this paper we study the characteristics of search queries submitted from mobile devices using various yahoo! {one-search} applications during a 2 months period in the second half of 2007, and report the query patterns derived from 20 million english sample queries submitted by users in {us}, canada, europe, and asia. we examine the query distribution and topical categories the queries belong to in order to find new trends. we compare and contrast the search patterns between {us} vs international queries, and between queries from various search interfaces ({xhtml}/{wap}, java widgets, and {sms}). we also compare our results with previous studies wherever possible, either to confirm previous findings, or to find interesting differences in the query distribution and pattern.\"london, uk\",how are habits formed: modelling habit formation in the real world,\"to investigate the process of habit formation in everyday life, 96 volunteers chose an eating, drinking or activity behaviour to carry out daily in the same context (for example \\'after breakfast\\') for 12 weeks. they completed the self-report habit index ({srhi}) each day and recorded whether they carried out the behaviour. the majority (82) of participants provided sufficient data for analysis, and increases in automaticity (calculated with a sub-set of {srhi} items) were examined over the study period. nonlinear regressions fitted an asymptotic curve to each individual\\'s automaticity scores over the 84 days. the model fitted for 62 individuals, of whom 39 showed a good fit. performing the behaviour more consistently was associated with better model fit. the time it took participants to reach 95\\\\% of their asymptote of automaticity ranged from 18 to 254 days; indicating considerable variation in how long it takes people to reach their limit of automaticity and highlighting that it can take a very long time. missing one opportunity to perform the behaviour did not materially affect the habit formation process. with repetition of a behaviour in a consistent context, automaticity increases following an asymptotic curve which can be modelled at the individual level. copyright {\\\\copyright} 2009 john wiley \\\\& sons, ltd.\"usa\",discovering users\\' topics of interest on twitter: a first look,\"twitter, a micro-blogging service, provides users with a framework for writing brief, often-noisy postings about their lives. these posts are called \"\"tweets.\"\" in this paper we present early results on discovering twitter users\\' topics of interest by examining the entities they mention in their tweets. our approach leverages a knowledge base to disambiguate and categorize the entities in the tweets. we then develop a \"\"topic profile,\"\" which characterizes users\\' topics of interest, by discerning which categories appear frequently and cover the entities. we demonstrate that even in this early work we are able to successfully discover the main topics of interest for the users in our study.\"usa\",finding the right facts in the crowd: factoid question answering over social media,\"community question answering has emerged as a popular and effective paradigm for a wide range of information needs. for example, to find out an obscure piece of trivia, it is now possible and even very effective to post a question on a popular community {qa} site such as yahoo! answers, and to rely on other users to provide answers, often within minutes. the importance of such community {qa} sites is magnified as they create archives of millions of questions and hundreds of millions of answers, many of which are invaluable for the information needs of other searchers. however, to make this immense body of knowledge accessible, effective answer retrieval is required. in particular, as any user can contribute an answer to a question, the majority of the content reflects personal, often unsubstantiated opinions. a ranking that combines both relevance and quality is required to make such archives usable for factual information retrieval. this task is challenging, as the structure and the contents of community {qa} archives differ significantly from the web setting. to address this problem we present a general ranking framework for factual information retrieval from social media. results of a large scale evaluation demonstrate that our method is highly effective at retrieving well-formed, factual answers to questions, as evaluated on a standard factoid {qa} benchmark. we also show that our learning framework can be tuned with the minimum of manual labeling. finally, we provide result analysis to gain deeper understanding of which features are significant for social media search and retrieval. our system can be used as a crucial building block for combining results from a variety of social media content with general web search results, and to better integrate social media content for effective information access.\"'),\n",
       " ('589b870a611c25fa99bd3d7295ac0622',\n",
       "  '\"one of the main challenges of securing broadcast communication is source authentication, or enabling receivers of broadcast data to verify that the received data really originates from the claimed source and was not modified en route. this problem is complicated by mutually untrusted receivers and unreliable communication environments where the sender does not retransmit lost packets.\"practical broadcast authentication in sensor networks,\"broadcast authentication is a critical security service in sensor networks; it allows a sender to broadcast messages to multiple nodes in an authenticated way. /spl {mu/tesla} and multi-level /spl {mu/tesla} have been proposed to provide such services for sensor networks. however, none of these techniques are scalable in terms of the number of senders. though multi-level /spl {mu/tesla} schemes can scale up to large sensor networks (in terms of receivers), they either use substantial bandwidth and storage at sensor nodes, or require significant resources at senders to deal with {dos} attacks. this paper presents efficient techniques to support a potentially large number of broadcast senders using /spl {mu/tesla} instances as building blocks. the proposed techniques are immune to the {dos} attacks. this paper also provides two approaches, a revocation tree based scheme and a proactive distribution based scheme, to revoke the broadcast authentication capability from compromised senders. the proposed techniques are implemented, and evaluated through simulation on {tinyos}. the analysis and experiment show that these techniques are efficient and practical, and can achieve better performance than the previous approaches.\"usa\",multilevel \\\\&\\\\#{956;tesla}: broadcast authentication for distributed sensor networks,\"broadcast authentication is a fundamental security service in distributed sensor networks. this paper presents the development of a scalable broadcast authentication scheme named <i>multilevel {μtesla}</i> based on {μtesla}, a broadcast authentication protocol whose scalability is limited by its unicast-based initial parameter distribution. multilevel {μtesla} satisfies several nice properties, including low overhead, tolerance of message loss, scalability to large networks, and resistance to replay attacks as well as denial-of-service attacks. this paper also presents the experimental results obtained through simulation, which demonstrate the performance of the proposed scheme under severe denial-of-service attacks and poor channel quality.\"\"recent advancement in wireless communica- tions and electronics has enabled the develop- ment of low-cost sensor networks. the sensor networks can be used for various application areas (e.g., health, military, home). for different application areas, there are different technical issues that researchers are currently resolving. the current state of the art of sensor networks is captured in this article, where solutions are discussed under their related protocol stack layer sections. this article ...\",on the survivability of routing protocols in ad hoc wireless networks,\"survivable routing protocols are able to provide service in the presence of attacks and failures. the strongest attacks that protocols can experience are attacks where adversaries have full control of a number of authenticated nodes that behave arbitrarily to disrupt the network, also referred to as byzantine attacks. this work examines the survivability of ad hoc wireless routing protocols in the presence of several byzantine attacks: black holes, flood rushing, wormholes and overlay network wormholes. traditional secure routing protocols that assume authenticated nodes can always be trusted, fail to defend against such attacks. our protocol, {odsbr}, is an on-demand wireless routing protocol able to provide correct service in the presence of failures and byzantine attacks. we demonstrate through simulation its effectiveness in mitigating such attacks. our analysis of the impact of these attacks versus the adversary\\x92s effort gives insights into their relative strengths, their interaction and their importance when designing wireless routing protocols.\"wormhole attacks in wireless networks,\"as mobile ad hoc network applications are deployed, security emerges as a central requirement. in this paper, we introduce the wormhole attack, a severe attack in ad hoc networks that is particularly challenging to defend against. the wormhole attack is possible even if the attacker has not compromised any hosts, and even if all communication provides authenticity and confidentiality. in the wormhole attack, an attacker records packets (or bits) at one location in the network, tunnels them (possibly selectively) to another location, and retransmits them there into the network. the wormhole attack can form a serious threat in wireless networks, especially against many ad hoc network routing protocols and location-based wireless security systems. for example, most existing ad hoc network routing protocols, without some mechanism to defend against the wormhole attack, would be unable to find routes longer than one or two hops, severely disrupting communication. we present a general mechanism, called packet leashes, for detecting and, thus defending against wormhole attacks, and we present a specific protocol, called {tik}, that implements leashes. we also discuss topology-based wormhole detection, and show that it is impossible for these approaches to detect some wormhole topologies.\"\"due to the broadcast nature of radio transmissions, communications in mobile ad hoc networks ({manets}) are more susceptible to malicious traffic analysis. in this paper we propose a novel anonymous on-demand routing protocol, termed {mask}, to enable anonymous communications thereby thwarting possible traffic analysis attacks. based on a new cryptographic concept called pairing, we first propose an anonymous neighborhood authentication protocol which allows neighboring nodes to authenticate each other without revealing their identities. then utilizing the secret pairwise link identifiers and keys established between neighbors during the neighborhood authentication process, {mask} fulfills the routing and packet forwarding tasks nicely without disclosing the identities of participating nodes under a rather strong adversarial model. {mask} provides the desirable sender and receiver anonymity, as well as the relationship anonymity of the sender and receiver. it is also resistant to a wide range of adversarial attacks. moreover, {mask} preserves the routing efficiency in contrast to previous proposals. detailed anonymity analysis and simulation studies are carried out to validate and justify the effectiveness of {mask}.\"\"as mobile ad hoc network applications are deployed, security emerges as a central requirement. in this paper, we introduce the wormhole attack, a severe attack in ad hoc networks that is particularly challenging to defend against. the wormhole attack is possible even if the attacker has not compromised any hosts, and even if all communication provides authenticity and confidentiality. in the wormhole attack, an attacker records packets (or bits) at one location in the network, tunnels them (possibly selectively) to another location, and retransmits them there into the network. the wormhole attack can form a serious threat in wireless networks, especially against many ad hoc network routing protocols and location-based wireless security systems. for example, most existing ad hoc network routing protocols, without some mechanism to defend against the wormhole attack, would be unable to find routes longer than one or two hops, severely disrupting communication. we present a new, general mechanism, called packet leashes, for detecting and thus defending against wormhole attacks, and we present a specific protocol, called {tik}, that implements leashes.\"'),\n",
       " ('90f1a3e6fcdbf9bc550e866116bbcea5',\n",
       "  '\"the transition from unicellular to differentiated multicellular organisms constitutes an increase in the level complexity, because previously existing individuals are combined to form a new, higher-level individual. the volvocine algae represent a unique opportunity to study this transition because they diverged relatively recently from unicellular relatives and because extant species display a range of intermediate grades between unicellular and multicellular, with functional specialization of cells. following the approach darwin used to understand  ” organs of extreme perfection” such as the vertebrate eye, this jump in complexity can be reduced to a series of small steps that cumulatively describe a gradual transition between the two levels. we use phylogenetic reconstructions of ancestral character states to trace the evolution of steps involved in this transition in volvocine algae. the history of these characters includes several well-supported instances of multiple origins and reversals. the inferred changes can be understood as components of cooperation–conflict–conflict mediation cycles as predicted by multilevel selection theory. one such cycle may have taken place early in volvocine evolution, leading to the highly integrated colonies seen in extant volvocine algae. a second cycle, in which the defection of somatic cells must be prevented, may still be in progress.\"2005-03-07 02:40:55,\"department of biology, washington university, campus box 1229, st. louis, mo 63130, usa.\",a twelve-step program for evolving multicellularity and a division of labor.,\"the volvocine algae provide an unrivalled opportunity to explore details of an evolutionary pathway leading from a unicellular ancestor to multicellular organisms with a division of labor between different cell types. members of this monophyletic group of green flagellates range in complexity from unicellular chlamydomonas through a series of extant organisms of intermediate size and complexity to volvox, a genus of spherical organisms that have thousands of cells and a germ-soma division of labor. it is estimated that these organisms all shared a common ancestor about 50 +/- 20 {mya}. here we outline twelve important ways in which the developmental repertoire of an ancestral unicell similar to modern c. reinhardtii was modified to produce first a small colonial organism like gonium that was capable of swimming directionally, then a sequence of larger organisms (such as pandorina, eudorina and pleodorina) in which there was an increasing tendency to differentiate two cell types, and eventually volvox carteri with its complete germ-soma division of labor.\"\"individuality is a complex trait, yet a series of stages each advantageous in itself can be shown to exist allowing evolution to get from unicellular individuals to multicellular individuals. we consider several of the key stages involved in this transition: the initial advantage of group formation, the origin of reproductive altruism within the group, and the further specialization of cell types as groups increase in size. how do groups become individuals? this is the central question we address. our hypothesis is that fitness tradeoffs drive the transition of a cell group into a multicellular individual through the evolution of cells specialized at reproductive and vegetative functions of the group. we have modeled this hypothesis and have tested our models in two ways. we have studied the origin of the genetic basis for reproductive altruism (somatic cells specialized at vegetative functions) in the multicellular volvox carteri by showing how an altruistic gene may have originated through cooption of a life-history tradeoff gene present in a unicellular ancestor. second, we ask why reproductive altruism and individuality arise only in the larger members of the volvocine group (recognizing that high levels of kinship are present in all volvocine algae groups). our answer is that the selective pressures leading to reproductive altruism stem from the increasing cost of reproduction with increasing group size. concepts from population genetics and evolutionary biology appear to be sufficient to explain complexity, at least as it relates to the problem of the major transitions between the different kinds of evolutionary individuals.\"\"{gc}-biased gene conversion ({gbgc}) is a process that tends to increase the {gc} content of recombining {dna} over evolutionary time and is thought to explain the evolution of {gc} content in mammals and yeasts. evidence for {gbgc} outside these two groups is growing but is still limited. here, we analyzed 36 completely sequenced genomes representing four of the five major groups in eukaryotes (unikonts, excavates, chromalveolates and plantae). {gbgc} was investigated by directly comparing {gc} content and recombination rates in species where recombination data are available, that is, half of them. to study all species of our dataset, we used chromosome size as a proxy for recombination rate and compared it with {gc} content. among the 17 species showing a significant relationship between {gc} content and chromosome size, 15 are consistent with the predictions of the {gbgc} model. importantly, the species showing a pattern consistent with {gbgc} are found in all the four major groups of eukaryotes studied, which suggests that {gbgc} may be widespread in eukaryotes.\"\"background and aims consensus higher-level molecular phylogenies present a compelling case that an ancient divergence separates eukaryotic green algae into two major monophyletic lineages, chlorophyta and streptophyta, and a residuum of green algae, which have been referred to prasinophytes or micromonadophytes. nuclear {dna} content estimates have been published for less than 1\\\\% of the described green algal members of chlorophyta, which includes multicellular green marine algae and freshwater flagellates (e.g. chlamydomonas and volvox). the present investigation summarizes the state of our knowledge and adds substantially to our database of c-values, especially for the streptophyte charophycean lineage which is the sister group of the land plants. a recent list of {2c} nuclear {dna} contents for isolates and species of green algae is expanded by 72 to {157.methods} the {dna}-localizing fluorochrome {dapi} (4′,6-diamidino-2-phenylindole) and red blood cell (chicken erythrocytes) standard were used to estimate {2c} values with static {microspectrophotometry.key} results in chlorophyta, including chlorophyceae, prasinophyceae, trebouxiophyceae and ulvophyceae, {2c} {dna} estimates range from 0·01 to 5·8\\xa0pg. nuclear {dna} content variation trends are noted and discussed for specific problematic taxon pairs, including {ulotrichales–ulvales}, and {cladophorales–siphonocladales}. for streptophyta, {2c} nuclear {dna} contents range from 0·2 to 6·4\\xa0pg, excluding the highly polyploid charales and desmidiales, which have genome sizes of up to 14·8 and 46·8\\xa0pg, respectively. nuclear {dna} content data for streptophyta superimposed on a contemporary molecular phylogeny indicate that early diverging lineages, including some members of chlorokybales, coleochaetales and klebsormidiales, have genomes as small as 0·1–0·5\\xa0pg. it is proposed that the streptophyte ancestral nuclear genome common to both the charophyte and the embryophyte lineages can be characterized as {1c} = 0·2\\xa0pg and 1n = {6.conclusions} these data will help pre-screen candidate species for the on-going construction of bacterial artificial chromosome nuclear genome libraries for land plant ancestors. data for the prasinophyte mesostigma are of particular interest as this alga reportedly most closely resembles the \\'ancestral green flagellate\\'. both mechanistic and ecological processes are discussed that could have produced the observed c-value increase of >100-fold in the charophyte green algae whereas the ancestral genome was conserved in the embryophytes.\"'),\n",
       " ('6527bbd2024cf027a9ba75b3e28ed4db',\n",
       "  '\"many networks of interest in the sciences, including social networks, computer networks, and metabolic and regulatory networks, are found to divide naturally into communities or modules. the problem of detecting and characterizing this community structure is one of the outstanding issues in the study of networked systems. one highly effective approach is the optimization of the quality function known as  ” modularity” over the possible divisions of a network. here i show that the modularity can be expressed in terms of the eigenvectors of a characteristic matrix for the network, which i call the modularity matrix, and that this expression leads to a spectral algorithm for community detection that returns results of demonstrably higher quality than competing methods in shorter running times. i illustrate the method with applications to several published network data sets.\"via della ricerca scientifica 1, 00133 rome, italy.\",defining and identifying communities in networks,\"the investigation of community structures in networks is an important issue in many domains and disciplines. this problem is relevant for social tasks (objective analysis of relationships on the web), biological inquiries (functional studies in metabolic and protein networks), or technological problems (optimization of large infrastructures). several types of algorithms exist for revealing the community structure in networks, but a general and quantitative definition of community is not implemented in the algorithms, leading to an intrinsic difficulty in the interpretation of the results without any additional nontopological information. in this article we deal with this problem by showing how quantitative definitions of community are implemented in practice in the existing algorithms. in this way the algorithms for the identification of the community structure become fully self-contained. furthermore, we propose a local algorithm to detect communities which outperforms the existing algorithms with respect to computational cost, keeping the same level of reliability. the algorithm is tested on artificial and real-world graphs. in particular, we show how the algorithm applies to a network of scientific collaborations, which, for its size, cannot be attacked with the usual methods. this type of local algorithm could open the way to applications to large-scale technological and biological systems.\"'),\n",
       " ('d705113f772771b23bcd17303ee0860a',\n",
       "  '\"wikis are web-based applications that allow all users not only to view pages but also to change them. the recent success of the internet encyclopedia wikipedia has drawn increasing attention from private users, small organizations and enterprises to the various possible uses of wikis. their simple structure and straightforward operation make them a serious alternative to expensive content management systems and also provide a basis for many applications in the area of collaborative work. we show the practical use of wikis in carrying out projects for users as well as for maintainers. this includes a step-by-step introduction to wiki philosophy, social effects and functions, a survey of their controls and components, and the installation and configuration of the wiki clones {mediawiki} and {twiki}. in order to exemplify the possibilities of the software, we use it as a project tool for planning a conference.\"\"wikis provide new opportunities for learning and for collaborative knowledge building as well as for understanding these processes. this article presents a theoretical framework for describing how learning and collaborative knowledge building take place. in order to understand these processes, three aspects need to be considered: the social processes facilitated by a wiki, the cognitive processes of the users, and how both processes influence each other mutually. for this purpose, the model presented in this article borrows from the systemic approach of luhmann as well as from piaget\\'s theory of equilibration and combines these approaches. the model analyzes processes which take place in the social system of a wiki as well as in the cognitive systems of the users. the model also describes learning activities as processes of externalization and internalization. individual learning happens through internal processes of assimilation and accommodation, whereas changes in a wiki are due to activities of external assimilation and accommodation which in turn lead to collaborative knowledge building. this article provides empirical examples for these equilibration activities by analyzing wikipedia articles. equilibration activities are described as being caused by subjectively perceived incongruities between an individuals\\' knowledge and the information provided by a wiki. incongruities of medium level cause cognitive conflicts which in turn activate the described processes of equilibration and facilitate individual learning and collaborative knowledge building.\"\"{<strong>toward an anthropology of graphing: semiotic and activity-theoretic perspectives</strong> presents the results of several studies involving scientists and technicians. in part one of the book, \"\"graphing in captivity\"\", the author describes and analyses the interpretation scientists volunteered given graphs that had been culled from an introductory course and textbook in ecology. surprisingly, the scientists were not the experts that the author expected them to be on the basis of the existing expert-novice literature. the section ends with the analysis of graphs that the scientists had culled from their own work. here, they articulated a tremendous amount of background understanding before talking about the content of their graphs. in part two, \"\"graphing in the wild\"\", the author reports on graph usage in three different workplaces based on his ethnographic research among scientists and technicians. based on these data, the author concludes that graphs and graphing are meaningful to the extent that they are deeply embedded in and connected to the familiarity with the workplace.}\"cognitive and computational perspectives)\",\"one of the key means by which knowledge is disseminated in the academic discourse community is the spoken presentation of papers at an academic conference. in contrast to the written research article, the spoken presentation remains relatively under-researched from a linguistic perspective, limiting the knowledge available for explicating this kind of discourse in academic language programs. in this paper, we draw on a social semiotic theory of language (systemic functional linguistics) and of gesture, to frame a multi-layered exploration of interpersonal meaning in this register that incorporates attention to generic staging, to expressions of attitude, and to the co-expression of attitudinal language and gesture. the data are a set of plenary presentations at an academic conference, and the study aims to explore means by which the speakers construe a relationship of solidarity with their audiences in the introductory or \\'set-up\\' stage of their talk.\"a commentary of gonzalez\\'s adaptation of activity theory for {hci} is presented. it critiques his proposal for a new level of analysis that is called working spheres/engagements. while considered insightful it questions whether his new framework will be used by other researchers.cognitive and computational perspectives)\",3rd edition\",\"{there\\'s a \"\"frank \\\\& ernest\"\" comic strip showing a chick breaking out of its shell, looking around, and saying, \"\"oh, wow! paradigm shift!\"\" blame the late thomas kuhn. few indeed are the philosophers or historians influential enough to make it into the funny papers, but kuhn is one.<p>  <i>the structure of scientific revolutions</i> is indeed a paradigmatic work in the history of science. kuhn\\'s use of terms such as \"\"paradigm shift\"\" and \"\"normal science,\"\" his ideas of how scientists move from disdain through doubt to acceptance of a new theory, his stress on social and psychological factors in science--all have had profound effects on historians, scientists, philosophers, critics, writers, business gurus, and even the cartoonist in the street.<p>  some scientists (such as steven weinberg and ernst mayr) are profoundly irritated by kuhn, especially by the doubts he casts--or the way  his work has been used to cast doubt--on the idea of scientific progress. yet it has been said that the acceptance of plate tectonics in the 1960s, for instance, was sped by geologists\\' reluctance to be on the downside of a paradigm shift. even weinberg has said that \"\"<i>structure</i> has had a wider influence than any other book on the history of science.\"\" as one of kuhn\\'s obituaries noted, \"\"we all live in a post-kuhnian age.\"\" <i>--mary ellen curtin</i> }\"\"cultural-historical activity theory is a new framework aimed at transcending the dichotomies of micro- and macro-, mental and material, observation and intervention in analysis and redesign of work. the approach distinguishes between short-lived goal-directed actions and durable, object-oriented activity systems. a historically evolving collective activity system, seen in its network relations to other activity systems, is taken as the prime unit of analysis against which scripted strings of goal-directed actions and automatic operations are interpreted. activity systems are driven by communal motives that are often difficult to articulate for individual participants. activity systems are in constant movement and internally contradictory. their systemic contradictions, manifested in disturbances and mundane innovations, offer possibilities for expansive developmental transformations. such transformations proceed through stepwise cycles of expansive learning which begin with actions of questioning the existing standard practice, then proceed to actions of analyzing its contradictions and modelling a vision for its zone of proximal development, then to actions of examining and implementing the new model in practice. new forms of work organization increasingly require negotiated ?knotworking? across boundaries. correspondingly, expansive learning increasingly involves horizontal widening of collective expertise by means of debating, negotiating and hybridizing different perspectives and conceptualizations. findings from a longitudinal intervention study of children\\'s medical care illuminate the theoretical arguments.\"\"this interdisciplinary work presents an integration of theory and research on how children develop their thinking as they participate in cultural activity with the guidance and challenge of their caregivers and other companions. the author, a leading developmental psychologist, views development as an apprenticeship in which children engage in the use of intellectual tools in societally structured activities with parents, other adults, and children. the author has gathered evidence from various disciplines--cognitive, developmental, and cultural psychology; anthropology; infancy studies; and communication research--furnishing a coherent and broadly based account of cognitive development in its sociocultural context. this work examines the mutual roles of the individual and the sociocultural world, and the culturally based processes by which children appropriate and extend skill and understanding from their involvement in shared thinking with other people. the book is written in a lively and engaging style and is supplemented by photographs and original illustrations by the author.\"activities, contexts, technologies\",\"intended for designers and researchers, context and consciousness brings together 13 contributions that apply activity theory to problems of human-computer interaction. understanding how people actually use computers in their everyday lives is essential to good design and evaluation. this insight necessitates a move out of the laboratory and into the field. the research described in context and consciousness presents activity theory as a means of structuring and guiding field studies of human-computer interaction, from practical design to theoretical development. activity theory is a psychological theory with a naturalistic emphasis, with roots going back to the 1920s in the soviet union. it provides a hierarchical framework for describing activity and a set of perspectives on practice. activity theory has been fruitfully applied in many areas of human need, including problems of mentally and physically handicapped children, educational testing, curriculum design, and ergonomics. there is growing interest in applying activity theory to problems of human- computer interaction, and an international community of researchers is contributing to the effort. contributors: rachel bellamy. susanne b{\\\\o}dker. ellen christiansen. yrjo engestr\\\\\"\"{o}m. virginia escalante. dorothy holland. victor kaptelinin. kari kuutti. bonnie a. nardi. arne raeithel. james reeves. boris velichkovksy. vladimir p. zinchenko.\"usa\",why we blog,\"bloggers are driven to document their lives, provide commentary and opinions, express deeply felt emotions, articulate ideas through writing, and form and maintain community forums.\"\"{this book brings together contributions from researchers within various social science disciplines who seek to redefine the methods and topics that constitute the study of work. they investigate work activity in ways that do not reduce it to a \"\"psychology\"\" of individual cognition or to a \"\"sociology\"\" of societal structures and communication. a key theme in the material is the relationship between theory and practice. mindful practices and communicative interaction are examined as situated issues at work in the reproduction of communities of practice in a variety of settings including: courts of law, computer software design, the piloting of airliners, the coordination of air traffic control,  and traffic management in underground railway systems.}\"\"{<p> in a book of intellectual breadth, james wertsch not only     offers a synthesis and critique of all vygotsky\\'s major ideas, but also presents a program for     using vygotskian theory as a guide to contemporary research in the social sciences and     humanities. he draws extensively on all vygotsky\\'s works, both in russian and in english, as     well as on his own studies in the soviet union with colleagues and students of vygotsky.     </p><p> vygotsky\\'s writings are an enormously rich source of ideas     for those who seek an account of the mind as it relates to the social and physical world.     wertsch explores three central themes that run through vygotsky\\'s work: his insistence on using     genetic, or developmental, analysis; his claim that higher mental functioning in the individual     has social origins; and his beliefs about the role of tools and signs in human social and     psychological activity wertsch demonstrates how the notion of semiotic mediation is essential to     understanding vygotsky\\'s unique contribution to the study of human consciousness.     </p><p> in the last four chapters wertsch extends vygotsky\\'s claims     in light of recent research in linguistics, semiotics, and literary theory. the focus on     semiotic phenomena, especially human language, enables him to integrate findings from the wide     variety of disciplines with which vygotsky was concerned wertsch shows how vygotsky\\'s approach     provides a principled way to link the various strands of human science that seem more isolated     than ever today. </p>}\"'),\n",
       " ('60a321bf89d186c88a3a1d2b2b165ce0',\n",
       "  '\"we review the biogeography of microorganisms in light of the biogeography of macroorganisms. a large body of research supports the idea that free-living microbial taxa exhibit biogeographic patterns. current evidence confirms that, as proposed by the {baas-becking} hypothesis, \\'the environment selects\\' and is, in part, responsible for spatial variation in microbial diversity. however, recent studies also dispute the idea that \\'everything is everywhere\\'. we also consider how the processes that generate and maintain biogeographic patterns in macroorganisms could operate in the microbial world.\"\"last year marked the 10th anniversary of the birth of phylogeography as a formal discipline. however, the field\\'s gestation began in the mid-1970s with the introduction of mitochondrial (mt) {dna} analyses to population genetics, and to the profound shift toward genealogical thought at the intraspecific level (now formalized as coalescent theory) that these methods prompted. this paper traces the early history and explosive growth of phylogeography, and closes with predictions about future challenges for the field that centre on several facets of genealogical concordance.\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function \n",
    "def joinListElements(x):\n",
    "    return ''.join(x)\n",
    "#Create new RDD containing userHash and abstract from RDD of join result\n",
    "#Using groupByKey() and mapValues to group values for each key into a single list\n",
    "transJoinResultRDD = joinResultRDD.map(lambda x: (x[1][0], x[1][1].strip())).groupByKey().mapValues(list).mapValues(joinListElements)\n",
    "transJoinResultRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('f05bcffe7951de9e5a32fff4a42eb088',\n",
       "  ['a',\n",
       "   'protein',\n",
       "   'domain',\n",
       "   'database',\n",
       "   'for',\n",
       "   'functional',\n",
       "   'characterization',\n",
       "   'and',\n",
       "   'annotation.\",\"{prosite}',\n",
       "   'consists',\n",
       "   'of',\n",
       "   'documentation',\n",
       "   'entries',\n",
       "   'describing',\n",
       "   'protein',\n",
       "   'domains,',\n",
       "   'families',\n",
       "   'and',\n",
       "   'functional',\n",
       "   'sites,',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'associated',\n",
       "   'patterns',\n",
       "   'and',\n",
       "   'profiles',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'them.',\n",
       "   'it',\n",
       "   'is',\n",
       "   'complemented',\n",
       "   'by',\n",
       "   '{prorule},',\n",
       "   'a',\n",
       "   'collection',\n",
       "   'of',\n",
       "   'rules',\n",
       "   'based',\n",
       "   'on',\n",
       "   'profiles',\n",
       "   'and',\n",
       "   'patterns,',\n",
       "   'which',\n",
       "   'increases',\n",
       "   'the',\n",
       "   'discriminatory',\n",
       "   'power',\n",
       "   'of',\n",
       "   'these',\n",
       "   'profiles',\n",
       "   'and',\n",
       "   'patterns',\n",
       "   'by',\n",
       "   'providing',\n",
       "   'additional',\n",
       "   'information',\n",
       "   'about',\n",
       "   'functionally',\n",
       "   'and/or',\n",
       "   'structurally',\n",
       "   'critical',\n",
       "   'amino',\n",
       "   'acids.',\n",
       "   '{prosite}',\n",
       "   'is',\n",
       "   'largely',\n",
       "   'used',\n",
       "   'for',\n",
       "   'the',\n",
       "   'annotation',\n",
       "   'of',\n",
       "   'domain',\n",
       "   'features',\n",
       "   'of',\n",
       "   '{uniprotkb}/{swiss-prot}',\n",
       "   'entries.',\n",
       "   'among',\n",
       "   'the',\n",
       "   '983',\n",
       "   '({dna}-binding)',\n",
       "   'domains,',\n",
       "   'repeats',\n",
       "   'and',\n",
       "   'zinc',\n",
       "   'fingers',\n",
       "   'present',\n",
       "   'in',\n",
       "   '{swiss-prot}',\n",
       "   '(release',\n",
       "   '57.8',\n",
       "   'of',\n",
       "   '22',\n",
       "   'september',\n",
       "   '2009),',\n",
       "   '696',\n",
       "   '(',\n",
       "   'approximately',\n",
       "   '70\\\\%)',\n",
       "   'are',\n",
       "   'annotated',\n",
       "   'with',\n",
       "   '{prosite}',\n",
       "   'descriptors',\n",
       "   'using',\n",
       "   'information',\n",
       "   'from',\n",
       "   '{prorule}.',\n",
       "   'in',\n",
       "   'order',\n",
       "   'to',\n",
       "   'allow',\n",
       "   'better',\n",
       "   'functional',\n",
       "   'characterization',\n",
       "   'of',\n",
       "   'domains,',\n",
       "   '{prosite}',\n",
       "   'developments',\n",
       "   'focus',\n",
       "   'on',\n",
       "   'subfamily',\n",
       "   'specific',\n",
       "   'profiles',\n",
       "   'and',\n",
       "   'a',\n",
       "   'new',\n",
       "   'profile',\n",
       "   'building',\n",
       "   'method',\n",
       "   'giving',\n",
       "   'more',\n",
       "   'weight',\n",
       "   'to',\n",
       "   'functionally',\n",
       "   'important',\n",
       "   'residues.',\n",
       "   'here,',\n",
       "   'we',\n",
       "   'describe',\n",
       "   '{amsa},',\n",
       "   'an',\n",
       "   'annotated',\n",
       "   'multiple',\n",
       "   'sequence',\n",
       "   'alignment',\n",
       "   'format',\n",
       "   'used',\n",
       "   'to',\n",
       "   'build',\n",
       "   'a',\n",
       "   'new',\n",
       "   'generation',\n",
       "   'of',\n",
       "   'generalized',\n",
       "   'profiles,',\n",
       "   'the',\n",
       "   'migration',\n",
       "   'of',\n",
       "   '{scanprosite}',\n",
       "   'to',\n",
       "   '{vital-it},',\n",
       "   'a',\n",
       "   'cluster',\n",
       "   'of',\n",
       "   '633',\n",
       "   '{cpus},',\n",
       "   'and',\n",
       "   'the',\n",
       "   'adoption',\n",
       "   'of',\n",
       "   'the',\n",
       "   'distributed',\n",
       "   'annotation',\n",
       "   'system',\n",
       "   '({das})',\n",
       "   'to',\n",
       "   'facilitate',\n",
       "   '{prosite}',\n",
       "   'data',\n",
       "   'integration',\n",
       "   'and',\n",
       "   'interchange',\n",
       "   'with',\n",
       "   'other',\n",
       "   'sources.',\n",
       "   'the',\n",
       "   'latest',\n",
       "   'version',\n",
       "   'of',\n",
       "   '{prosite}',\n",
       "   '(release',\n",
       "   '20.54,',\n",
       "   'of',\n",
       "   '22',\n",
       "   'september',\n",
       "   '2009)',\n",
       "   'contains',\n",
       "   '1308',\n",
       "   'patterns,',\n",
       "   '863',\n",
       "   'profiles',\n",
       "   'and',\n",
       "   '869',\n",
       "   '{prorules}.',\n",
       "   '{prosite}',\n",
       "   'is',\n",
       "   'accessible',\n",
       "   'at:',\n",
       "   'http://www.expasy.org/prosite/.\"\"impressive',\n",
       "   'advances',\n",
       "   'in',\n",
       "   '{ngs}',\n",
       "   'have',\n",
       "   'enabled',\n",
       "   'an',\n",
       "   'immense',\n",
       "   'diversity',\n",
       "   'of',\n",
       "   'novel',\n",
       "   'applications.',\n",
       "   'the',\n",
       "   'barrier',\n",
       "   'of',\n",
       "   'the',\n",
       "   '\\\\$1000',\n",
       "   'genome',\n",
       "   'has',\n",
       "   'recently',\n",
       "   'been',\n",
       "   'broken.',\n",
       "   'important',\n",
       "   'novel',\n",
       "   'tools',\n",
       "   'for',\n",
       "   'clinical',\n",
       "   'diagnostics',\n",
       "   'based',\n",
       "   'on',\n",
       "   '{ngs}',\n",
       "   'are',\n",
       "   'appearing.',\n",
       "   'third-generation',\n",
       "   'technologies',\n",
       "   'may',\n",
       "   'further',\n",
       "   'revolutionize',\n",
       "   'genomics',\n",
       "   'research.',\n",
       "   'significant',\n",
       "   'challenges',\n",
       "   'for',\n",
       "   '{ngs}',\n",
       "   'remain,',\n",
       "   'in',\n",
       "   'particular',\n",
       "   'data',\n",
       "   'storage',\n",
       "   'and',\n",
       "   'processing.',\n",
       "   'ten',\n",
       "   'years',\n",
       "   'ago',\n",
       "   'next-generation',\n",
       "   'sequencing',\n",
       "   '({ngs})',\n",
       "   'technologies',\n",
       "   'appeared',\n",
       "   'on',\n",
       "   'the',\n",
       "   'market.',\n",
       "   'during',\n",
       "   'the',\n",
       "   'past',\n",
       "   'decade,',\n",
       "   'tremendous',\n",
       "   'progress',\n",
       "   'has',\n",
       "   'been',\n",
       "   'made',\n",
       "   'in',\n",
       "   'terms',\n",
       "   'of',\n",
       "   'speed,',\n",
       "   'read',\n",
       "   'length,',\n",
       "   'and',\n",
       "   'throughput,',\n",
       "   'along',\n",
       "   'with',\n",
       "   'a',\n",
       "   'sharp',\n",
       "   'reduction',\n",
       "   'in',\n",
       "   'per-base',\n",
       "   'cost.',\n",
       "   'together,',\n",
       "   'these',\n",
       "   'advances',\n",
       "   'democratized',\n",
       "   '{ngs}',\n",
       "   'and',\n",
       "   'paved',\n",
       "   'the',\n",
       "   'way',\n",
       "   'for',\n",
       "   'the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'a',\n",
       "   'large',\n",
       "   'number',\n",
       "   'of',\n",
       "   'novel',\n",
       "   '{ngs}',\n",
       "   'applications',\n",
       "   'in',\n",
       "   'basic',\n",
       "   'science',\n",
       "   'as',\n",
       "   'well',\n",
       "   'as',\n",
       "   'in',\n",
       "   'translational',\n",
       "   'research',\n",
       "   'areas',\n",
       "   'such',\n",
       "   'as',\n",
       "   'clinical',\n",
       "   'diagnostics,',\n",
       "   'agrigenomics,',\n",
       "   'and',\n",
       "   'forensic',\n",
       "   'science.',\n",
       "   'here',\n",
       "   'we',\n",
       "   'provide',\n",
       "   'an',\n",
       "   'overview',\n",
       "   'of',\n",
       "   'the',\n",
       "   'evolution',\n",
       "   'of',\n",
       "   '{ngs}',\n",
       "   'and',\n",
       "   'discuss',\n",
       "   'the',\n",
       "   'most',\n",
       "   'significant',\n",
       "   'improvements',\n",
       "   'in',\n",
       "   'sequencing',\n",
       "   'technologies',\n",
       "   'and',\n",
       "   'library',\n",
       "   'preparation',\n",
       "   'protocols.',\n",
       "   'we',\n",
       "   'also',\n",
       "   'explore',\n",
       "   'the',\n",
       "   'current',\n",
       "   'landscape',\n",
       "   'of',\n",
       "   '{ngs}',\n",
       "   'applications',\n",
       "   'and',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'perspective',\n",
       "   'for',\n",
       "   'future',\n",
       "   'developments.\"{dna}',\n",
       "   'methylation:',\n",
       "   'roles',\n",
       "   'in',\n",
       "   'mammalian',\n",
       "   'development,\"{dna}',\n",
       "   'methylation',\n",
       "   'is',\n",
       "   'among',\n",
       "   'the',\n",
       "   'best',\n",
       "   'studied',\n",
       "   'epigenetic',\n",
       "   'modifications',\n",
       "   'and',\n",
       "   'is',\n",
       "   'essential',\n",
       "   'to',\n",
       "   'mammalian',\n",
       "   'development.',\n",
       "   'although',\n",
       "   'the',\n",
       "   'methylation',\n",
       "   'status',\n",
       "   'of',\n",
       "   'most',\n",
       "   '{cpg}',\n",
       "   'dinucleotides',\n",
       "   'in',\n",
       "   'the',\n",
       "   'genome',\n",
       "   'is',\n",
       "   'stably',\n",
       "   'propagated',\n",
       "   'through',\n",
       "   'mitosis,',\n",
       "   'improvements',\n",
       "   'to',\n",
       "   'methods',\n",
       "   'for',\n",
       "   'measuring',\n",
       "   'methylation',\n",
       "   'have',\n",
       "   'identified',\n",
       "   'numerous',\n",
       "   'regions',\n",
       "   'in',\n",
       "   'which',\n",
       "   'it',\n",
       "   'is',\n",
       "   'dynamically',\n",
       "   'regulated.',\n",
       "   'in',\n",
       "   'this',\n",
       "   'review,',\n",
       "   'we',\n",
       "   'discuss',\n",
       "   'key',\n",
       "   'concepts',\n",
       "   'in',\n",
       "   'the',\n",
       "   'function',\n",
       "   'of',\n",
       "   '{dna}',\n",
       "   'methylation',\n",
       "   'in',\n",
       "   'mammals,',\n",
       "   'stemming',\n",
       "   'from',\n",
       "   'more',\n",
       "   'than',\n",
       "   'two',\n",
       "   'decades',\n",
       "   'of',\n",
       "   'research,',\n",
       "   'including',\n",
       "   'many',\n",
       "   'recent',\n",
       "   'studies',\n",
       "   'that',\n",
       "   'have',\n",
       "   'elucidated',\n",
       "   'when',\n",
       "   'and',\n",
       "   'where',\n",
       "   '{dna}',\n",
       "   'methylation',\n",
       "   'has',\n",
       "   'a',\n",
       "   'regulatory',\n",
       "   'role',\n",
       "   'in',\n",
       "   'the',\n",
       "   'genome.',\n",
       "   'we',\n",
       "   'include',\n",
       "   'insights',\n",
       "   'from',\n",
       "   'early',\n",
       "   'development,',\n",
       "   'embryonic',\n",
       "   'stem',\n",
       "   'cells',\n",
       "   'and',\n",
       "   'adult',\n",
       "   'lineages,',\n",
       "   'particularly',\n",
       "   'haematopoiesis,',\n",
       "   'to',\n",
       "   'highlight',\n",
       "   'the',\n",
       "   'general',\n",
       "   'features',\n",
       "   'of',\n",
       "   'this',\n",
       "   'modification',\n",
       "   'as',\n",
       "   'it',\n",
       "   'participates',\n",
       "   'in',\n",
       "   'both',\n",
       "   'global',\n",
       "   'and',\n",
       "   'localized',\n",
       "   'epigenetic',\n",
       "   'regulation.\"storing,',\n",
       "   'and',\n",
       "   'querying',\n",
       "   'biological',\n",
       "   'pathways.\",\"biological',\n",
       "   'pathways,',\n",
       "   'including',\n",
       "   'metabolic',\n",
       "   'pathways,',\n",
       "   'protein',\n",
       "   'interaction',\n",
       "   'networks,',\n",
       "   'signal',\n",
       "   'transduction',\n",
       "   'pathways,',\n",
       "   'and',\n",
       "   'gene',\n",
       "   'regulatory',\n",
       "   'networks,',\n",
       "   'are',\n",
       "   'currently',\n",
       "   'represented',\n",
       "   'in',\n",
       "   'over',\n",
       "   '220',\n",
       "   'diverse',\n",
       "   'databases.',\n",
       "   'these',\n",
       "   'data',\n",
       "   'are',\n",
       "   'crucial',\n",
       "   'for',\n",
       "   'the',\n",
       "   'study',\n",
       "   'of',\n",
       "   'specific',\n",
       "   'biological',\n",
       "   'processes,',\n",
       "   'including',\n",
       "   'human',\n",
       "   'diseases.',\n",
       "   'standard',\n",
       "   'exchange',\n",
       "   'formats',\n",
       "   'for',\n",
       "   'pathway',\n",
       "   'information,',\n",
       "   'such',\n",
       "   'as',\n",
       "   '{biopax},',\n",
       "   '{cellml},',\n",
       "   '{sbml}',\n",
       "   'and',\n",
       "   '{psi}-{mi},',\n",
       "   'enable',\n",
       "   'convenient',\n",
       "   'collection',\n",
       "   'of',\n",
       "   'this',\n",
       "   'data',\n",
       "   'for',\n",
       "   'biological',\n",
       "   'research,',\n",
       "   'but',\n",
       "   'mechanisms',\n",
       "   'for',\n",
       "   'common',\n",
       "   'storage',\n",
       "   'and',\n",
       "   'communication',\n",
       "   'are',\n",
       "   'required.',\n",
       "   'we',\n",
       "   'have',\n",
       "   'developed',\n",
       "   '{cpath},',\n",
       "   'an',\n",
       "   'open',\n",
       "   'source',\n",
       "   'database',\n",
       "   'and',\n",
       "   'web',\n",
       "   'application',\n",
       "   'for',\n",
       "   'collecting,',\n",
       "   'storing,',\n",
       "   'and',\n",
       "   'querying',\n",
       "   'biological',\n",
       "   'pathway',\n",
       "   'data.',\n",
       "   '{cpath}',\n",
       "   'makes',\n",
       "   'it',\n",
       "   'easy',\n",
       "   'to',\n",
       "   'aggregate',\n",
       "   'custom',\n",
       "   'pathway',\n",
       "   'data',\n",
       "   'sets',\n",
       "   'available',\n",
       "   'in',\n",
       "   'standard',\n",
       "   'exchange',\n",
       "   'formats',\n",
       "   'from',\n",
       "   'multiple',\n",
       "   'databases,',\n",
       "   'present',\n",
       "   'pathway',\n",
       "   'data',\n",
       "   'to',\n",
       "   'biologists',\n",
       "   'via',\n",
       "   'a',\n",
       "   'customizable',\n",
       "   'web',\n",
       "   'interface,',\n",
       "   'and',\n",
       "   'export',\n",
       "   'pathway',\n",
       "   'data',\n",
       "   'via',\n",
       "   'a',\n",
       "   'web',\n",
       "   'service',\n",
       "   'to',\n",
       "   'third-party',\n",
       "   'software,',\n",
       "   'such',\n",
       "   'as',\n",
       "   'cytoscape,',\n",
       "   'for',\n",
       "   'visualization',\n",
       "   'and',\n",
       "   'analysis.',\n",
       "   '{cpath}',\n",
       "   'is',\n",
       "   'software',\n",
       "   'only,',\n",
       "   'and',\n",
       "   'does',\n",
       "   'not',\n",
       "   'include',\n",
       "   'new',\n",
       "   'pathway',\n",
       "   'information.',\n",
       "   'key',\n",
       "   'features',\n",
       "   'include:',\n",
       "   'a',\n",
       "   'built-in',\n",
       "   'identifier',\n",
       "   'mapping',\n",
       "   'service',\n",
       "   'for',\n",
       "   'linking',\n",
       "   'identical',\n",
       "   'interactors',\n",
       "   'and',\n",
       "   'linking',\n",
       "   'to',\n",
       "   'external',\n",
       "   'resources;',\n",
       "   'built-in',\n",
       "   'support',\n",
       "   'for',\n",
       "   '{psi}-{mi}',\n",
       "   'and',\n",
       "   '{biopax}',\n",
       "   'standard',\n",
       "   'pathway',\n",
       "   'exchange',\n",
       "   'formats;',\n",
       "   'a',\n",
       "   'web',\n",
       "   'service',\n",
       "   'interface',\n",
       "   'for',\n",
       "   'searching',\n",
       "   'and',\n",
       "   'retrieving',\n",
       "   'pathway',\n",
       "   'data',\n",
       "   'sets;',\n",
       "   'and',\n",
       "   'thorough',\n",
       "   'documentation.',\n",
       "   'the',\n",
       "   '{cpath}',\n",
       "   'software',\n",
       "   'is',\n",
       "   'freely',\n",
       "   'available',\n",
       "   'under',\n",
       "   'the',\n",
       "   '{lgpl}',\n",
       "   'open',\n",
       "   'source',\n",
       "   'license',\n",
       "   'for',\n",
       "   'academic',\n",
       "   'and',\n",
       "   'commercial',\n",
       "   'use.',\n",
       "   '{cpath}',\n",
       "   'is',\n",
       "   'a',\n",
       "   'robust,',\n",
       "   'scalable,',\n",
       "   'modular,',\n",
       "   'professional-grade',\n",
       "   'software',\n",
       "   'platform',\n",
       "   'for',\n",
       "   'collecting,',\n",
       "   'storing,',\n",
       "   'and',\n",
       "   'querying',\n",
       "   'biological',\n",
       "   'pathways.',\n",
       "   'it',\n",
       "   'can',\n",
       "   'serve',\n",
       "   'as',\n",
       "   'the',\n",
       "   'core',\n",
       "   'data',\n",
       "   'handling',\n",
       "   'component',\n",
       "   'in',\n",
       "   'information',\n",
       "   'systems',\n",
       "   'for',\n",
       "   'pathway',\n",
       "   'visualization,',\n",
       "   'analysis',\n",
       "   'and',\n",
       "   'modeling.\"new',\n",
       "   'haven,',\n",
       "   'connecticut',\n",
       "   '06520,',\n",
       "   'usa.\",annotation',\n",
       "   'transfer',\n",
       "   'between',\n",
       "   'genomes:',\n",
       "   'protein-protein',\n",
       "   'interologs',\n",
       "   'and',\n",
       "   '{protein-dna}',\n",
       "   'regulogs.,texts,',\n",
       "   'and',\n",
       "   'trees.\",\"{bionames}',\n",
       "   'is',\n",
       "   'a',\n",
       "   'web',\n",
       "   'database',\n",
       "   'of',\n",
       "   'taxonomic',\n",
       "   'names',\n",
       "   'for',\n",
       "   'animals,',\n",
       "   'linked',\n",
       "   'to',\n",
       "   'the',\n",
       "   'primary',\n",
       "   'literature',\n",
       "   'and,',\n",
       "   'wherever',\n",
       "   'possible,',\n",
       "   'to',\n",
       "   'phylogenetic',\n",
       "   'trees.',\n",
       "   'it',\n",
       "   'aims',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'taxonomic',\n",
       "   '\"\"dashboard\"\"',\n",
       "   'where',\n",
       "   'at',\n",
       "   'a',\n",
       "   'glance',\n",
       "   'we',\n",
       "   'can',\n",
       "   'see',\n",
       "   'a',\n",
       "   'summary',\n",
       "   'of',\n",
       "   'the',\n",
       "   'taxonomic',\n",
       "   'and',\n",
       "   'phylogenetic',\n",
       "   'information',\n",
       "   'we',\n",
       "   'have',\n",
       "   'for',\n",
       "   'a',\n",
       "   'given',\n",
       "   'taxon',\n",
       "   'and',\n",
       "   'hence',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'quick',\n",
       "   'answer',\n",
       "   'to',\n",
       "   'the',\n",
       "   'basic',\n",
       "   'question',\n",
       "   '\"\"what',\n",
       "   'is',\n",
       "   'this',\n",
       "   'taxon?\"\"',\n",
       "   '{bionames}',\n",
       "   'combines',\n",
       "   'classifications',\n",
       "   'from',\n",
       "   'the',\n",
       "   'global',\n",
       "   'biodiversity',\n",
       "   'information',\n",
       "   'facility',\n",
       "   '({gbif})',\n",
       "   'and',\n",
       "   '{genbank},',\n",
       "   'images',\n",
       "   'from',\n",
       "   'the',\n",
       "   'encyclopedia',\n",
       "   'of',\n",
       "   'life',\n",
       "   '({eol}),',\n",
       "   'animal',\n",
       "   'names',\n",
       "   'from',\n",
       "   'the',\n",
       "   'index',\n",
       "   'of',\n",
       "   'organism',\n",
       "   'names',\n",
       "   '({ion}),',\n",
       "   'and',\n",
       "   'bibliographic',\n",
       "   'data',\n",
       "   'from',\n",
       "   'multiple',\n",
       "   'sources',\n",
       "   'including',\n",
       "   'the',\n",
       "   'biodiversity',\n",
       "   'heritage',\n",
       "   'library',\n",
       "   '({bhl})',\n",
       "   'and',\n",
       "   '{crossref}.',\n",
       "   'the',\n",
       "   'user',\n",
       "   'interface',\n",
       "   'includes',\n",
       "   'display',\n",
       "   'of',\n",
       "   'full',\n",
       "   'text',\n",
       "   'articles,',\n",
       "   'interactive',\n",
       "   'timelines',\n",
       "   'of',\n",
       "   'taxonomic',\n",
       "   'publications,',\n",
       "   'and',\n",
       "   'zoomable',\n",
       "   'phylogenies.',\n",
       "   'it',\n",
       "   'is',\n",
       "   'available',\n",
       "   'at',\n",
       "   'http://bionames.org.\"\"the',\n",
       "   'prediction',\n",
       "   'of',\n",
       "   'regulatory',\n",
       "   'elements',\n",
       "   'is',\n",
       "   'a',\n",
       "   'problem',\n",
       "   'where',\n",
       "   'computational',\n",
       "   'methods',\n",
       "   'offer',\n",
       "   'great',\n",
       "   'hope.',\n",
       "   'over',\n",
       "   'the',\n",
       "   'past',\n",
       "   'few',\n",
       "   'years,',\n",
       "   'numerous',\n",
       "   'tools',\n",
       "   'have',\n",
       "   'become',\n",
       "   'available',\n",
       "   'for',\n",
       "   'this',\n",
       "   'task.',\n",
       "   'the',\n",
       "   'purpose',\n",
       "   'of',\n",
       "   'the',\n",
       "   'current',\n",
       "   'assessment',\n",
       "   'is',\n",
       "   'twofold:',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'some',\n",
       "   'guidance',\n",
       "   'to',\n",
       "   'users',\n",
       "   'regarding',\n",
       "   'the',\n",
       "   'accuracy',\n",
       "   'of',\n",
       "   'currently',\n",
       "   'available',\n",
       "   'tools',\n",
       "   'in',\n",
       "   'various',\n",
       "   'settings,',\n",
       "   'and',\n",
       "   'to',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'benchmark',\n",
       "   'of',\n",
       "   'data',\n",
       "   'sets',\n",
       "   'for',\n",
       "   'assessing',\n",
       "   'future',\n",
       "   'tools.\"\"unraveling',\n",
       "   'the',\n",
       "   'mechanisms',\n",
       "   'that',\n",
       "   'regulate',\n",
       "   'gene',\n",
       "   'expression',\n",
       "   'is',\n",
       "   'a',\n",
       "   'major',\n",
       "   'challenge',\n",
       "   'in',\n",
       "   'biology.',\n",
       "   'an',\n",
       "   'important',\n",
       "   'task',\n",
       "   'in',\n",
       "   'this',\n",
       "   'challenge',\n",
       "   'is',\n",
       "   'to',\n",
       "   'identify',\n",
       "   'regulatory',\n",
       "   'elements,',\n",
       "   'especially',\n",
       "   'the',\n",
       "   'binding',\n",
       "   'sites',\n",
       "   'in',\n",
       "   'deoxyribonucleic',\n",
       "   'acid',\n",
       "   '({dna})',\n",
       "   'for',\n",
       "   'transcription',\n",
       "   'factors.',\n",
       "   'these',\n",
       "   'binding',\n",
       "   'sites',\n",
       "   'are',\n",
       "   'short',\n",
       "   '{dna}',\n",
       "   'segments',\n",
       "   'that',\n",
       "   'are',\n",
       "   'called',\n",
       "   'motifs.',\n",
       "   'recent',\n",
       "   'advances',\n",
       "   'in',\n",
       "   'genome',\n",
       "   'sequence',\n",
       "   'availability',\n",
       "   'and',\n",
       "   'in',\n",
       "   'high-throughput',\n",
       "   'gene',\n",
       "   'expression',\n",
       "   'analysis',\n",
       "   'technologies',\n",
       "   'have',\n",
       "   'allowed',\n",
       "   ...]),\n",
       " ('beaca02b21b7cad6cb738c5e2682af8d',\n",
       "  ['new',\n",
       "   'haven,',\n",
       "   'connecticut',\n",
       "   '06520,',\n",
       "   'usa.\",annotation',\n",
       "   'transfer',\n",
       "   'between',\n",
       "   'genomes:',\n",
       "   'protein-protein',\n",
       "   'interologs',\n",
       "   'and',\n",
       "   '{protein-dna}',\n",
       "   'regulogs.,\"bioinformatics',\n",
       "   'program,',\n",
       "   'boston',\n",
       "   'university,',\n",
       "   'boston,',\n",
       "   'massachusetts;',\n",
       "   'department',\n",
       "   'of',\n",
       "   'biomedical',\n",
       "   'engineering,',\n",
       "   'boston',\n",
       "   'university,',\n",
       "   'boston,',\n",
       "   'massachusetts\",{zdock}:',\n",
       "   'an',\n",
       "   'initial-stage',\n",
       "   'protein-docking',\n",
       "   'algorithm,\"the',\n",
       "   'development',\n",
       "   'of',\n",
       "   'scoring',\n",
       "   'functions',\n",
       "   'is',\n",
       "   'of',\n",
       "   'great',\n",
       "   'importance',\n",
       "   'to',\n",
       "   'protein',\n",
       "   'docking.',\n",
       "   'here',\n",
       "   'we',\n",
       "   'present',\n",
       "   'a',\n",
       "   'new',\n",
       "   'scoring',\n",
       "   'function',\n",
       "   'for',\n",
       "   'the',\n",
       "   'initial',\n",
       "   'stage',\n",
       "   'of',\n",
       "   'unbound',\n",
       "   'docking.',\n",
       "   'it',\n",
       "   'combines',\n",
       "   'our',\n",
       "   'recently',\n",
       "   'developed',\n",
       "   'pairwise',\n",
       "   'shape',\n",
       "   'complementarity',\n",
       "   'with',\n",
       "   'desolvation',\n",
       "   'and',\n",
       "   'electrostatics.',\n",
       "   'we',\n",
       "   'compare',\n",
       "   'this',\n",
       "   'scoring',\n",
       "   'function',\n",
       "   'with',\n",
       "   'three',\n",
       "   'other',\n",
       "   'functions',\n",
       "   'on',\n",
       "   'a',\n",
       "   'large',\n",
       "   'benchmark',\n",
       "   'of',\n",
       "   '49',\n",
       "   'nonredundant',\n",
       "   'test',\n",
       "   'cases',\n",
       "   'and',\n",
       "   'show',\n",
       "   'its',\n",
       "   'superior',\n",
       "   'performance,',\n",
       "   'especially',\n",
       "   'for',\n",
       "   'the',\n",
       "   'antibody-antigen',\n",
       "   'category',\n",
       "   'of',\n",
       "   'test',\n",
       "   'cases.',\n",
       "   'for',\n",
       "   '44',\n",
       "   'test',\n",
       "   'cases',\n",
       "   '(90\\\\%',\n",
       "   'of',\n",
       "   'the',\n",
       "   'benchmark),',\n",
       "   'we',\n",
       "   'can',\n",
       "   'retain',\n",
       "   'at',\n",
       "   'least',\n",
       "   'one',\n",
       "   'near-native',\n",
       "   'structure',\n",
       "   'within',\n",
       "   'the',\n",
       "   'top',\n",
       "   '2000',\n",
       "   'predictions',\n",
       "   'at',\n",
       "   'the',\n",
       "   '6°',\n",
       "   'rotational',\n",
       "   'sampling',\n",
       "   'density,',\n",
       "   'with',\n",
       "   'an',\n",
       "   'average',\n",
       "   'of',\n",
       "   '52',\n",
       "   'near-native',\n",
       "   'structures',\n",
       "   'per',\n",
       "   'test',\n",
       "   'case.',\n",
       "   'the',\n",
       "   'remaining',\n",
       "   'five',\n",
       "   'difficult',\n",
       "   'test',\n",
       "   'cases',\n",
       "   'can',\n",
       "   'be',\n",
       "   'explained',\n",
       "   'by',\n",
       "   'a',\n",
       "   'combination',\n",
       "   'of',\n",
       "   'poor',\n",
       "   'binding',\n",
       "   'affinity,',\n",
       "   'large',\n",
       "   'backbone',\n",
       "   'conformational',\n",
       "   'changes,',\n",
       "   'and',\n",
       "   'our',\n",
       "   \"algorithm's\",\n",
       "   'strong',\n",
       "   'tendency',\n",
       "   'for',\n",
       "   'identifying',\n",
       "   'large',\n",
       "   'concave',\n",
       "   'binding',\n",
       "   'pockets.',\n",
       "   'all',\n",
       "   'four',\n",
       "   'scoring',\n",
       "   'functions',\n",
       "   'have',\n",
       "   'been',\n",
       "   'integrated',\n",
       "   'into',\n",
       "   'our',\n",
       "   'fast',\n",
       "   'fourier',\n",
       "   'transform',\n",
       "   'based',\n",
       "   'docking',\n",
       "   'algorithm',\n",
       "   '{zdock},',\n",
       "   'which',\n",
       "   'is',\n",
       "   'freely',\n",
       "   'available',\n",
       "   'to',\n",
       "   'academic',\n",
       "   'users',\n",
       "   'at',\n",
       "   'http://zlab.bu.edu/∼rong/dock.',\n",
       "   'proteins',\n",
       "   '2003;52:80–87.',\n",
       "   '{\\\\copyright}',\n",
       "   '2003',\n",
       "   '{wiley-liss},',\n",
       "   'inc.\"\"human',\n",
       "   'protein',\n",
       "   'reference',\n",
       "   'database',\n",
       "   '({hprd}--http://www.hprd.org/),',\n",
       "   'initially',\n",
       "   'described',\n",
       "   'in',\n",
       "   '2003,',\n",
       "   'is',\n",
       "   'a',\n",
       "   'database',\n",
       "   'of',\n",
       "   'curated',\n",
       "   'proteomic',\n",
       "   'information',\n",
       "   'pertaining',\n",
       "   'to',\n",
       "   'human',\n",
       "   'proteins.',\n",
       "   'we',\n",
       "   'have',\n",
       "   'recently',\n",
       "   'added',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'new',\n",
       "   'features',\n",
       "   'in',\n",
       "   '{hprd}.',\n",
       "   'these',\n",
       "   'include',\n",
       "   '{phosphomotif}',\n",
       "   'finder,',\n",
       "   'which',\n",
       "   'allows',\n",
       "   'users',\n",
       "   'to',\n",
       "   'find',\n",
       "   'the',\n",
       "   'presence',\n",
       "   'of',\n",
       "   'over',\n",
       "   '320',\n",
       "   'experimentally',\n",
       "   'verified',\n",
       "   'phosphorylation',\n",
       "   'motifs',\n",
       "   'in',\n",
       "   'proteins',\n",
       "   'of',\n",
       "   'interest.',\n",
       "   'another',\n",
       "   'new',\n",
       "   'feature',\n",
       "   'is',\n",
       "   'a',\n",
       "   'protein',\n",
       "   'distributed',\n",
       "   'annotation',\n",
       "   '{system--human}',\n",
       "   'proteinpedia',\n",
       "   '(http://www.humanproteinpedia.org/)--through',\n",
       "   'which',\n",
       "   'laboratories',\n",
       "   'can',\n",
       "   'submit',\n",
       "   'their',\n",
       "   'data,',\n",
       "   'which',\n",
       "   'is',\n",
       "   'mapped',\n",
       "   'onto',\n",
       "   'protein',\n",
       "   'entries',\n",
       "   'in',\n",
       "   '{hprd}.',\n",
       "   'over',\n",
       "   '75',\n",
       "   'laboratories',\n",
       "   'involved',\n",
       "   'in',\n",
       "   'proteomics',\n",
       "   'research',\n",
       "   'have',\n",
       "   'already',\n",
       "   'participated',\n",
       "   'in',\n",
       "   'this',\n",
       "   'effort',\n",
       "   'by',\n",
       "   'submitting',\n",
       "   'data',\n",
       "   'for',\n",
       "   'over',\n",
       "   '15,000',\n",
       "   'human',\n",
       "   'proteins.',\n",
       "   'the',\n",
       "   'submitted',\n",
       "   'data',\n",
       "   'includes',\n",
       "   'mass',\n",
       "   'spectrometry',\n",
       "   'and',\n",
       "   'protein',\n",
       "   'microarray-derived',\n",
       "   'data,',\n",
       "   'among',\n",
       "   'other',\n",
       "   'data',\n",
       "   'types.',\n",
       "   'finally,',\n",
       "   '{hprd}',\n",
       "   'is',\n",
       "   'also',\n",
       "   'linked',\n",
       "   'to',\n",
       "   'a',\n",
       "   'compendium',\n",
       "   'of',\n",
       "   'human',\n",
       "   'signaling',\n",
       "   'pathways',\n",
       "   'developed',\n",
       "   'by',\n",
       "   'our',\n",
       "   'group,',\n",
       "   '{netpath}',\n",
       "   '(http://www.netpath.org/),',\n",
       "   'which',\n",
       "   'currently',\n",
       "   'contains',\n",
       "   'annotations',\n",
       "   'for',\n",
       "   'several',\n",
       "   'cancer',\n",
       "   'and',\n",
       "   'immune',\n",
       "   'signaling',\n",
       "   'pathways.',\n",
       "   'since',\n",
       "   'the',\n",
       "   'last',\n",
       "   'update,',\n",
       "   'more',\n",
       "   'than',\n",
       "   '5500',\n",
       "   'new',\n",
       "   'protein',\n",
       "   'sequences',\n",
       "   'have',\n",
       "   'been',\n",
       "   'added,',\n",
       "   'making',\n",
       "   '{hprd}',\n",
       "   'a',\n",
       "   'comprehensive',\n",
       "   'resource',\n",
       "   'for',\n",
       "   'studying',\n",
       "   'the',\n",
       "   'human',\n",
       "   'proteome.\"\"{background}:microarrays',\n",
       "   'revolutionized',\n",
       "   'biological',\n",
       "   'research',\n",
       "   'by',\n",
       "   'enabling',\n",
       "   'gene',\n",
       "   'expression',\n",
       "   'comparisons',\n",
       "   'on',\n",
       "   'a',\n",
       "   'transcriptome-wide',\n",
       "   'scale.',\n",
       "   'microarrays,',\n",
       "   'however,',\n",
       "   'do',\n",
       "   'not',\n",
       "   'estimate',\n",
       "   'absolute',\n",
       "   'expression',\n",
       "   'level',\n",
       "   'accurately.',\n",
       "   'at',\n",
       "   'present,',\n",
       "   'high',\n",
       "   'throughput',\n",
       "   'sequencing',\n",
       "   'is',\n",
       "   'emerging',\n",
       "   'as',\n",
       "   'an',\n",
       "   'alternative',\n",
       "   'methodology',\n",
       "   'for',\n",
       "   'transcriptome',\n",
       "   'studies.',\n",
       "   'although',\n",
       "   'free',\n",
       "   'of',\n",
       "   'many',\n",
       "   'limitations',\n",
       "   'imposed',\n",
       "   'by',\n",
       "   'microarray',\n",
       "   'design,',\n",
       "   'its',\n",
       "   'potential',\n",
       "   'to',\n",
       "   'estimate',\n",
       "   'absolute',\n",
       "   'transcript',\n",
       "   'levels',\n",
       "   'is',\n",
       "   '{unknown.results}:in',\n",
       "   'this',\n",
       "   'study,',\n",
       "   'we',\n",
       "   'evaluate',\n",
       "   'relative',\n",
       "   'accuracy',\n",
       "   'of',\n",
       "   'microarrays',\n",
       "   'and',\n",
       "   'transcriptome',\n",
       "   'sequencing',\n",
       "   '({rna}-seq)',\n",
       "   'using',\n",
       "   'third',\n",
       "   'methodology:',\n",
       "   'proteomics.',\n",
       "   'we',\n",
       "   'find',\n",
       "   'that',\n",
       "   '{rna}-seq',\n",
       "   'provides',\n",
       "   'a',\n",
       "   'better',\n",
       "   'estimate',\n",
       "   'of',\n",
       "   'absolute',\n",
       "   'expression',\n",
       "   '{levels.conclusion}:our',\n",
       "   'result',\n",
       "   'shows',\n",
       "   'that',\n",
       "   'in',\n",
       "   'terms',\n",
       "   'of',\n",
       "   'overall',\n",
       "   'technical',\n",
       "   'performance,',\n",
       "   '{rna}-seq',\n",
       "   'is',\n",
       "   'the',\n",
       "   'technique',\n",
       "   'of',\n",
       "   'choice',\n",
       "   'for',\n",
       "   'studies',\n",
       "   'that',\n",
       "   'require',\n",
       "   'accurate',\n",
       "   'estimation',\n",
       "   'of',\n",
       "   'absolute',\n",
       "   'transcript',\n",
       "   'levels.\"\"cancers',\n",
       "   'evolve',\n",
       "   'by',\n",
       "   'a',\n",
       "   'reiterative',\n",
       "   'process',\n",
       "   'of',\n",
       "   'clonal',\n",
       "   'expansion,',\n",
       "   'genetic',\n",
       "   'diversification',\n",
       "   'and',\n",
       "   'clonal',\n",
       "   'selection',\n",
       "   'within',\n",
       "   'the',\n",
       "   'adaptive',\n",
       "   'landscapes',\n",
       "   'of',\n",
       "   'tissue',\n",
       "   'ecosystems.',\n",
       "   'the',\n",
       "   'dynamics',\n",
       "   'are',\n",
       "   'complex,',\n",
       "   'with',\n",
       "   'highly',\n",
       "   'variable',\n",
       "   'patterns',\n",
       "   'of',\n",
       "   'genetic',\n",
       "   'diversity',\n",
       "   'and',\n",
       "   'resulting',\n",
       "   'clonal',\n",
       "   'architecture.',\n",
       "   'therapeutic',\n",
       "   'intervention',\n",
       "   'may',\n",
       "   'destroy',\n",
       "   'cancer',\n",
       "   'clones',\n",
       "   'and',\n",
       "   'erode',\n",
       "   'their',\n",
       "   'habitats,',\n",
       "   'but',\n",
       "   'it',\n",
       "   'can',\n",
       "   'also',\n",
       "   'inadvertently',\n",
       "   'provide',\n",
       "   'a',\n",
       "   'potent',\n",
       "   'selective',\n",
       "   'pressure',\n",
       "   'for',\n",
       "   'the',\n",
       "   'expansion',\n",
       "   'of',\n",
       "   'resistant',\n",
       "   'variants.',\n",
       "   'the',\n",
       "   'inherently',\n",
       "   'darwinian',\n",
       "   'character',\n",
       "   'of',\n",
       "   'cancer',\n",
       "   'is',\n",
       "   'the',\n",
       "   'primary',\n",
       "   'reason',\n",
       "   'for',\n",
       "   'this',\n",
       "   'therapeutic',\n",
       "   'failure,',\n",
       "   'but',\n",
       "   'it',\n",
       "   'may',\n",
       "   'also',\n",
       "   'hold',\n",
       "   'the',\n",
       "   'key',\n",
       "   'to',\n",
       "   'more',\n",
       "   'effective',\n",
       "   'control.\"\"intratumoral',\n",
       "   'heterogeneity',\n",
       "   'arises',\n",
       "   'through',\n",
       "   'evolution',\n",
       "   'of',\n",
       "   'genetically',\n",
       "   'diverse',\n",
       "   'subclones',\n",
       "   'during',\n",
       "   'tumor',\n",
       "   'progression.',\n",
       "   'however,',\n",
       "   'whether',\n",
       "   'cells',\n",
       "   'within',\n",
       "   'single',\n",
       "   'genetic',\n",
       "   'clones',\n",
       "   'are',\n",
       "   'functionally',\n",
       "   'equivalent',\n",
       "   'remains',\n",
       "   'unknown.',\n",
       "   'by',\n",
       "   'combining',\n",
       "   '{dna}',\n",
       "   'copy',\n",
       "   'number',\n",
       "   'alteration',\n",
       "   '({cna})',\n",
       "   'profiling,',\n",
       "   'sequencing,',\n",
       "   'and',\n",
       "   'lentiviral',\n",
       "   'lineage',\n",
       "   'tracking,',\n",
       "   'we',\n",
       "   'followed',\n",
       "   'the',\n",
       "   'repopulation',\n",
       "   'dynamics',\n",
       "   'of',\n",
       "   '150',\n",
       "   'single',\n",
       "   'lentivirus-marked',\n",
       "   'lineages',\n",
       "   'from',\n",
       "   '10',\n",
       "   'human',\n",
       "   'colorectal',\n",
       "   'cancers',\n",
       "   'through',\n",
       "   'serial',\n",
       "   'xenograft',\n",
       "   'passages',\n",
       "   'in',\n",
       "   'mice.',\n",
       "   '{cna}',\n",
       "   'and',\n",
       "   'mutational',\n",
       "   'analysis',\n",
       "   'distinguished',\n",
       "   'individual',\n",
       "   'clones',\n",
       "   'and',\n",
       "   'showed',\n",
       "   'that',\n",
       "   'clones',\n",
       "   'remained',\n",
       "   'stable',\n",
       "   'on',\n",
       "   'serial',\n",
       "   'transplantation.',\n",
       "   'despite',\n",
       "   'this',\n",
       "   'stability,',\n",
       "   'the',\n",
       "   'proliferation,',\n",
       "   'persistence,',\n",
       "   'and',\n",
       "   'chemotherapy',\n",
       "   'tolerance',\n",
       "   'of',\n",
       "   'lentivirally',\n",
       "   'marked',\n",
       "   'lineages',\n",
       "   'were',\n",
       "   'variable',\n",
       "   'within',\n",
       "   'each',\n",
       "   'clone.',\n",
       "   'chemotherapy',\n",
       "   'promoted',\n",
       "   'dominance',\n",
       "   'of',\n",
       "   'previously',\n",
       "   'minor',\n",
       "   'or',\n",
       "   'dormant',\n",
       "   'lineages.',\n",
       "   'thus,',\n",
       "   'apart',\n",
       "   'from',\n",
       "   'genetic',\n",
       "   'diversity,',\n",
       "   'tumor',\n",
       "   'cells',\n",
       "   'display',\n",
       "   'inherent',\n",
       "   'functional',\n",
       "   'variability',\n",
       "   'in',\n",
       "   'tumor',\n",
       "   'propagation',\n",
       "   'potential,',\n",
       "   'a',\n",
       "   'mechanism',\n",
       "   'that',\n",
       "   'contributes',\n",
       "   'both',\n",
       "   'to',\n",
       "   'cancer',\n",
       "   'growth',\n",
       "   'and',\n",
       "   'therapy',\n",
       "   'tolerance.\"\"recent',\n",
       "   'advances',\n",
       "   'in',\n",
       "   'functional',\n",
       "   'genomics',\n",
       "   'have',\n",
       "   'helped',\n",
       "   'generate',\n",
       "   'large-scale',\n",
       "   'high-throughput',\n",
       "   'protein',\n",
       "   'interaction',\n",
       "   'data.',\n",
       "   'such',\n",
       "   'networks,',\n",
       "   'though',\n",
       "   'extremely',\n",
       "   'valuable',\n",
       "   'towards',\n",
       "   'molecular',\n",
       "   'level',\n",
       "   'understanding',\n",
       "   'of',\n",
       "   'cells,',\n",
       "   'do',\n",
       "   'not',\n",
       "   'provide',\n",
       "   'any',\n",
       "   'direct',\n",
       "   'information',\n",
       "   'about',\n",
       "   'the',\n",
       "   'regions',\n",
       "   '(domains)',\n",
       "   'in',\n",
       "   'the',\n",
       "   'proteins',\n",
       "   'that',\n",
       "   'mediate',\n",
       "   'the',\n",
       "   'interaction.',\n",
       "   'here,',\n",
       "   'we',\n",
       "   'performed',\n",
       "   'co-evolutionary',\n",
       "   'analysis',\n",
       "   'of',\n",
       "   'domains',\n",
       "   'in',\n",
       "   'interacting',\n",
       "   'proteins',\n",
       "   'in',\n",
       "   'order',\n",
       "   'to',\n",
       "   'understand',\n",
       "   'the',\n",
       "   'degree',\n",
       "   'of',\n",
       "   'co-evolution',\n",
       "   'of',\n",
       "   'interacting',\n",
       "   'and',\n",
       "   'non-interacting',\n",
       "   'domains.',\n",
       "   'using',\n",
       "   'a',\n",
       "   'combination',\n",
       "   'of',\n",
       "   'sequence',\n",
       "   'and',\n",
       "   'structural',\n",
       "   'analysis,',\n",
       "   'we',\n",
       "   'analyzed',\n",
       "   'protein–protein',\n",
       "   'interactions',\n",
       "   'in',\n",
       "   '{f1-atpase},',\n",
       "   '{sec23p/sec24p},',\n",
       "   '{dna}-directed',\n",
       "   '{rna}',\n",
       "   'polymerase',\n",
       "   'and',\n",
       "   'nuclear',\n",
       "   'pore',\n",
       "   'complexes,',\n",
       "   'and',\n",
       "   'found',\n",
       "   'that',\n",
       "   'interacting',\n",
       "   'domain',\n",
       "   'pair(s)',\n",
       "   'for',\n",
       "   'a',\n",
       "   'given',\n",
       "   'interaction',\n",
       "   'exhibits',\n",
       "   'higher',\n",
       "   'level',\n",
       "   'of',\n",
       "   'co-evolution',\n",
       "   'than',\n",
       "   'the',\n",
       "   'non-interacting',\n",
       "   'domain',\n",
       "   'pairs.',\n",
       "   'motivated',\n",
       "   'by',\n",
       "   'this',\n",
       "   'finding,',\n",
       "   'we',\n",
       "   'developed',\n",
       "   'a',\n",
       "   'computational',\n",
       "   'method',\n",
       "   'to',\n",
       "   'test',\n",
       "   'the',\n",
       "   'generality',\n",
       "   'of',\n",
       "   'the',\n",
       "   'observed',\n",
       "   'trend,',\n",
       "   'and',\n",
       "   'to',\n",
       "   'predict',\n",
       "   'large-scale',\n",
       "   'domain–domain',\n",
       "   'interactions.',\n",
       "   'given',\n",
       "   'a',\n",
       "   'protein–protein',\n",
       "   'interaction,',\n",
       "   'the',\n",
       "   'proposed',\n",
       "   'method',\n",
       "   'predicts',\n",
       "   'the',\n",
       "   'domain',\n",
       "   'pair(s)',\n",
       "   'that',\n",
       "   'is',\n",
       "   'most',\n",
       "   'likely',\n",
       "   'to',\n",
       "   'mediate',\n",
       "   'the',\n",
       "   'protein',\n",
       "   'interaction.',\n",
       "   'we',\n",
       "   'applied',\n",
       "   'this',\n",
       "   'method',\n",
       "   'on',\n",
       "   'the',\n",
       "   'yeast',\n",
       "   'interactome',\n",
       "   'to',\n",
       "   'predict',\n",
       "   'domain–domain',\n",
       "   'interactions,',\n",
       "   'and',\n",
       "   'used',\n",
       "   'known',\n",
       "   'domain–domain',\n",
       "   'interactions',\n",
       "   'found',\n",
       "   'in',\n",
       "   '{pdb}',\n",
       "   'crystal',\n",
       "   'structures',\n",
       "   'to',\n",
       "   'validate',\n",
       "   'our',\n",
       "   'predictions.',\n",
       "   'our',\n",
       "   'results',\n",
       "   'show',\n",
       "   'that',\n",
       "   'the',\n",
       "   'prediction',\n",
       "   'accuracy',\n",
       "   'of',\n",
       "   'the',\n",
       "   'proposed',\n",
       "   'method',\n",
       "   'is',\n",
       "   'statistically',\n",
       "   'significant.',\n",
       "   'comparison',\n",
       "   'of',\n",
       "   'our',\n",
       "   'prediction',\n",
       "   'results',\n",
       "   'with',\n",
       "   'those',\n",
       "   'from',\n",
       "   'two',\n",
       "   'other',\n",
       "   'methods',\n",
       "   'reveals',\n",
       "   'that',\n",
       "   'only',\n",
       "   'a',\n",
       "   'fraction',\n",
       "   'of',\n",
       "   'predictions',\n",
       "   'are',\n",
       "   'shared',\n",
       "   'by',\n",
       "   'all',\n",
       "   'the',\n",
       "   'three',\n",
       "   'methods,',\n",
       "   'indicating',\n",
       "   'that',\n",
       "   'the',\n",
       "   'proposed',\n",
       "   'method',\n",
       "   'can',\n",
       "   'detect',\n",
       "   'known',\n",
       "   'interactions',\n",
       "   'missed',\n",
       "   'by',\n",
       "   'other',\n",
       "   'methods.',\n",
       "   'we',\n",
       "   'believe',\n",
       "   'that',\n",
       "   'the',\n",
       "   'proposed',\n",
       "   'method',\n",
       "   'can',\n",
       "   'be',\n",
       "   'used',\n",
       "   'with',\n",
       "   'other',\n",
       "   'methods',\n",
       "   'to',\n",
       "   'help',\n",
       "   'identify',\n",
       "   'previously',\n",
       "   'unrecognized',\n",
       "   'domain–domain',\n",
       "   'interactions',\n",
       "   'on',\n",
       "   'a',\n",
       "   'genome',\n",
       "   'scale,',\n",
       "   'and',\n",
       "   'could',\n",
       "   'potentially',\n",
       "   'help',\n",
       "   'reduce',\n",
       "   'the',\n",
       "   'search',\n",
       "   'space',\n",
       "   'for',\n",
       "   'identifying',\n",
       "   'interaction',\n",
       "   'sites.\"boston,',\n",
       "   'ma',\n",
       "   '02215.\",\"structure,',\n",
       "   'function,',\n",
       "   'and',\n",
       "   'evolution',\n",
       "   'of',\n",
       "   'transient',\n",
       "   'and',\n",
       "   'obligate',\n",
       "   'protein–protein',\n",
       "   'interactions\",\"recent',\n",
       "   'analyses',\n",
       "   'of',\n",
       "   'high-throughput',\n",
       "   'protein',\n",
       "   'interaction',\n",
       "   'data',\n",
       "   'coupled',\n",
       "   'with',\n",
       "   'large-scale',\n",
       "   'investigations',\n",
       "   'of',\n",
       "   'evolutionary',\n",
       "   'properties',\n",
       "   'of',\n",
       "   'interaction',\n",
       "   'networks',\n",
       "   'have',\n",
       "   'left',\n",
       "   'some',\n",
       "   'unanswered',\n",
       "   'questions.',\n",
       "   'to',\n",
       "   'what',\n",
       "   'extent',\n",
       "   'do',\n",
       "   'protein',\n",
       "   'interactions',\n",
       "   'act',\n",
       "   'as',\n",
       "   'constraints',\n",
       "   'during',\n",
       "   'evolution',\n",
       "   'of',\n",
       "   'the',\n",
       "   'protein',\n",
       "   'sequence?',\n",
       "   'how',\n",
       "   'does',\n",
       "   'the',\n",
       "   'type',\n",
       "   'of',\n",
       "   'interaction,',\n",
       "   'specifically',\n",
       "   'transient',\n",
       "   'or',\n",
       "   'obligate,',\n",
       "   'play',\n",
       "   'into',\n",
       "   'these',\n",
       "   ...])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting the values every time a blank space is encountered\n",
    "splitRDD = transJoinResultRDD.map(lambda x: (x[0], x[1].split(' ')))\n",
    "splitRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('f05bcffe7951de9e5a32fff4a42eb088', 'database'), 61),\n",
       " (('f05bcffe7951de9e5a32fff4a42eb088', 'functional'), 40),\n",
       " (('f05bcffe7951de9e5a32fff4a42eb088', 'of'), 1050),\n",
       " (('f05bcffe7951de9e5a32fff4a42eb088', 'documentation'), 4),\n",
       " (('f05bcffe7951de9e5a32fff4a42eb088', 'families'), 5),\n",
       " (('f05bcffe7951de9e5a32fff4a42eb088', 'as'), 181),\n",
       " (('f05bcffe7951de9e5a32fff4a42eb088', 'identify'), 21),\n",
       " (('f05bcffe7951de9e5a32fff4a42eb088', 'rules'), 3),\n",
       " (('f05bcffe7951de9e5a32fff4a42eb088', 'power'), 8),\n",
       " (('f05bcffe7951de9e5a32fff4a42eb088', '{uniprotkb}/{swiss-prot}'), 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting the frequency of each individual words used by every user\n",
    "splitRDD2 = splitRDD.flatMapValues(lambda x: x).map(lambda x: (x,1)).reduceByKey(lambda a, b: a + b)\n",
    "#splitRDD2.persist()\n",
    "splitRDD2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('f05bcffe7951de9e5a32fff4a42eb088', ('database', 61)),\n",
       " ('f05bcffe7951de9e5a32fff4a42eb088', ('functional', 40)),\n",
       " ('f05bcffe7951de9e5a32fff4a42eb088', ('of', 1050)),\n",
       " ('f05bcffe7951de9e5a32fff4a42eb088', ('documentation', 4)),\n",
       " ('f05bcffe7951de9e5a32fff4a42eb088', ('families', 5)),\n",
       " ('f05bcffe7951de9e5a32fff4a42eb088', ('as', 181)),\n",
       " ('f05bcffe7951de9e5a32fff4a42eb088', ('identify', 21)),\n",
       " ('f05bcffe7951de9e5a32fff4a42eb088', ('rules', 3)),\n",
       " ('f05bcffe7951de9e5a32fff4a42eb088', ('power', 8)),\n",
       " ('f05bcffe7951de9e5a32fff4a42eb088', ('{uniprotkb}/{swiss-prot}', 1))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearrangedRDD = splitRDD2.map(lambda x: ((x[0][0], (x[0][1], x[1]))))\n",
    "rearrangedRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('f05bcffe7951de9e5a32fff4a42eb088',\n",
       "  [('the', 1157),\n",
       "   ('of', 1050),\n",
       "   ('and', 862),\n",
       "   ('a', 541),\n",
       "   ('to', 502),\n",
       "   ('in', 418),\n",
       "   ('for', 384),\n",
       "   ('is', 317),\n",
       "   ('data', 207),\n",
       "   ('that', 206),\n",
       "   ('as', 181),\n",
       "   ('are', 174),\n",
       "   ('we', 157),\n",
       "   ('with', 153),\n",
       "   ('from', 139),\n",
       "   ('by', 117),\n",
       "   ('on', 111),\n",
       "   ('be', 108),\n",
       "   ('an', 107),\n",
       "   ('this', 107),\n",
       "   ('can', 105),\n",
       "   ('information', 97),\n",
       "   ('gene', 90),\n",
       "   ('binding', 86),\n",
       "   ('analysis', 85),\n",
       "   ('biological', 84),\n",
       "   ('at', 79),\n",
       "   ('have', 78),\n",
       "   ('available', 77),\n",
       "   ('protein', 77),\n",
       "   ('sequence', 72),\n",
       "   ('or', 71),\n",
       "   ('which', 71),\n",
       "   ('transcription', 65),\n",
       "   ('these', 65),\n",
       "   ('their', 62),\n",
       "   ('database', 61),\n",
       "   ('genes', 58),\n",
       "   ('also', 56),\n",
       "   ('sites', 56),\n",
       "   ('such', 56),\n",
       "   ('has', 55),\n",
       "   ('regulatory', 51),\n",
       "   ('all', 51),\n",
       "   ('been', 50),\n",
       "   ('it', 50),\n",
       "   ('new', 50),\n",
       "   ('motif', 50),\n",
       "   ('used', 48),\n",
       "   ('sequences', 47),\n",
       "   ('networks', 46),\n",
       "   ('different', 45),\n",
       "   ('provides', 44),\n",
       "   ('most', 43),\n",
       "   ('web', 43),\n",
       "   ('using', 43),\n",
       "   ('into', 43),\n",
       "   ('model', 43),\n",
       "   ('genome', 42),\n",
       "   ('one', 42),\n",
       "   ('tools', 41),\n",
       "   ('functional', 40),\n",
       "   ('multiple', 40),\n",
       "   ('{dna}', 40),\n",
       "   ('our', 40),\n",
       "   ('other', 39),\n",
       "   ('between', 39),\n",
       "   ('provide', 38),\n",
       "   ('number', 37),\n",
       "   ('interaction', 37),\n",
       "   ('present', 36),\n",
       "   ('databases', 36),\n",
       "   ('set', 35),\n",
       "   ('each', 35),\n",
       "   ('not', 35),\n",
       "   ('methods', 34),\n",
       "   ('more', 34),\n",
       "   ('but', 34),\n",
       "   ('network', 34),\n",
       "   ('sets', 33),\n",
       "   ('well', 33),\n",
       "   ('factor', 33),\n",
       "   ('pathway', 32),\n",
       "   ('its', 32),\n",
       "   ('use', 31),\n",
       "   ('algorithm', 31),\n",
       "   ('mass', 31),\n",
       "   ('than', 31),\n",
       "   ('software', 31),\n",
       "   ('ontology', 31),\n",
       "   ('alignment', 31),\n",
       "   ('many', 31),\n",
       "   ('two', 30),\n",
       "   ('method', 30),\n",
       "   ('interactions', 30),\n",
       "   ('models', 30),\n",
       "   ('experimental', 30),\n",
       "   ('large', 29),\n",
       "   ('molecular', 28),\n",
       "   ('users', 27),\n",
       "   ('based', 27),\n",
       "   ('only', 26),\n",
       "   ('annotation', 26),\n",
       "   ('expression', 26),\n",
       "   ('motifs', 26),\n",
       "   ('developed', 25),\n",
       "   ('pathways', 25),\n",
       "   ('features', 25),\n",
       "   ('data,', 25),\n",
       "   ('human', 25),\n",
       "   ('systems', 25),\n",
       "   ('tool', 25),\n",
       "   ('', 24),\n",
       "   ('including', 24),\n",
       "   ('computational', 24),\n",
       "   ('integrated', 24),\n",
       "   ('patterns', 24),\n",
       "   ('over', 24),\n",
       "   ('data.', 24),\n",
       "   ('resource', 24),\n",
       "   ('identification', 24),\n",
       "   ('research', 23),\n",
       "   ('{kegg}', 23),\n",
       "   ('may', 22),\n",
       "   ('metabolic', 22),\n",
       "   ('factors', 22),\n",
       "   ('known', 22),\n",
       "   ('about', 22),\n",
       "   ('interface', 22),\n",
       "   ('novel', 22),\n",
       "   ('approach', 22),\n",
       "   ('identify', 21),\n",
       "   ('specific', 21),\n",
       "   ('proteins', 21),\n",
       "   ('knowledge', 21),\n",
       "   ('were', 21),\n",
       "   ('within', 21),\n",
       "   ('bioinformatics', 21),\n",
       "   ('both', 21),\n",
       "   ('ontologies', 21),\n",
       "   ('through', 21),\n",
       "   ('when', 20),\n",
       "   ('source', 20),\n",
       "   ('algorithms', 20),\n",
       "   ('results', 20),\n",
       "   ('system', 20),\n",
       "   ('development', 19),\n",
       "   ('studies', 19),\n",
       "   ('sequencing', 19),\n",
       "   ('open', 19),\n",
       "   ('important', 19),\n",
       "   ('various', 19),\n",
       "   ('find', 19),\n",
       "   ('high-throughput', 19),\n",
       "   ('here', 19),\n",
       "   ('show', 19),\n",
       "   ('however,', 18),\n",
       "   ('finding', 18),\n",
       "   ('biology', 18),\n",
       "   ('genomic', 18),\n",
       "   ('significant', 18),\n",
       "   ('single', 18),\n",
       "   ('signaling', 18),\n",
       "   ('comprehensive', 18),\n",
       "   ('arabidopsis', 18),\n",
       "   ('contains', 18),\n",
       "   ('made', 18),\n",
       "   ('among', 18),\n",
       "   ('elements', 18),\n",
       "   ('freely', 17),\n",
       "   ('sequences.', 17),\n",
       "   ('standard', 17),\n",
       "   ('describe', 17),\n",
       "   ('phylogenetic', 17),\n",
       "   ('species', 17),\n",
       "   ('where', 16),\n",
       "   ('entities', 16),\n",
       "   ('regions', 16),\n",
       "   ('study', 16),\n",
       "   ('mapping', 16),\n",
       "   ('under', 16),\n",
       "   ('given', 16),\n",
       "   ('access', 16),\n",
       "   ('collection', 16),\n",
       "   ('integration', 16),\n",
       "   ('support', 16),\n",
       "   ('application', 16),\n",
       "   ('discovery', 16),\n",
       "   ('allows', 16),\n",
       "   ('several', 16),\n",
       "   ('associated', 16),\n",
       "   ('function', 16),\n",
       "   ('how', 16),\n",
       "   ('current', 15),\n",
       "   ('major', 15),\n",
       "   ('public', 15),\n",
       "   ('facilitate', 15),\n",
       "   ('user', 15),\n",
       "   ('include', 15),\n",
       "   ('was', 15),\n",
       "   ('genomes', 15),\n",
       "   ('recent', 15),\n",
       "   ('understanding', 15),\n",
       "   ('databases.', 14),\n",
       "   ('common', 14),\n",
       "   ('experiments', 14),\n",
       "   ('will', 14),\n",
       "   ('currently', 14),\n",
       "   ('search', 14),\n",
       "   ('approaches', 14),\n",
       "   ('site', 14),\n",
       "   ('structure', 14),\n",
       "   ('they', 14),\n",
       "   ('orthologous', 13),\n",
       "   ('cis-regulatory', 13),\n",
       "   ('any', 13),\n",
       "   ('networks,', 13),\n",
       "   ('online', 13),\n",
       "   ('scientific', 13),\n",
       "   ('query', 13),\n",
       "   ('there', 13),\n",
       "   ('probabilistic', 13),\n",
       "   ('cellular', 13),\n",
       "   ('identified', 13),\n",
       "   ('sources', 13),\n",
       "   ('chromatin', 13),\n",
       "   ('evolutionary', 13),\n",
       "   ('groups', 13),\n",
       "   ('profiles', 13),\n",
       "   ('terms', 13),\n",
       "   ('accuracy', 13),\n",
       "   ('complex', 13),\n",
       "   ('curated', 13),\n",
       "   ('annotated', 12),\n",
       "   ('significantly', 12),\n",
       "   ('community', 12),\n",
       "   ('conservation', 12),\n",
       "   ('usa.', 12),\n",
       "   ('project', 12),\n",
       "   ('those', 12),\n",
       "   ('updated', 12),\n",
       "   ('service', 12),\n",
       "   ('annotations', 12),\n",
       "   ('alignments', 12),\n",
       "   ('visualization', 12),\n",
       "   ('yeast', 12),\n",
       "   ('metabolite', 12),\n",
       "   ('depth', 12),\n",
       "   ('statistical', 12),\n",
       "   ('related', 12),\n",
       "   ('them', 12),\n",
       "   ('direct', 12),\n",
       "   ('detection', 12),\n",
       "   ('microarray', 12),\n",
       "   ('diverse', 11),\n",
       "   ('{chip}-seq', 11),\n",
       "   ('three', 11),\n",
       "   ('graph', 11),\n",
       "   ('individual', 11),\n",
       "   ('includes', 11),\n",
       "   ('some', 11),\n",
       "   ('across', 11),\n",
       "   ('highly', 11),\n",
       "   ('need', 11),\n",
       "   ('framework', 11),\n",
       "   ('promoter', 11),\n",
       "   ('previously', 11),\n",
       "   ('because', 11),\n",
       "   ('often', 11),\n",
       "   ('conserved', 11),\n",
       "   ('local', 11),\n",
       "   ('although', 11),\n",
       "   ('pathways,', 11),\n",
       "   ('input', 11),\n",
       "   ('gibbs', 11),\n",
       "   ('spectrometry', 11),\n",
       "   ('no', 10),\n",
       "   ('derived', 10),\n",
       "   ('process', 10),\n",
       "   ('models,', 10),\n",
       "   ('metabolites', 10),\n",
       "   ('integrate', 10),\n",
       "   ('applications', 10),\n",
       "   ('information,', 10),\n",
       "   ('problem', 10),\n",
       "   ('organisms.', 10),\n",
       "   ('processing', 10),\n",
       "   ('variety', 10),\n",
       "   ('being', 10),\n",
       "   ('prior', 10),\n",
       "   ('analyze', 10),\n",
       "   ('version', 10),\n",
       "   ('biochemical', 10),\n",
       "   ('organisms', 10),\n",
       "   ('high', 10),\n",
       "   ('{tf}', 10),\n",
       "   ('better', 10),\n",
       "   ('target', 10),\n",
       "   ('introduce', 10),\n",
       "   ('server', 10),\n",
       "   ('protein-protein', 10),\n",
       "   ('prediction', 10),\n",
       "   ('web-based', 10),\n",
       "   ('reference', 10),\n",
       "   ('sequences,', 10),\n",
       "   ('communities', 10),\n",
       "   ('key', 10),\n",
       "   ('literature', 10),\n",
       "   ('motifs.', 10),\n",
       "   ('aligned', 10),\n",
       "   ('types', 10),\n",
       "   ('large-scale', 10),\n",
       "   ('become', 9),\n",
       "   ('biomedical', 9),\n",
       "   ('datasets', 9),\n",
       "   ('generated', 9),\n",
       "   ('widely', 9),\n",
       "   ('predicted', 9),\n",
       "   ('nodes', 9),\n",
       "   ('{uniprot}', 9),\n",
       "   ('easily', 9),\n",
       "   ('methods,', 9),\n",
       "   ('automated', 9),\n",
       "   ('distribution', 9),\n",
       "   ('reliability', 9),\n",
       "   ('extensive', 9),\n",
       "   ('cell', 9),\n",
       "   ('maximum', 9),\n",
       "   ('link', 9),\n",
       "   ('without', 9),\n",
       "   ('existing', 9),\n",
       "   ('supplementary', 9),\n",
       "   ('up', 9),\n",
       "   ('properties', 9),\n",
       "   ('providing', 9),\n",
       "   ('allow', 9),\n",
       "   ('here,', 9),\n",
       "   ('global', 9),\n",
       "   ('designed', 9),\n",
       "   ('underlying', 9),\n",
       "   ('state', 9),\n",
       "   ('useful', 9),\n",
       "   ('positions', 9),\n",
       "   ('peak', 9),\n",
       "   ('order', 9),\n",
       "   ('recently', 9),\n",
       "   ('explore', 9),\n",
       "   ('searching', 9),\n",
       "   ('called', 9),\n",
       "   ('paper', 9),\n",
       "   ('upstream', 9),\n",
       "   ('genes,', 9),\n",
       "   ('same', 9),\n",
       "   ('able', 9),\n",
       "   ('perform', 9),\n",
       "   ('improve', 9),\n",
       "   ('identifying', 9),\n",
       "   ('functions', 9),\n",
       "   ('interpretation', 9),\n",
       "   ('interactive', 9),\n",
       "   ('demonstrate', 9),\n",
       "   ('chemical', 9),\n",
       "   ('complete', 9),\n",
       "   ('contain', 9),\n",
       "   ('provided', 9),\n",
       "   ('power', 8),\n",
       "   ('storage', 8),\n",
       "   ('formats', 8),\n",
       "   ('enable', 8),\n",
       "   ('applied', 8),\n",
       "   ('very', 8),\n",
       "   ('collections', 8),\n",
       "   ('analyses', 8),\n",
       "   ('simple', 8),\n",
       "   ('processes', 8),\n",
       "   ('publicly', 8),\n",
       "   ('possible', 8),\n",
       "   ('database,', 8),\n",
       "   ('description', 8),\n",
       "   ('website', 8),\n",
       "   ('{mrna}', 8),\n",
       "   ('generates', 8),\n",
       "   ('position', 8),\n",
       "   ('test', 8),\n",
       "   ('obtained', 8),\n",
       "   ('heterogeneous', 8),\n",
       "   ('controlled', 8),\n",
       "   ('required', 8),\n",
       "   ('code', 8),\n",
       "   ('develop', 8),\n",
       "   ('biologically', 8),\n",
       "   ('lists', 8),\n",
       "   ('cancer', 8),\n",
       "   ('characterization', 8),\n",
       "   ('additional', 8),\n",
       "   ('modifications', 8),\n",
       "   ('analysis.', 8),\n",
       "   ('information.', 8),\n",
       "   ('work', 8),\n",
       "   ('networks.', 8),\n",
       "   ('downloaded', 8),\n",
       "   ('combination', 8),\n",
       "   ('structures', 8),\n",
       "   ('proposed', 8),\n",
       "   ('compared', 8),\n",
       "   ('promoters', 8),\n",
       "   ('proteomics', 8),\n",
       "   ('comparison', 8),\n",
       "   ('bayesian', 8),\n",
       "   ('represent', 8),\n",
       "   ('proteins,', 8),\n",
       "   ('corresponding', 8),\n",
       "   ('time', 8),\n",
       "   ('intergenic', 8),\n",
       "   ('{deme}', 8),\n",
       "   ('via', 8),\n",
       "   ('identifier', 8),\n",
       "   ('full', 8),\n",
       "   ('challenge', 8),\n",
       "   ('increase', 8),\n",
       "   ('coverage', 8),\n",
       "   ('java', 8),\n",
       "   ('spectra', 8),\n",
       "   ('user-friendly', 8),\n",
       "   ('similar', 8),\n",
       "   ('cluster', 8),\n",
       "   ('modules', 8),\n",
       "   ('genes.', 8),\n",
       "   ('directly', 8),\n",
       "   ('2', 8),\n",
       "   ('profiling', 8),\n",
       "   ('positional', 8),\n",
       "   ('small-world', 8),\n",
       "   ('electronic', 8),\n",
       "   ('{phylogibbs}', 8),\n",
       "   ('further', 7),\n",
       "   ('linking', 7),\n",
       "   ('performance', 7),\n",
       "   ('design', 7),\n",
       "   ('do', 7),\n",
       "   ('created', 7),\n",
       "   ('take', 7),\n",
       "   ('stored', 7),\n",
       "   ('improved', 7),\n",
       "   ('robust', 7),\n",
       "   ('whose', 7),\n",
       "   ('make', 7),\n",
       "   ('logos', 7),\n",
       "   ('accessed', 7),\n",
       "   ('step', 7),\n",
       "   ('{biocyc}', 7),\n",
       "   ('{biana}', 7),\n",
       "   ('{ngs}', 7),\n",
       "   ('technologies', 7),\n",
       "   ('level', 7),\n",
       "   ('nature', 7),\n",
       "   ('describes', 7),\n",
       "   ('described', 7),\n",
       "   ('content', 7),\n",
       "   ('view', 7),\n",
       "   ('background', 7),\n",
       "   ('real', 7),\n",
       "   ('while', 7),\n",
       "   ('(arabidopsis', 7),\n",
       "   ('thaliana', 7),\n",
       "   ('efforts', 7),\n",
       "   ('sequenced', 7),\n",
       "   ('systematic', 7),\n",
       "   ('would', 7),\n",
       "   ('maps', 7),\n",
       "   ('differences', 7),\n",
       "   ('representation', 7),\n",
       "   ('saccharomyces', 7),\n",
       "   ('largely', 7),\n",
       "   ('exchange', 7),\n",
       "   ('core', 7),\n",
       "   ('five', 7),\n",
       "   ('create', 7),\n",
       "   ('exploration', 7),\n",
       "   ('repositories', 7),\n",
       "   ('sample', 7),\n",
       "   ('should', 7),\n",
       "   ('supported', 7),\n",
       "   ('de', 7),\n",
       "   ('hierarchical', 7),\n",
       "   ('library', 7),\n",
       "   ('especially', 7),\n",
       "   ('suggest', 7),\n",
       "   ('enhanced', 7),\n",
       "   ('traditional', 7),\n",
       "   ('sites.', 7),\n",
       "   ('potential', 7),\n",
       "   ('semantic', 7),\n",
       "   ('quantitative', 7),\n",
       "   ('quality', 7),\n",
       "   ('uses', 7),\n",
       "   ('published', 7),\n",
       "   ('central', 7),\n",
       "   ('evidence', 7),\n",
       "   ('help', 7),\n",
       "   ('searches', 7),\n",
       "   ('regulation', 7),\n",
       "   ('unknown', 7),\n",
       "   ('genomics', 7),\n",
       "   ('encyclopedia', 7),\n",
       "   ('acid', 7),\n",
       "   ('detect', 7),\n",
       "   ('minimum', 7),\n",
       "   ('powerful', 7),\n",
       "   ('classes', 7),\n",
       "   ('relations', 7),\n",
       "   ('first', 7),\n",
       "   ('mapped', 7),\n",
       "   ('connectivity', 7),\n",
       "   ('found', 7),\n",
       "   ('sequence.', 7),\n",
       "   ('services', 7),\n",
       "   ('arbitrary', 7),\n",
       "   ('advances', 6),\n",
       "   ('basic', 6),\n",
       "   ('{biopax}', 6),\n",
       "   ('reported', 6),\n",
       "   ('broad', 6),\n",
       "   ('comparative', 6),\n",
       "   ('techniques', 6),\n",
       "   ('rapidly', 6),\n",
       "   ('identifiers', 6),\n",
       "   ('relevant', 6),\n",
       "   ('systems.', 6),\n",
       "   ('now', 6),\n",
       "   ('free', 6),\n",
       "   ('novo', 6),\n",
       "   ('newly', 6),\n",
       "   ('social', 6),\n",
       "   ('annotations,', 6),\n",
       "   ('browser', 6),\n",
       "   ('insights', 6),\n",
       "   ('aims', 6),\n",
       "   ('sampler', 6),\n",
       "   ('ways', 6),\n",
       "   ('markov', 6),\n",
       "   ('probability', 6),\n",
       "   ('range', 6),\n",
       "   ('problems', 6),\n",
       "   ('map', 6),\n",
       "   ('completely', 6),\n",
       "   ('characterized', 6),\n",
       "   ('caenorhabditis', 6),\n",
       "   ('nucleotide', 6),\n",
       "   ('download', 6),\n",
       "   ('relative', 6),\n",
       "   ('domain', 6),\n",
       "   ('amino', 6),\n",
       "   ('{prosite}', 6),\n",
       "   ('{sbml}', 6),\n",
       "   ('guide', 6),\n",
       "   ('results.', 6),\n",
       "   ('histone', 6),\n",
       "   ('accurate', 6),\n",
       "   ('towards', 6),\n",
       "   ('synthetic', 6),\n",
       "   ('following', 6),\n",
       "   ('sampling', 6),\n",
       "   ('initial', 6),\n",
       "   ('species.', 6),\n",
       "   ('000', 6),\n",
       "   ('offers', 6),\n",
       "   ('spatial', 6),\n",
       "   ('experiment', 6),\n",
       "   ('initiation', 6),\n",
       "   ('consists', 6),\n",
       "   ('signal', 6),\n",
       "   ('license', 6),\n",
       "   ('package', 6),\n",
       "   ('propose', 6),\n",
       "   ('environment', 6),\n",
       "   ('either', 6),\n",
       "   ('addition', 6),\n",
       "   ('manually', 6),\n",
       "   ('uk.', 6),\n",
       "   ('unique', 6),\n",
       "   ('logo', 6),\n",
       "   ('{agris}', 6),\n",
       "   ('release', 6),\n",
       "   ('moreover,', 6),\n",
       "   ('drosophila', 6),\n",
       "   ('functionally', 6),\n",
       "   ('and/or', 6),\n",
       "   ('challenges', 6),\n",
       "   ('relationships', 6),\n",
       "   ('defined', 6),\n",
       "   ('links', 6),\n",
       "   ('{pazar}', 6),\n",
       "   ('{go}', 6),\n",
       "   ('allowing', 6),\n",
       "   ('rate', 6),\n",
       "   ('tests', 6),\n",
       "   ('plant', 6),\n",
       "   ('transcriptional', 6),\n",
       "   ('genetic', 6),\n",
       "   ('inferred', 6),\n",
       "   ('genome-wide', 6),\n",
       "   ('discriminative', 6),\n",
       "   ('distributed', 6),\n",
       "   ('role', 6),\n",
       "   ('general', 6),\n",
       "   ('represented', 6),\n",
       "   ('crucial', 6),\n",
       "   ('databases,', 6),\n",
       "   ('available.', 6),\n",
       "   ('enrichment', 6),\n",
       "   ('changes', 6),\n",
       "   ('products', 6),\n",
       "   ('extended', 6),\n",
       "   ('regions.', 6),\n",
       "   ('{ondex}', 6),\n",
       "   ('expressed', 6),\n",
       "   ('address', 6),\n",
       "   ('families', 5),\n",
       "   ('generation', 5),\n",
       "   ('mechanisms', 5),\n",
       "   ('linked', 5),\n",
       "   ('immunoprecipitation', 5),\n",
       "   ('considered', 5),\n",
       "   ('raw', 5),\n",
       "   ('eukaryotic', 5),\n",
       "   ('algorithm,', 5),\n",
       "   ('levels', 5),\n",
       "   ('mining', 5),\n",
       "   ('start', 5),\n",
       "   ('length', 5),\n",
       "   ('growth', 5),\n",
       "   ('point', 5),\n",
       "   ('information:', 5),\n",
       "   ('online.', 5),\n",
       "   ('visualized', 5),\n",
       "   ('thus', 5),\n",
       "   ('1', 5),\n",
       "   ('alternative', 5),\n",
       "   ('reveal', 5),\n",
       "   ('output', 5),\n",
       "   ('{atcisdb}', 5),\n",
       "   ('regular', 5),\n",
       "   ('finally,', 5),\n",
       "   ('viewer', 5),\n",
       "   ('interactions.', 5),\n",
       "   ('future', 5),\n",
       "   ('research,', 5),\n",
       "   ('component', 5),\n",
       "   ('computer', 5),\n",
       "   ('unaligned', 5),\n",
       "   ('tools.', 5),\n",
       "   ('simulated', 5),\n",
       "   ('commonly', 5),\n",
       "   ('{meme}', 5),\n",
       "   ('systematically', 5),\n",
       "   ('discovering', 5),\n",
       "   ('clustering', 5),\n",
       "   ('objects', 5),\n",
       "   ('biology,', 5),\n",
       "   ('discovery,', 5),\n",
       "   ('hundreds', 5),\n",
       "   ('measures', 5),\n",
       "   ('fully', 5),\n",
       "   ('expected', 5),\n",
       "   ('thousands', 5),\n",
       "   ('{attfdb}', 5),\n",
       "   ('descriptions', 5),\n",
       "   ('composition', 5),\n",
       "   ('coli', 5),\n",
       "   ('simulation', 5),\n",
       "   ('past', 5),\n",
       "   ('progress', 5),\n",
       "   ('methylation', 5),\n",
       "   ('best', 5),\n",
       "   ('essential', 5),\n",
       "   ('querying', 5),\n",
       "   ('processes,', 5),\n",
       "   ('taxonomic', 5),\n",
       "   ('short', 5),\n",
       "   ('combinations', 5),\n",
       "   ('methods.', 5),\n",
       "   ('university', 5),\n",
       "   ('due', 5),\n",
       "   ('unified', 5),\n",
       "   ('compare', 5),\n",
       "   ('supports', 5),\n",
       "   ('metabolomics', 5),\n",
       "   ('extends', 5),\n",
       "   ('resources', 5),\n",
       "   ('vertebrate', 5),\n",
       "   ('network.', 5),\n",
       "   ('{dna}-binding', 5),\n",
       "   ('predictive', 5),\n",
       "   ('york', 5),\n",
       "   ('interacting', 5),\n",
       "   ('visual', 5),\n",
       "   ('{cagrid}', 5),\n",
       "   ('{stamp}', 5),\n",
       "   ('critical', 5),\n",
       "   ('discuss', 5),\n",
       "   ('display', 5),\n",
       "   ('elements,', 5),\n",
       "   ('statistically', 5),\n",
       "   ('mechanism', 5),\n",
       "   ('increasing', 5),\n",
       "   ('likelihood', 5),\n",
       "   ('enables', 5),\n",
       "   ('report', 5),\n",
       "   ('included', 5),\n",
       "   ('availability:', 5),\n",
       "   ('resources,', 5),\n",
       "   ('evaluate', 5),\n",
       "   ('previous', 5),\n",
       "   ('results:', 5),\n",
       "   ('relationship', 5),\n",
       "   ('escherichia', 5),\n",
       "   ('consensus', 5),\n",
       "   ('ability', 5),\n",
       "   ('”', 5),\n",
       "   ('annotations.', 5),\n",
       "   ('ceres', 5),\n",
       "   ('analyse', 5),\n",
       "   ('overlap', 5),\n",
       "   ('utilized', 5),\n",
       "   ('proteins.', 5),\n",
       "   ('architecture', 5),\n",
       "   ('meaningful', 5),\n",
       "   ('{source}', 5),\n",
       "   ('object', 5),\n",
       "   ('discovered', 5),\n",
       "   ('ppm', 5),\n",
       "   ('sites,', 5),\n",
       "   ('format', 5),\n",
       "   ('mammalian', 5),\n",
       "   ('serve', 5),\n",
       "   ('primary', 5),\n",
       "   ('availability', 5),\n",
       "   ('higher', 5),\n",
       "   ('researchers', 5),\n",
       "   ('incomplete', 5),\n",
       "   ('mathematical', 5),\n",
       "   ('strongly', 5),\n",
       "   ('means', 5),\n",
       "   ('suitable', 5),\n",
       "   ('own', 5),\n",
       "   ('regulators', 5),\n",
       "   ('{pir}', 5),\n",
       "   ('{genemania}', 5),\n",
       "   ('unification', 5),\n",
       "   ('describing', 5),\n",
       "   ('biology.', 5),\n",
       "   ('functionality', 5),\n",
       "   ('monte', 5),\n",
       "   ('flexible', 5),\n",
       "   ('majority', 5),\n",
       "   ('edges', 5),\n",
       "   ('health,', 5),\n",
       "   ('strategy', 5),\n",
       "   ('functions.', 5),\n",
       "   ('automatically', 5),\n",
       "   ('associations', 5),\n",
       "   ('another', 5),\n",
       "   ('experimentally', 5),\n",
       "   ('studies.', 5),\n",
       "   ('drug', 5),\n",
       "   ('methodology', 5),\n",
       "   ('location', 5),\n",
       "   ('small', 5),\n",
       "   ('{vobn}', 5),\n",
       "   ('so', 5),\n",
       "   ('developmental', 5),\n",
       "   ('whole', 5),\n",
       "   ('graph-based', 5),\n",
       "   ('documentation', 4),\n",
       "   ('customizable', 4),\n",
       "   ('question', 4),\n",
       "   ('task', 4),\n",
       "   ('algorithms.', 4),\n",
       "   ('stand-alone', 4),\n",
       "   ('reads', 4),\n",
       "   ('conditions', 4),\n",
       "   ('accurately', 4),\n",
       "   ('indicate', 4),\n",
       "   ('tasks', 4),\n",
       "   ('update', 4),\n",
       "   ('making', 4),\n",
       "   ('against', 4),\n",
       "   ('share', 4),\n",
       "   ('scattered', 4),\n",
       "   ('sets.', 4),\n",
       "   (\"5'\", 4),\n",
       "   ('four', 4),\n",
       "   ('selected', 4),\n",
       "   ('entire', 4),\n",
       "   ('must', 4),\n",
       "   ('needs', 4),\n",
       "   ('{atregnet}', 4),\n",
       "   ('coordinated', 4),\n",
       "   ('mutated', 4),\n",
       "   ('is,', 4),\n",
       "   ('management', 4),\n",
       "   ('fraction', 4),\n",
       "   ('e.', 4),\n",
       "   ('reveals', 4),\n",
       "   ('enabled', 4),\n",
       "   ('research.', 4),\n",
       "   ('numerous', 4),\n",
       "   ('highlight', 4),\n",
       "   ('{cpath}', 4),\n",
       "   ('makes', 4),\n",
       "   ('custom', 4),\n",
       "   ('combines', 4),\n",
       "   ('organism', 4),\n",
       "   ('factors.', 4),\n",
       "   ('overrepresented', 4),\n",
       "   ('shown', 4),\n",
       "   ('considerable', 4),\n",
       "   ('difficult', 4),\n",
       "   ('browse', 4),\n",
       "   ('entry', 4),\n",
       "   ('followed', 4),\n",
       "   ('define', 4),\n",
       "   ('million', 4),\n",
       "   ('total', 4),\n",
       "   ('robustness', 4),\n",
       "   ('wide', 4),\n",
       "   ('file', 4),\n",
       "   ('illustrate', 4),\n",
       "   ('incorporates', 4),\n",
       "   ('interactions,', 4),\n",
       "   ('graphs', 4),\n",
       "   ('gene,', 4),\n",
       "   ('metabolomic', 4),\n",
       "   ('(i)', 4),\n",
       "   ('contact:', 4),\n",
       "   ('then', 4),\n",
       "   ('demonstrating', 4),\n",
       "   ('popular', 4),\n",
       "   ('efficient', 4),\n",
       "   ('ensembl', 4),\n",
       "   ('algorithms,', 4),\n",
       "   ('combined', 4),\n",
       "   ('dynamics', 4),\n",
       "   ('principles', 4),\n",
       "   ('vast', 4),\n",
       "   ('compounds', 4),\n",
       "   ('tools,', 4),\n",
       "   ('signals', 4),\n",
       "   ('{hprd},', 4),\n",
       "   ('expression.', 4),\n",
       "   ('processes.', 4),\n",
       "   ('automatic', 4),\n",
       "   ('together', 4),\n",
       "   ('fixed', 4),\n",
       "   ('activity', 4),\n",
       "   ('main', 4),\n",
       "   ('visualize', 4),\n",
       "   ('navigation', 4),\n",
       "   ('correlation', 4),\n",
       "   ('client', 4),\n",
       "   ('storing,', 4),\n",
       "   ('organisms,', 4),\n",
       "   ('estimates', 4),\n",
       "   ('whereas', 4),\n",
       "   ('cambridge', 4),\n",
       "   ('department', 4),\n",
       "   ('facilitates', 4),\n",
       "   ('optimization', 4),\n",
       "   ('pattern', 4),\n",
       "   ('resulting', 4),\n",
       "   ('graphical', 4),\n",
       "   ('(ii)', 4),\n",
       "   ('occur', 4),\n",
       "   ('showed', 4),\n",
       "   ('{ids}', 4),\n",
       "   ('effective', 4),\n",
       "   ('specificity', 4),\n",
       "   ('correlations', 4),\n",
       "   ('sequence,', 4),\n",
       "   ('({tfs})', 4),\n",
       "   ('lack', 4),\n",
       "   ('scientists', 4),\n",
       "   ('elements.', 4),\n",
       "   ('amounts', 4),\n",
       "   ('subsets', 4),\n",
       "   ('upon', 4),\n",
       "   ('even', 4),\n",
       "   ('needed', 4),\n",
       "   ('{tsss}', 4),\n",
       "   ('sources.', 4),\n",
       "   ('studied', 4),\n",
       "   ('does', 4),\n",
       "   ('offer', 4),\n",
       "   ('assessment', 4),\n",
       "   ('implemented', 4),\n",
       "   ('programmatic', 4),\n",
       "   ('reliable', 4),\n",
       "   ('example', 4),\n",
       "   ('types.', 4),\n",
       "   ('extending', 4),\n",
       "   ('frequently', 4),\n",
       "   ('modeling', 4),\n",
       "   ('bases', 4),\n",
       "   ('program', 4),\n",
       "   ('groups.', 4),\n",
       "   ('article,', 4),\n",
       "   ('signalling', 4),\n",
       "   ('organization', 4),\n",
       "   ('energy', 4),\n",
       "   ('libraries', 4),\n",
       "   ('pages', 4),\n",
       "   ('error', 4),\n",
       "   ('necessary', 4),\n",
       "   ('{wikipathways}', 4),\n",
       "   ('determine', 4),\n",
       "   ('analyzing', 4),\n",
       "   ('{ppis}', 4),\n",
       "   ('fast', 4),\n",
       "   ('grid', 4),\n",
       "   ('distinct', 4),\n",
       "   ('k', 4),\n",
       "   ('{mzmine}', 4),\n",
       "   ('domains,', 4),\n",
       "   ('accessible', 4),\n",
       "   ('particular', 4),\n",
       "   ('speed,', 4),\n",
       "   ('convenient', 4),\n",
       "   ('software,', 4),\n",
       "   ('external', 4),\n",
       "   ('academic', 4),\n",
       "   ('platform', 4),\n",
       "   ('cb10', 4),\n",
       "   ('{ols}', 4),\n",
       "   ('obtain', 4),\n",
       "   ('estimate', 4),\n",
       "   ('parsimony', 4),\n",
       "   ('carlo', 4),\n",
       "   ('result', 4),\n",
       "   ('added', 4),\n",
       "   ('lightweight', 4),\n",
       "   ('open-source', 4),\n",
       "   ('predicting', 4),\n",
       "   ('generalised', 4),\n",
       "   ('{pathway}', 4),\n",
       "   ('components', 4),\n",
       "   ('3,', 4),\n",
       "   ('coupled', 4),\n",
       "   ('stable', 4),\n",
       "   ('increased', 4),\n",
       "   ('currently,', 4),\n",
       "   ('{string}', 4),\n",
       "   ('aspects', 4),\n",
       "   ('effectively', 4),\n",
       "   ('account', 4),\n",
       "   ('classification', 4),\n",
       "   ('out', 4),\n",
       "   ('universal', 4),\n",
       "   ('localization', 4),\n",
       "   ('like', 4),\n",
       "   ('database.', 4),\n",
       "   ('consistent', 4),\n",
       "   ('sets,', 4),\n",
       "   ('registry', 4),\n",
       "   ('protocol', 4),\n",
       "   ('understand', 4),\n",
       "   ('approximately', 4),\n",
       "   ('translational', 4),\n",
       "   ('pathways.', 4),\n",
       "   ('and,', 4),\n",
       "   ('specifically', 4),\n",
       "   ('since', 4),\n",
       "   ('sufficient', 4),\n",
       "   ('{soap}', 4),\n",
       "   ('flexibility', 4),\n",
       "   ('detailed', 4),\n",
       "   ('advantage', 4),\n",
       "   ('false', 4),\n",
       "   ('generic', 4),\n",
       "   ('precise', 4),\n",
       "   ('standardized', 4),\n",
       "   ('base', 4),\n",
       "   ('enzyme', 4),\n",
       "   ('repository', 4),\n",
       "   ('(iii)', 4),\n",
       "   ('addition,', 4),\n",
       "   ('least', 4),\n",
       "   ('proteomic', 4),\n",
       "   ('decay', 4),\n",
       "   ('{ot}', 4),\n",
       "   ('{jaspar}', 4),\n",
       "   ('subset', 4),\n",
       "   ('elucidation', 4),\n",
       "   ('synergizer', 4),\n",
       "   ('requires', 4),\n",
       "   ('annotation,', 4),\n",
       "   ('require', 4),\n",
       "   ('capturing', 4),\n",
       "   ('{tfs}', 4),\n",
       "   ('university,', 4),\n",
       "   ...]),\n",
       " ('beaca02b21b7cad6cb738c5e2682af8d',\n",
       "  [('the', 2810),\n",
       "   ('of', 2747),\n",
       "   ('and', 1653),\n",
       "   ('in', 1278),\n",
       "   ('to', 1174),\n",
       "   ('a', 1164),\n",
       "   ('that', 675),\n",
       "   ('for', 649),\n",
       "   ('is', 601),\n",
       "   ('we', 568),\n",
       "   ('are', 504),\n",
       "   ('with', 455),\n",
       "   ('by', 339),\n",
       "   ('as', 330),\n",
       "   ('protein', 318),\n",
       "   ('on', 307),\n",
       "   ('from', 289),\n",
       "   ('this', 272),\n",
       "   ('be', 240),\n",
       "   ('cancer', 230),\n",
       "   ('these', 213),\n",
       "   ('an', 209),\n",
       "   ('have', 204),\n",
       "   ('binding', 170),\n",
       "   ('gene', 169),\n",
       "   ('data', 166),\n",
       "   ('or', 162),\n",
       "   ('proteins', 161),\n",
       "   ('which', 160),\n",
       "   ('at', 155),\n",
       "   ('genes', 154),\n",
       "   ('can', 146),\n",
       "   ('sequence', 139),\n",
       "   ('between', 139),\n",
       "   ('mutations', 134),\n",
       "   ('new', 127),\n",
       "   ('our', 126),\n",
       "   ('human', 125),\n",
       "   ('{dna}', 125),\n",
       "   ('their', 124),\n",
       "   ('method', 123),\n",
       "   ('using', 120),\n",
       "   ('been', 118),\n",
       "   ('more', 118),\n",
       "   ('has', 117),\n",
       "   ('analysis', 114),\n",
       "   ('expression', 112),\n",
       "   ('it', 105),\n",
       "   ('interaction', 104),\n",
       "   ('also', 103),\n",
       "   ('interactions', 103),\n",
       "   ('different', 101),\n",
       "   ('not', 99),\n",
       "   ('than', 99),\n",
       "   ('used', 99),\n",
       "   ('tumor', 98),\n",
       "   ('number', 94),\n",
       "   ('all', 92),\n",
       "   ('such', 90),\n",
       "   ('sites', 89),\n",
       "   ('two', 89),\n",
       "   ('results', 88),\n",
       "   ('functional', 87),\n",
       "   ('transcription', 87),\n",
       "   ('were', 87),\n",
       "   ('protein-protein', 85),\n",
       "   ('into', 85),\n",
       "   ('methods', 85),\n",
       "   ('structural', 83),\n",
       "   ('known', 83),\n",
       "   ('many', 82),\n",
       "   ('most', 82),\n",
       "   ('structure', 80),\n",
       "   ('genome', 80),\n",
       "   ('molecular', 78),\n",
       "   ('other', 77),\n",
       "   ('was', 77),\n",
       "   ('may', 76),\n",
       "   ('information', 74),\n",
       "   ('interface', 74),\n",
       "   ('but', 74),\n",
       "   ('genomic', 74),\n",
       "   ('based', 73),\n",
       "   ('multiple', 73),\n",
       "   ('show', 72),\n",
       "   ('one', 71),\n",
       "   ('its', 71),\n",
       "   ('evolutionary', 70),\n",
       "   ('prediction', 70),\n",
       "   ('each', 69),\n",
       "   ('cell', 69),\n",
       "   ('model', 68),\n",
       "   ('biological', 67),\n",
       "   ('approach', 64),\n",
       "   ('structures', 64),\n",
       "   ('evolution', 63),\n",
       "   ('present', 63),\n",
       "   ('interacting', 62),\n",
       "   ('set', 61),\n",
       "   ('regulatory', 60),\n",
       "   ('genetic', 58),\n",
       "   ('within', 58),\n",
       "   ('found', 58),\n",
       "   ('large', 58),\n",
       "   ('network', 57),\n",
       "   ('available', 57),\n",
       "   ('including', 57),\n",
       "   ('well', 57),\n",
       "   ('only', 57),\n",
       "   ('interfaces', 56),\n",
       "   ('sequencing', 56),\n",
       "   ('identify', 55),\n",
       "   ('residues', 55),\n",
       "   ('complex', 54),\n",
       "   ('important', 53),\n",
       "   ('cells', 53),\n",
       "   ('database', 53),\n",
       "   ('identified', 53),\n",
       "   ('specific', 52),\n",
       "   ('here', 52),\n",
       "   ('sequences', 52),\n",
       "   ('complexes', 51),\n",
       "   ('conserved', 50),\n",
       "   ('single', 49),\n",
       "   ('however,', 49),\n",
       "   ('here,', 49),\n",
       "   ('breast', 49),\n",
       "   ('', 48),\n",
       "   ('computational', 48),\n",
       "   ('those', 48),\n",
       "   ('patterns', 47),\n",
       "   ('regions', 47),\n",
       "   ('both', 46),\n",
       "   ('similar', 45),\n",
       "   ('how', 45),\n",
       "   ('protein–protein', 45),\n",
       "   ('associated', 44),\n",
       "   ('function', 44),\n",
       "   ('developed', 44),\n",
       "   ('novel', 44),\n",
       "   ('through', 44),\n",
       "   ('some', 44),\n",
       "   ('mutation', 43),\n",
       "   ('several', 43),\n",
       "   ('cellular', 43),\n",
       "   ('provides', 43),\n",
       "   ('common', 43),\n",
       "   ('somatic', 43),\n",
       "   ('when', 42),\n",
       "   ('across', 42),\n",
       "   ('predict', 41),\n",
       "   ('use', 41),\n",
       "   ('alignment', 41),\n",
       "   ('surface', 41),\n",
       "   ('high', 40),\n",
       "   ('understanding', 40),\n",
       "   ('about', 40),\n",
       "   ('domain', 40),\n",
       "   ('observed', 40),\n",
       "   ('experimental', 40),\n",
       "   ('previously', 39),\n",
       "   ('factor', 39),\n",
       "   ('data.', 38),\n",
       "   ('factors', 38),\n",
       "   ('accuracy', 38),\n",
       "   ('proteins,', 37),\n",
       "   ('find', 36),\n",
       "   ('will', 36),\n",
       "   ('properties', 36),\n",
       "   ('algorithm', 36),\n",
       "   ('cancers', 36),\n",
       "   ('provide', 35),\n",
       "   ('there', 35),\n",
       "   ('heterogeneity', 35),\n",
       "   ('they', 35),\n",
       "   ('samples', 35),\n",
       "   ('demonstrate', 35),\n",
       "   ('individual', 35),\n",
       "   ('predicted', 35),\n",
       "   ('role', 35),\n",
       "   ('three', 34),\n",
       "   ('predictions', 34),\n",
       "   ('genes.', 34),\n",
       "   ('same', 34),\n",
       "   ('tumors', 34),\n",
       "   ('residue', 34),\n",
       "   ('study', 34),\n",
       "   ('transcriptional', 34),\n",
       "   ('phylogenetic', 34),\n",
       "   ('statistical', 34),\n",
       "   ('predicting', 33),\n",
       "   ('copy', 33),\n",
       "   ('models', 33),\n",
       "   ('studies', 33),\n",
       "   ('university', 33),\n",
       "   ('methylation', 33),\n",
       "   ('proteins.', 33),\n",
       "   ('site', 33),\n",
       "   ('small', 33),\n",
       "   ('motifs', 32),\n",
       "   ('research', 32),\n",
       "   ('networks', 32),\n",
       "   ('genomes', 32),\n",
       "   ('related', 32),\n",
       "   ('recent', 32),\n",
       "   ('clustering', 32),\n",
       "   ('interactions.', 32),\n",
       "   ('compared', 32),\n",
       "   ('over', 32),\n",
       "   ('among', 32),\n",
       "   ('distinct', 32),\n",
       "   ('sets', 31),\n",
       "   ('identification', 31),\n",
       "   ('significant', 31),\n",
       "   ('derived', 31),\n",
       "   ('approaches', 31),\n",
       "   ('highly', 31),\n",
       "   ('during', 30),\n",
       "   ('acid', 30),\n",
       "   ('target', 30),\n",
       "   ('pathways', 30),\n",
       "   ('higher', 30),\n",
       "   ('where', 30),\n",
       "   ('yeast', 30),\n",
       "   ('test', 29),\n",
       "   ('conservation', 29),\n",
       "   ('involved', 29),\n",
       "   ('although', 29),\n",
       "   ('chromatin', 29),\n",
       "   ('clinical', 29),\n",
       "   ('significantly', 29),\n",
       "   ('first', 29),\n",
       "   ('”', 29),\n",
       "   ('{tfs}', 29),\n",
       "   ('docking', 28),\n",
       "   ('applied', 28),\n",
       "   ('even', 28),\n",
       "   ('level', 28),\n",
       "   ('microarray', 28),\n",
       "   ('key', 28),\n",
       "   ('functions', 28),\n",
       "   ('tumour', 28),\n",
       "   ('major', 28),\n",
       "   ('comparison', 28),\n",
       "   ('promoter', 28),\n",
       "   ('amino', 27),\n",
       "   ('hot', 27),\n",
       "   ('essential', 27),\n",
       "   ('time', 27),\n",
       "   ('tools', 27),\n",
       "   ('allows', 27),\n",
       "   ('given', 27),\n",
       "   ('changes', 27),\n",
       "   ('targets', 27),\n",
       "   ('propose', 27),\n",
       "   ('likely', 26),\n",
       "   ('hydrogen', 26),\n",
       "   ('development', 25),\n",
       "   ('disease', 25),\n",
       "   ('alignments', 25),\n",
       "   ('primary', 25),\n",
       "   ('them', 25),\n",
       "   ('patients', 25),\n",
       "   ('features', 25),\n",
       "   ('contact', 25),\n",
       "   ('cells.', 25),\n",
       "   ('colorectal', 24),\n",
       "   ('differences', 24),\n",
       "   ('dynamics', 24),\n",
       "   ('biology', 24),\n",
       "   ('algorithms', 24),\n",
       "   ('pairs', 24),\n",
       "   ('modules', 24),\n",
       "   ('rate', 24),\n",
       "   ('families', 24),\n",
       "   ('suggest', 24),\n",
       "   ('profiles', 24),\n",
       "   ('recognition', 24),\n",
       "   ('cases', 23),\n",
       "   ('any', 23),\n",
       "   ('while', 23),\n",
       "   ('types', 23),\n",
       "   ('various', 23),\n",
       "   ('few', 23),\n",
       "   ('combination', 23),\n",
       "   ('four', 23),\n",
       "   ('potential', 23),\n",
       "   ('lung', 23),\n",
       "   ('server', 23),\n",
       "   ('under', 23),\n",
       "   ('program', 23),\n",
       "   ('relevant', 23),\n",
       "   ('mutational', 23),\n",
       "   ('normal', 23),\n",
       "   ('department', 22),\n",
       "   ('analyses', 22),\n",
       "   ('clonal', 22),\n",
       "   ('often', 22),\n",
       "   ('secondary', 22),\n",
       "   ('evidence', 22),\n",
       "   ('mutated', 22),\n",
       "   ('furthermore,', 22),\n",
       "   ('form', 22),\n",
       "   ('driver', 22),\n",
       "   ('generated', 22),\n",
       "   ('better', 22),\n",
       "   ('review', 22),\n",
       "   ('performed', 22),\n",
       "   ('classification', 22),\n",
       "   ('performance', 22),\n",
       "   ('relative', 22),\n",
       "   ('could', 22),\n",
       "   ('does', 22),\n",
       "   ('web', 22),\n",
       "   ('approximately', 22),\n",
       "   ('much', 22),\n",
       "   ('scoring', 21),\n",
       "   ('possible', 21),\n",
       "   ('specificity', 21),\n",
       "   ('basis', 21),\n",
       "   ('suggesting', 21),\n",
       "   ('addition,', 21),\n",
       "   ('tool', 21),\n",
       "   ('comprehensive', 21),\n",
       "   ('large-scale', 21),\n",
       "   ('play', 21),\n",
       "   ('called', 21),\n",
       "   ('cluster', 21),\n",
       "   ('interfaces.', 21),\n",
       "   ('hubs', 21),\n",
       "   ('process', 21),\n",
       "   ('similarity', 21),\n",
       "   ('techniques', 21),\n",
       "   ('biclustering', 21),\n",
       "   ('identifying', 20),\n",
       "   ('current', 20),\n",
       "   ('{\\\\copyright}', 20),\n",
       "   ('random', 20),\n",
       "   ('local', 20),\n",
       "   ('species', 20),\n",
       "   ('compare', 20),\n",
       "   ('proposed', 20),\n",
       "   ('perform', 20),\n",
       "   ('parameters', 20),\n",
       "   ('then', 20),\n",
       "   ('provided', 20),\n",
       "   ('accurate', 20),\n",
       "   ('alterations', 20),\n",
       "   ('contacts', 20),\n",
       "   ('describe', 20),\n",
       "   ('hydrophobic', 20),\n",
       "   ('correlated', 20),\n",
       "   ('discovery', 20),\n",
       "   ('previous', 20),\n",
       "   ('less', 20),\n",
       "   ('functionally', 19),\n",
       "   ('growth', 19),\n",
       "   ('defined', 19),\n",
       "   ('knowledge', 19),\n",
       "   ('selection', 19),\n",
       "   ('insights', 19),\n",
       "   ('majority', 19),\n",
       "   ('characterized', 19),\n",
       "   ('ability', 19),\n",
       "   ('include', 19),\n",
       "   ('score', 19),\n",
       "   ('further', 19),\n",
       "   ('able', 19),\n",
       "   ('{tfbs}', 19),\n",
       "   ('thus', 19),\n",
       "   ('per', 19),\n",
       "   ('shows', 19),\n",
       "   ('order', 19),\n",
       "   ('resistance', 19),\n",
       "   ('signal', 19),\n",
       "   ('existing', 19),\n",
       "   ('domains', 19),\n",
       "   ('underlying', 19),\n",
       "   ('mouse', 19),\n",
       "   ('usa.', 19),\n",
       "   ('complete', 19),\n",
       "   ('de', 19),\n",
       "   ('variation', 19),\n",
       "   ('variety', 19),\n",
       "   ('simple', 18),\n",
       "   ('parallel', 18),\n",
       "   ('comparative', 18),\n",
       "   ('cancer.', 18),\n",
       "   ('loss', 18),\n",
       "   ('without', 18),\n",
       "   ('detection', 18),\n",
       "   ('subset', 18),\n",
       "   ('bonds', 18),\n",
       "   ('optimal', 18),\n",
       "   ('unbound', 18),\n",
       "   ('basic', 18),\n",
       "   ('chains', 18),\n",
       "   ('shown', 18),\n",
       "   ('measure', 18),\n",
       "   ('useful', 18),\n",
       "   ('mechanisms', 18),\n",
       "   ('metastatic', 18),\n",
       "   ('particularly', 18),\n",
       "   ('systems', 18),\n",
       "   ('university,', 18),\n",
       "   ('benchmark', 18),\n",
       "   ('annotation', 18),\n",
       "   ('data,', 18),\n",
       "   ('general', 18),\n",
       "   ('independent', 18),\n",
       "   ('organization', 18),\n",
       "   ('hub', 18),\n",
       "   ('because', 18),\n",
       "   ('very', 17),\n",
       "   ('method,', 17),\n",
       "   ('indicate', 17),\n",
       "   ('spots', 17),\n",
       "   ('homologous', 17),\n",
       "   ('terms', 17),\n",
       "   ('enriched', 17),\n",
       "   ('solvent', 17),\n",
       "   ('software', 17),\n",
       "   ('diverse', 17),\n",
       "   ('search', 17),\n",
       "   ('no', 17),\n",
       "   ('work', 17),\n",
       "   ('particular', 17),\n",
       "   ('whose', 17),\n",
       "   ('implications', 17),\n",
       "   ('revealed', 17),\n",
       "   ('tissue-specific', 17),\n",
       "   ('programs', 17),\n",
       "   ('transient', 17),\n",
       "   ('what', 17),\n",
       "   ('range', 17),\n",
       "   ('analyze', 17),\n",
       "   ('help', 17),\n",
       "   ('sites.', 17),\n",
       "   ('difficult', 17),\n",
       "   ('if', 17),\n",
       "   ('genes,', 17),\n",
       "   ('total', 17),\n",
       "   ('represent', 17),\n",
       "   ('overall', 16),\n",
       "   ('selective', 16),\n",
       "   ('high-throughput', 16),\n",
       "   ('diversity', 16),\n",
       "   ('structures.', 16),\n",
       "   ('tend', 16),\n",
       "   ('correlation', 16),\n",
       "   ('sequences.', 16),\n",
       "   ('pattern', 16),\n",
       "   ('detect', 16),\n",
       "   ('contribute', 16),\n",
       "   ('do', 16),\n",
       "   ('type', 16),\n",
       "   ('complexes.', 16),\n",
       "   ('clusters', 16),\n",
       "   ('chain', 16),\n",
       "   ('demonstrated', 16),\n",
       "   ('project', 16),\n",
       "   ('promoters', 16),\n",
       "   ('includes', 16),\n",
       "   ('currently', 16),\n",
       "   ('combined', 16),\n",
       "   ('made', 16),\n",
       "   ('report', 16),\n",
       "   ('being', 16),\n",
       "   ('main', 16),\n",
       "   ('complexes,', 15),\n",
       "   ('constraints', 15),\n",
       "   ('whereas', 15),\n",
       "   ('{kras}', 15),\n",
       "   ('crystal', 15),\n",
       "   ('dataset', 15),\n",
       "   ('databases', 15),\n",
       "   ('early', 15),\n",
       "   ('chromosomal', 15),\n",
       "   ('resulting', 15),\n",
       "   ('understand', 15),\n",
       "   ('become', 15),\n",
       "   ('{tf}', 15),\n",
       "   ('fast', 15),\n",
       "   ('biology,', 15),\n",
       "   ('elements', 15),\n",
       "   ('architecture', 15),\n",
       "   ('up', 15),\n",
       "   ('expressed', 15),\n",
       "   ('levels', 15),\n",
       "   ('stable', 15),\n",
       "   ('{rna}', 15),\n",
       "   ('increase', 15),\n",
       "   ('determined', 15),\n",
       "   ('sites,', 15),\n",
       "   ('formation', 15),\n",
       "   ('designed', 15),\n",
       "   ('thus,', 15),\n",
       "   ('genomics', 15),\n",
       "   ('analysis,', 15),\n",
       "   ('application', 15),\n",
       "   ('ovarian', 15),\n",
       "   ('context', 15),\n",
       "   ('bound', 15),\n",
       "   ('patient', 15),\n",
       "   ('methods.', 14),\n",
       "   ('processes', 14),\n",
       "   ('version', 14),\n",
       "   ('{dna}-binding', 14),\n",
       "   ('detected', 14),\n",
       "   ('structurally', 14),\n",
       "   ('molecules', 14),\n",
       "   ('acquired', 14),\n",
       "   ('low', 14),\n",
       "   ('regulation', 14),\n",
       "   ('water', 14),\n",
       "   ('mathematical', 14),\n",
       "   ('variants', 14),\n",
       "   ('=', 14),\n",
       "   ('experimentally', 14),\n",
       "   ('six', 14),\n",
       "   ('finding', 14),\n",
       "   ('showed', 14),\n",
       "   ('additional', 14),\n",
       "   ('histone', 14),\n",
       "   ('motif', 14),\n",
       "   ('relationships', 14),\n",
       "   ('would', 14),\n",
       "   ('now', 14),\n",
       "   ('frequency', 14),\n",
       "   ('tumours', 14),\n",
       "   ('framework', 14),\n",
       "   ('another', 14),\n",
       "   ('remains', 14),\n",
       "   ('problem', 14),\n",
       "   ('functions.', 14),\n",
       "   ('develop', 14),\n",
       "   ('progression', 14),\n",
       "   ('drug', 14),\n",
       "   ('trees', 14),\n",
       "   ('association', 14),\n",
       "   ('core', 14),\n",
       "   ('integrated', 14),\n",
       "   ('freely', 14),\n",
       "   ('{wiley-liss},', 14),\n",
       "   ('feature', 14),\n",
       "   ('reveals', 14),\n",
       "   ('accessible', 14),\n",
       "   ('state', 14),\n",
       "   ('providing', 14),\n",
       "   ('responsible', 14),\n",
       "   ('recurrent', 14),\n",
       "   ('systematic', 14),\n",
       "   ('organisms', 14),\n",
       "   ('pathway', 14),\n",
       "   ('distribution', 14),\n",
       "   ('bioinformatics', 14),\n",
       "   ('should', 14),\n",
       "   ('energy', 14),\n",
       "   ('suggests', 14),\n",
       "   ('program,', 13),\n",
       "   ('conformational', 13),\n",
       "   ('signaling', 13),\n",
       "   ('result', 13),\n",
       "   ('cells,', 13),\n",
       "   ('extent', 13),\n",
       "   ('larger', 13),\n",
       "   ('metabolic', 13),\n",
       "   ('global', 13),\n",
       "   ('interactions,', 13),\n",
       "   ('partners', 13),\n",
       "   ('particular,', 13),\n",
       "   ('discuss', 13),\n",
       "   ('atomic', 13),\n",
       "   ('original', 13),\n",
       "   ('associations', 13),\n",
       "   ('genomes.', 13),\n",
       "   ('md', 13),\n",
       "   ('making', 13),\n",
       "   ('fraction', 13),\n",
       "   ('source', 13),\n",
       "   ('obtained', 13),\n",
       "   ('best', 13),\n",
       "   ('biomolecular', 13),\n",
       "   ('area', 13),\n",
       "   ('step', 13),\n",
       "   ('signatures', 13),\n",
       "   ('size', 13),\n",
       "   ('contains', 13),\n",
       "   ('package', 13),\n",
       "   ('{pkm2}', 13),\n",
       "   ('effects', 13),\n",
       "   ('nature', 13),\n",
       "   ('extensive', 13),\n",
       "   ('intermediate', 13),\n",
       "   ('evaluation', 13),\n",
       "   ('combining', 13),\n",
       "   ('clear', 13),\n",
       "   ('open', 13),\n",
       "   ('detecting', 13),\n",
       "   ('cancer,', 13),\n",
       "   ('representation', 13),\n",
       "   ('rearrangements', 13),\n",
       "   ('upon', 13),\n",
       "   ('certain', 13),\n",
       "   ('allelic', 13),\n",
       "   ('due', 13),\n",
       "   ('recently', 13),\n",
       "   ('average', 13),\n",
       "   ('therefore', 13),\n",
       "   ('unique', 13),\n",
       "   ('introduce', 13),\n",
       "   ('strategy', 13),\n",
       "   ('had', 13),\n",
       "   ('increasing', 13),\n",
       "   ('still', 13),\n",
       "   ('allow', 12),\n",
       "   ('support', 12),\n",
       "   ('robust', 12),\n",
       "   ('insight', 12),\n",
       "   ('us', 12),\n",
       "   ('genome-wide', 12),\n",
       "   ('orthologous', 12),\n",
       "   ('whether', 12),\n",
       "   ('problems', 12),\n",
       "   ('efficient', 12),\n",
       "   ('need', 12),\n",
       "   ('directly', 12),\n",
       "   ('dynamic', 12),\n",
       "   ('whole', 12),\n",
       "   ('side', 12),\n",
       "   ('blood', 12),\n",
       "   ('modeling', 12),\n",
       "   ('finally,', 12),\n",
       "   ('estimate', 12),\n",
       "   ('free', 12),\n",
       "   ('increased', 12),\n",
       "   ('groups', 12),\n",
       "   ('long', 12),\n",
       "   ('areas', 12),\n",
       "   ('evolve', 12),\n",
       "   ('explore', 12),\n",
       "   ('future', 12),\n",
       "   ('events', 12),\n",
       "   ('system', 12),\n",
       "   ('representing', 12),\n",
       "   ('thousands', 12),\n",
       "   ('monomers', 12),\n",
       "   ('sources', 12),\n",
       "   ('reference', 12),\n",
       "   ('region', 12),\n",
       "   ('central', 12),\n",
       "   ('class', 12),\n",
       "   ('experiments', 12),\n",
       "   ('make', 12),\n",
       "   ('consistent', 12),\n",
       "   ('distance', 12),\n",
       "   ('cases,', 12),\n",
       "   ('require', 12),\n",
       "   ('contribution', 12),\n",
       "   ('separate', 12),\n",
       "   ('sensitivity', 12),\n",
       "   ('expression,', 12),\n",
       "   ('nucleotide', 12),\n",
       "   ('biclusters', 12),\n",
       "   ('apply', 12),\n",
       "   ('presence', 11),\n",
       "   ('methods,', 11),\n",
       "   ('function,', 11),\n",
       "   ('graph', 11),\n",
       "   ('roles', 11),\n",
       "   ('widely', 11),\n",
       "   ('influence', 11),\n",
       "   ('studies,', 11),\n",
       "   ('interfacial', 11),\n",
       "   ('five', 11),\n",
       "   ('strong', 11),\n",
       "   ('described', 11),\n",
       "   ('predictions.', 11),\n",
       "   ('bayesian', 11),\n",
       "   ('et', 11),\n",
       "   ('either', 11),\n",
       "   ('located', 11),\n",
       "   ('chemical', 11),\n",
       "   ('moreover,', 11),\n",
       "   ('resolution', 11),\n",
       "   ('might', 11),\n",
       "   ('contain', 11),\n",
       "   ('expression.', 11),\n",
       "   ('determine', 11),\n",
       "   ('genomes,', 11),\n",
       "   ('cis-regulatory', 11),\n",
       "   ('especially', 11),\n",
       "   ('backbone', 11),\n",
       "   ('effective', 11),\n",
       "   ('reported', 11),\n",
       "   ('1', 11),\n",
       "   ('assess', 11),\n",
       "   ('abundance', 11),\n",
       "   ('{kegg}', 11),\n",
       "   ('reveal', 11),\n",
       "   ('that,', 11),\n",
       "   ('strongly', 11),\n",
       "   ('aberrations', 11),\n",
       "   ('direct', 11),\n",
       "   ('address', 11),\n",
       "   ('user', 11),\n",
       "   ('pair', 11),\n",
       "   ('subclonal', 11),\n",
       "   ('addition', 11),\n",
       "   ('p53', 11),\n",
       "   ('heterogeneous', 11),\n",
       "   ('therapeutic', 11),\n",
       "   ('institute', 11),\n",
       "   ('selected', 11),\n",
       "   ('effect', 11),\n",
       "   ('group', 11),\n",
       "   ('published', 11),\n",
       "   ('machine', 11),\n",
       "   ('mutant', 11),\n",
       "   ('power', 11),\n",
       "   ('regions,', 11),\n",
       "   ('{cpg}', 11),\n",
       "   ('modules.', 11),\n",
       "   ('boston,', 11),\n",
       "   ('importance', 11),\n",
       "   ('users', 11),\n",
       "   ('since', 11),\n",
       "   ('statistically', 11),\n",
       "   ('little', 11),\n",
       "   ('simulations', 11),\n",
       "   ('hierarchical', 11),\n",
       "   ('distinguish', 11),\n",
       "   ('sample', 11),\n",
       "   ('activity', 11),\n",
       "   ('control', 11),\n",
       "   ('biomedical', 10),\n",
       "   ('nonredundant', 10),\n",
       "   ('study,', 10),\n",
       "   ('via', 10),\n",
       "   ('weighted', 10),\n",
       "   ('three-dimensional', 10),\n",
       "   ('response', 10),\n",
       "   ('evaluated', 10),\n",
       "   ('interact', 10),\n",
       "   ('derive', 10),\n",
       "   ('affymetrix', 10),\n",
       "   ('cancers.', 10),\n",
       "   ('collection', 10),\n",
       "   ('enable', 10),\n",
       "   ('sets,', 10),\n",
       "   ('lower', 10),\n",
       "   ('hypothesis', 10),\n",
       "   ('eukaryotic', 10),\n",
       "   ('{pten}', 10),\n",
       "   ('networks.', 10),\n",
       "   ('account', 10),\n",
       "   ('suggested', 10),\n",
       "   ('occur', 10),\n",
       "   ('family', 10),\n",
       "   ('affinity', 10),\n",
       "   ('types.', 10),\n",
       "   ('evaluate', 10),\n",
       "   ('relatively', 10),\n",
       "   ('complexity', 10),\n",
       "   ('kinase', 10),\n",
       "   ('calculated', 10),\n",
       "   ('body', 10),\n",
       "   ('profiling', 10),\n",
       "   ('oligomeric', 10),\n",
       "   ('every', 10),\n",
       "   ('buried', 10),\n",
       "   ('achieved', 10),\n",
       "   ('already', 10),\n",
       "   ('clones', 10),\n",
       "   ('predicts', 10),\n",
       "   ('potentially', 10),\n",
       "   ('developing', 10),\n",
       "   ('coding', 10),\n",
       "   ('positions', 10),\n",
       "   ('regulate', 10),\n",
       "   ('limited', 10),\n",
       "   ('true', 10),\n",
       "   ('procedure', 10),\n",
       "   ('phenotype', 10),\n",
       "   ('observation', 10),\n",
       "   ('x-ray', 10),\n",
       "   ('analyzed', 10),\n",
       "   ('second', 10),\n",
       "   ('standard', 10),\n",
       "   ('inhibition', 10),\n",
       "   ('authors', 10),\n",
       "   ('therapy', 10),\n",
       "   ('point', 10),\n",
       "   ('therefore,', 10),\n",
       "   ('thereby', 10),\n",
       "   ('challenge', 10),\n",
       "   ('public', 10),\n",
       "   ('rapid', 10),\n",
       "   ('electrostatic', 10),\n",
       "   ('learning', 10),\n",
       "   ('change', 10),\n",
       "   ('after', 10),\n",
       "   ('numbers', 10),\n",
       "   ('genetically', 10),\n",
       "   ('subclones', 10),\n",
       "   ('degree', 10),\n",
       "   ('shared', 10),\n",
       "   ('required', 10),\n",
       "   ('generally', 10),\n",
       "   ('expected', 10),\n",
       "   ('suppressor', 10),\n",
       "   ('fact', 10),\n",
       "   ('comparing', 10),\n",
       "   ('samples.', 10),\n",
       "   ('according', 10),\n",
       "   ('so', 10),\n",
       "   ('wide', 10),\n",
       "   ('mutations.', 10),\n",
       "   ('critical', 10),\n",
       "   ('subtypes', 10),\n",
       "   ('genotyping', 10),\n",
       "   ('resource', 9),\n",
       "   ('advances', 9),\n",
       "   ('obligate', 9),\n",
       "   ('inferred', 9),\n",
       "   ('coverage', 9),\n",
       "   ('series', 9),\n",
       "   ('matrix', 9),\n",
       "   ('center', 9),\n",
       "   ('processes,', 9),\n",
       "   ('targets.', 9),\n",
       "   ('findings', 9),\n",
       "   ('components', 9),\n",
       "   ('profile', 9),\n",
       "   ('efforts', 9),\n",
       "   ('estimates', 9),\n",
       "   ('characteristic', 9),\n",
       "   ('substantially', 9),\n",
       "   ('tree', 9),\n",
       "   ('template', 9),\n",
       "   ('cases.', 9),\n",
       "   ('expansion', 9),\n",
       "   ('modelling', 9),\n",
       "   ('successful', 9),\n",
       "   ('genome.', 9),\n",
       "   ('families,', 9),\n",
       "   ('along', 9),\n",
       "   ('putative', 9),\n",
       "   ('determining', 9),\n",
       "   ('automatic', 9),\n",
       "   ('simulation', 9),\n",
       "   ('folding', 9),\n",
       "   ('requires', 9),\n",
       "   ('signature', 9),\n",
       "   ('treatment', 9),\n",
       "   ('sufficient', 9),\n",
       "   ('enables', 9),\n",
       "   ('physical', 9),\n",
       "   ('initial', 9),\n",
       "   ('respectively.', 9),\n",
       "   ('analysis.', 9),\n",
       "   ('characterize', 9),\n",
       "   ('graphical', 9),\n",
       "   ('theoretical', 9),\n",
       "   ('obligatory', 9),\n",
       "   ('interpretation', 9),\n",
       "   ('medical', 9),\n",
       "   ('rather', 9),\n",
       "   ('pathways,', 9),\n",
       "   ('quality', 9),\n",
       "   ('mutations,', 9),\n",
       "   ('disordered', 9),\n",
       "   ('members', 9),\n",
       "   ('full', 9),\n",
       "   ('beyond', 9),\n",
       "   ('macromolecular', 9),\n",
       "   ('offers', 9),\n",
       "   ('together', 9),\n",
       "   ('outperforms', 9),\n",
       "   ('advanced', 9),\n",
       "   ('measurements', 9),\n",
       "   ('cancer-related', 9),\n",
       "   ('amplification', 9),\n",
       "   ('corresponding', 9),\n",
       "   ('instability', 9),\n",
       "   ('malignant', 9),\n",
       "   ('weak', 9),\n",
       "   ('non-obligate', 9),\n",
       "   ('variable', 9),\n",
       "   ('despite', 9),\n",
       "   ('distinguishing', 9),\n",
       "   ('maryland', 9),\n",
       "   ('tumorigenesis', 9),\n",
       "   ('characteristics', 9),\n",
       "   ('database,', 9),\n",
       "   ('systems.', 9),\n",
       "   ('contrast', 9),\n",
       "   ('size,', 9),\n",
       "   ('toward', 9),\n",
       "   ('generating', 9),\n",
       "   ('showing', 9),\n",
       "   ('before', 9),\n",
       "   ('comparable', 9),\n",
       "   ('community', 9),\n",
       "   ('way', 9),\n",
       "   ('emerging', 9),\n",
       "   ('mechanism', 9),\n",
       "   ('leads', 9),\n",
       "   ('monomeric', 9),\n",
       "   ('patch', 9),\n",
       "   ('nearly', 9),\n",
       "   ('times', 9),\n",
       "   ('tumors.', 9),\n",
       "   ('arrays', 9),\n",
       "   ('illustrate', 9),\n",
       "   ('packing', 9),\n",
       "   ('states', 9),\n",
       "   ('mapping', 9),\n",
       "   ('containing', 9),\n",
       "   ('indicates', 9),\n",
       "   ('intratumor', 9),\n",
       "   ('structure,', 9),\n",
       "   ('replication', 9),\n",
       "   ('measured', 9),\n",
       "   ('natural', 9),\n",
       "   ('studying', 8),\n",
       "   ('10', 8),\n",
       "   ('partners.', 8),\n",
       "   ('validation', 8),\n",
       "   ('presented', 8),\n",
       "   ('progress', 8),\n",
       "   ('modification', 8),\n",
       "   ('correct', 8),\n",
       "   ('computationally', 8),\n",
       "   ('population', 8),\n",
       "   ('technology', 8),\n",
       "   ('antibody', 8),\n",
       "   ('repetitive', 8),\n",
       "   ('occurred', 8),\n",
       "   ('integrates', 8),\n",
       "   ('years', 8),\n",
       "   ('{wt1}', 8),\n",
       "   ('easily', 8),\n",
       "   ('fundamental', 8),\n",
       "   ('conditions', 8),\n",
       "   ('stability', 8),\n",
       "   ('improve', 8),\n",
       "   ('epigenetic', 8),\n",
       "   ('activation', 8),\n",
       "   ('improved', 8),\n",
       "   ('pathways.', 8),\n",
       "   ('usually', 8),\n",
       "   ('location', 8),\n",
       "   ('bond', 8),\n",
       "   ('flexibility', 8),\n",
       "   ('model.', 8),\n",
       "   ('highlight', 8),\n",
       "   ('part', 8),\n",
       "   ('extended', 8),\n",
       "   ('applications', 8),\n",
       "   ('non-obligatory', 8),\n",
       "   ('suited', 8),\n",
       "   ('crucial', 8),\n",
       "   ('biologically', 8),\n",
       "   ('regulated', 8),\n",
       "   ('patients.', 8),\n",
       "   ('relationship', 8),\n",
       "   ('sampling', 8),\n",
       "   ('united', 8),\n",
       "   ('acids', 8),\n",
       "   ('(ii)', 8),\n",
       "   ('solvation', 8),\n",
       "   ('evolution.', 8),\n",
       "   ('sequence,', 8),\n",
       "   ('seen', 8),\n",
       "   ('interface,', 8),\n",
       "   ('out', 8),\n",
       "   ('yet', 8),\n",
       "   ('against', 8),\n",
       "   ('positive', 8),\n",
       "   ('increasingly', 8),\n",
       "   ('patterns,', 8),\n",
       "   ('quantification', 8),\n",
       "   ('circulating', 8),\n",
       "   ('tested', 8),\n",
       "   ('interface.', 8),\n",
       "   ('goal', 8),\n",
       "   ('preferences', 8),\n",
       "   ('arise', 8),\n",
       "   ('is,', 8),\n",
       "   ('datasets', 8),\n",
       "   ('benefit', 8),\n",
       "   ('caused', 8),\n",
       "   ('exhibit', 8),\n",
       "   ('good', 8),\n",
       "   ('parameter', 8),\n",
       "   ('microarrays', 8),\n",
       "   ('11', 8),\n",
       "   ('implicated', 8),\n",
       "   ('factors,', 8),\n",
       "   ...]),\n",
       " ('dacb9d5d7b6e1b8090aa2d4cf6542ea1',\n",
       "  [('the', 1443),\n",
       "   ('of', 1346),\n",
       "   ('and', 1133),\n",
       "   ('to', 733),\n",
       "   ('in', 684),\n",
       "   ('a', 549),\n",
       "   ('for', 367),\n",
       "   ('that', 256),\n",
       "   ('is', 252),\n",
       "   ('on', 192),\n",
       "   ('are', 184),\n",
       "   ('this', 179),\n",
       "   ('with', 173),\n",
       "   ('as', 171),\n",
       "   ('we', 169),\n",
       "   ('information', 159),\n",
       "   ('from', 148),\n",
       "   ('an', 129),\n",
       "   ('by', 112),\n",
       "   ('be', 108),\n",
       "   ('were', 106),\n",
       "   ('their', 102),\n",
       "   ('or', 99),\n",
       "   ('have', 99),\n",
       "   ('data', 97),\n",
       "   ('these', 96),\n",
       "   ('research', 80),\n",
       "   ('health', 79),\n",
       "   ('which', 78),\n",
       "   ('new', 71),\n",
       "   ('can', 68),\n",
       "   ('has', 68),\n",
       "   ('use', 66),\n",
       "   ('it', 63),\n",
       "   ('using', 61),\n",
       "   ('at', 61),\n",
       "   ('results', 60),\n",
       "   ('knowledge', 60),\n",
       "   ('medical', 59),\n",
       "   ('clinical', 59),\n",
       "   ('not', 57),\n",
       "   ('was', 57),\n",
       "   ('been', 56),\n",
       "   ('used', 55),\n",
       "   ('drug', 55),\n",
       "   ('web', 55),\n",
       "   ('between', 55),\n",
       "   ('search', 54),\n",
       "   ('more', 53),\n",
       "   ('about', 51),\n",
       "   ('other', 49),\n",
       "   ('study', 49),\n",
       "   ('all', 48),\n",
       "   ('biomedical', 48),\n",
       "   ('but', 47),\n",
       "   ('studies', 47),\n",
       "   ('based', 46),\n",
       "   ('they', 45),\n",
       "   ('such', 45),\n",
       "   ('most', 43),\n",
       "   ('its', 43),\n",
       "   ('analysis', 43),\n",
       "   ('semantic', 42),\n",
       "   ('review', 42),\n",
       "   ('number', 42),\n",
       "   ('articles', 41),\n",
       "   ('provide', 40),\n",
       "   ('our', 40),\n",
       "   ('each', 39),\n",
       "   ('also', 39),\n",
       "   ('tools', 38),\n",
       "   ('how', 38),\n",
       "   ('literature', 38),\n",
       "   ('available', 37),\n",
       "   ('journals', 36),\n",
       "   ('through', 36),\n",
       "   ('text', 36),\n",
       "   ('methods', 36),\n",
       "   ('both', 36),\n",
       "   ('scientific', 36),\n",
       "   ('will', 35),\n",
       "   ('systematic', 35),\n",
       "   ('different', 34),\n",
       "   ('one', 34),\n",
       "   ('learning', 33),\n",
       "   ('many', 33),\n",
       "   ('drugs', 33),\n",
       "   ('two', 33),\n",
       "   ('into', 32),\n",
       "   ('may', 31),\n",
       "   ('approach', 31),\n",
       "   ('evidence', 30),\n",
       "   ('journal', 30),\n",
       "   ('there', 30),\n",
       "   ('published', 30),\n",
       "   ('biological', 30),\n",
       "   ('electronic', 30),\n",
       "   ('than', 29),\n",
       "   ('identify', 29),\n",
       "   ('care', 29),\n",
       "   ('systems', 29),\n",
       "   ('', 29),\n",
       "   ('database', 28),\n",
       "   ('approaches', 28),\n",
       "   ('article', 28),\n",
       "   ('within', 28),\n",
       "   ('model', 27),\n",
       "   ('three', 26),\n",
       "   ('paper', 26),\n",
       "   ('impact', 26),\n",
       "   ('identified', 26),\n",
       "   ('performance', 26),\n",
       "   ('technology', 26),\n",
       "   ('had', 26),\n",
       "   ('reviews', 26),\n",
       "   ('quality', 26),\n",
       "   ('set', 26),\n",
       "   ('when', 26),\n",
       "   ('education', 26),\n",
       "   ('skills', 26),\n",
       "   ('network', 25),\n",
       "   ('training', 25),\n",
       "   ('students', 25),\n",
       "   ('system', 25),\n",
       "   ('only', 25),\n",
       "   ('bioinformatics', 25),\n",
       "   ('however,', 24),\n",
       "   ('databases', 24),\n",
       "   ('relevant', 24),\n",
       "   ('development', 24),\n",
       "   ('terms', 24),\n",
       "   ('human', 23),\n",
       "   ('chemical', 23),\n",
       "   ('some', 23),\n",
       "   ('practice', 23),\n",
       "   ('over', 23),\n",
       "   ('large', 23),\n",
       "   ('trials', 23),\n",
       "   ('questions', 23),\n",
       "   ('complex', 23),\n",
       "   ('assessment', 23),\n",
       "   ('science', 22),\n",
       "   ('interaction', 22),\n",
       "   ('well', 22),\n",
       "   ('those', 22),\n",
       "   ('discovery', 22),\n",
       "   ('developed', 22),\n",
       "   ('support', 22),\n",
       "   ('existing', 21),\n",
       "   ('should', 21),\n",
       "   ('found', 21),\n",
       "   ('including', 21),\n",
       "   ('literacy', 21),\n",
       "   ('structure', 21),\n",
       "   ('recent', 21),\n",
       "   ('=', 20),\n",
       "   ('molecular', 20),\n",
       "   ('better', 20),\n",
       "   ('related', 19),\n",
       "   ('need', 19),\n",
       "   ('relationships', 19),\n",
       "   ('specific', 19),\n",
       "   ('no', 19),\n",
       "   ('best', 19),\n",
       "   ('theory', 19),\n",
       "   ('strategies', 19),\n",
       "   ('present', 19),\n",
       "   ('reports', 19),\n",
       "   ('ontology', 18),\n",
       "   ('novel', 18),\n",
       "   ('potential', 18),\n",
       "   ('publication', 18),\n",
       "   ('content', 18),\n",
       "   ('what', 18),\n",
       "   ('computer', 18),\n",
       "   ('evaluation', 18),\n",
       "   ('social', 18),\n",
       "   ('application', 18),\n",
       "   ('several', 17),\n",
       "   ('process', 17),\n",
       "   ('offers', 17),\n",
       "   ('key', 17),\n",
       "   ('future', 17),\n",
       "   ('describe', 17),\n",
       "   ('compared', 17),\n",
       "   ('controlled', 17),\n",
       "   ('applications', 17),\n",
       "   ('often', 17),\n",
       "   ('provides', 17),\n",
       "   ('concepts', 17),\n",
       "   ('reporting', 17),\n",
       "   ('google', 17),\n",
       "   ('work', 17),\n",
       "   ('first', 17),\n",
       "   ('disease', 17),\n",
       "   ('field', 16),\n",
       "   ('biology', 16),\n",
       "   ('demonstrate', 16),\n",
       "   ('general', 16),\n",
       "   ('method', 16),\n",
       "   ('presents', 16),\n",
       "   ('free', 16),\n",
       "   ('access', 16),\n",
       "   ('patient', 16),\n",
       "   ('them', 16),\n",
       "   ('levels', 16),\n",
       "   ('known', 16),\n",
       "   ('across', 16),\n",
       "   ('time', 16),\n",
       "   ('show', 16),\n",
       "   ('being', 16),\n",
       "   ('then', 16),\n",
       "   ('online', 15),\n",
       "   ('student', 15),\n",
       "   ('further', 15),\n",
       "   ('significant', 15),\n",
       "   ('could', 15),\n",
       "   ('retrieval', 15),\n",
       "   ('factors', 15),\n",
       "   ('academic', 15),\n",
       "   ('useful', 15),\n",
       "   ('criteria', 15),\n",
       "   ('outcomes', 15),\n",
       "   ('digital', 15),\n",
       "   ('order', 15),\n",
       "   ('university', 15),\n",
       "   ('scholarly', 15),\n",
       "   ('randomized', 15),\n",
       "   ('citation', 15),\n",
       "   ('traditional', 15),\n",
       "   ('included', 15),\n",
       "   ('critical', 15),\n",
       "   ('evidence-based', 15),\n",
       "   ('{ebp}', 15),\n",
       "   ('higher', 14),\n",
       "   ('authors', 14),\n",
       "   ('citations', 14),\n",
       "   ('”', 14),\n",
       "   ('trial', 14),\n",
       "   ('medicine', 14),\n",
       "   ('survey', 14),\n",
       "   ('design', 14),\n",
       "   ('types', 14),\n",
       "   ('main', 14),\n",
       "   ('if', 14),\n",
       "   ('problems', 14),\n",
       "   ('ontologies', 14),\n",
       "   ('groups', 14),\n",
       "   ('performed', 14),\n",
       "   ('help', 14),\n",
       "   ('resources', 14),\n",
       "   ('increasingly', 14),\n",
       "   ('language', 14),\n",
       "   ('classification', 14),\n",
       "   ('made', 14),\n",
       "   ('sample', 14),\n",
       "   ('least', 14),\n",
       "   ('international', 14),\n",
       "   ('models', 14),\n",
       "   ('interest', 13),\n",
       "   ('growth', 13),\n",
       "   ('framework', 13),\n",
       "   ('include', 13),\n",
       "   ('who', 13),\n",
       "   ('associated', 13),\n",
       "   ('various', 13),\n",
       "   ('given', 13),\n",
       "   ('effective', 13),\n",
       "   ('{pubmed}', 13),\n",
       "   ('empirical', 13),\n",
       "   ('paper,', 13),\n",
       "   ('cochrane', 13),\n",
       "   ('current', 13),\n",
       "   ('them.', 13),\n",
       "   ('patterns', 13),\n",
       "   ('technologies', 13),\n",
       "   ('educational', 13),\n",
       "   ('evaluate', 13),\n",
       "   ('course', 13),\n",
       "   ('during', 12),\n",
       "   ('increase', 12),\n",
       "   ('teaching', 12),\n",
       "   ('items', 12),\n",
       "   ('gene', 12),\n",
       "   ('overall', 12),\n",
       "   ('{qa}', 12),\n",
       "   ('range', 12),\n",
       "   ('healthcare', 12),\n",
       "   ('analysis,', 12),\n",
       "   ('supporting', 12),\n",
       "   ('collaboration', 12),\n",
       "   ('multiple', 12),\n",
       "   ('facilitate', 12),\n",
       "   ('professional', 12),\n",
       "   ('out', 12),\n",
       "   ('while', 12),\n",
       "   ('understanding', 12),\n",
       "   ('much', 12),\n",
       "   ('scholar', 12),\n",
       "   ('(95\\\\%', 12),\n",
       "   ('conducted', 12),\n",
       "   ('own', 12),\n",
       "   ('researchers', 12),\n",
       "   ('techniques', 12),\n",
       "   ('type', 12),\n",
       "   ('whether', 12),\n",
       "   ('evaluating', 12),\n",
       "   ('confidence', 12),\n",
       "   ('program', 12),\n",
       "   ('validity', 12),\n",
       "   ('improve', 11),\n",
       "   ('past', 11),\n",
       "   ('presented', 11),\n",
       "   ('analyses', 11),\n",
       "   ('individual', 11),\n",
       "   ('question', 11),\n",
       "   ('pathway', 11),\n",
       "   ('good', 11),\n",
       "   ('high', 11),\n",
       "   ('library', 11),\n",
       "   ('2.0', 11),\n",
       "   ('components', 11),\n",
       "   ('communication', 11),\n",
       "   ('ease', 11),\n",
       "   ('strategy', 11),\n",
       "   ('variety', 11),\n",
       "   ('any', 11),\n",
       "   ('libraries', 11),\n",
       "   ('subject', 11),\n",
       "   ('making', 11),\n",
       "   ('therapeutic', 11),\n",
       "   ('resource', 11),\n",
       "   ('interactions', 11),\n",
       "   ('case', 11),\n",
       "   ('find', 11),\n",
       "   ('role', 11),\n",
       "   ('small', 11),\n",
       "   ('required', 11),\n",
       "   ('pathways', 10),\n",
       "   ('become', 10),\n",
       "   ('years', 10),\n",
       "   ('possible', 10),\n",
       "   ('effect', 10),\n",
       "   ('suggest', 10),\n",
       "   ('genes,', 10),\n",
       "   ('principles', 10),\n",
       "   ('focus', 10),\n",
       "   ('research.', 10),\n",
       "   ('research,', 10),\n",
       "   ('examples', 10),\n",
       "   ('networks', 10),\n",
       "   ('make', 10),\n",
       "   ('issues', 10),\n",
       "   ('open', 10),\n",
       "   ('would', 10),\n",
       "   ('after', 10),\n",
       "   ('identifying', 10),\n",
       "   ('perceived', 10),\n",
       "   ('evaluated', 10),\n",
       "   ('guidance', 10),\n",
       "   ('sources', 10),\n",
       "   ('users', 10),\n",
       "   ('\\\\^{a}\\x96º', 10),\n",
       "   ('combined', 10),\n",
       "   ('diseases', 10),\n",
       "   ('test', 10),\n",
       "   ('decision', 10),\n",
       "   ('domain', 10),\n",
       "   ('residents', 10),\n",
       "   ('tool', 10),\n",
       "   ('life', 10),\n",
       "   ('direct', 10),\n",
       "   ('cell', 10),\n",
       "   ('among', 10),\n",
       "   ('important', 10),\n",
       "   ('lectures', 10),\n",
       "   ('standard', 10),\n",
       "   ('integration', 9),\n",
       "   ('thus', 9),\n",
       "   ('representation', 9),\n",
       "   ('searches', 9),\n",
       "   ('tested', 9),\n",
       "   ('considered', 9),\n",
       "   ('rapid', 9),\n",
       "   ('rate', 9),\n",
       "   ('challenges', 9),\n",
       "   ('developing', 9),\n",
       "   ('user', 9),\n",
       "   ('ability', 9),\n",
       "   ('visualization', 9),\n",
       "   ('{oa}', 9),\n",
       "   ('list', 9),\n",
       "   ('determine', 9),\n",
       "   ('recommendations', 9),\n",
       "   ('little', 9),\n",
       "   ('videos', 9),\n",
       "   ('part', 9),\n",
       "   ('simulation', 9),\n",
       "   ('aspects', 9),\n",
       "   ('sciences', 9),\n",
       "   ('understand', 9),\n",
       "   ('low', 9),\n",
       "   ('areas', 9),\n",
       "   ('group', 9),\n",
       "   ('expression', 9),\n",
       "   ('methodological', 9),\n",
       "   ('utility', 9),\n",
       "   ('taken', 9),\n",
       "   ('held', 9),\n",
       "   ('effects', 9),\n",
       "   ('experimental', 9),\n",
       "   ('searching', 9),\n",
       "   ('journals,', 9),\n",
       "   ('statistical', 9),\n",
       "   ('experiments', 9),\n",
       "   ('do', 9),\n",
       "   ('mining', 9),\n",
       "   ('public', 9),\n",
       "   ('genes', 9),\n",
       "   ('attributes', 9),\n",
       "   ('ways', 9),\n",
       "   ('explore', 9),\n",
       "   ('science,', 9),\n",
       "   ('provided', 9),\n",
       "   ('wide', 9),\n",
       "   ('having', 9),\n",
       "   ('95\\\\%', 9),\n",
       "   ('created', 8),\n",
       "   ('generation', 8),\n",
       "   ('patients', 8),\n",
       "   ('competence', 8),\n",
       "   ('standards', 8),\n",
       "   ('showed', 8),\n",
       "   ('yet', 8),\n",
       "   ('implications', 8),\n",
       "   ('(e.g.,', 8),\n",
       "   ('professionals', 8),\n",
       "   ('particular', 8),\n",
       "   ('extract', 8),\n",
       "   ('measure', 8),\n",
       "   ('identification', 8),\n",
       "   ('usefulness', 8),\n",
       "   ('together', 8),\n",
       "   ('efficient', 8),\n",
       "   ('literature.', 8),\n",
       "   ('propose', 8),\n",
       "   ('forms', 8),\n",
       "   ('you', 8),\n",
       "   ('still', 8),\n",
       "   ('{powerpoint}', 8),\n",
       "   ('questions.', 8),\n",
       "   ('mappings', 8),\n",
       "   ('very', 8),\n",
       "   ('skills,', 8),\n",
       "   ('four', 8),\n",
       "   ('education.', 8),\n",
       "   ('methods,', 8),\n",
       "   ('limited', 8),\n",
       "   ('proposed', 8),\n",
       "   ('publications', 8),\n",
       "   ('larger', 8),\n",
       "   ('sets', 8),\n",
       "   ('{his}', 8),\n",
       "   ('increasing', 8),\n",
       "   ('difficult', 8),\n",
       "   ('original', 8),\n",
       "   ('article,', 8),\n",
       "   ('described', 8),\n",
       "   ('drugs,', 8),\n",
       "   ('growing', 8),\n",
       "   ('{ddx}', 8),\n",
       "   ('features', 8),\n",
       "   ('currently', 8),\n",
       "   ('targets', 8),\n",
       "   ('comparison', 8),\n",
       "   ('evolution', 8),\n",
       "   ('statistically', 8),\n",
       "   ('recommended', 8),\n",
       "   ('dictionary', 8),\n",
       "   ('advances', 8),\n",
       "   ('theoretical', 8),\n",
       "   ('investigate', 8),\n",
       "   ('common', 8),\n",
       "   ('must', 8),\n",
       "   ('statement', 8),\n",
       "   ('indicate', 8),\n",
       "   ('needed', 8),\n",
       "   ('scientists', 8),\n",
       "   ('lack', 8),\n",
       "   ('learning.', 8),\n",
       "   ('{hts}', 8),\n",
       "   ('extent', 8),\n",
       "   ('produce', 8),\n",
       "   ('-', 8),\n",
       "   ('computational', 8),\n",
       "   ('qualitative', 8),\n",
       "   ('{il}', 8),\n",
       "   ('context', 8),\n",
       "   ('large-scale', 8),\n",
       "   ('medicine,', 8),\n",
       "   ('participants', 8),\n",
       "   ('instruments', 8),\n",
       "   ('relations', 8),\n",
       "   ('service', 7),\n",
       "   ('prediction', 7),\n",
       "   ('form', 7),\n",
       "   ('area', 7),\n",
       "   ('opportunities', 7),\n",
       "   ('essential', 7),\n",
       "   ('organisations', 7),\n",
       "   ('goal', 7),\n",
       "   ('needs', 7),\n",
       "   ('where', 7),\n",
       "   ('abstracts', 7),\n",
       "   ('likely', 7),\n",
       "   ('value', 7),\n",
       "   ('resources.', 7),\n",
       "   ('evidence.', 7),\n",
       "   ('directions', 7),\n",
       "   ('means', 7),\n",
       "   ('basis', 7),\n",
       "   ('similar', 7),\n",
       "   ('reported', 7),\n",
       "   ('learners', 7),\n",
       "   ('trials.', 7),\n",
       "   ('2002),', 7),\n",
       "   ('either', 7),\n",
       "   ('repositories', 7),\n",
       "   ('address', 7),\n",
       "   ('education,', 7),\n",
       "   ('materials', 7),\n",
       "   ('bibliographic', 7),\n",
       "   ('few', 7),\n",
       "   ('checklist', 7),\n",
       "   ('explain', 7),\n",
       "   ('aim', 7),\n",
       "   ('level', 7),\n",
       "   ('conclusions', 7),\n",
       "   ('here', 7),\n",
       "   ('conference', 7),\n",
       "   ('methodology', 7),\n",
       "   ('concept', 7),\n",
       "   ('3', 7),\n",
       "   ('knowledge,', 7),\n",
       "   ('{sbme}', 7),\n",
       "   ('department', 7),\n",
       "   ('entities', 7),\n",
       "   ('characteristics', 7),\n",
       "   ('institutional', 7),\n",
       "   ('simple', 7),\n",
       "   ('central', 7),\n",
       "   ('scores', 7),\n",
       "   ('assess', 7),\n",
       "   ('providing', 7),\n",
       "   ('developers', 7),\n",
       "   ('amount', 7),\n",
       "   ('extraction', 7),\n",
       "   ('acceptance', 7),\n",
       "   ('extracted', 7),\n",
       "   ('fields', 7),\n",
       "   ('sizes', 7),\n",
       "   ('includes', 7),\n",
       "   ('under', 7),\n",
       "   ('enhance', 7),\n",
       "   ('report', 7),\n",
       "   ('practical', 7),\n",
       "   ('relationship', 7),\n",
       "   ('model.', 7),\n",
       "   ('indexed', 7),\n",
       "   ('efficacy', 7),\n",
       "   ('lead', 7),\n",
       "   ('offer', 7),\n",
       "   ('cognitive', 7),\n",
       "   ('{if}', 7),\n",
       "   ('information,', 7),\n",
       "   ('requires', 7),\n",
       "   ('precision', 7),\n",
       "   ('target', 7),\n",
       "   ('{snomed}', 7),\n",
       "   ('perform', 7),\n",
       "   ('although', 7),\n",
       "   ('{consort}', 7),\n",
       "   ('particularly', 7),\n",
       "   ('here,', 7),\n",
       "   (\"students'\", 7),\n",
       "   ('full', 7),\n",
       "   ('way', 7),\n",
       "   ('global', 7),\n",
       "   ('selected', 7),\n",
       "   ('even', 7),\n",
       "   ('care,', 7),\n",
       "   ('influence', 7),\n",
       "   ('metadata', 7),\n",
       "   ('guideline', 7),\n",
       "   ('(n', 7),\n",
       "   ('activity', 7),\n",
       "   ('problem', 7),\n",
       "   ('same', 7),\n",
       "   ('assessing', 7),\n",
       "   ('implementation', 7),\n",
       "   ('emerging', 7),\n",
       "   ('commonly', 7),\n",
       "   ('total', 7),\n",
       "   ('cited', 7),\n",
       "   ('rank', 7),\n",
       "   ('process,', 7),\n",
       "   ('suggests', 7),\n",
       "   ('five', 7),\n",
       "   ('highly', 7),\n",
       "   ('reasoning', 7),\n",
       "   ('discuss', 7),\n",
       "   ('services', 7),\n",
       "   ('informatics', 7),\n",
       "   ('instructional', 7),\n",
       "   ('create', 7),\n",
       "   ('answer', 7),\n",
       "   ('attention', 7),\n",
       "   ('{ci}', 7),\n",
       "   ('secondary', 7),\n",
       "   ('{ci},', 7),\n",
       "   ('prestige', 7),\n",
       "   ('inclusion', 6),\n",
       "   ('papers', 6),\n",
       "   ('active', 6),\n",
       "   ('underlying', 6),\n",
       "   ('use.', 6),\n",
       "   ('management', 6),\n",
       "   ('countries', 6),\n",
       "   ('action', 6),\n",
       "   ('reviewed', 6),\n",
       "   ('consider', 6),\n",
       "   ('enable', 6),\n",
       "   ('defined', 6),\n",
       "   ('web-based', 6),\n",
       "   ('systems.', 6),\n",
       "   ('librarians', 6),\n",
       "   ('major', 6),\n",
       "   ('comprehensive', 6),\n",
       "   ('linked', 6),\n",
       "   ('approved', 6),\n",
       "   ('algorithm', 6),\n",
       "   ('innovations', 6),\n",
       "   ('changes', 6),\n",
       "   ('historical', 6),\n",
       "   ('frequently', 6),\n",
       "   ('extensive', 6),\n",
       "   ('examine', 6),\n",
       "   ('collaborative', 6),\n",
       "   ('selection', 6),\n",
       "   ('associations', 6),\n",
       "   ('addition,', 6),\n",
       "   ('virtual', 6),\n",
       "   ('distinct', 6),\n",
       "   ('instruction', 6),\n",
       "   ('affect', 6),\n",
       "   ('experience', 6),\n",
       "   ('utilized', 6),\n",
       "   ('{psi}', 6),\n",
       "   ('protein', 6),\n",
       "   ('multidimensional', 6),\n",
       "   ('categories', 6),\n",
       "   ('{smpdb}', 6),\n",
       "   ('freely', 6),\n",
       "   ('focused', 6),\n",
       "   ('domains', 6),\n",
       "   ('demonstrated', 6),\n",
       "   ('term', 6),\n",
       "   ('adoption', 6),\n",
       "   ('documents', 6),\n",
       "   ('diseases.', 6),\n",
       "   ('subscription', 6),\n",
       "   ('scopus', 6),\n",
       "   ('rather', 6),\n",
       "   ('benefit', 6),\n",
       "   ('studies,', 6),\n",
       "   ('steps', 6),\n",
       "   ('{medline}', 6),\n",
       "   ('lower', 6),\n",
       "   ('internet', 6),\n",
       "   ('designed', 6),\n",
       "   ('uses', 6),\n",
       "   ('structural', 6),\n",
       "   ('curriculum', 6),\n",
       "   ('category', 6),\n",
       "   ('update', 6),\n",
       "   ('y', 6),\n",
       "   ('class', 6),\n",
       "   ('coded', 6),\n",
       "   ('trends', 6),\n",
       "   ('example', 6),\n",
       "   ('average', 6),\n",
       "   ('because', 6),\n",
       "   ('definitions', 6),\n",
       "   ('observed', 6),\n",
       "   ('databases,', 6),\n",
       "   ('objective', 6),\n",
       "   ('discussed', 6),\n",
       "   ('previous', 6),\n",
       "   ('without', 6),\n",
       "   ('safety', 6),\n",
       "   ('screening', 6),\n",
       "   ('explained', 6),\n",
       "   ('top', 6),\n",
       "   ('document', 6),\n",
       "   ('less', 6),\n",
       "   ('measures', 6),\n",
       "   ('records', 6),\n",
       "   ('integrated', 6),\n",
       "   ('processes', 6),\n",
       "   ('point', 6),\n",
       "   ('information.', 6),\n",
       "   ('predictive', 6),\n",
       "   ('review.', 6),\n",
       "   ('benefits', 6),\n",
       "   ('significantly', 6),\n",
       "   ('findings', 6),\n",
       "   ('presentations', 6),\n",
       "   ('diagnostic', 6),\n",
       "   ('world', 6),\n",
       "   ('primary', 6),\n",
       "   ('relating', 6),\n",
       "   ('themselves', 6),\n",
       "   ('structures', 6),\n",
       "   ('becoming', 6),\n",
       "   ('development.', 6),\n",
       "   ('collection', 6),\n",
       "   ('task', 6),\n",
       "   ('molecules', 6),\n",
       "   ('natural', 6),\n",
       "   ('testing', 6),\n",
       "   ('drugs.', 6),\n",
       "   ('called', 6),\n",
       "   ('argue', 6),\n",
       "   ('purpose', 6),\n",
       "   ('compounds', 6),\n",
       "   ('publishing', 6),\n",
       "   ('prior', 6),\n",
       "   ('processing', 6),\n",
       "   ('develop', 6),\n",
       "   ('describing', 6),\n",
       "   ('shows', 6),\n",
       "   ('involved', 6),\n",
       "   ('retrieved', 6),\n",
       "   ('pharmacology', 6),\n",
       "   ('2', 6),\n",
       "   ('examines', 6),\n",
       "   ('sense', 6),\n",
       "   ('{youtube}', 6),\n",
       "   ('pivotal', 6),\n",
       "   ('model,', 6),\n",
       "   ('go', 5),\n",
       "   ('years,', 5),\n",
       "   ('conceptual', 5),\n",
       "   ('integrate', 5),\n",
       "   ('result', 5),\n",
       "   ('retrieval,', 5),\n",
       "   ('business', 5),\n",
       "   ('agreement', 5),\n",
       "   ('compare', 5),\n",
       "   ('generated', 5),\n",
       "   ('supports', 5),\n",
       "   ('reference', 5),\n",
       "   ('combinations', 5),\n",
       "   ('response', 5),\n",
       "   ('sciences,', 5),\n",
       "   ('explicitly', 5),\n",
       "   ('usa.', 5),\n",
       "   ('change', 5),\n",
       "   ('applying', 5),\n",
       "   ('behaviors', 5),\n",
       "   ('broad', 5),\n",
       "   ('datasets', 5),\n",
       "   ('manual', 5),\n",
       "   ('consistently', 5),\n",
       "   ('database.', 5),\n",
       "   ('work,', 5),\n",
       "   ('ranking', 5),\n",
       "   ('positive', 5),\n",
       "   ('handsearching', 5),\n",
       "   ('entry', 5),\n",
       "   ('linguistic', 5),\n",
       "   ('popular', 5),\n",
       "   ('acquisition', 5),\n",
       "   ('successful', 5),\n",
       "   ('practice.', 5),\n",
       "   ('curricula', 5),\n",
       "   ('achieved', 5),\n",
       "   ('scale', 5),\n",
       "   ('inform', 5),\n",
       "   ('age', 5),\n",
       "   ('per', 5),\n",
       "   ('and,', 5),\n",
       "   ('indicating', 5),\n",
       "   ('valuable', 5),\n",
       "   ('care.', 5),\n",
       "   ('diseases,', 5),\n",
       "   ('optimal', 5),\n",
       "   ('aims', 5),\n",
       "   ('base', 5),\n",
       "   ('integrating', 5),\n",
       "   ('data,', 5),\n",
       "   ('highlight', 5),\n",
       "   ('serves', 5),\n",
       "   ('values', 5),\n",
       "   ('people', 5),\n",
       "   ('improving', 5),\n",
       "   ('summary', 5),\n",
       "   ('2009', 5),\n",
       "   ('nature', 5),\n",
       "   ('recorded', 5),\n",
       "   ('map', 5),\n",
       "   ('questions,', 5),\n",
       "   ('collected', 5),\n",
       "   ('vast', 5),\n",
       "   ('system.', 5),\n",
       "   ('recall', 5),\n",
       "   (\"{ct}'s\", 5),\n",
       "   ('query', 5),\n",
       "   ('diverse', 5),\n",
       "   ('system,', 5),\n",
       "   ('interventions', 5),\n",
       "   ('effectively', 5),\n",
       "   ('readers', 5),\n",
       "   ('relation', 5),\n",
       "   ('site', 5),\n",
       "   ('guidelines', 5),\n",
       "   ('algorithms', 5),\n",
       "   ('expert', 5),\n",
       "   ('similarity', 5),\n",
       "   ('guide', 5),\n",
       "   ('guided', 5),\n",
       "   ('behavior', 5),\n",
       "   ('established', 5),\n",
       "   ('amounts', 5),\n",
       "   ('pairwise', 5),\n",
       "   ('compound', 5),\n",
       "   ('pathways,', 5),\n",
       "   ('act', 5),\n",
       "   ('results.', 5),\n",
       "   ('whereas', 5),\n",
       "   ('signaling', 5),\n",
       "   ('against', 5),\n",
       "   ('chapter', 5),\n",
       "   ('products', 5),\n",
       "   ('component', 5),\n",
       "   ('real', 5),\n",
       "   ('{copub}', 5),\n",
       "   ('debate', 5),\n",
       "   ('classified', 5),\n",
       "   ('medicine.', 5),\n",
       "   ('early', 5),\n",
       "   ('final', 5),\n",
       "   ('structured', 5),\n",
       "   ('relatively', 5),\n",
       "   ('lists', 5),\n",
       "   ('appropriate', 5),\n",
       "   ('difference', 5),\n",
       "   ('require', 5),\n",
       "   ('subset', 5),\n",
       "   ('({or}', 5),\n",
       "   ('national', 5),\n",
       "   ('despite', 5),\n",
       "   (\"patients'\", 5),\n",
       "   ('establish', 5),\n",
       "   ('keyword', 5),\n",
       "   ('able', 5),\n",
       "   ('initial', 5),\n",
       "   ('cases', 5),\n",
       "   ('risk', 5),\n",
       "   ('heart', 5),\n",
       "   ('series', 5),\n",
       "   ('allow', 5),\n",
       "   ('additional', 5),\n",
       "   ('1', 5),\n",
       "   ('like', 5),\n",
       "   ('success', 5),\n",
       "   ('led', 5),\n",
       "   ('suggested', 5),\n",
       "   ('molecule', 5),\n",
       "   ('now', 5),\n",
       "   ('automated', 5),\n",
       "   ('e-learning', 5),\n",
       "   ('offered', 5),\n",
       "   ('school', 5),\n",
       "   ('typical', 5),\n",
       "   ('claims', 5),\n",
       "   ('{fda}', 5),\n",
       "   ('combination', 5),\n",
       "   ('{kegg}', 5),\n",
       "   ('extended', 5),\n",
       "   ('mechanism', 5),\n",
       "   ('properties', 5),\n",
       "   ('makes', 5),\n",
       "   ('theories', 5),\n",
       "   ('generally', 5),\n",
       "   ('intended', 5),\n",
       "   ('journals.', 5),\n",
       "   ('respect', 5),\n",
       "   ('personal', 5),\n",
       "   ('might', 5),\n",
       "   ('search.', 5),\n",
       "   ('random', 5),\n",
       "   ('status', 5),\n",
       "   ('articles,', 5),\n",
       "   ('apply', 5),\n",
       "   ('drug-drug', 5),\n",
       "   ('topics', 5),\n",
       "   ('multidisciplinary', 5),\n",
       "   ('question.', 5),\n",
       "   ('production', 4),\n",
       "   ('genetic', 4),\n",
       "   ('in-depth', 4),\n",
       "   ('candidates', 4),\n",
       "   ('following', 4),\n",
       "   ('proliferation', 4),\n",
       "   ('mechanisms', 4),\n",
       "   ('discipline', 4),\n",
       "   ('(the', 4),\n",
       "   ('weighted', 4),\n",
       "   ('differences', 4),\n",
       "   ('study,', 4),\n",
       "   ('studying', 4),\n",
       "   ('towards', 4),\n",
       "   ('exists', 4),\n",
       "   ('scope', 4),\n",
       "   ('applications,', 4),\n",
       "   ('interpretation', 4),\n",
       "   ('encode', 4),\n",
       "   (\"clinicians'\", 4),\n",
       "   ('cause', 4),\n",
       "   ('({ebp})', 4),\n",
       "   ('availability', 4),\n",
       "   ('financial', 4),\n",
       "   ('barriers', 4),\n",
       "   ('together,', 4),\n",
       "   ('tags', 4),\n",
       "   ('methods.', 4),\n",
       "   ('improvement', 4),\n",
       "   ('metabolic', 4),\n",
       "   ('generating', 4),\n",
       "   ('introduction', 4),\n",
       "   ('fields,', 4),\n",
       "   ('educators', 4),\n",
       "   ('examined', 4),\n",
       "   ('maps', 4),\n",
       "   ('names,', 4),\n",
       "   ('sophisticated', 4),\n",
       "   ('artificial', 4),\n",
       "   ('correct', 4),\n",
       "   ('names', 4),\n",
       "   ('captures', 4),\n",
       "   ('coding', 4),\n",
       "   ('working', 4),\n",
       "   ('{ehealth}', 4),\n",
       "   ('{srs}', 4),\n",
       "   ('parametric', 4),\n",
       "   ('association', 4),\n",
       "   ('blog', 4),\n",
       "   ('allows', 4),\n",
       "   ('blogs', 4),\n",
       "   ('trend', 4),\n",
       "   ('facilitating', 4),\n",
       "   ('universities', 4),\n",
       "   ('aimed', 4),\n",
       "   ('contained', 4),\n",
       "   ('represents', 4),\n",
       "   ('corresponding', 4),\n",
       "   ('history', 4),\n",
       "   ('sought', 4),\n",
       "   ('scored', 4),\n",
       "   ('function', 4),\n",
       "   ('fully', 4),\n",
       "   ('detailed', 4),\n",
       "   ('paradigm', 4),\n",
       "   ('importance', 4),\n",
       "   ('outcome', 4),\n",
       "   ('greatly', 4),\n",
       "   ('actions', 4),\n",
       "   ('discusses', 4),\n",
       "   ('tagging', 4),\n",
       "   ('{la}', 4),\n",
       "   ('numerous', 4),\n",
       "   ('promising', 4),\n",
       "   ('statistics', 4),\n",
       "   ('5', 4),\n",
       "   ('bias', 4),\n",
       "   ('complexity', 4),\n",
       "   ('organization', 4),\n",
       "   ('meta-narrative', 4),\n",
       "   ('synthesis', 4),\n",
       "   ('certification', 4),\n",
       "   (\"{ksh97}-p's\", 4),\n",
       "   ('proteins', 4),\n",
       "   ('derived', 4),\n",
       "   ('via', 4),\n",
       "   ('20', 4),\n",
       "   ('mean', 4),\n",
       "   ('quantitative', 4),\n",
       "   ('literacy,', 4),\n",
       "   ('conducting', 4),\n",
       "   ...]),\n",
       " ('6dd817ab0c2f730fc206faf5f5847bbb',\n",
       "  [('the', 3244),\n",
       "   ('of', 2407),\n",
       "   ('and', 1966),\n",
       "   ('to', 1570),\n",
       "   ('a', 1443),\n",
       "   ('in', 1289),\n",
       "   ('for', 804),\n",
       "   ('search', 788),\n",
       "   ('that', 752),\n",
       "   ('we', 700),\n",
       "   ('information', 698),\n",
       "   ('is', 579),\n",
       "   ('on', 564),\n",
       "   ('this', 476),\n",
       "   ('are', 445),\n",
       "   ('user', 401),\n",
       "   ('with', 397),\n",
       "   ('as', 390),\n",
       "   ('web', 374),\n",
       "   ('from', 345),\n",
       "   ('by', 309),\n",
       "   ('an', 301),\n",
       "   ('retrieval', 287),\n",
       "   ('query', 264),\n",
       "   ('can', 257),\n",
       "   ('be', 254),\n",
       "   ('results', 249),\n",
       "   ('context', 249),\n",
       "   ('our', 224),\n",
       "   ('users', 219),\n",
       "   ('using', 209),\n",
       "   ('their', 191),\n",
       "   ('queries', 181),\n",
       "   ('relevance', 174),\n",
       "   ('these', 174),\n",
       "   ('have', 161),\n",
       "   ('study', 160),\n",
       "   ('it', 159),\n",
       "   ('based', 158),\n",
       "   ('which', 157),\n",
       "   ('or', 157),\n",
       "   ('data', 153),\n",
       "   ('used', 153),\n",
       "   ('different', 153),\n",
       "   ('health', 150),\n",
       "   ('more', 149),\n",
       "   ('has', 145),\n",
       "   ('not', 138),\n",
       "   ('use', 136),\n",
       "   ('how', 136),\n",
       "   ('research', 134),\n",
       "   ('such', 133),\n",
       "   ('system', 131),\n",
       "   ('were', 130),\n",
       "   ('model', 128),\n",
       "   ('show', 123),\n",
       "   ('systems', 115),\n",
       "   ('paper', 113),\n",
       "   ('evaluation', 113),\n",
       "   ('task', 112),\n",
       "   ('two', 110),\n",
       "   ('at', 108),\n",
       "   ('was', 105),\n",
       "   ('they', 104),\n",
       "   ('been', 103),\n",
       "   ('between', 102),\n",
       "   (\"user's\", 102),\n",
       "   ('behavior', 99),\n",
       "   ('approach', 99),\n",
       "   ('also', 98),\n",
       "   ('when', 95),\n",
       "   ('analysis', 94),\n",
       "   ('terms', 94),\n",
       "   ('new', 93),\n",
       "   ('provide', 91),\n",
       "   ('', 91),\n",
       "   ('than', 89),\n",
       "   ('most', 88),\n",
       "   ('medical', 87),\n",
       "   ('contextual', 87),\n",
       "   ('present', 86),\n",
       "   ('relevant', 85),\n",
       "   ('about', 84),\n",
       "   ('feedback', 84),\n",
       "   ('engine', 84),\n",
       "   ('each', 82),\n",
       "   ('methods', 81),\n",
       "   ('other', 79),\n",
       "   ('documents', 78),\n",
       "   ('into', 77),\n",
       "   ('improve', 75),\n",
       "   ('three', 73),\n",
       "   ('work', 73),\n",
       "   ('both', 72),\n",
       "   ('one', 72),\n",
       "   ('but', 71),\n",
       "   ('document', 71),\n",
       "   ('may', 70),\n",
       "   ('techniques', 70),\n",
       "   ('engines', 69),\n",
       "   ('some', 68),\n",
       "   ('propose', 68),\n",
       "   ('number', 68),\n",
       "   ('effective', 68),\n",
       "   ('performance', 68),\n",
       "   ('models', 66),\n",
       "   ('paper,', 66),\n",
       "   ('result', 66),\n",
       "   ('many', 62),\n",
       "   ('tasks', 62),\n",
       "   ('{ir}', 62),\n",
       "   ('all', 62),\n",
       "   ('however,', 62),\n",
       "   ('its', 61),\n",
       "   ('ranking', 61),\n",
       "   ('searching', 60),\n",
       "   ('design', 59),\n",
       "   ('current', 59),\n",
       "   ('time', 59),\n",
       "   ('important', 59),\n",
       "   ('quality', 59),\n",
       "   ('through', 59),\n",
       "   ('method', 58),\n",
       "   ('better', 58),\n",
       "   ('online', 57),\n",
       "   ('over', 57),\n",
       "   ('personalized', 57),\n",
       "   ('implicit', 57),\n",
       "   ('framework', 56),\n",
       "   ('interaction', 56),\n",
       "   ('effectiveness', 55),\n",
       "   ('context-aware', 55),\n",
       "   ('seeking', 54),\n",
       "   ('well', 54),\n",
       "   ('there', 54),\n",
       "   ('social', 53),\n",
       "   (\"users'\", 53),\n",
       "   ('while', 53),\n",
       "   ('only', 52),\n",
       "   ('set', 52),\n",
       "   ('semantic', 52),\n",
       "   ('knowledge', 52),\n",
       "   ('previous', 51),\n",
       "   ('test', 51),\n",
       "   ('provides', 51),\n",
       "   ('related', 51),\n",
       "   ('term', 51),\n",
       "   ('studies', 50),\n",
       "   ('what', 50),\n",
       "   ('general', 49),\n",
       "   ('measures', 48),\n",
       "   ('people', 48),\n",
       "   ('types', 48),\n",
       "   ('within', 48),\n",
       "   ('information.', 47),\n",
       "   ('experiments', 47),\n",
       "   ('logs', 47),\n",
       "   ('content', 46),\n",
       "   ('very', 46),\n",
       "   ('identify', 45),\n",
       "   ('evaluate', 45),\n",
       "   ('specific', 45),\n",
       "   ('experimental', 45),\n",
       "   ('large', 45),\n",
       "   ('find', 45),\n",
       "   ('significant', 44),\n",
       "   ('individual', 44),\n",
       "   ('where', 44),\n",
       "   ('algorithms', 44),\n",
       "   ('questions', 44),\n",
       "   ('approaches', 44),\n",
       "   ('text', 44),\n",
       "   ('do', 43),\n",
       "   ('several', 43),\n",
       "   ('preferences', 43),\n",
       "   ('applications', 42),\n",
       "   ('proposed', 42),\n",
       "   ('domain', 42),\n",
       "   ('interactive', 42),\n",
       "   ('browsing', 42),\n",
       "   ('learning', 41),\n",
       "   ('describe', 41),\n",
       "   ('features', 41),\n",
       "   ('findings', 41),\n",
       "   ('help', 41),\n",
       "   ('further', 40),\n",
       "   ('internet', 40),\n",
       "   ('language', 40),\n",
       "   ('pages', 40),\n",
       "   ('found', 40),\n",
       "   ('clinical', 40),\n",
       "   ('should', 40),\n",
       "   ('will', 40),\n",
       "   ('then', 39),\n",
       "   ('traditional', 39),\n",
       "   ('personalization', 39),\n",
       "   ('existing', 38),\n",
       "   ('expansion', 38),\n",
       "   ('often', 38),\n",
       "   ('support', 37),\n",
       "   ('factors', 37),\n",
       "   ('understanding', 37),\n",
       "   ('access', 37),\n",
       "   ('strategies', 37),\n",
       "   ('real', 37),\n",
       "   ('first', 37),\n",
       "   ('human', 37),\n",
       "   ('given', 37),\n",
       "   ('including', 37),\n",
       "   ('needs', 37),\n",
       "   ('demonstrate', 37),\n",
       "   ('significantly', 36),\n",
       "   ('need', 36),\n",
       "   ('queries.', 36),\n",
       "   ('sources', 36),\n",
       "   ('query.', 35),\n",
       "   ('information,', 35),\n",
       "   ('topic', 34),\n",
       "   ('article', 34),\n",
       "   ('relationship', 34),\n",
       "   ('log', 34),\n",
       "   ('problem', 34),\n",
       "   ('order', 34),\n",
       "   ('useful', 33),\n",
       "   ('algorithm', 33),\n",
       "   ('automatically', 33),\n",
       "   ('review', 33),\n",
       "   ('page', 33),\n",
       "   ('type', 33),\n",
       "   ('concepts', 33),\n",
       "   ('same', 33),\n",
       "   ('presents', 32),\n",
       "   ('multiple', 32),\n",
       "   ('after', 32),\n",
       "   ('associated', 32),\n",
       "   ('queries,', 32),\n",
       "   ('part', 32),\n",
       "   ('automatic', 32),\n",
       "   ('modeling', 32),\n",
       "   ('novel', 32),\n",
       "   ('interest', 32),\n",
       "   ('{trec}', 32),\n",
       "   ('developed', 31),\n",
       "   ('recommender', 31),\n",
       "   ('{pubmed}', 31),\n",
       "   ('aspects', 31),\n",
       "   ('systems.', 31),\n",
       "   ('develop', 31),\n",
       "   ('future', 31),\n",
       "   ('click', 31),\n",
       "   ('best', 31),\n",
       "   ('subjects', 31),\n",
       "   ('high', 30),\n",
       "   ('”', 30),\n",
       "   ('retrieved', 30),\n",
       "   ('collaborative', 30),\n",
       "   ('wide', 30),\n",
       "   ('interests', 30),\n",
       "   ('biomedical', 30),\n",
       "   ('collection', 30),\n",
       "   ('results.', 30),\n",
       "   ('conducted', 30),\n",
       "   ('them', 29),\n",
       "   ('session', 29),\n",
       "   ('during', 29),\n",
       "   ('ability', 29),\n",
       "   ('computer', 29),\n",
       "   ('world', 29),\n",
       "   ('literature', 28),\n",
       "   ('searches', 28),\n",
       "   ('search.', 28),\n",
       "   ('care', 28),\n",
       "   ('concept', 28),\n",
       "   ('context,', 28),\n",
       "   ('make', 28),\n",
       "   ('challenges', 28),\n",
       "   ('out', 27),\n",
       "   ('development', 27),\n",
       "   ('question', 27),\n",
       "   ('identified', 27),\n",
       "   ('cognitive', 27),\n",
       "   ('those', 27),\n",
       "   ('information-seeking', 27),\n",
       "   ('finding', 27),\n",
       "   ('explicit', 27),\n",
       "   ('initial', 26),\n",
       "   ('precision', 26),\n",
       "   ('understand', 26),\n",
       "   ('empirical', 26),\n",
       "   ('according', 26),\n",
       "   ('reading', 26),\n",
       "   ('way', 26),\n",
       "   ('classification', 26),\n",
       "   ('evidence', 26),\n",
       "   ('users.', 26),\n",
       "   ('differences', 26),\n",
       "   ('focus', 26),\n",
       "   ('provided', 26),\n",
       "   ('level', 26),\n",
       "   ('various', 26),\n",
       "   ('common', 25),\n",
       "   ('researchers', 25),\n",
       "   ('participants', 25),\n",
       "   ('improving', 25),\n",
       "   ('topics', 25),\n",
       "   ('computing', 25),\n",
       "   ('key', 25),\n",
       "   ('system.', 25),\n",
       "   ('explore', 25),\n",
       "   ('consumer', 25),\n",
       "   ('behavior.', 25),\n",
       "   ('field', 25),\n",
       "   ('whether', 25),\n",
       "   ('average', 25),\n",
       "   ('implications', 25),\n",
       "   ('impact', 25),\n",
       "   ('documents.', 25),\n",
       "   ('rather', 24),\n",
       "   ('no', 24),\n",
       "   ('consumers', 24),\n",
       "   ('few', 24),\n",
       "   ('highly', 24),\n",
       "   ('discuss', 24),\n",
       "   ('usability', 24),\n",
       "   ('answer', 24),\n",
       "   ('collections', 24),\n",
       "   ('influence', 24),\n",
       "   ('potential', 24),\n",
       "   ('suggest', 24),\n",
       "   ('include', 24),\n",
       "   ('providing', 24),\n",
       "   ('predict', 24),\n",
       "   ('if', 24),\n",
       "   ('judgments', 24),\n",
       "   ('likely', 23),\n",
       "   ('retrieval.', 23),\n",
       "   ('finally,', 23),\n",
       "   ('process', 23),\n",
       "   ('tasks.', 23),\n",
       "   ('main', 23),\n",
       "   ('examine', 23),\n",
       "   ('interface', 23),\n",
       "   ('interactions', 23),\n",
       "   ('major', 23),\n",
       "   ('google', 23),\n",
       "   ('compare', 22),\n",
       "   ('compared', 22),\n",
       "   ('context.', 22),\n",
       "   ('role', 22),\n",
       "   ('list', 22),\n",
       "   ('characteristics', 22),\n",
       "   ('single', 22),\n",
       "   ('mobile', 22),\n",
       "   ('allows', 22),\n",
       "   ('personal', 22),\n",
       "   ('even', 22),\n",
       "   ('application', 22),\n",
       "   ('consider', 21),\n",
       "   ('good', 21),\n",
       "   ('i', 21),\n",
       "   ('little', 21),\n",
       "   ('available', 21),\n",
       "   ('problems', 21),\n",
       "   ('contexts', 21),\n",
       "   ('although', 21),\n",
       "   ('search,', 21),\n",
       "   ('possible', 21),\n",
       "   ('standard', 21),\n",
       "   ('does', 21),\n",
       "   ('issues', 21),\n",
       "   ('mining', 21),\n",
       "   ('survey', 21),\n",
       "   ('criteria', 20),\n",
       "   ('accurate', 20),\n",
       "   ('describes', 20),\n",
       "   ('might', 20),\n",
       "   ('still', 20),\n",
       "   ('shows', 20),\n",
       "   ('recommendation', 20),\n",
       "   ('patient', 20),\n",
       "   ('investigate', 20),\n",
       "   ('presented', 20),\n",
       "   ('long', 20),\n",
       "   ('measure', 20),\n",
       "   ('combination', 20),\n",
       "   ('utility', 20),\n",
       "   ('particular,', 20),\n",
       "   ('patterns', 20),\n",
       "   ('users,', 20),\n",
       "   ('history', 20),\n",
       "   ('range', 20),\n",
       "   ('any', 20),\n",
       "   ('experience', 20),\n",
       "   ('searcher', 20),\n",
       "   ('being', 20),\n",
       "   ('who', 20),\n",
       "   ('nature', 20),\n",
       "   ('obtained', 19),\n",
       "   ('able', 19),\n",
       "   ('four', 19),\n",
       "   ('less', 19),\n",
       "   ('perform', 19),\n",
       "   ('experiment', 19),\n",
       "   ('representation', 19),\n",
       "   ('form', 19),\n",
       "   ('means', 19),\n",
       "   ('behaviors', 19),\n",
       "   ('evaluating', 19),\n",
       "   ('determine', 19),\n",
       "   ('controlled', 19),\n",
       "   ('variety', 19),\n",
       "   ('national', 19),\n",
       "   ('sessions', 19),\n",
       "   ('address', 19),\n",
       "   ('usage', 19),\n",
       "   ('data.', 19),\n",
       "   ('take', 19),\n",
       "   ('much', 19),\n",
       "   ('articles', 19),\n",
       "   ('query,', 19),\n",
       "   ('successful', 19),\n",
       "   ('known', 18),\n",
       "   ('identifying', 18),\n",
       "   ('groups', 18),\n",
       "   ('past', 18),\n",
       "   ('rank', 18),\n",
       "   ('large-scale', 18),\n",
       "   ('user.', 18),\n",
       "   ('evaluated', 18),\n",
       "   ('yet', 18),\n",
       "   ('topical', 18),\n",
       "   ('underlying', 18),\n",
       "   ('group', 18),\n",
       "   ('investigated', 18),\n",
       "   ('view', 18),\n",
       "   ('build', 18),\n",
       "   ('effects', 18),\n",
       "   ('simple', 18),\n",
       "   ('task.', 18),\n",
       "   ('sites', 18),\n",
       "   ('comparison', 18),\n",
       "   ('under', 18),\n",
       "   ('correlation', 18),\n",
       "   ('point', 17),\n",
       "   ('proposes', 17),\n",
       "   ('profile', 17),\n",
       "   ('prediction', 17),\n",
       "   ('original', 17),\n",
       "   ('ways', 17),\n",
       "   ('model.', 17),\n",
       "   ('engine.', 17),\n",
       "   ('overall', 17),\n",
       "   ('article,', 17),\n",
       "   ('analyzed', 17),\n",
       "   ('web.', 17),\n",
       "   ('so', 17),\n",
       "   ('goal', 17),\n",
       "   ('relationships', 17),\n",
       "   ('feature', 17),\n",
       "   ('extraction', 17),\n",
       "   ('uses', 17),\n",
       "   ('area', 17),\n",
       "   ('statistical', 17),\n",
       "   ('data,', 17),\n",
       "   ('without', 17),\n",
       "   ('recent', 17),\n",
       "   ('experts', 17),\n",
       "   ('critical', 17),\n",
       "   ('case', 17),\n",
       "   ('first,', 17),\n",
       "   ('-', 17),\n",
       "   ('baseline', 16),\n",
       "   ('extracted', 16),\n",
       "   ('improved', 16),\n",
       "   ('activity', 16),\n",
       "   ('ratings', 16),\n",
       "   ('across', 16),\n",
       "   ('students', 16),\n",
       "   ('attention', 16),\n",
       "   ('building', 16),\n",
       "   ('activities', 16),\n",
       "   ('global', 16),\n",
       "   ('showed', 16),\n",
       "   ('difficult', 16),\n",
       "   ('five', 16),\n",
       "   ('needs.', 16),\n",
       "   ('via', 16),\n",
       "   ('({ir})', 16),\n",
       "   ('indicate', 16),\n",
       "   ('designed', 16),\n",
       "   ('estimate', 16),\n",
       "   ('state', 16),\n",
       "   ('actual', 16),\n",
       "   ('predicting', 16),\n",
       "   ('directly', 16),\n",
       "   ('digital', 16),\n",
       "   ('task,', 16),\n",
       "   ('suggests', 16),\n",
       "   ('since', 16),\n",
       "   ('studied', 16),\n",
       "   ('short', 15),\n",
       "   ('towards', 15),\n",
       "   ('library', 15),\n",
       "   ('database', 15),\n",
       "   ('focused', 15),\n",
       "   ('representations', 15),\n",
       "   ('commercial', 15),\n",
       "   ('answers', 15),\n",
       "   ('relative', 15),\n",
       "   ('tool', 15),\n",
       "   ('valuable', 15),\n",
       "   ('services', 15),\n",
       "   ('called', 15),\n",
       "   ('strategy', 15),\n",
       "   ('because', 15),\n",
       "   ('searchers', 15),\n",
       "   ('primary', 15),\n",
       "   ('web-based', 15),\n",
       "   ('considered', 15),\n",
       "   ('subject', 15),\n",
       "   ('produce', 15),\n",
       "   ('change', 15),\n",
       "   ('automated', 15),\n",
       "   ('appropriate', 15),\n",
       "   ('performance.', 15),\n",
       "   ('outperforms', 15),\n",
       "   ('results,', 15),\n",
       "   ('needed', 15),\n",
       "   ('effect', 15),\n",
       "   ('thus', 15),\n",
       "   ('desktop', 15),\n",
       "   ('clicks', 15),\n",
       "   ('popular', 15),\n",
       "   ('tasks,', 15),\n",
       "   ('changes', 15),\n",
       "   ('among', 15),\n",
       "   ('natural', 15),\n",
       "   ('clickthrough', 14),\n",
       "   ('basic', 14),\n",
       "   ('theoretical', 14),\n",
       "   ('account', 14),\n",
       "   ('instead', 14),\n",
       "   ('training', 14),\n",
       "   ('particularly', 14),\n",
       "   ('looking', 14),\n",
       "   ('effectively', 14),\n",
       "   ('ambiguous', 14),\n",
       "   ('clicked', 14),\n",
       "   ('usefulness', 14),\n",
       "   ('importance', 14),\n",
       "   ('laboratory', 14),\n",
       "   ('local', 14),\n",
       "   ('resources', 14),\n",
       "   ('his', 14),\n",
       "   ('value', 14),\n",
       "   ('improvement', 14),\n",
       "   ('similar', 14),\n",
       "   ('site', 14),\n",
       "   ('could', 14),\n",
       "   ('introduce', 14),\n",
       "   ('privacy', 14),\n",
       "   ('interesting', 14),\n",
       "   ('literacy', 14),\n",
       "   ('insight', 14),\n",
       "   ('comprehensive', 14),\n",
       "   ('study,', 14),\n",
       "   ('become', 14),\n",
       "   ('objective', 14),\n",
       "   ('addition,', 14),\n",
       "   ('profiles', 14),\n",
       "   ('recall', 14),\n",
       "   ('limited', 13),\n",
       "   ('systems,', 13),\n",
       "   ('links', 13),\n",
       "   ('satisfaction', 13),\n",
       "   ('applications.', 13),\n",
       "   ('additional', 13),\n",
       "   ('public', 13),\n",
       "   ('applied', 13),\n",
       "   ('wikipedia', 13),\n",
       "   ('structure', 13),\n",
       "   ('purpose', 13),\n",
       "   ('computational', 13),\n",
       "   ('journal', 13),\n",
       "   ('discussed', 13),\n",
       "   ('model,', 13),\n",
       "   ('methodology', 13),\n",
       "   ('assess', 13),\n",
       "   ('metrics', 13),\n",
       "   ('physicians', 13),\n",
       "   ('science', 13),\n",
       "   ('abstract', 13),\n",
       "   ('source', 13),\n",
       "   ('levels', 13),\n",
       "   ('content-based', 13),\n",
       "   ('shown', 13),\n",
       "   ('enhance', 13),\n",
       "   ('retrieve', 13),\n",
       "   ('trails', 13),\n",
       "   ('accuracy', 13),\n",
       "   ('formulate', 13),\n",
       "   ('made', 13),\n",
       "   ('efficiency', 13),\n",
       "   ('second,', 13),\n",
       "   ('time,', 13),\n",
       "   ('essential', 13),\n",
       "   ('making', 13),\n",
       "   ('conceptual', 13),\n",
       "   ('her', 13),\n",
       "   ('offers', 13),\n",
       "   ('community', 13),\n",
       "   ('lack', 13),\n",
       "   ('challenge', 13),\n",
       "   ('{medline}', 13),\n",
       "   ('models.', 13),\n",
       "   ('why', 13),\n",
       "   ('benefit', 13),\n",
       "   ('ranked', 13),\n",
       "   ('news', 13),\n",
       "   ('electronic', 13),\n",
       "   ('changing', 13),\n",
       "   ('learn', 13),\n",
       "   ('process.', 13),\n",
       "   ('practical', 13),\n",
       "   ('research.', 13),\n",
       "   ('experiments,', 13),\n",
       "   ('represent', 13),\n",
       "   ('interfaces', 12),\n",
       "   ('systematic', 12),\n",
       "   ('due', 12),\n",
       "   ('beyond', 12),\n",
       "   ('relevance,', 12),\n",
       "   ('evaluations', 12),\n",
       "   ('suggestion', 12),\n",
       "   ('demonstrated', 12),\n",
       "   ('long-term', 12),\n",
       "   ('browser', 12),\n",
       "   ('involves', 12),\n",
       "   ('particular', 12),\n",
       "   ('improvements', 12),\n",
       "   ('increasing', 12),\n",
       "   ('total', 12),\n",
       "   ('comparing', 12),\n",
       "   ('communication', 12),\n",
       "   ('paradigm', 12),\n",
       "   ('tagging', 12),\n",
       "   ('done', 12),\n",
       "   ('collected', 12),\n",
       "   ('clustering', 12),\n",
       "   ('require', 12),\n",
       "   ('rich', 12),\n",
       "   ('million', 12),\n",
       "   ('basis', 12),\n",
       "   ('creating', 12),\n",
       "   ('guide', 12),\n",
       "   ('work,', 12),\n",
       "   ('would', 12),\n",
       "   ('preference', 12),\n",
       "   ('complex', 12),\n",
       "   ('aim', 12),\n",
       "   ('(1)', 12),\n",
       "   ('science,', 12),\n",
       "   ('respect', 12),\n",
       "   ('design,', 12),\n",
       "   ('preliminary', 12),\n",
       "   ('certain', 12),\n",
       "   ('(2)', 12),\n",
       "   ('(e.g.,', 12),\n",
       "   ('patients', 12),\n",
       "   ('characterize', 12),\n",
       "   ('conclude', 12),\n",
       "   ('illustrate', 12),\n",
       "   ('word', 12),\n",
       "   ('relatively', 12),\n",
       "   ('relevance.', 12),\n",
       "   ('environment', 12),\n",
       "   ('scale', 12),\n",
       "   ('dynamic', 12),\n",
       "   ('state-of-the-art', 12),\n",
       "   ('manually', 12),\n",
       "   ('answering', 12),\n",
       "   ('older', 12),\n",
       "   ('service', 11),\n",
       "   ('faceted', 11),\n",
       "   ('match', 11),\n",
       "   ('select', 11),\n",
       "   ('substantial', 11),\n",
       "   ('developing', 11),\n",
       "   ('distribution', 11),\n",
       "   ('widely', 11),\n",
       "   ('machine', 11),\n",
       "   ('items', 11),\n",
       "   ('derived', 11),\n",
       "   ('factor', 11),\n",
       "   ('context-sensitive', 11),\n",
       "   ('facilitate', 11),\n",
       "   ('full', 11),\n",
       "   ('intent', 11),\n",
       "   ('analyze', 11),\n",
       "   ('taken', 11),\n",
       "   ('hierarchical', 11),\n",
       "   ('actions', 11),\n",
       "   ('goals', 11),\n",
       "   ('promising', 11),\n",
       "   ('methods,', 11),\n",
       "   ('studying', 11),\n",
       "   ('book', 11),\n",
       "   ('allow', 11),\n",
       "   ('diverse', 11),\n",
       "   ('expertise', 11),\n",
       "   ('keyword', 11),\n",
       "   (\"subjects'\", 11),\n",
       "   ('applications,', 11),\n",
       "   ('vector', 11),\n",
       "   ('keywords', 11),\n",
       "   ('format', 11),\n",
       "   ('extensive', 11),\n",
       "   ('advanced', 11),\n",
       "   ('complete', 11),\n",
       "   ('learned', 11),\n",
       "   ('had', 11),\n",
       "   ('efficient', 11),\n",
       "   ('before', 11),\n",
       "   ('example,', 11),\n",
       "   ('use,', 11),\n",
       "   ('techniques,', 11),\n",
       "   ('tools', 11),\n",
       "   ('must', 11),\n",
       "   ('explicitly', 11),\n",
       "   ('space', 11),\n",
       "   ('especially', 11),\n",
       "   ('behavior,', 11),\n",
       "   ('supporting', 11),\n",
       "   ('either', 11),\n",
       "   ('categories', 11),\n",
       "   ('lead', 11),\n",
       "   ('websites', 11),\n",
       "   ('visited', 11),\n",
       "   ('authors', 11),\n",
       "   ('sample', 11),\n",
       "   ('gain', 11),\n",
       "   ('rating', 11),\n",
       "   ('assist', 11),\n",
       "   ('similarity', 10),\n",
       "   ('networks', 10),\n",
       "   ('extract', 10),\n",
       "   ('typically', 10),\n",
       "   ('annotations', 10),\n",
       "   ('filtering', 10),\n",
       "   ('usa.', 10),\n",
       "   ('fundamental', 10),\n",
       "   ('reported', 10),\n",
       "   ('cannot', 10),\n",
       "   ('examined', 10),\n",
       "   ('scores', 10),\n",
       "   ('models,', 10),\n",
       "   ('reformulations', 10),\n",
       "   ('classify', 10),\n",
       "   ('open', 10),\n",
       "   ('*', 10),\n",
       "   ('carried', 10),\n",
       "   ('navigation', 10),\n",
       "   ('topics.', 10),\n",
       "   ('framework.', 10),\n",
       "   ('task-based', 10),\n",
       "   ('techniques.', 10),\n",
       "   ('combined', 10),\n",
       "   ('time.', 10),\n",
       "   ('implemented', 10),\n",
       "   ('having', 10),\n",
       "   ('theory', 10),\n",
       "   ('ambiguity', 10),\n",
       "   ('usually', 10),\n",
       "   ('display', 10),\n",
       "   ('effectiveness.', 10),\n",
       "   ('media', 10),\n",
       "   ('discusses', 10),\n",
       "   ('unique', 10),\n",
       "   ('analyzing', 10),\n",
       "   ('advantage', 10),\n",
       "   ('adults', 10),\n",
       "   ('indicates', 10),\n",
       "   ('background', 10),\n",
       "   ('{mesh}', 10),\n",
       "   ('estimating', 10),\n",
       "   ('probabilistic', 10),\n",
       "   ('university', 10),\n",
       "   ('functions', 10),\n",
       "   ('give', 10),\n",
       "   ('project', 10),\n",
       "   ('technique', 10),\n",
       "   ('us', 10),\n",
       "   ('network', 10),\n",
       "   ('manual', 10),\n",
       "   ('overview', 10),\n",
       "   ('observed', 10),\n",
       "   ('per', 10),\n",
       "   ('previously', 10),\n",
       "   ('examines', 10),\n",
       "   ('processing', 10),\n",
       "   ('searching,', 10),\n",
       "   ('includes', 10),\n",
       "   ('records', 10),\n",
       "   ('reformulation', 10),\n",
       "   ('capturing', 10),\n",
       "   ('second', 10),\n",
       "   ('decision', 10),\n",
       "   ('investigation', 10),\n",
       "   ('positive', 10),\n",
       "   ('expert', 10),\n",
       "   ('(e.g.', 10),\n",
       "   ('alternative', 10),\n",
       "   ('score', 10),\n",
       "   ('predictive', 10),\n",
       "   ('medicine', 10),\n",
       "   ('obtain', 10),\n",
       "   ('following', 10),\n",
       "   ('success', 10),\n",
       "   ('sets', 10),\n",
       "   ('searches,', 10),\n",
       "   ('terms,', 10),\n",
       "   ('report', 10),\n",
       "   ('followed', 10),\n",
       "   ('elements', 10),\n",
       "   ('generally', 10),\n",
       "   ('doctors', 10),\n",
       "   ('english', 10),\n",
       "   ('recommendations', 10),\n",
       "   ('up', 10),\n",
       "   ('engines,', 10),\n",
       "   (\"one's\", 10),\n",
       "   ('terms.', 10),\n",
       "   ('ongoing', 10),\n",
       "   ('specifically,', 9),\n",
       "   ('differ', 9),\n",
       "   ('ontology', 9),\n",
       "   ('own', 9),\n",
       "   ('topics,', 9),\n",
       "   ('generate', 9),\n",
       "   ('{md}', 9),\n",
       "   ('resulting', 9),\n",
       "   ('assistance', 9),\n",
       "   ('latent', 9),\n",
       "   ('integrate', 9),\n",
       "   ('reference', 9),\n",
       "   ('re-finding', 9),\n",
       "   ('cost', 9),\n",
       "   ('apply', 9),\n",
       "   ('2', 9),\n",
       "   ('adaptive', 9),\n",
       "   ('create', 9),\n",
       "   ('makes', 9),\n",
       "   ('currently', 9),\n",
       "   ('architecture', 9),\n",
       "   ('switching', 9),\n",
       "   ('built', 9),\n",
       "   ('ad', 9),\n",
       "   ('exploratory', 9),\n",
       "   ('retrieving', 9),\n",
       "   ('contain', 9),\n",
       "   ('difference', 9),\n",
       "   ('performed', 9),\n",
       "   ('suggested', 9),\n",
       "   ('integration', 9),\n",
       "   ('rely', 9),\n",
       "   ('toward', 9),\n",
       "   ('combine', 9),\n",
       "   ('approach,', 9),\n",
       "   ('higher', 9),\n",
       "   ('engines.', 9),\n",
       "   ('tags', 9),\n",
       "   ('situational', 9),\n",
       "   ('seeking.', 9),\n",
       "   ('tests', 9),\n",
       "   ('length', 9),\n",
       "   ('enhanced', 9),\n",
       "   ('mode', 9),\n",
       "   ('frequently', 9),\n",
       "   ('detection', 9),\n",
       "   ('exploration', 9),\n",
       "   ('largely', 9),\n",
       "   ('influenced', 9),\n",
       "   ('seek', 9),\n",
       "   ('suggestions', 9),\n",
       "   ('qualitative', 9),\n",
       "   ('therefore', 9),\n",
       "   ('log-based', 9),\n",
       "   ('sufficient', 9),\n",
       "   ('relations', 9),\n",
       "   ('{loinc}', 9),\n",
       "   ('documents,', 9),\n",
       "   ('domains', 9),\n",
       "   ('opportunities', 9),\n",
       "   ('mean', 9),\n",
       "   ('resource', 9),\n",
       "   ('explores', 9),\n",
       "   ('description', 9),\n",
       "   ('questionnaire', 9),\n",
       "   ('amount', 9),\n",
       "   ('texts', 9),\n",
       "   ('tested', 9),\n",
       "   ('increase', 9),\n",
       "   ('is,', 9),\n",
       "   ('taxonomy', 9),\n",
       "   ('realistic', 9),\n",
       "   ('definitions', 9),\n",
       "   ('multidimensional', 9),\n",
       "   ('interests.', 9),\n",
       "   ('adapt', 9),\n",
       "   ('thus,', 9),\n",
       "   ('see', 9),\n",
       "   ('association', 9),\n",
       "   ('pages,', 9),\n",
       "   ('bayesian', 9),\n",
       "   ('logs.', 9),\n",
       "   ('viewed', 9),\n",
       "   ('containing', 9),\n",
       "   ('judgment', 9),\n",
       "   ('pseudo-relevance', 9),\n",
       "   ('direct', 9),\n",
       "   ('words', 9),\n",
       "   ('practice', 9),\n",
       "   ('meaningful', 9),\n",
       "   ('crucial', 9),\n",
       "   ('categorization', 9),\n",
       "   ('appears', 8),\n",
       "   ('matching', 8),\n",
       "   ('1', 8),\n",
       "   ('strongly', 8),\n",
       "   ('around', 8),\n",
       "   ('difficulty', 8),\n",
       "   ('explored', 8),\n",
       "   ('engine,', 8),\n",
       "   ('capture', 8),\n",
       "   ('end', 8),\n",
       "   ('third,', 8),\n",
       "   ('{pico}', 8),\n",
       "   ('behaviour', 8),\n",
       "   ('algorithms,', 8),\n",
       "   ('{results}:', 8),\n",
       "   ('involved', 8),\n",
       "   ('vocabulary', 8),\n",
       "   ('greater', 8),\n",
       "   ('ontological', 8),\n",
       "   ('achieve', 8),\n",
       "   ('extracting', 8),\n",
       "   ('probability', 8),\n",
       "   ('trail', 8),\n",
       "   ('variation', 8),\n",
       "   ('selection', 8),\n",
       "   ('{uptodate}', 8),\n",
       "   ('art', 8),\n",
       "   ('mining,', 8),\n",
       "   ('drug', 8),\n",
       "   ('organize', 8),\n",
       "   ('enable', 8),\n",
       "   ('refer', 8),\n",
       "   ('gathered', 8),\n",
       "   ('increasingly', 8),\n",
       "   ('system,', 8),\n",
       "   ('=', 8),\n",
       "   ('incorporate', 8),\n",
       "   ('latter', 8),\n",
       "   ('seen', 8),\n",
       "   ('output', 8),\n",
       "   ('millions', 8),\n",
       "   ('semantically', 8),\n",
       "   ('whose', 8),\n",
       "   ('structures', 8),\n",
       "   ('lexical', 8),\n",
       "   ('revealed', 8),\n",
       "   ('reliability', 8),\n",
       "   ('active', 8),\n",
       "   ('incorporating', 8),\n",
       "   ('infer', 8),\n",
       "   ('improves', 8),\n",
       "   ('university,', 8),\n",
       "   ('situations.', 8),\n",
       "   ('ranking,', 8),\n",
       "   ('organized', 8),\n",
       "   ('systematically', 8),\n",
       "   ('indexing', 8),\n",
       "   ('e.g.,', 8),\n",
       "   ('ubiquitous', 8),\n",
       "   ('rankings', 8),\n",
       "   ('analysis,', 8),\n",
       "   ('aims', 8),\n",
       "   ('implement', 8),\n",
       "   ('least', 8),\n",
       "   ('measured', 8),\n",
       "   ('examining', 8),\n",
       "   ('accuracy.', 8),\n",
       "   ('mapping', 8),\n",
       "   ('index', 8),\n",
       "   ('integrated', 8),\n",
       "   ('attempt', 8),\n",
       "   ('series', 8),\n",
       "   ('just', 8),\n",
       "   ('situations', 8),\n",
       "   ('response', 8),\n",
       "   ...]),\n",
       " ('37769fdc48d9a70ccf3a3245f7375230',\n",
       "  [('the', 353),\n",
       "   ('of', 326),\n",
       "   ('and', 265),\n",
       "   ('to', 217),\n",
       "   ('a', 185),\n",
       "   ('in', 180),\n",
       "   ('we', 134),\n",
       "   ('for', 98),\n",
       "   ('that', 74),\n",
       "   ('on', 69),\n",
       "   ('as', 62),\n",
       "   ('social', 61),\n",
       "   ('are', 59),\n",
       "   ('is', 58),\n",
       "   ('users', 49),\n",
       "   ('with', 48),\n",
       "   ('this', 44),\n",
       "   ('their', 42),\n",
       "   ('an', 41),\n",
       "   ('from', 40),\n",
       "   ('information', 38),\n",
       "   ('content', 38),\n",
       "   ('user', 34),\n",
       "   ('by', 33),\n",
       "   ('our', 33),\n",
       "   ('at', 32),\n",
       "   ('online', 32),\n",
       "   ('such', 30),\n",
       "   ('they', 29),\n",
       "   ('these', 28),\n",
       "   ('community', 27),\n",
       "   ('search', 27),\n",
       "   ('not', 25),\n",
       "   ('people', 24),\n",
       "   ('be', 24),\n",
       "   ('can', 24),\n",
       "   ('or', 24),\n",
       "   ('which', 22),\n",
       "   ('have', 22),\n",
       "   ('quality', 21),\n",
       "   ('web', 21),\n",
       "   ('more', 21),\n",
       "   ('question', 20),\n",
       "   ('how', 20),\n",
       "   ('results', 19),\n",
       "   ('mobile', 18),\n",
       "   ('study', 18),\n",
       "   ('research', 18),\n",
       "   ('other', 17),\n",
       "   ('sites', 17),\n",
       "   ('media', 17),\n",
       "   ('value', 16),\n",
       "   ('answer', 16),\n",
       "   ('knowledge', 16),\n",
       "   ('privacy', 16),\n",
       "   ('who', 15),\n",
       "   ('but', 15),\n",
       "   ('sharing', 15),\n",
       "   ('has', 15),\n",
       "   ('using', 14),\n",
       "   ('answers', 14),\n",
       "   ('analysis', 14),\n",
       "   ('large', 13),\n",
       "   ('questions', 13),\n",
       "   ('through', 13),\n",
       "   ('design', 13),\n",
       "   ('behavior', 13),\n",
       "   ('while', 13),\n",
       "   ('twitter', 13),\n",
       "   ('query', 12),\n",
       "   ('than', 12),\n",
       "   ('new', 12),\n",
       "   ('between', 12),\n",
       "   ('show', 12),\n",
       "   ('over', 12),\n",
       "   ('data', 12),\n",
       "   ('paper', 12),\n",
       "   ('different', 12),\n",
       "   ('find', 12),\n",
       "   ('it', 12),\n",
       "   ('', 11),\n",
       "   ('types', 11),\n",
       "   ('where', 11),\n",
       "   ('yahoo!', 11),\n",
       "   ('however,', 10),\n",
       "   ('potential', 10),\n",
       "   ('only', 10),\n",
       "   ('queries', 10),\n",
       "   ('present', 10),\n",
       "   ('one', 10),\n",
       "   ('use', 10),\n",
       "   ('members', 10),\n",
       "   ('many', 10),\n",
       "   ('about', 10),\n",
       "   ('were', 10),\n",
       "   ('answering', 10),\n",
       "   ('context', 10),\n",
       "   ('provide', 9),\n",
       "   ('participants', 9),\n",
       "   ('there', 9),\n",
       "   ('based', 9),\n",
       "   ('site', 9),\n",
       "   ('experts', 9),\n",
       "   ('given', 9),\n",
       "   ('popular', 9),\n",
       "   ('better', 9),\n",
       "   ('{sqa}', 9),\n",
       "   ('university', 9),\n",
       "   ('problem', 9),\n",
       "   ('also', 9),\n",
       "   ('some', 9),\n",
       "   ('investigate', 8),\n",
       "   (\"users'\", 8),\n",
       "   ('framework', 8),\n",
       "   ('{q\\\\&a}', 8),\n",
       "   ('general', 8),\n",
       "   ('contributions', 8),\n",
       "   ('experiments', 8),\n",
       "   ('group', 8),\n",
       "   ('often', 8),\n",
       "   ('impact', 8),\n",
       "   ('system', 8),\n",
       "   ('may', 8),\n",
       "   ('communities', 8),\n",
       "   ('number', 8),\n",
       "   ('categories', 8),\n",
       "   ('patterns', 8),\n",
       "   ('answers,', 8),\n",
       "   ('accuracy', 8),\n",
       "   ('out', 7),\n",
       "   ('characteristics', 7),\n",
       "   ('others', 7),\n",
       "   ('caching', 7),\n",
       "   ('when', 7),\n",
       "   ('recommender', 7),\n",
       "   ('student', 7),\n",
       "   ('been', 7),\n",
       "   ('behind', 7),\n",
       "   ('understanding', 7),\n",
       "   ('explore', 7),\n",
       "   ('will', 7),\n",
       "   ('both', 7),\n",
       "   ('systems', 7),\n",
       "   ('high', 7),\n",
       "   ('paper,', 7),\n",
       "   ('particularly', 7),\n",
       "   ('participation', 7),\n",
       "   ('identify', 7),\n",
       "   ('millions', 7),\n",
       "   ('into', 7),\n",
       "   ('model', 7),\n",
       "   ('effective', 7),\n",
       "   ('leaks', 7),\n",
       "   ('discuss', 7),\n",
       "   ('controlled', 7),\n",
       "   ('topics', 7),\n",
       "   ('small', 7),\n",
       "   ('available', 7),\n",
       "   ('multiple', 6),\n",
       "   ('classification', 6),\n",
       "   ('particular', 6),\n",
       "   ('community.', 6),\n",
       "   ('order', 6),\n",
       "   ('analyze', 6),\n",
       "   ('few', 6),\n",
       "   ('users.', 6),\n",
       "   ('work', 6),\n",
       "   ('his', 6),\n",
       "   ('examine', 6),\n",
       "   ('each', 6),\n",
       "   ('early', 6),\n",
       "   ('future', 6),\n",
       "   ('report', 6),\n",
       "   ('demonstrate', 6),\n",
       "   ('technology', 6),\n",
       "   ('increasingly', 6),\n",
       "   ('higher', 6),\n",
       "   ('implications', 6),\n",
       "   ('share', 6),\n",
       "   ('high-quality', 6),\n",
       "   ('science', 6),\n",
       "   ('ph.d.', 6),\n",
       "   ('interests', 6),\n",
       "   ('its', 6),\n",
       "   ('relevance', 6),\n",
       "   ('questions,', 6),\n",
       "   ('previous', 6),\n",
       "   ('methods', 6),\n",
       "   ('value,', 6),\n",
       "   ('content.', 6),\n",
       "   ('recommendations', 6),\n",
       "   ('create', 6),\n",
       "   ('computer', 6),\n",
       "   ('understand', 5),\n",
       "   ('able', 5),\n",
       "   ('combining', 5),\n",
       "   ('set', 5),\n",
       "   ('interested', 5),\n",
       "   ('earthquakes', 5),\n",
       "   ('important', 5),\n",
       "   ('might', 5),\n",
       "   ('particular,', 5),\n",
       "   ('recent', 5),\n",
       "   ('tweets', 5),\n",
       "   ('goals', 5),\n",
       "   ('celebrity', 5),\n",
       "   ('studies', 5),\n",
       "   ('make', 5),\n",
       "   ('us', 5),\n",
       "   ('increase', 5),\n",
       "   ('likely', 5),\n",
       "   ('two', 5),\n",
       "   ('posting', 5),\n",
       "   ('used', 5),\n",
       "   ('items', 5),\n",
       "   ('sexual', 5),\n",
       "   ('theory', 5),\n",
       "   ('most', 5),\n",
       "   ('those', 5),\n",
       "   ('well', 5),\n",
       "   ('--', 5),\n",
       "   ('within', 5),\n",
       "   ('including', 5),\n",
       "   ('what', 5),\n",
       "   ('contribution', 5),\n",
       "   ('collaborative', 5),\n",
       "   ('urgent', 5),\n",
       "   ('school', 5),\n",
       "   ('specific', 5),\n",
       "   ('amount', 5),\n",
       "   ('interest', 5),\n",
       "   ('newcomers', 5),\n",
       "   ('factual', 5),\n",
       "   ('tweets.', 5),\n",
       "   ('automaticity', 5),\n",
       "   ('time', 5),\n",
       "   ('finding', 5),\n",
       "   ('wide', 5),\n",
       "   ('first', 5),\n",
       "   ('develop', 5),\n",
       "   ('personal', 5),\n",
       "   ('networking', 5),\n",
       "   ('insights', 5),\n",
       "   ('automatically', 5),\n",
       "   ('interactions', 5),\n",
       "   ('process', 4),\n",
       "   ('interesting', 4),\n",
       "   ('issues', 4),\n",
       "   ('thus', 4),\n",
       "   ('network', 4),\n",
       "   ('three', 4),\n",
       "   ('identifying', 4),\n",
       "   ('would', 4),\n",
       "   ('allow', 4),\n",
       "   ('become', 4),\n",
       "   ('algorithm', 4),\n",
       "   ('type', 4),\n",
       "   ('finally,', 4),\n",
       "   ('it.', 4),\n",
       "   ('interaction', 4),\n",
       "   ('her', 4),\n",
       "   ('criteria', 4),\n",
       "   ('{qa}', 4),\n",
       "   ('personalization', 4),\n",
       "   ('decisions', 4),\n",
       "   ('either', 4),\n",
       "   ('all', 4),\n",
       "   ('activity', 4),\n",
       "   ('time.', 4),\n",
       "   ('various', 4),\n",
       "   ('shared', 4),\n",
       "   ('user-generated', 4),\n",
       "   ('earthquake', 4),\n",
       "   ('selection', 4),\n",
       "   ('opportunities', 4),\n",
       "   ('facebook,', 4),\n",
       "   ('mining', 4),\n",
       "   ('he', 4),\n",
       "   ('approaches', 4),\n",
       "   ('very', 4),\n",
       "   ('was', 4),\n",
       "   ('{cqa}', 4),\n",
       "   ('widely', 4),\n",
       "   ('significantly', 4),\n",
       "   ('static', 4),\n",
       "   ('characterize', 4),\n",
       "   ('question/answering', 4),\n",
       "   ('carnegie', 4),\n",
       "   ('mellon', 4),\n",
       "   ('minnesota.', 4),\n",
       "   ('behaviour', 4),\n",
       "   ('offer', 4),\n",
       "   ('identified', 4),\n",
       "   ('retrieval', 4),\n",
       "   ('same', 4),\n",
       "   ('scale', 4),\n",
       "   ('({cqa})', 4),\n",
       "   ('could', 4),\n",
       "   ('whom', 4),\n",
       "   ('post', 4),\n",
       "   ('rich', 4),\n",
       "   ('variety', 4),\n",
       "   ('photo', 4),\n",
       "   ('detect', 4),\n",
       "   ('techniques', 4),\n",
       "   ('during', 4),\n",
       "   ('becoming', 4),\n",
       "   ('individuals', 4),\n",
       "   ('several', 4),\n",
       "   ('study,', 4),\n",
       "   ('challenges', 4),\n",
       "   ('approach', 4),\n",
       "   ('networks', 4),\n",
       "   ('include', 4),\n",
       "   ('habit', 4),\n",
       "   ('evaluation', 4),\n",
       "   ('highly', 4),\n",
       "   ('whether', 4),\n",
       "   ('practice', 4),\n",
       "   ('analyzed', 4),\n",
       "   ('cover', 4),\n",
       "   ('aardvark,', 4),\n",
       "   ('technologies', 4),\n",
       "   ('audience', 4),\n",
       "   ('information,', 4),\n",
       "   ('submitted', 4),\n",
       "   ('archives', 4),\n",
       "   ('experience', 4),\n",
       "   ('target', 4),\n",
       "   ('considerations', 3),\n",
       "   ('tasks', 3),\n",
       "   ('puzzling', 3),\n",
       "   ('experiment', 3),\n",
       "   ('aardvark', 3),\n",
       "   ('them', 3),\n",
       "   ('human', 3),\n",
       "   ('links', 3),\n",
       "   ('findings', 3),\n",
       "   ('rely', 3),\n",
       "   ('focused', 3),\n",
       "   ('privacy,', 3),\n",
       "   ('exploring', 3),\n",
       "   ('feedback', 3),\n",
       "   ('especially', 3),\n",
       "   ('reveal', 3),\n",
       "   ('example,', 3),\n",
       "   ('university.', 3),\n",
       "   ('department', 3),\n",
       "   ('derived', 3),\n",
       "   ('lies', 3),\n",
       "   ('satisfy', 3),\n",
       "   ('motivation', 3),\n",
       "   ('ability', 3),\n",
       "   ('introduce', 3),\n",
       "   ('article', 3),\n",
       "   ('varies', 3),\n",
       "   ('-', 3),\n",
       "   ('engagement', 3),\n",
       "   ('contributions.', 3),\n",
       "   ('becomes', 3),\n",
       "   ('location', 3),\n",
       "   ('contributed', 3),\n",
       "   ('professor', 3),\n",
       "   ('psychology', 3),\n",
       "   ('mechanical', 3),\n",
       "   ('micro-task', 3),\n",
       "   ('perform', 3),\n",
       "   ('weeks', 3),\n",
       "   ('help', 3),\n",
       "   ('ask', 3),\n",
       "   ('why', 3),\n",
       "   ('world', 3),\n",
       "   ('ranking', 3),\n",
       "   ('system,', 3),\n",
       "   ('way', 3),\n",
       "   ('service,', 3),\n",
       "   ('themselves', 3),\n",
       "   ('improve', 3),\n",
       "   ('recognize', 3),\n",
       "   ('four', 3),\n",
       "   ('communities.', 3),\n",
       "   ('individual', 3),\n",
       "   ('share.', 3),\n",
       "   ('needs', 3),\n",
       "   ('good', 3),\n",
       "   ('increased', 3),\n",
       "   ('event', 3),\n",
       "   ('organizational', 3),\n",
       "   ('systems,', 3),\n",
       "   ('formation', 3),\n",
       "   ('whole', 3),\n",
       "   (\"user's\", 3),\n",
       "   ('detection', 3),\n",
       "   ('approximately', 3),\n",
       "   ('clustering', 3),\n",
       "   ('time,', 3),\n",
       "   ('observe', 3),\n",
       "   ('across', 3),\n",
       "   ('address', 3),\n",
       "   ('process.', 3),\n",
       "   ('affect', 3),\n",
       "   ('theories', 3),\n",
       "   ('every', 3),\n",
       "   ('if', 3),\n",
       "   ('tuned', 3),\n",
       "   ('needs.', 3),\n",
       "   ('associated', 3),\n",
       "   ('seek', 3),\n",
       "   ('everyday', 3),\n",
       "   ('gain', 3),\n",
       "   ('real', 3),\n",
       "   ('prevalence', 3),\n",
       "   ('long', 3),\n",
       "   ('relevant', 3),\n",
       "   ('appears', 3),\n",
       "   ('question,', 3),\n",
       "   ('right', 3),\n",
       "   ('engine', 3),\n",
       "   ('like', 3),\n",
       "   ('now', 3),\n",
       "   ('look', 3),\n",
       "   ('produce', 3),\n",
       "   ('problem,', 3),\n",
       "   ('do', 3),\n",
       "   ('management', 3),\n",
       "   ('select', 3),\n",
       "   ('chances', 3),\n",
       "   ('microblogging', 3),\n",
       "   ('public', 3),\n",
       "   ('united', 3),\n",
       "   ('responses', 3),\n",
       "   ('sites.', 3),\n",
       "   ('distribution', 3),\n",
       "   ('contribution.', 3),\n",
       "   ('broad', 3),\n",
       "   ('go', 3),\n",
       "   ('task', 3),\n",
       "   ('test', 3),\n",
       "   ('real-time', 3),\n",
       "   ('expertise', 3),\n",
       "   ('best', 3),\n",
       "   ('incriminating', 3),\n",
       "   ('business,', 3),\n",
       "   ('practitioners', 3),\n",
       "   ('addition', 3),\n",
       "   ('trust', 3),\n",
       "   ('build', 3),\n",
       "   ('evaluate', 3),\n",
       "   ('represent', 3),\n",
       "   ('effectiveness', 3),\n",
       "   ('posts', 3),\n",
       "   ('provided', 3),\n",
       "   ('log', 3),\n",
       "   ('propose', 3),\n",
       "   ('changes', 3),\n",
       "   ('add', 3),\n",
       "   ('communities,', 3),\n",
       "   ('simply', 3),\n",
       "   ('range', 3),\n",
       "   ('dramatically', 3),\n",
       "   ('focus', 3),\n",
       "   ('question-answering', 3),\n",
       "   ('devices', 3),\n",
       "   ('contexts', 3),\n",
       "   ('focuses', 3),\n",
       "   ('computing', 3),\n",
       "   ('serendipity', 2),\n",
       "   ('root', 2),\n",
       "   ('single', 2),\n",
       "   ('engine,', 2),\n",
       "   ('catastrophes', 2),\n",
       "   ('chain', 2),\n",
       "   ('combination', 2),\n",
       "   ('accounts', 2),\n",
       "   ('leave', 2),\n",
       "   ('machine', 2),\n",
       "   ('collapse,', 2),\n",
       "   ('estimate', 2),\n",
       "   ('confirm', 2),\n",
       "   ('witkey', 2),\n",
       "   ('solution', 2),\n",
       "   ('paying', 2),\n",
       "   (\"site's\", 2),\n",
       "   ('reference', 2),\n",
       "   ('years,', 2),\n",
       "   ('alternative', 2),\n",
       "   ('caching,', 2),\n",
       "   ('year', 2),\n",
       "   ('members.', 2),\n",
       "   ('broader', 2),\n",
       "   ('central', 2),\n",
       "   ('opportunity', 2),\n",
       "   ('block', 2),\n",
       "   ('facebook', 2),\n",
       "   ('drastically', 2),\n",
       "   ('excellent', 2),\n",
       "   ('array', 2),\n",
       "   ('case,', 2),\n",
       "   ('evidence', 2),\n",
       "   ('domain,', 2),\n",
       "   ('taxonomy', 2),\n",
       "   ('i', 2),\n",
       "   ('searches', 2),\n",
       "   ('motivating', 2),\n",
       "   ('{sns}', 2),\n",
       "   ('empirical', 2),\n",
       "   ('graduate', 2),\n",
       "   ('participation,', 2),\n",
       "   ('human-computer', 2),\n",
       "   ('presents', 2),\n",
       "   ('intrinsic', 2),\n",
       "   ('significant', 2),\n",
       "   ('compare', 2),\n",
       "   ('curve', 2),\n",
       "   ('even', 2),\n",
       "   ('sample', 2),\n",
       "   ('monetary', 2),\n",
       "   ('special', 2),\n",
       "   ('part', 2),\n",
       "   ('outcomes', 2),\n",
       "   ('experimental', 2),\n",
       "   ('see', 2),\n",
       "   ('then', 2),\n",
       "   ('keep', 2),\n",
       "   ('along', 2),\n",
       "   ('lively', 2),\n",
       "   ('events', 2),\n",
       "   ('shows', 2),\n",
       "   ('had', 2),\n",
       "   ('dynamic', 2),\n",
       "   ('imagined', 2),\n",
       "   ('resemble', 2),\n",
       "   ('communication', 2),\n",
       "   ('tend', 2),\n",
       "   ('tasks,', 2),\n",
       "   ('manage', 2),\n",
       "   ('effects', 2),\n",
       "   ('you', 2),\n",
       "   ('second,', 2),\n",
       "   ('emerged', 2),\n",
       "   ('automatic', 2),\n",
       "   ('manually', 2),\n",
       "   ('observations', 2),\n",
       "   ('dynamics', 2),\n",
       "   ('corpus', 2),\n",
       "   ('frequently', 2),\n",
       "   ('specifically', 2),\n",
       "   ('half', 2),\n",
       "   ('abuse', 2),\n",
       "   ('sources:', 2),\n",
       "   ('itself,', 2),\n",
       "   ('non-content', 2),\n",
       "   ('available,', 2),\n",
       "   ('explicit', 2),\n",
       "   ('need', 2),\n",
       "   ('knows', 2),\n",
       "   ('setting.', 2),\n",
       "   ('sharing.', 2),\n",
       "   ('attention', 2),\n",
       "   ('singling', 2),\n",
       "   ('receiving', 2),\n",
       "   ('revelation', 2),\n",
       "   ('variation', 2),\n",
       "   ('knowledge,', 2),\n",
       "   ('features', 2),\n",
       "   ('apply', 2),\n",
       "   ('collaboration', 2),\n",
       "   ('called', 2),\n",
       "   ('applications', 2),\n",
       "   ('goals.', 2),\n",
       "   ('previously', 2),\n",
       "   ('trade', 2),\n",
       "   ('low', 2),\n",
       "   ('remote', 2),\n",
       "   ('months', 2),\n",
       "   ('getting', 2),\n",
       "   ('compared', 2),\n",
       "   ('traditional', 2),\n",
       "   ('fatal', 2),\n",
       "   ('open', 2),\n",
       "   ('result', 2),\n",
       "   ('beyond', 2),\n",
       "   ('center', 2),\n",
       "   ('in-depth', 2),\n",
       "   ('take', 2),\n",
       "   ('indicate', 2),\n",
       "   ('current', 2),\n",
       "   ('behavior.', 2),\n",
       "   ('conceptualize', 2),\n",
       "   ('service', 2),\n",
       "   ('primarily', 2),\n",
       "   ('activity.', 2),\n",
       "   ('factors', 2),\n",
       "   ('coupled', 2),\n",
       "   ('mutual', 2),\n",
       "   ('fraction', 2),\n",
       "   ('amounts', 2),\n",
       "   ('improves', 2),\n",
       "   ('limitations', 2),\n",
       "   ('methods.', 2),\n",
       "   ('measure', 2),\n",
       "   ('designers', 2),\n",
       "   ('affinity', 2),\n",
       "   ('with;', 2),\n",
       "   ('monitor', 2),\n",
       "   ('groups', 2),\n",
       "   ('join', 2),\n",
       "   ('processes', 2),\n",
       "   ('diverse', 2),\n",
       "   ('simple', 2),\n",
       "   ('long-term', 2),\n",
       "   ('must', 2),\n",
       "   ('unshared', 2),\n",
       "   ('spam.', 2),\n",
       "   ('availability', 2),\n",
       "   ('increases,', 2),\n",
       "   ('rest', 2),\n",
       "   ('close', 2),\n",
       "   ('further', 2),\n",
       "   ('risk', 2),\n",
       "   ('studies,', 2),\n",
       "   ('sign-up', 2),\n",
       "   ('contributing', 2),\n",
       "   ('usage', 2),\n",
       "   ('notion', 2),\n",
       "   ('call', 2),\n",
       "   ('related', 2),\n",
       "   ('broadcast', 2),\n",
       "   ('private', 2),\n",
       "   ('revealing', 2),\n",
       "   ('economics', 2),\n",
       "   ('\\\\&', 2),\n",
       "   ('learning', 2),\n",
       "   ('sites,', 2),\n",
       "   ('successfully', 2),\n",
       "   ('generating', 2),\n",
       "   ('2007,', 2),\n",
       "   ('multiplayer', 2),\n",
       "   ('fitted', 2),\n",
       "   ('entities', 2),\n",
       "   ('requires', 2),\n",
       "   ('paradigm', 2),\n",
       "   ('market', 2),\n",
       "   ('engines', 2),\n",
       "   ('targeted', 2),\n",
       "   ('web.', 2),\n",
       "   ('a.', 2),\n",
       "   ('get', 2),\n",
       "   ('awareness', 2),\n",
       "   ('should', 2),\n",
       "   ('document', 2),\n",
       "   ('among', 2),\n",
       "   ('grouping', 2),\n",
       "   ('suggests', 2),\n",
       "   ('findings,', 2),\n",
       "   ('limits', 2),\n",
       "   ('reasons', 2),\n",
       "   ('asked', 2),\n",
       "   ('efficient', 2),\n",
       "   ('audiences,', 2),\n",
       "   ('china,', 2),\n",
       "   ('does', 2),\n",
       "   ('core', 2),\n",
       "   ('come', 2),\n",
       "   ('congresspeople', 2),\n",
       "   ('places', 2),\n",
       "   ('predictors', 2),\n",
       "   ('require', 2),\n",
       "   ('estimation', 2),\n",
       "   ('lists.', 2),\n",
       "   ('achieve', 2),\n",
       "   ('server', 2),\n",
       "   ('lead', 2),\n",
       "   ('complex', 2),\n",
       "   ('combines', 2),\n",
       "   ('method', 2),\n",
       "   ('samples', 2),\n",
       "   ('considerable', 2),\n",
       "   ('groups,', 2),\n",
       "   ('desired', 2),\n",
       "   ('separate', 2),\n",
       "   ('searching', 2),\n",
       "   ('me', 2),\n",
       "   ('questions.', 2),\n",
       "   ('emerge', 2),\n",
       "   ('physical', 2),\n",
       "   ('settings.', 2),\n",
       "   ('highlight', 2),\n",
       "   ('health', 2),\n",
       "   ('assessments.', 2),\n",
       "   ('little', 2),\n",
       "   ('encouraging', 2),\n",
       "   ('contribute', 2),\n",
       "   ('friends', 2),\n",
       "   ('services', 2),\n",
       "   ('emerging', 2),\n",
       "   ('problem.', 2),\n",
       "   ('department,', 2),\n",
       "   ('literature', 2),\n",
       "   ('analysis,', 2),\n",
       "   ('comments,', 2),\n",
       "   ('million', 2),\n",
       "   ('needs,', 2),\n",
       "   ('asymptotic', 2),\n",
       "   ('fit.', 2),\n",
       "   ('aspects', 2),\n",
       "   ('size,', 2),\n",
       "   ('developing', 2),\n",
       "   ('via', 2),\n",
       "   ('took', 2),\n",
       "   ('decision', 2),\n",
       "   ('engine.', 2),\n",
       "   ('person', 2),\n",
       "   ('challenge', 2),\n",
       "   ('describe', 2),\n",
       "   ('them.', 2),\n",
       "   ('power', 2),\n",
       "   ('answers.', 2),\n",
       "   ('managers', 2),\n",
       "   ('effectively', 2),\n",
       "   ('already', 2),\n",
       "   ('highlighting', 2),\n",
       "   ('strategic', 2),\n",
       "   ('form', 2),\n",
       "   ('taskcn,', 2),\n",
       "   ('site.', 2),\n",
       "   ('field', 2),\n",
       "   ('naver', 2),\n",
       "   ('past', 2),\n",
       "   ('examples', 2),\n",
       "   ('vs.', 2),\n",
       "   ('levels', 2),\n",
       "   ('showing', 2),\n",
       "   ('intent', 2),\n",
       "   ('comprehensive', 2),\n",
       "   ('conversation', 2),\n",
       "   ('classify', 2),\n",
       "   ('--social', 2),\n",
       "   ('exhibit', 2),\n",
       "   ('exploiting', 2),\n",
       "   ('so-called', 2),\n",
       "   ('concerns', 2),\n",
       "   ('interviews', 2),\n",
       "   ('self-report', 2),\n",
       "   ('topics,', 2),\n",
       "   ('methodological', 2),\n",
       "   ('yield', 2),\n",
       "   ('results,', 2),\n",
       "   ('understood.', 2),\n",
       "   ('learning,', 2),\n",
       "   ('inclined', 2),\n",
       "   ('defined', 2),\n",
       "   ('information.', 2),\n",
       "   ('sensitive', 2),\n",
       "   ('major', 2),\n",
       "   ('categorization,', 2),\n",
       "   ('socio-emotional', 2),\n",
       "   ('carried', 2),\n",
       "   ('games', 2),\n",
       "   ('factoid', 2),\n",
       "   ('retrieval.', 2),\n",
       "   ('engaging', 2),\n",
       "   ('costs.', 2),\n",
       "   ('collecting', 2),\n",
       "   ('tasks.', 2),\n",
       "   ('found', 2),\n",
       "   ('return', 2),\n",
       "   ('directly', 2),\n",
       "   ('experiments.', 2),\n",
       "   ('tremendous', 2),\n",
       "   ('message,', 2),\n",
       "   ('text', 2),\n",
       "   ('interface', 2),\n",
       "   ('appeals', 2),\n",
       "   ('(a', 2),\n",
       "   ('future.', 2),\n",
       "   ('chiles', 2),\n",
       "   ('experts.', 2),\n",
       "   ('left', 2),\n",
       "   ('want', 2),\n",
       "   ('users,', 2),\n",
       "   ('investigates', 2),\n",
       "   ('targeting', 2),\n",
       "   ('thriving', 2),\n",
       "   ('because', 2),\n",
       "   ('daily', 2),\n",
       "   ('less', 2),\n",
       "   ('first,', 2),\n",
       "   ('forum', 2),\n",
       "   ('unfortunately,', 2),\n",
       "   ('usable', 2),\n",
       "   ('retrieving', 2),\n",
       "   ('labeled', 2),\n",
       "   ('leads', 2),\n",
       "   ('applicable', 2),\n",
       "   ('existing', 2),\n",
       "   ('services.', 2),\n",
       "   ('venue', 2),\n",
       "   ('sufficient', 2),\n",
       "   ('95\\\\%', 2),\n",
       "   ('self-censorship', 2),\n",
       "   ('interviewed', 2),\n",
       "   ('application', 2),\n",
       "   ('important.', 2),\n",
       "   ('ratings', 2),\n",
       "   ('portal', 2),\n",
       "   ('sources', 2),\n",
       "   ('definition.', 2),\n",
       "   ('nature', 2),\n",
       "   ('review', 2),\n",
       "   ('evaluating', 2),\n",
       "   ('space.', 2),\n",
       "   ('received', 2),\n",
       "   ('newcomer', 2),\n",
       "   ('motivations', 2),\n",
       "   ('weeks.', 2),\n",
       "   ('initially', 2),\n",
       "   ('much', 2),\n",
       "   ('particle', 2),\n",
       "   ('advice,', 2),\n",
       "   ('participate,', 2),\n",
       "   ('entropy', 2),\n",
       "   ('tepper', 2),\n",
       "   ('economics,', 2),\n",
       "   ('sociotechnical', 2),\n",
       "   ('day', 2),\n",
       "   ('game', 2),\n",
       "   ('largest', 2),\n",
       "   ('collected', 2),\n",
       "   ('majority', 2),\n",
       "   ('increases', 2),\n",
       "   ('reach', 2),\n",
       "   ('surveys', 1),\n",
       "   ('markets', 1),\n",
       "   ('needed', 1),\n",
       "   ('harness', 1),\n",
       "   ('search,\"the', 1),\n",
       "   ('act', 1),\n",
       "   ('concern', 1),\n",
       "   ('rather', 1),\n",
       "   ('explained,\"online', 1),\n",
       "   ('{ebay},', 1),\n",
       "   ('google,', 1),\n",
       "   (\"fisher's\", 1),\n",
       "   ('1920s,', 1),\n",
       "   ('learnings', 1),\n",
       "   ('happened:', 1),\n",
       "   ('isolated', 1),\n",
       "   ('incidents;', 1),\n",
       "   ('heightened', 1),\n",
       "   ('readers', 1),\n",
       "   ('revenue', 1),\n",
       "   ('include:', 1),\n",
       "   ('click', 1),\n",
       "   ('power,', 1),\n",
       "   ('instant', 1),\n",
       "   ('voice.', 1),\n",
       "   ('chiles,', 1),\n",
       "   ('unusual', 1),\n",
       "   ('book:', 1),\n",
       "   ('discoursing', 1),\n",
       "   ('preventable', 1),\n",
       "   ('age.', 1),\n",
       "   ('deadly', 1),\n",
       "   ('maine,', 1),\n",
       "   ('offers', 1),\n",
       "   ('planes,', 1),\n",
       "   ('ships,', 1),\n",
       "   ('--<i>howard', 1),\n",
       "   ('concorde', 1),\n",
       "   ('nuclear', 1),\n",
       "   ('demonstrates', 1),\n",
       "   ('catastrophe.', 1),\n",
       "   ('structures', 1),\n",
       "   ('flaw?</p>}\"\"early', 1),\n",
       "   ('modeling,', 1),\n",
       "   ('adaption', 1),\n",
       "   ('quite', 1),\n",
       "   ('engagement.\"usa\",topical', 1),\n",
       "   ('query.', 1),\n",
       "   ('ideally,', 1),\n",
       "   ('abstract', 1),\n",
       "   ('decomposition', 1),\n",
       "   ('tackle', 1),\n",
       "   ('variant', 1),\n",
       "   ('kind', 1),\n",
       "   ('programming.', 1),\n",
       "   ('actual', 1),\n",
       "   ('audience\",\"social', 1),\n",
       "   ('making', 1),\n",
       "   ('conversation.', 1),\n",
       "   ('techniques,', 1),\n",
       "   ('site,', 1),\n",
       "   ('inactive', 1),\n",
       "   ('perhaps', 1),\n",
       "   ('winning,', 1),\n",
       "   ('decrease.', 1),\n",
       "   ('picture,', 1),\n",
       "   ('win', 1),\n",
       "   ('win-to-submission', 1),\n",
       "   ('20\\\\%', 1),\n",
       "   ('buzz', 1),\n",
       "   ('claims', 1),\n",
       "   ('figures', 1),\n",
       "   ('legislators', 1),\n",
       "   ('self-promotion.', 1),\n",
       "   ('rhetorical', 1),\n",
       "   ('google', 1),\n",
       "   ('studied,', 1),\n",
       "   ('money', 1),\n",
       "   ('contributes', 1),\n",
       "   ('anybody', 1),\n",
       "   ('outperformed', 1),\n",
       "   ('reinforcement,\"community', 1),\n",
       "   ('answer.', 1),\n",
       "   ('portals', 1),\n",
       "   ('domains.', 1),\n",
       "   ('engines,\"in', 1),\n",
       "   ('designing', 1),\n",
       "   ('engines.', 1),\n",
       "   ('spanning', 1),\n",
       "   ('rates', 1),\n",
       "   ('faced', 1),\n",
       "   ('sufficent', 1),\n",
       "   ('suggest', 1),\n",
       "   ('benefit', 1),\n",
       "   ('additional', 1),\n",
       "   ('ways:', 1),\n",
       "   ('self;', 1),\n",
       "   ('(2)', 1),\n",
       "   ('160', 1),\n",
       "   ('wealth', 1),\n",
       "   ('interactions.', 1),\n",
       "   ('change', 1),\n",
       "   ('population', 1),\n",
       "   ('inductive', 1),\n",
       "   ('non-social,', 1),\n",
       "   ('class', 1),\n",
       "   ('sidelines', 1),\n",
       "   ('formation,', 1),\n",
       "   ('growth,', 1),\n",
       "   ('dissolution', 1),\n",
       "   ('descriptive', 1),\n",
       "   ('metadata', 1),\n",
       "   ('engagements.', 1),\n",
       "   ('before', 1),\n",
       "   ('analytical', 1),\n",
       "   ('groups.\"usa\",the', 1),\n",
       "   (\"{wasn't}:\", 1),\n",
       "   ('networks,', 1),\n",
       "   ('sometimes', 1),\n",
       "   ('self-censor,', 1),\n",
       "   ('thought', 1),\n",
       "   ('liked', 1),\n",
       "   ('reported', 1),\n",
       "   ('exactly', 1),\n",
       "   ('community-based', 1),\n",
       "   ('answering\",\"the', 1),\n",
       "   ('predicated', 1),\n",
       "   ('navigational', 1),\n",
       "   ('url', 1),\n",
       "   ('e.g.', 1),\n",
       "   ('baidu', 1),\n",
       "   ('truly', 1),\n",
       "   ('is,', 1),\n",
       "   ('pushed', 1),\n",
       "   ('arriving', 1),\n",
       "   ('until', 1),\n",
       "   ('appear,', 1),\n",
       "   ('answered', 1),\n",
       "   ('useful', 1),\n",
       "   ('ineffective', 1),\n",
       "   ('details', 1),\n",
       "   ('disclosure,', 1),\n",
       "   ('applications,', 1),\n",
       "   ('prevent', 1),\n",
       "   ('self-reports.,\"assessing', 1),\n",
       "   ('computers', 1),\n",
       "   ('early-intervention', 1),\n",
       "   ('engaged', 1),\n",
       "   ('rate.\"usa\",feed', 1),\n",
       "   ('sites,\"social', 1),\n",
       "   ('true', 1),\n",
       "   ('predict', 1),\n",
       "   ('support', 1),\n",
       "   ('communities.\"usa\",information', 1),\n",
       "   ('often,', 1),\n",
       "   ('unknown', 1),\n",
       "   ('implications.', 1),\n",
       "   ('privacy,\"mobile', 1),\n",
       "   ('ubicomp', 1),\n",
       "   ('poorly', 1),\n",
       "   ('approach,', 1),\n",
       "   ('sampling', 1),\n",
       "   ('socio-cultural', 1),\n",
       "   ('sociological', 1),\n",
       "   ('work,', 1),\n",
       "   ('objectively', 1),\n",
       "   ('sensors,\"twitter,', 1),\n",
       "   ('characteristic', 1),\n",
       "   ...])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupedSortedRDD = rearrangedRDD.groupByKey().map(lambda x: (x[0], sorted(list(x[1]), key=lambda x: x[1], reverse=True)))\n",
    "groupedSortedRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['f05bcffe7951de9e5a32fff4a42eb088'],\n",
       "  [('the', 1157),\n",
       "   ('of', 1050),\n",
       "   ('and', 862),\n",
       "   ('a', 541),\n",
       "   ('to', 502),\n",
       "   ('in', 418),\n",
       "   ('for', 384),\n",
       "   ('is', 317),\n",
       "   ('data', 207),\n",
       "   ('that', 206)]),\n",
       " (['beaca02b21b7cad6cb738c5e2682af8d'],\n",
       "  [('the', 2810),\n",
       "   ('of', 2747),\n",
       "   ('and', 1653),\n",
       "   ('in', 1278),\n",
       "   ('to', 1174),\n",
       "   ('a', 1164),\n",
       "   ('that', 675),\n",
       "   ('for', 649),\n",
       "   ('is', 601),\n",
       "   ('we', 568)]),\n",
       " (['dacb9d5d7b6e1b8090aa2d4cf6542ea1'],\n",
       "  [('the', 1443),\n",
       "   ('of', 1346),\n",
       "   ('and', 1133),\n",
       "   ('to', 733),\n",
       "   ('in', 684),\n",
       "   ('a', 549),\n",
       "   ('for', 367),\n",
       "   ('that', 256),\n",
       "   ('is', 252),\n",
       "   ('on', 192)]),\n",
       " (['6dd817ab0c2f730fc206faf5f5847bbb'],\n",
       "  [('the', 3244),\n",
       "   ('of', 2407),\n",
       "   ('and', 1966),\n",
       "   ('to', 1570),\n",
       "   ('a', 1443),\n",
       "   ('in', 1289),\n",
       "   ('for', 804),\n",
       "   ('search', 788),\n",
       "   ('that', 752),\n",
       "   ('we', 700)]),\n",
       " (['37769fdc48d9a70ccf3a3245f7375230'],\n",
       "  [('the', 353),\n",
       "   ('of', 326),\n",
       "   ('and', 265),\n",
       "   ('to', 217),\n",
       "   ('a', 185),\n",
       "   ('in', 180),\n",
       "   ('we', 134),\n",
       "   ('for', 98),\n",
       "   ('that', 74),\n",
       "   ('on', 69)])]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topTenGroupRDD = groupedSortedRDD.map(lambda x: ([x[0]] ,x[1][0:10]))\n",
    "topTenGroupRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['f05bcffe7951de9e5a32fff4a42eb088'],\n",
       "  ['the', 'of', 'and', 'a', 'to', 'in', 'for', 'is', 'data', 'that']),\n",
       " (['beaca02b21b7cad6cb738c5e2682af8d'],\n",
       "  ['the', 'of', 'and', 'in', 'to', 'a', 'that', 'for', 'is', 'we']),\n",
       " (['dacb9d5d7b6e1b8090aa2d4cf6542ea1'],\n",
       "  ['the', 'of', 'and', 'to', 'in', 'a', 'for', 'that', 'is', 'on']),\n",
       " (['6dd817ab0c2f730fc206faf5f5847bbb'],\n",
       "  ['the', 'of', 'and', 'to', 'a', 'in', 'for', 'search', 'that', 'we']),\n",
       " (['37769fdc48d9a70ccf3a3245f7375230'],\n",
       "  ['the', 'of', 'and', 'to', 'a', 'in', 'we', 'for', 'that', 'on']),\n",
       " (['589b870a611c25fa99bd3d7295ac0622'],\n",
       "  ['the', 'and', 'to', 'of', 'in', 'a', 'wormhole', 'is', 'are', 'routing']),\n",
       " (['90f1a3e6fcdbf9bc550e866116bbcea5'],\n",
       "  ['of', 'the', 'and', 'to', 'a', 'in', 'that', 'for', 'is', 'have']),\n",
       " (['6527bbd2024cf027a9ba75b3e28ed4db'],\n",
       "  ['the', 'of', 'in', 'and', 'to', 'this', 'community', 'is', 'a', 'for']),\n",
       " (['d705113f772771b23bcd17303ee0860a'],\n",
       "  ['the', 'of', 'and', '', 'in', 'a', 'to', 'as', 'for', 'that']),\n",
       " (['60a321bf89d186c88a3a1d2b2b165ce0'],\n",
       "  ['the',\n",
       "   'of',\n",
       "   'in',\n",
       "   'and',\n",
       "   'that',\n",
       "   'as',\n",
       "   'microbial',\n",
       "   'biogeographic',\n",
       "   'a',\n",
       "   'for'])]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalResultRDD = finalResultRDD.mapValues(lambda x: [t[0] for t in x])\n",
    "finalResultRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalResultRDD.saveAsTextFile(\"output_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
